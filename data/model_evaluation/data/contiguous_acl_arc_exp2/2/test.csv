unique_id,citing_id,citing_title,cited_title,cited_authors,section_title,cited_abstract,citation_context,cite_context_paragraph,citation_class_label,dynamic_contexts_combined
CC774,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a multipurpose interface to an online dictionary,"['Branimir Boguraev', 'David Carter', 'Ted Briscoe']",,,"In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords, dictionary search through the pronunciation field is available; Carter (1987) has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure (Huttenlocher and Zue, 1983).', 'In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']",0,"['In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .']"
CC775,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,the grammar of english predicate complement constructions,['P S Rosenbaum'],,"A set of phrase structure rules and a set of transformational rules are proposed for which the claim is made that these rules enumerate the underlying and derived sentential structures which exemplify two productive classes of sentential embedding in English. These are sentential embedding in noun phrases and sentential embedding in verb phrases. First, following a statement of the grammatical rules, the phrase structure rules are analyzed and defended. Second, the transformational rules which map the underlying structures generated by the phrase structure rules onto appropriate derived structures are justified with respect to noun phrase and verb phrase complementation. Finally, a brief treatment is offered for the extension of the proposed descriptive apparatus to noun phrase and verb phrase complementation in predicate adjectival constructions. Thesis Supervisor: Noam Chomsky Title: Professor of Modern Languages",We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .,"['We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .', 'Figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system.']",5,['We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .']
CC776,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,processing dictionary definitions with phrasal pattern hierarchies in this issue,['Hiyan Alshawi'],,"This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns. An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English. A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions. The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary. Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific phrasal analysis rules as determined by a hierarchy of patterns in which less specific patterns dominate more specific ones. This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism. Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions.","As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.","['A series of systems in Cambridge are implemented in Lisp running under UnixTM.', 'They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams.', 'A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.', 'To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.']",0,"['A series of systems in Cambridge are implemented in Lisp running under UnixTM.', 'They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams.', 'A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.', 'To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.']"
CC777,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a natural language toolkit reconciling theory with practice,['Branimir Boguraev'],introduction,"Generalized Phrase Structure Grammar (GPSG) is a highly restrictive theory of natural language syntax, characterised by complex interaction between its various rule types and constraints. Motivated by desire for declarative semantics, the theory defines these as applying simultaneously in the process of licensing local trees. As a result, as far as practical implementations of GPSG are concerned, the theory loses its apparent efficient parsability and becomes computationally intractable. This paper describes one aspect of an UK collaborative effort to produce a general purpose morphological and syntactic analyser for English within the theoretical framework of Generalized Phrase Structure Grammar, namely the development of a tractable grammatical formalism with clear semantics, capable of supporting the task of writing a substantial grammar. The paper outlines the intellectual and pragmatic background of the development effort and traces the incremental evolution of this formalism, following discussions concerning the fundamental issues of rules interpretation, feature system, grammar organisation, parser strategy, environment for grammar writing and support, and the construction of a lexicon linked to the grammar. Particular emphasis is placed on the quesiton of how theoretical standpoints have been reconciled with practical constraints, and how the commitment to deliver a functional morphological and syntactic analyser of wide scope and coverage of English has influenced the current state of the grammatical formalism.","The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .","['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"
CC778,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,synthesis of speech from unrestricted text,['J Allen'],experiments,"E diatribe ekhei san stokho ten melete ton prosodikon kanonon tes ellenikes glossas. Ta apotelesmata mporoun na ensomatothoun se opoiodepote sustema suntheses omilias anexarteta apo ten epilegmene strategike suntheses. E prosodia parousiazetai san polumetrike sunartese tes themeliodous sukhnotetas tes entases kai tes diarkeias ton phonematon kai parousiazontai montela se epipedo lexes toso gia argo oso kai gia gregoro ruthmo ekphoras. Epises parousiazontai montela kai kanones se epipleon protaseis me tropo pou exantlei ola ta suntaktika phainomena tes ellenikes. Proteinetai oti e prosodia se epipedo protases mporei na suntethei apo montela epipedou lexes upertithemena pano se mia pherousa se epipedo protases e klise tes opoias exartatai apo ten uparxe phainomenon emphases.This thesis aims at the study of the prosodic rules of the Greek language for use in a text to speech synthesis from unrestricted text. Regardless of the underlying synthesis stratregie (diphones, phonemes, etc). Prosody is treated as a polymetric function of fundamental frequency, intensity and duration of the phonemes. Prosodic models are presented first for isolated intonation words for various tempos including slow and fast. Models for large sentences are also presented in a way that all syntactic phenomena of the language are included and respected. It is suggested that sentence level prosodic models can be derived and synthesized from word-level models that are superimposed on a carrier spanning the whole sentence. The trend of the carrier is dependent upon various emphatic phenomena such as local stress or sentence emphasis","Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']",4,"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.']"
CC779,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,texttospeechan overview,"['S P Olive', 'M Y Liberman']",introduction,,"In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .","['In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']",0,"['In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .']"
CC780,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,from sad to glad emotional computer voices,['J Cahn'],experiments,"Synthesized English speech is readily distinguished from human speech on the basis of inappropriate intonation and insu cient expressiveness. This is a drawback for conversational computer systems. Intonation is the carrier of emphasis or de-emphasis, serving to clarify meaning for the spoken word much as variations in typeface and punctuation do for the written word. Expressiveness is not tied to word or phrase meaning but is global in scope. It provides the context in which the intonation occurs, and reveals the speaker's intentions and general mental state. In synthesized speech, intonation makes the message easier to understand; enhanced expressiveness contributes to dramatic e ect, making the message easier to listen to.","Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']",4,"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .']"
CC781,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,texttospeechan overview,"['S P Olive', 'M Y Liberman']",experiments,,We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .,"['We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'Two concerns motivated our implementation.', 'First, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences.', 'Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text-to-speech synthesizer.']",5,['We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .']
CC782,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,now lets talk about now identifying cue phrases intonationally,"['J Hirschberg', 'D Litman']",introduction,"Cue phrases are words and phrases such as now and by the way which may be used to convey explicit information about the structure of a discourse. However, while cue phrases may convey discourse structure, each may also be used to different effect. The question of how speakers and hearers distinguish between such uses of cue phrases has not been addressed in discourse studies to date. Based on a study of now in natural recorded discourse, we propose that cue and non-cue usage can be distinguished intonationally, on the basis of phrasing and accent.",#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",0,"['#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']"
CC783,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,aspects of the theory of syntax,['N Chomsky'],introduction,Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon,"Sentences like 12 , from #AUTHOR_TAG , are frequently cited .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from #AUTHOR_TAG , are frequently cited .', '(Square brackets mark off the NP constituents that contain embed- ded sentences.)']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from #AUTHOR_TAG , are frequently cited .']"
CC784,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,the contribution of parsing to prosodic phrasing in an experimental texttospeech system,"['J Bachenko', 'E Fitzpatrick', 'C E Wright']",introduction,"While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody, the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody. We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules. The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries. These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence.We discuss the results of an experiment to determine the performance of our system. We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate.","In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .","['In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']",2,"['In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .']"
CC785,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,finitestate parsing of phrasestructure languages and the status of readjustment rules in grammar,['D T Langendoen'],introduction,,"#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"
CC786,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,performance structures a psycholinguistic and linguistic appraisal,"['J P Gee', 'F Grosjean']",introduction,,"The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .","['The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths; this is the case in Chickens were eating I[ the remaining green vegetables, where the subject-predicate boundary finds no prosodic correspondent. 4']",0,"['The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .']"
CC787,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,on stress and linguistic rhythm,"['M Y Liberman', 'A Prince']",introduction,"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to Linguistic Inquiry.","3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .","['Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .', 'This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing.']",1,"['3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .']"
CC788,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,the sound pattern of english,"['N Chomsky', 'M Halle']",introduction,"Since this classic work in phonology was published in 1968, there has been no other book that gives as broad a view of the subject, combining generally applicable theoretical contributions with analysis of the details of a single language. The theoretical issues raised in The Sound Pattern of English continue to be critical to current phonology, and in many instances the solutions proposed by Chomsky and Halle have yet to be improved upon.Noam Chomsky and Morris Halle are Institute Professors of Linguistics and Philosophy at MIT.","In #AUTHOR_TAG , this flattening process is not part of the grammar .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In #AUTHOR_TAG , this flattening process is not part of the grammar .', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'In #AUTHOR_TAG , this flattening process is not part of the grammar .', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"
CC789,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,on stress and linguistic rhythm,"['M Y Liberman', 'A Prince']",experiments,"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to Linguistic Inquiry.","An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .","['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', 'Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing.']",1,"['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', 'Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing.']"
CC790,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,prosodic systems and intonation in english,['D Crystal'],introduction,Preface 1. Some preliminary considerations 2. Past work on prosodic features 3. Voice-quality and sound attributes in prosodic study 4. The prosodic features of English 5. The intonation system of English 6. The grammar of intonation 7. The semantics of intonation Bibliography Index of persons Index of subjects.,"#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .","['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",0,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .']"
CC791,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,speech rhythm its relation to performance universals and articulatory timing,['G Allen'],introduction,,"The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .","['The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths; this is the case in Chickens were eating I[ the remaining green vegetables, where the subject-predicate boundary finds no prosodic correspondent. 4']",0,"['The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .']"
CC792,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,capacity demands in shortterm memory for synthetic and natural speech,"['P A Luce', 'T C Feustel', 'D B Pisoni']",experiments,"Three experiments were performed that compared recall for synthetic and natural lists of monosyllabic words. In the first experiment, presentation intervals of 1, 2, and 5 s per word were used. Although free recall was consistently poorer overall for the synthetic lists at all presentation rates, the decrement for synthetic stimuli did not increase differentially with faster rates. In a second experiment, zero, three, and six digits were presented visually for retention prior to free recall of each spoken word list in a preload paradigm. Fewer subjects were able to correctly recall all of the digits for the six-digit list than the three-digit list when the following word lists were synthetic. The third experiment required ordered recall of lists of natural and synthetic words. Differences in ordered recall between the synthetic and natural word lists were substantially larger for the primacy portion of the serial position curve than the recency portion. These results indicate that difficulties observed in the perception and comprehension of synthetic speech are due, in part, to increased processing demands in short-term memory.","Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']",4,"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .']"
CC793,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,toward treating english nominals correctly,"['R W Sproat', 'M Y Liberman']",experiments,We describe a program for assigning correct stress contours to nominals in English. It makes use of idiosyncratic knowledge about the stress behavior of various nominal types and general knowledge about English stress rules. We have also investigated the related issue of parsing complex nominals in English. The importance of this work and related research to the problem of text-to-speech is &apos;discussed,"Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .","['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .']",1,"['Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .']"
CC794,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,the contribution of parsing to prosodic phrasing in an experimental texttospeech system,"['J Bachenko', 'E Fitzpatrick', 'C E Wright']",introduction,"While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody, the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody. We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules. The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries. These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence.We discuss the results of an experiment to determine the performance of our system. We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate.","Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .","['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",2,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .']"
CC795,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,syntax and speech,"['W Cooper', 'J Paccia-Cooper']",introduction,"Interactions between phonology and syntax are inspected in continuous speech samples from 30 speech-delayed children. Two types of interactions are examined: The co-occurrence of speech and language delay and the effects of phonological reduction on the realization of phonetically complex morphophonemes. Four possible patterns of association between the phonological and syntactic systems are outlined, and subjects are assigned to these patterns based on their phonological and syntactic performance. Results indicate that two-thirds of the subjects display evidence of overall syntactic delay, whereas half show some limitation in the use of phonetically complex morphophonemes, their performance in that area being below the level of their syntactic production. Implications of these findings for a theory of speech delay and for management programming are discussed","This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.']"
CC796,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,performance structures a psycholinguistic and linguistic appraisal,"['J P Gee', 'F Grosjean']",introduction,,"Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .","['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']",1,"['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']"
CC797,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,aspects of prosody,['J Bing'],introduction,,"The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .","['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",0,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']"
CC798,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,prosodic structure and spoken word recognition,"['F Grosjean', 'J P Gee']",introduction,,"Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .","['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'Function words, e.g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.', 'Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone.']",5,"['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .']"
CC799,J91-2003,On compositional semantics,logic and conversationquot,['H P Grice'],introduction,,"Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .","['However, there are at least three arguments against iterating PT.', 'First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.', 'Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .', 'Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.']",4,"['Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .', 'Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.']"
CC800,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .","['At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .', 'The quoted works seem to be good representatives for each of the directions; they also point to related literature.']",1,"['We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .']"
CC801,J91-2003,On compositional semantics,domain circumscription a reevaluationquot,"['D W Etherington', 'R E Mercer']",introduction,,"Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.', 'Similarly, the notion of R+ M-abduction is spiritually related to the ""abduc- tive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of Berwick (1986).', 'But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.']"
CC802,J91-2003,On compositional semantics,the boundaries of words and their meaningsquot,['W Labov'],introduction,,W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .,['W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .'],0,['W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .']
CC803,J91-2003,On compositional semantics,a grammar of contemporary english,"['R Quirk', 'S Greenbaum', 'G Leech', 'J Svartvik']",introduction,"The publication of this important volume fills the need for an up-to-date survey of the entire scope of English syntax. Though it falls short of a perfectly balanced treatment of the whole system, it touches upon all the essential topics and treats in depth a number of crucial problems of current interest such as case, ellipsis, and information focus. Even the publishers' claims are vindicated to a surprising degree. The statement that it ""constitutes a standard reference grammar"" is reasonably well justified. Recent investigations, including the authors' own research, are integrated into the ""accumulated grammatical tradition"" quite effectively. But whether it is ""the fullest and most comprehensive synchronic description of English grammar ever written"" is arguable. No one acquainted with Poutsma's work would agree with that. Very advanced foreign students o r native speakers of English who want to learn about basic grammar will find some of thel sections suitable for their needs, such as the lesson about restrictive and nonrestrictive relative clauses, though even here some of the explanations require very intensive study. Most of the chapters are rather like an advanced textbook for teachers or linguists. The organization and viewpoint give the impression of a carefully planned university lecture supplemented by diagrams, charts, and lists. A good example is the lesson on auxiliaries and verb phrases, which starts with a set of sample sentences demonstrating that ""should see"" and ""happen to see"" behave differently under various transformations and expansions. After the essential concepts are explained and exemplified-lexical verb, semi-auxiliary, operator, and the like-lists and paradigms are given as in the usual reference work. A particularly useful feature of this chapter is the outline of modal auxiliaries with examples of their divergent meanings.","Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .","['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .']",0,"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .']"
CC804,J91-2003,On compositional semantics,cohesion in english,"['M A K Halliday', 'R Hasan']",introduction,"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good","Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.']"
CC805,J91-2003,On compositional semantics,a dictionary of modern english usage,['H W Fowler'],introduction,,"This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .","['Still, our definition of coherence may not be restrictive enough: two collections of sentences, one referring to ""black"" (about black pencils, black pullovers, and black poodles), the other one about ""death"" (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic ""black + death.""', ""This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .""]",0,"[""This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .""]"
CC806,J91-2003,On compositional semantics,mental models,['P N Johnson-Laird'],introduction,"The complexity of conceptualizing mental models has made Virtual Reality an interesting way to enhance communication and understanding between individuals working together on a project or idea. Here, the authors discuss practical applications of using VR for this purpose","This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']",1,"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']"
CC807,J91-2003,On compositional semantics,a theory of truth and semantic representationquot,['H Kamp'],introduction,,"But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora .","['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']",1,"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"
CC808,J91-2003,On compositional semantics,so what can we talk about nowquot,['B Webber'],introduction,Impure water is made suitable for drinking in an apparatus comprising a pressurizable holding tank attached to a purification cartridge containing an impurities adsorbent and a fine filter. A gas-containing cartridge is pierced to provide a bactericidal gas for killing pathogenic microorganisms and for pressurizing the holding tank to force the water through the purification cartridge.,"Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .","['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']",0,"['Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']"
CC809,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .","['According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', 'However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.)', 'suddenly (for Hobbs) becomes coherent.', ""It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added."", 'On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'And the paragraph obtained by adding the third sentence is coherent.', 'Moreover, coherence here is clearly the result of the existence of the topic ""John likes spinach.""']",1,"['According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .']"
CC810,J91-2003,On compositional semantics,artificial intelligence the very idea,['J Haugeland'],introduction,"The idea that human thinking and machine computing are ""radically the same"" provides the central theme for this marvelously lucid and witty book on what artificial intelligence is all about. Although presented entirely in nontechnical","For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap","['We adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning.', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap']",0,"['For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap']"
CC811,J91-2003,On compositional semantics,the episode schema in story processingquot,"['K Haberlandt', 'C Berian', 'J Sandson']",introduction,,Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .,"['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .']",0,['Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .']
CC812,J91-2003,On compositional semantics,paragraph structure inference,['E J Crothers'],introduction,,"Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .","['However, there are at least three arguments against iterating PT.', 'First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.', 'Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .', 'Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.']",4,"['Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .']"
CC813,J91-2003,On compositional semantics,krypton a functional approach to knowledge representationquot,"['R J Brachman', 'R E Fikes', 'H J Levesque']",introduction,,"The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .","['The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .', 'Brachman et al. 1985).', ""KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL."", ""However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior."", 'From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard.', 'In our system, we also distinguish between the ""definitional"" and factual information, but the ""definitional"" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, ""coherence"" and ""dominance,"" which are not variants of the standard first order entailment, but abduction.']",1,"['The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .']"
CC814,J91-2003,On compositional semantics,abductive inferencequot,['J A Reggia'],introduction,,"Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"[""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .""]"
CC815,J91-2003,On compositional semantics,the interpretation of tense in discoursequot,['B Webber'],introduction,,The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.).""]",0,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents.', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.']"
CC816,J91-2003,On compositional semantics,organizational patterns in discoursequot,['J Hinds'],introduction,,"According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .","['The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists, classifies, and discusses various types of inference, by which he means, generally, ""the linguistic-logical notions of consequent and presupposition"" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences).', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']",0,"['The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists, classifies, and discusses various types of inference, by which he means, generally, ""the linguistic-logical notions of consequent and presupposition"" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences).', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.']"
CC817,J91-2003,On compositional semantics,a theory of diagnosis from first principlesquot,['R Reiter'],introduction,,"Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .""]"
CC818,J91-2003,On compositional semantics,inference without chainingquot,['A Frisch'],introduction,,"Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .","['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]",1,"['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]"
CC819,J91-2003,On compositional semantics,the game of language,['J Hintikka'],introduction,,"This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG .","['Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references.', ""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG ."", 'Hintikka (1985).']",1,"[""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG .""]"
CC820,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).","['Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).', 'We will have, however, no need for ""strong"" notions in this paper.', 'Also, in a practical system, ""satisfies"" should be probably replaced by ""violates fewest.""']",1,"['Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).', 'We will have, however, no need for ""strong"" notions in this paper.']"
CC821,J91-2003,On compositional semantics,38 examples of elusive antecedents from published texts,['J R Hobbs'],introduction,,"Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .","['Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', 'Later, Hobbs (1979Hobbs ( , 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ""salience"" in choosing facts from this knowledge base.']",0,"['Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .']"
CC822,J91-2003,On compositional semantics,temporal ontology in natural languagequot,"['M Moens', 'M Steedman']",introduction,,The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.).""]",0,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.']"
CC823,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .","['Since it is the ""highest"" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence.', 'Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .', 'Zadrozny 1987aZadrozny , 1987b.', 'Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering.', 'As it turns out, f~ is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story.']",0,"['Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .']"
CC824,J91-2003,On compositional semantics,the paragraph as a grammatical unit,['R E Longacre'],introduction,,"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.']"
CC825,J91-2003,On compositional semantics,analysis without actual infinityquot,['J Mycielski'],introduction,,"As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983, Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics.', 'As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .', 'Mycielski 1981).']",0,"['As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .', 'Mycielski 1981).']"
CC826,J91-2003,On compositional semantics,disambiguating prepositional phrase attachments by using online dictionary definitionsquot computational linguistics 1334251260 special issue on the lexicon,"['K Jensen', 'J-L Binot']",introduction,,"We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .","['We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']",2,"['We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .']"
CC827,J91-2003,On compositional semantics,introduction to artificial intelligence,"['E Charniak', 'D McDermott']",introduction,"This book is an introduction on artificial intelligence. Topics include reasoning under uncertainty, robot plans, language understanding, and learning. The history of the field as well as intellectual ties to related disciplines are presented.","The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .","['The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']",0,"['The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']"
CC828,J91-2003,On compositional semantics,universal grammarquot,['R Montague'],introduction,,"The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .","['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']",1,"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"
CC829,J91-2003,On compositional semantics,semantics and cognition,['R Jackendoff'],introduction,"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication","This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']",1,"['This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .']"
CC830,J91-2003,On compositional semantics,paragraph structure inference,['E J Crothers'],introduction,,"He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .","['The textualist approach to paragraph analysis is exemplified by E. J. Crothers.', 'His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'Paragraphs therefore give hierarchical structure to sentences.', 'Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), expository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']",0,"['The textualist approach to paragraph analysis is exemplified by E. J. Crothers.', 'His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'Paragraphs therefore give hierarchical structure to sentences.', 'Hinds discusses three major types of paragraphs, and their corresponding segment types.']"
CC831,J91-2003,On compositional semantics,dont blame the toolquot,['W Woods'],introduction,,"However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .","['Once the word ""up"" is given its meaning relative to our experience with gravity, it is not free to ""slip"" into its opposite.', '""Up"" means up and not down ....', 'We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model.', 'Mothers have a different role than fathers in this model, and thus there is a reason why ""Death is the father of beauty"" fails poetically while ""Death is the mother of beauty"" succeeds ....', 'It is precisely this ""grounding"" of logical predicates in other conceptual structures that we would like to capture.', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .', 'Woods 1987).']",0,"['Once the word ""up"" is given its meaning relative to our experience with gravity, it is not free to ""slip"" into its opposite.', '""Up"" means up and not down ....', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .', 'Woods 1987).']"
CC832,J91-2003,On compositional semantics,semantics and cognition,['R Jackendoff'],introduction,"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication","#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''","['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']",1,"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"
CC833,J91-2003,On compositional semantics,cues people use to paragraph text,"['S J Bond', 'J R Hayes']",introduction,"This paper reports the results of three studies on the paragraph. In Study 1, subjects were asked to paragraph a text from which paragraph indentations had been removed. Results indicate that readers can consistently paragraph unparagraphed text, thus supporting Young and Becker's (1966) assertion that the paragraph is a psychologically real unit of discourse. Results also reveal that readers rely heavily on breaks in text cohesion (e.g., topic shift) and on paragraph length as paragraphing cues. In Study 2, four new subjects were asked to paragraph the same text used in Study 1, and to ""think aloud,"" giving their reasons for paragraphing, as they did so. Analysis of these thinking aloud protocols reveal that, in the absence of a strong paragraphing cue, readers will read ahead in a text, sometimes flagging weaker paragraphing cues as they go. If they feel the unparagraphed text is too long, they will go back and paragraph at these weaker cues until all paragraphs in the text are an acceptable length. Based on the results of Studies 1 and 2, a model of how readers paragraph was devised. The model was tested in Study 3 on new subjects who were asked to think aloud as they paragraphed the same text used in Study 1 , and another, longer text. The model predicted the new data quite accurately. Deciding whether paragraph boundaries are psychologically real or arbitrary is very much like deciding whether geographical boundaries are psychologically real or arbitrary. Typically, state boundaries are not psychologically real because travelers cannot find them without the help of signs. Coast lines, on the other hand, are very real. People who miss them fall into the ocean. In the same way, we would consider paragraph boundaries artificial if people could find them only with the help of paragraphing marks, and real if people could consistently find them in texts from which paragraphing marks had been removed. Some linguists such as Hodges (1941) have viewed the paragraph as an arbitrary device used by the writer to ""give the reader a breathing spell"" (p. 311). Similarly, Rodgers (1967) has suggested that a section of text ""becomes a paragraph not by virtue of its structure, but because the writer elects to indent"" (p. 182). However, Young and Becker (1966) and Koen, Becker, and Young (1969) have provided strong evidence that paragraphs are indeed psychologically real. They asked readers to paragraph text from which all paragraphing markers had been removed and found that their readers Research in the Teaching of English, Vol. 18, No. 2, May 1984",An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .,"['An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980).']",0,"['An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980).']"
CC834,J91-2003,On compositional semantics,paragraph structure inference,['E J Crothers'],introduction,,"#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' .","['This demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '(Other reference works could be treated as additional sources of world knowledge.)', 'This type of consultation uses existing natural language texts as a referential level for processing purposes.', 'It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", 'With respect to that independent source of knowledge, our main contributions are two.', 'First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation.', 'In other words, we recognize it as a separate logical level--the referential level.', 'Second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level.']",0,"[""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' .""]"
CC835,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?","['6.1.1 Was the Use of a Gricean Maxim Necessary?', 'Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?', 'It seems to us that the answer is no.']",0,"['Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?']"
CC836,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .","['We adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance, relating ""they"" to ""apples"" in the sentence (cf.', 'Haugeland 1985 p. 195;Zadrozny 1987a):']",5,"['This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .']"
CC837,J91-2003,On compositional semantics,learning from positiveonly examples the subset principle and three case studiesquot,['R C Berwick'],introduction,,"Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', 'Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .', 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', 'Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .']"
CC838,J91-2003,On compositional semantics,the episode schema in story processingquot,"['K Haberlandt', 'C Berian', 'J Sandson']",introduction,,"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']"
CC839,J91-2003,On compositional semantics,focusing in the comprehension of definite anaphoraquot,['C Sidner'],introduction,,"Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .","['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']",0,"['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']"
CC840,J91-2003,On compositional semantics,death is the mother of beauty,['M Turner'],introduction,"Let's read! We will often find out this sentence everywhere. When still being a kid, mom used to order us to always read, so did the teacher. Some books are fully read in a week and we need the obligation to support reading. What about now? Do you still love reading? Is reading only for you who have obligation? Absolutely not! We here offer you a new book enPDFd death is the mother of beauty to read.","Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .","['The referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of up and down that are based by the way our bodies actually function.']",0,"['The referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of up and down that are based by the way our bodies actually function.']"
CC841,J91-2003,On compositional semantics,passing markers a theory of contextual influence in language comprehensionquot,['E Charniak'],introduction,,"`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .","['The idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]",1,"[""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]"
CC842,J91-2003,On compositional semantics,a logic of implicit and explicit beliefsquot,['H J Levesque'],introduction,,"Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .","['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]",1,"['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]"
CC843,J91-2003,On compositional semantics,the representation and use of focus in a system for understanding dialogsquot,['B J Grosz'],introduction,"As a dialog progresses the objects and actions that are most relevant to the conversation, and hence in the focus of attention of the dialog participants, change. This paper describes a representation of focus for language understanding systems, emphasizing its use in understanding task-oriented dialogs. The representation highlights that part of the knowledge base relevant at a given point in a dialog. A model of the task is used both to structure the focus representation and to provide an index into potentially relevant concepts in the knowledge base The use of the focus representation to make retrieval of items from the knowledge base more efficient is described.","Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .","['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']",0,"['Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .']"
CC844,J91-2003,On compositional semantics,cohesion in english,"['M A K Halliday', 'R Hasan']",introduction,"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good","This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .","['Note: In our translation from English to logic we are assuming that ""it"" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around).', 'This means that the ""it"" that brought the disease in P1 will not be considered to refer to the infection ""i"" or the death ""d"" in P3.', 'This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .']",4,"['This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .']"
CC845,J91-2003,On compositional semantics,parsing strategies in a broadcoverage grammar of english research report rc 12147 ibm tj watson research center,['K Jensen'],introduction,," #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .","['We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples.', 'This approach is taken in computational syntactic grammars (e.g.', ' #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .']",0,"[' #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .']"
CC846,J91-2003,On compositional semantics,semantic interpretation and the resolution of ambiguity,['G Hirst'],introduction,"Preface 1. Introduction 2. Semantic interpretation 3. The Absity semantic interpreter 4. Lexical disambiguation 5. Polaroid words 6. Structural disambiguation 7. The semantic enquiry desk 8. Conclusion 9. Speculations, partially baked ideas, and exercises for the reader References Index of names Index of subjects.","`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .","['The idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]",1,"[""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]"
CC847,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .","['We do not claim that Gla is the best or unique way of expressing the rule ""assume that the writer did not say too much.""', 'Rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .']",2,"['We do not claim that Gla is the best or unique way of expressing the rule ""assume that the writer did not say too much.""', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .']"
CC848,J91-2003,On compositional semantics,cohesion in english,"['M A K Halliday', 'R Hasan']",introduction,"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good","Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .","['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']",0,"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"
CC849,J91-2003,On compositional semantics,the flow of thought and the flow of languagequot,['W L Chafe'],introduction,,"#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .']"
CC850,J91-2003,On compositional semantics,a theory of truth and semantic representationquot,['H Kamp'],introduction,,"This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']",1,"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .']"
CC851,J91-2003,On compositional semantics,episodes as chunks in narrative memoryquot,"['J B Black', 'G H Bower']",introduction,,Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .,"['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']",0,['Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']
CC852,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .","['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']",2,"['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']"
CC853,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.",It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .,"['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']",1,"['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']"
CC854,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .","['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]",0,"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"
CC855,J91-2003,On compositional semantics,resolving pronoun referencesquot,['J R Hobbs'],introduction,,"The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .","['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']",0,"['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']"
CC856,J91-2003,On compositional semantics,languages with self reference i foundationsquot,['D Perlis'],introduction,,"Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .","['5.1.1', 'Translation to Logic.', 'The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject (e.g.', 'Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .', 'Extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (""that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon""---ibid.).', 'However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.', 'After these remarks we can begin constructing the model of the example paragraph.', 'We assume that constants are introduced by NPs.', 'We have then (i) Constants s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d), infection(i), death(b), year(1347).']",0,"['5.1.1', 'Translation to Logic.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']"
CC857,J91-2003,On compositional semantics,lectures on contemporary syntactic theories csli lecture notes,['P Sells'],introduction,,"For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''","['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.', ""That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end."", 'We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.']",4,"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.']"
CC858,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the interaction of word recognition and linguistic processing in speech understandingquot,['H Niemann'],introduction,"This contribution describes an approach to integrate a speech understanding and dialog system into a homogeneous architecture based on semantic networks. The definition of the network as well as its use in speech understanding is described briefly. A scoring function for word hypotheses meeting the requirements of a graph search algorithm is presented. The main steps of the linguistic analysis, i.e. syntax, semantics, and pragmatics, are described and their realization in the semantic network is shown. The processing steps alternating between data- and model-driven phases are outlined using an example sentence which demonstrates a tight interaction between word recognition and linguistic processing.","Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .']",0,"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .']"
CC859,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,automatic speech recognition the development of the sphinx system appendix i,['K F Lee'],,,A formula for the test set perplexity ( #AUTHOR_TAG ) is :13,['A formula for the test set perplexity ( #AUTHOR_TAG ) is :13'],0,['A formula for the test set perplexity ( #AUTHOR_TAG ) is :13']
CC860,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the voyager speech understanding system preliminary development and evaluationquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",,"Early experience with the development of the MIT VOYAGER spoken language system is described, and its current performance is documented. The three components of VOYAGER, the speech recognition component, the natural language component, and the application back-end, are described.>",The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .,"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']",5,['The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .']
CC861,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the collection and preliminary analysis of a spontaneous speech databasequot darpa speech and natural language workshop,"['V Zue', 'N Daly', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff', 'M Soclof']",,,Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .,"['To obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information.', 'Their speech was recorded in a simulation mode in which the speech recognition component was excluded.', 'Instead, an experimenter in a separate room typed in the utterances as spoken by the subject.', 'Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .']",5,['Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .']
CC862,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,on whmovementquot in formal syntax edited by,['Noam Chomsky'],,,( #AUTHOR_TAG ) .,['( #AUTHOR_TAG ) .'],0,['( #AUTHOR_TAG ) .']
CC863,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,grammaticallybased automatic word class formationquot,"['L Hirschman', 'R Grishman', 'N Sager']",,,This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .,"['Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.', 'This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']",1,"['This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.']"
CC864,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,error bounds for convolutional codes and an asymptotically optimal decoding algorithmquot,['A Viterbi'],,"The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.","The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .","['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system (Zue et al. 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing.', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .']",5,"['The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .']"
CC865,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the case for casequot in universals in linguistic theory,['C J Fillmore'],,,Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .,"['Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .', 'For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as ""leave.""', 'Thus a flight can ""leave for Chicago from Boston at nine,"" or, equivalently, ""leave at nine for Chicago from Boston.""', 'If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place]s, [to-place]s and [at-time]s is possible.', 'This is of course unacceptable, but it is straightforward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in turn, fail if that bit has already been set.', 'We have found that this strategy, in conjunction with the capability of erasing all semantic bits whenever a new clause is entered (through the meta level ""detach"" operation mentioned previously) serves the desired goal of eliminating the unwanted redundancies.']",5,['Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .']
CC866,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the bbn spoken language systemquot,"['S Boisen', 'Y-L Chow', 'A Haas', 'R Ingria', 'S Roukos', 'D Stallard']",introduction,,"Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .']",0,"['Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .']"
CC867,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,full integration of speech and language understanding in the mit spoken language systemquot,"['D Goodine', 'S Seneff', 'L Hirschman', 'M Phillips']",,,"We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) .","['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions (Zue et al. 1991), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) ."", ""Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]",5,"[""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) .""]"
CC868,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,speech database development design and analysis of the acousticphonetic corpusquot,"['L Lamel', 'R H Kassel', 'S Seneff']",,,The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .,"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .', 'The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.', 'The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']",5,['The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .']
CC869,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,development and preliminary evaluation of the mit atis systemquot,"['S Seneff', 'J Glass', 'D Goddeau', 'D Goodine', 'L Hirschman', 'H Leung', 'M Phillips', 'J Polifroni', 'V Zue']",,,"However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .","['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.', 'It then passes along to the right sibling the same frame that was handed to it from above, with the completed topic slot filled with the information delivered by the children.']",3,"['However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.']"
CC870,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,development and preliminary evaluation of the mit atis systemquot,"['S Seneff', 'J Glass', 'D Goddeau', 'D Goodine', 'L Hirschman', 'H Leung', 'M Phillips', 'J Polifroni', 'V Zue']",,,"The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.","['We currently have two application domains that can carry on a spoken dialog with a user.', 'One, the VOYAGER domain (Zue et al. 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University.', 'The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues.']",5,"['We currently have two application domains that can carry on a spoken dialog with a user.', 'The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues.']"
CC871,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,discovery procedures for sublanguage selectional patterns initial experimentsquot,"['R Grishman', 'L Hirschman', 'N T Nhan']",,"Selectional constraints specify, for a particular domain, the combinations of semantic classes acceptable in subject-verb-object relationships and other syntactic structures. These constraints are important in blocking incorrect analyses in natural language processing systems. However, these constraints are domain-specific and hence must be developed anew when a system is ported to a new domain. A discovery procedure for selectional constraints is therefore essential in enhancing the portability of such systems.This paper describes a semi-automated procedure for collecting the co-occurrence patterns from a sample of texts in a domain, and then using these patterns as the basis for selectional constraints in analyzing further texts. We discuss some of the difficulties in automating the collection process, and describe two experiments that measure the completeness of these patterns and their effectiveness compared with manually-prepared patterns. We then describe and evaluate a procedure for selectional constraint relaxation, intended to compensate for gaps in the set of patterns. Finally, we suggest how these procedures could be combined with a system that queries a domain expert, in order to produce a more efficient discovery procedure.",This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .,"['Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.', 'This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']",1,['This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .']
CC872,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,estimation of probabilities from sparse data for the language model component of a speech recognizerquot assp35,['S M Katz'],,"The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises.","Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .","['This appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in ""three hundred and sixteen.""', 'Included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'Since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'This is a problem to be aware of in building grammars from example sentences.', 'In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .']",0,"['This appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in ""three hundred and sixteen.""', 'Included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'Since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'This is a problem to be aware of in building grammars from example sentences.', 'In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .']"
CC873,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,on whmovementquot in formal syntax edited by,['Noam Chomsky'],,,"To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .","['2.5.1 Gaps.', 'The mechanism to deal with gaps resembles in certain respects the Hold register idea of ATNs, but with an important difference, reflecting the design philoso-phy that no node can have access to information outside of its immediate domain.', 'The mechanism involves two slots that are available in the feature vector of each parse node.', 'These are called the CURRENT-FOCUS and the FLOAT-OBJECT, respectively.', 'The CURRENT-FOCUS slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'If the FLOAT-OBJECT slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the FLOAT-OBJECT.', 'The process of getting into the FLOAT-OBJECT slot (which is analogous to the Hold register) requires two steps, executed independently by two different nodes.', 'The first node, the generator, fills the CURRENT-FOCUS slot with the subparse returned to it by its children.', 'The second node, the activator, moves the CURRENT-FOCUS into the FLOAT-OBJECT position, for its children, during the top-down cycle.', 'It also requires that the FLOAT-OBJECT be absorbed somewhere among its descendants by a designated absorber node, a condition that is checked during the bottom-up cycle.', 'The CURRENT-FOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.', 'That is to say, the CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an [end] node is encountered.', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .', 'Finally, certain blocker nodes block the transfer of the FLOAT-OBJECT to their children.']",0,"['2.5.1 Gaps.', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .']"
CC874,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the voyager speech understanding system preliminary development and evaluationquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",conclusion,"Early experience with the development of the MIT VOYAGER spoken language system is described, and its current performance is documented. The three components of VOYAGER, the speech recognition component, the natural language component, and the application back-end, are described.>","One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .","['We_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', 'The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains.']",5,"['We_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', 'The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains.']"
CC875,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,semantics and quantification in natural language question answeringquot,['W A Woods'],,,"The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator .","[""The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."", 'Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator.', 'The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)?', 'The CURRENT-FOCUS slot is not restricted to nodes that represent nouns.', 'Some of the generators are adverbial or adjectival parts of speech (pos).', 'An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse.', 'As an example, the question, ""(How oily)/do you like your salad dressing (ti)?"" contains a [q-subject] ""how oily"" that is an adjective.', 'The absorber [pred-adjective] accepts the available float-object as its subparse, but only after confirming that POS is ADJECTIVE.']",1,"[""The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator .""]"
CC876,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the use of a semantic network in speech dialoguequot,['G Th Niedermair'],introduction,,"Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .']",0,"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .']"
CC877,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,benchmark tests for darpa resource management database performance evaluationsquot,['D Pallett'],,"A nominally 1000-word resource management database for continuous speech recognition was developed for use in the DARPA Speech Research Program. This database has now been used at several sites for benchmark tests, and the database is expected to be made available to a wider community in the near future. The author documents the structure of the benchmark tests, including the selection of test material and details of studies of scoring algorithms.>",The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .,"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .', 'The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']",5,['The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .']
CC878,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,a formal basis for the heuristic determination of minimum cost pathsquot,"['P Hart', 'N J Nilsson', 'B Raphael']",,"Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.","For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .","['Some modification of this scheme is necessary when the input stream is not deterministic.', 'For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', 'Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found.']",5,"['For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', 'Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found.']"
CC879,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the collection and preliminary analysis of a spontaneous speech databasequot darpa speech and natural language workshop,"['V Zue', 'N Daly', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff', 'M Soclof']",,,"The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .","['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence.']",5,"['The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .']"
CC880,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,transition network grammars for natural language analysisquot,['W A Woods'],,"The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described.","The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.","['This section describes how TINA handles several issues that are often considered to be part of the task of a parser.', 'These include agreement constraints, semantic restrictions, subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in ""(which article)/do you think I should read (ti)?"") (Chomsky 1977).', 'The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.']",1,"['This section describes how TINA handles several issues that are often considered to be part of the task of a parser.', 'The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.']"
CC881,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the minds system using context and dialog to enhance speech recognitionquot,['S R Young'],introduction,,"Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']",0,"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"
CC882,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,integration of speech recognition and natural language processing in the mit voyager systemquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",,"The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>","We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .","['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', 'A simple word-pair grammar constrained the search space.', 'If the parse failed, then the sentence was rejected.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."", 'Both the A* and the Viterbi search are left-to-right search algorithms.', 'However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.', 'That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.']",5,"[""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"
CC883,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,integration of speech recognition and natural language processing in the mit voyager systemquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",,"The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>","Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .","['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]",5,"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"
CC884,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .,"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .', 'This description can then be given the standard set-theoretical interpretation of King (1989, 1994).']",0,"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .']"
CC885,J97-4003,On Expressing Lexical Generalizations in HPSG,offline constraint propagation for efficient hpsg processing,"['Detmar Meurers', 'Guido Minnen']",introduction,We investigate the use of a technique developed in the constraint programming community called constraint propagation to automatically make a HPSG theory more specific at those places where linguistically motivated underspecification would lead to inefficient processing. We discuss two concrete HPSG examples showing how off-line constraint propagation helps improve processing efficiency.,The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.,"['The most specific generalization does not necessarily provide additional constrain- ing information.', 'However, usually it is the case that lexical entries resulting from lexical rule application differ in very few specifications compared to the number of specifica- tions in a base lexical entry.', 'Most of the specifications of a lexical entry are assumed to be passed unchanged via the automatically generated frame specification.', 'Therefore, after lifting the common information into the extended lexical entry, the out-argument in many cases contains enough information to permit a postponed execution of the interaction predicate.', 'When C is the common information, and D1, ..., Dk are the definitions of the interaction predicate called, we use distributivity to factor out C in (C A D1) V -.. V (C A Dk): We compute C A (D1 V ... V Dk), where the r) are assumed to contain no further common factors.', 'Once we have computed c, we use it to make the extended lexical entry more specific.', 'This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.']",0,['The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.']
CC886,J97-4003,On Expressing Lexical Generalizations in HPSG,the compleat lkb,['Ann Copestake'],related work,,"This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .","['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .']"
CC887,J97-4003,On Expressing Lexical Generalizations in HPSG,controlling the application of lexical rules,"['Ted Briscoe', 'Ann Copestake']",introduction,"In this paper, we describe an item-familiarity account of the semi-productivity of morphological and lexical rules, and illustrate how it can be applied to practical issues which arise when building large scale lexical knowledge bases which utilize lexical rules. Our approach assumes that attested uses of derived words and senses are explicitly recorded, but that productive use of lexical rules is also possible, though controlled by probabilities associated with rule application. We discuss how the necessary probabilities and estimates of lexical rule productivity may be acquired from corpora.","27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .","['The way these predicates interconnect is represented in Figure 19.', '27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', '28 In order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', 'Since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given.']",0,"['27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .']"
CC888,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC889,J97-4003,On Expressing Lexical Generalizations in HPSG,the representation of lexical semantic information cognitive science research paper csrp 280,['Ann Copestake'],introduction,,"This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .","['This disjunction thus constitutes the base lexicon.', 'The disjuncts in the constraint on derived-word, on the other hand, encode the lexical rules.', 'The in-specification of a lexical rule specifies the IN feature, the out-specification, the derived word itself.', 'Note that the value of the IN feature is of type word and thus also has to satisfy either a base lexical entry or an out-specification of a lexical rule.', 'While this introduces the recursion necessary to permit successive lexical rule application, it also grounds the recursion in a word described by a base lexical entry.', 'Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .', 'Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.', 'As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.', 'The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.', '9']",0,"['This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .']"
CC890,J97-4003,On Expressing Lexical Generalizations in HPSG,transformations of logic programs foundations and techniques,"['Alberto Pettorossi', 'Maurizio Proietti']",introduction,,"As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .","['The automata resulting from word class specialization group the lexical entries into natural classes.', 'In case the automata corresponding to two lexical entries are identical, the entries belong to the same natural class.', 'However, each lexical rule application, i.e., each transition in an automaton, calls a frame predicate that can have a large number of defining clauses.', 'Intuitively understood, each defining clause of a frame predicate corresponds to a subclass of the class of lexical entries to which a lexical rule can be applied.', 'During word class specialization, though, when the finite-state automaton representing global lexical rule application is pruned with respect to a particular base lexical entry, we know which subclass we are dealing with.', 'For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones.', 'We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (Tamaki and Sato 1984).', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .', 'Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.', 'To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.', '3° The successive unfolding steps are schematically represented in Figure 20.']",1,"['As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .']"
CC891,J97-4003,On Expressing Lexical Generalizations in HPSG,modularizing contexted constraints,['John Griffith'],introduction,"This paper describes a method for compiling a constraint-based grammar into a potentially more efficient form for processing. This method takes dependent disjunctions within a constraint formula and factors them into non-interacting groups whenever possible by determining their independence. When a group of dependent disjunctions is split into smaller groups, an exponential amount of redundant information is reduced. At runtime, this means that an exponential amount of processing can be saved as well. Since the performance of an algorithm for processing constraints with dependent disjunctions is highly determined by its input, the transformation presented in this paper should prove beneficial for all such algorithms. 1 Introduction  There are two facts that conspire to make the treatment of disjunction an important consideration when building a natural language processing (NLP) system. The first fact is that natural languages are full of ambiguities, and in a grammar many of these ambi..",32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .,"['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing.']",0,['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .']
CC892,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,"12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .","['The translation of the lexical rule into a predicate is trivial.', 'The result is displayed description language.', '12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A more detailed presentation can be found in Minnen (in preparation).', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 Hinrichs and Nakazawa (1996) show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler.']",0,"['12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .']"
CC893,J97-4003,On Expressing Lexical Generalizations in HPSG,partialvp and splitnp topicalization in german an hpsg analysis,"['Erhard Hinrichs', 'Tsuneko Nakazawa']",introduction,,"6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .","['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world (Gerdemann and King 1994;Gerdemann 1995).', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']",0,"['6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .']"
CC894,J97-4003,On Expressing Lexical Generalizations in HPSG,open and closed world types in nlp systems,['Dale Gerdemann'],introduction,,4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .,"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']",0,"['The terminology used in the literature varies.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .']"
CC895,J97-4003,On Expressing Lexical Generalizations in HPSG,on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars,['Detmar Meurers'],introduction,"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.","Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .","['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AI (McCarthy and Hayes 1969), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .']",0,"['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .']"
CC896,J97-4003,On Expressing Lexical Generalizations in HPSG,on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars,['Detmar Meurers'],introduction,"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.",As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .,"['A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.', 'Most current HPSG analyses of Dutch, German, Italian, and French fall into that category.', '1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.', 'Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.', 'As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .']",4,"['A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.', 'As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .']"
CC897,J97-4003,On Expressing Lexical Generalizations in HPSG,unfoldfold transformation of logic programs,"['Hisao Tamaki', 'Taisuke Sato']",introduction,,The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .,"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of the clause.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']",5,['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .']
CC898,J97-4003,On Expressing Lexical Generalizations in HPSG,statische programmtransformationen zur effizienten verarbeitung constraintbasierter grammatiken diplomarbeit,['Annette Opalka'],related work,,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC899,J97-4003,On Expressing Lexical Generalizations in HPSG,the formalism and implementation of patr ii,"['Stuart Shieber', 'Hans Uszkoreit', 'Fernando Pereira', 'Jane Robinson', 'Mabry Tyson']",related work,,A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC900,J97-4003,On Expressing Lexical Generalizations in HPSG,the typed feature structure representation formalism,['Martin Emele'],related work,,A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC901,J97-4003,On Expressing Lexical Generalizations in HPSG,lexical rules in hpsg what are they,"['Mike Calcagno', 'Carl Pollard']",introduction,,"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .","['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']",0,"['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).']"
CC902,J97-4003,On Expressing Lexical Generalizations in HPSG,an expanded logical formalism for headdriven phrase structure grammar,['Paul King'],,"Though Pollard and Sag 1994] assumes that an unspeciied variant of the formal logic of Carpenter 1992] will provide a formalism for HPSG, a precise formulation of the envisaged formalism is not immediately obvious, primarily because a principal tenet of Carpenter 1992], that feature structures represent partial information, seems to connict with a principal tenet of Pollard and Sag 1994], that feature structures represent abstract linguistic entities. This has caused many HPSGians to be mistakenly concerned with partial-information speciic notions, such as subsumption, that are appropriate for the Carpenter 1992] logic but inappropriate for the formalism Pollard and Sag 1994] envisages. This paper hopes to allay this concern and the confusion it engenders by substituting King 1989] for Carpenter 1992] as the basis of the envisaged formalism. It demonstrates that the formal logic of King 1989] provides a formalism for HPSG that meets all Pollard and Sag 1994] asks of the envisaged formalism. It further shows that the most credible variant of the Carpenter 1992] logic consistent with the aims of Pollard and Sag 1994] is not only incompatible with the tenet that feature structures represent partial information, but also an instance of the King 1989] logic.",The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .,"['The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .', 'We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).', 'This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'Our compiler distinguished seven word classes.', 'Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations.']",5,"['The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .', 'We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).', 'This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.']"
CC903,J97-4003,On Expressing Lexical Generalizations in HPSG,interpreting lexical rules,['Mike Calcagno'],introduction,,"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .","['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']",0,"['Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).']"
CC904,J97-4003,On Expressing Lexical Generalizations in HPSG,flipped out aux in german,"['Erhard Hinrichs', 'Tsuneko Nakazawa']",introduction,,"/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC905,J97-4003,On Expressing Lexical Generalizations in HPSG,ale—the attribute logic engine users guide version 201,"['Bob Carpenter', 'Gerald Penn']",related work,"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ...","A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .","['A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'In the ALE system, for example, a depth bound can be specified for this purpose.', 'Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding.']",1,"['A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.']"
CC906,J97-4003,On Expressing Lexical Generalizations in HPSG,a logical formalism for headdriven phrase structure grammar,['Paul King'],introduction,,"A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .","['A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .', 'The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects.', 'These atomic expressions can be combined using conjunction, disjunction, and negation.', 'The expressions are interpreted by a set-theoretical semantics.']",0,"['A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .']"
CC907,J97-4003,On Expressing Lexical Generalizations in HPSG,prolog and natural language analysis csli lecture notes center for the study of language and information,"['Fernando Pereira', 'Stuart Shieber']",introduction,,"The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .","['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29', 'The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of the clause.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']",0,"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29', 'The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.']"
CC908,J97-4003,On Expressing Lexical Generalizations in HPSG,typed unification grammars,"['Martin Emele', 'Remi Zajac']",related work,"This paper defines unification based ID/LP grammars based on typed feature structures as nonterminals and proposes a variant of Earley's algorithm to decide whether a given input sentence is a member of the language generated by a particular typed unification ID/LP grammar. A solution to the problem of the nonlocal flow of information in unification ID/LP grammars as discussed in Seiffert (1991) is incorporated into the algorithm. At the same time, it tries to connect this technical work with linguistics by presenting an example of the problem resulting from HPSG approaches to linguistics (Hinrichs and Nakasawa 1994, Richter and Sailer 1995) and with computational linguistics by drawing connections from this approach to systems implementing HPSG, especially the TROLL system, Gerdemann et al. (forthcoming).Comment: paper (81 pages), appendix (17 pages, Prolog code), format: .ps   compressed and uuencode",A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC909,J97-4003,On Expressing Lexical Generalizations in HPSG,a logical formalism for headdriven phrase structure grammar,['Paul King'],introduction,,"This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '","['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '"", '11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism.', 'We do not make the linguistic claim that passives should be analyzed using such a lexical rule.', 'For space reasons, the SYNSEM feature is abbreviated by its first letter.', 'The traditional (First I Rest) list notation is used, and the operator • stands for the append relation in the usual way.', '1l Manandhar (1995) proposes to unify these two steps by including an update operator in the The computational treatment we discuss in the rest of the paper follows this setup in that it automatically computes, for each lexical rule specification, the frames necessary to preserve the properties not changed by it.', '12 We will show that the detection and specification of frames and the use of program transformation to advance their integration into the lexicon encoding is one of the key ingredients of the covariation approach to HPSG lexical rules.']",0,"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '""]"
CC910,J97-4003,On Expressing Lexical Generalizations in HPSG,hpsg lexicon without lexical rules,['Karel Oliva'],related work,"this paper, I shall try  (i) to show that ueglecting standtu:d insights of tile orgauization of lexicon is detrimental both to the linguistic adequacy and to the practical useful- hess of the lexicon,  (ii) to make a proposal of an alternative reconcil ing the needs of HPSG with the usual lexicographic practic","In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC911,J97-4003,On Expressing Lexical Generalizations in HPSG,the update operation in feature logic,['Suresh Manandhar'],introduction,,11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.,['11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.'],0,['11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.']
CC912,J97-4003,On Expressing Lexical Generalizations in HPSG,an overview of disjunctive constraint satisfaction,"['John Maxwell', 'Ronald Kaplan']",introduction,This paper presents a new algorithm for solving disjunctive systems of constraints. The algorithm determines whether a system is satisfiable and produces the models if the system is satisfiable. There are three main steps for determining whether or not the system is satisfiable: 1 ) turn the disjunctive system into an equi-satisfiable conjunctive system in polynomial time 2) convert the conjunctive system into canonical form using extensions of standard techniques #3) extract and solve a propositional 'disjunctive residue',32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .,"['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing.']",0,['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .']
CC913,J97-4003,On Expressing Lexical Generalizations in HPSG,the generative power of categorial grammars and headdriven phrase structure grammars with lexical rules,['Bob Carpenter'],related work,"In this paper, it is shown that the addition of simple and linguistically motivated forms of lexical rules to grammatical theories based on subcategorization lists, such as categorial grammars (CG) or head-driven phrase structure grammars (HPSG), results in a system that can generate all and only the recursively enumerable languages. The proof of this result is carried out by means of a reduction of generalized rewriting systems. Two restrictions are considered, each of which constrains the generative power of the resulting system to context-free languages.",The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .,"['The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .', 'In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper.']",0,['The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .']
CC914,J97-4003,On Expressing Lexical Generalizations in HPSG,some philosophical problems from the standpoint of artificial intelligence,"['John McCarthy', 'Patrick Hayes']",introduction,,"This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .","['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (Meurers 1994).']",1,"['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (Meurers 1994).']"
CC915,J97-4003,On Expressing Lexical Generalizations in HPSG,the craft of prolog,"[""Richard O'Keefe""]",introduction,"Hacking your program is no substitute for understanding your problem. Prolog is different, but not that different. Elegance is not optional. These are the themes that unify Richard O'Keefe's very personal statement on how Prolog programs should be written. The emphasis in ""The Craft of Prolog"" is on using Prolog effectively. It presents a loose collection of topics that build on and elaborate concepts learning in a first course. These may be read in any order following the first chapter, ""Basic Topics in Prolog, "" which provides a basis for the rest of the material in the book.","Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .","['Encoding a finite-state automaton as definite relations is rather straightforward.', 'In fact, one can view the representations as notational variants of one another.', 'Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .', 'Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail.']",5,"['Encoding a finite-state automaton as definite relations is rather straightforward.', 'In fact, one can view the representations as notational variants of one another.', 'Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .']"
CC916,J97-4003,On Expressing Lexical Generalizations in HPSG,applying lexical rules under subsumption,"['Erhard Hinrichs', 'Tsuneko Nakazawa']",introduction,"Lexical rules are used in constraint based grammar formalisms such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994) to express generalizations among lexical entries. This paper discusses a number of lexical rules from recent HPSG analyses of German (Hinrichs and Nakazawa 1994) and shows that the grammar in some cases vastly overgenerates and in other cases introduces massive spurious structural ambiguity, if lexical rules apply under unification. Such problems of overgeneration or spurious ambiguity do not arise, if a lexical rule applies to a given lexical entry iff the lexical entry is subsumed by the left-hand side of the lexical rule. Finally, the paper discusses computational consequences of applying lexical rules under subsumption.",15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .,"['The translation of the lexical rule into a predicate is trivial.', 'The result is displayed description language.', '12 In order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in Meurers (1995).', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A more detailed presentation can be found in Minnen (in preparation).', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler.']",0,['15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .']
CC917,J97-4003,On Expressing Lexical Generalizations in HPSG,word formation in lexical type hierarchies a case study of baradjectives in german masters thesis,['Susanne Riehemann'],related work,,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC918,J97-4003,On Expressing Lexical Generalizations in HPSG,verb second by underspecification,['Annette Frank'],related work,,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .']"
CC919,J97-4003,On Expressing Lexical Generalizations in HPSG,an expanded logical formalism for headdriven phrase structure grammar,['Paul King'],introduction,"Though Pollard and Sag 1994] assumes that an unspeciied variant of the formal logic of Carpenter 1992] will provide a formalism for HPSG, a precise formulation of the envisaged formalism is not immediately obvious, primarily because a principal tenet of Carpenter 1992], that feature structures represent partial information, seems to connict with a principal tenet of Pollard and Sag 1994], that feature structures represent abstract linguistic entities. This has caused many HPSGians to be mistakenly concerned with partial-information speciic notions, such as subsumption, that are appropriate for the Carpenter 1992] logic but inappropriate for the formalism Pollard and Sag 1994] envisages. This paper hopes to allay this concern and the confusion it engenders by substituting King 1989] for Carpenter 1992] as the basis of the envisaged formalism. It demonstrates that the formal logic of King 1989] provides a formalism for HPSG that meets all Pollard and Sag 1994] asks of the envisaged formalism. It further shows that the most credible variant of the Carpenter 1992] logic consistent with the aims of Pollard and Sag 1994] is not only incompatible with the tenet that feature structures represent partial information, but also an instance of the King 1989] logic.",4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .,"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']",0,"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']"
CC920,J97-4003,On Expressing Lexical Generalizations in HPSG,french clitic climbing without clitics or climbing,"['Philip Miller', 'Ivan Sag']",introduction,,"de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', 'de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', 'de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC921,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).","['16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).', 'In such a Predicative Lexical Rule (which we only note as an example and not as a linguistic proposal) the subtype of the head object undergoing the rule as well as the value of the features only appropriate for the subtypes of substantive either is lost or must be specified by a separate rule for each of the subtypes.']",0,"['16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).']"
CC922,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,"The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .","['Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.', 'The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup.', 'The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .']",2,"['The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .']"
CC923,J97-4003,On Expressing Lexical Generalizations in HPSG,passive without lexical rules in,['Andreas Kathol'],related work,,"In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .']"
CC924,J97-4003,On Expressing Lexical Generalizations in HPSG,lexical polymorphism and word disambiguation,['Antonio Sanfilippo'],related work,We present an approach to lexical ambiguity where regularities about sense/u~ge extensibillty are represented by underepecifying word entries through lexic~d polymorphism. Word diumbiguation is carried out using contextual information gathered during language processing to ground polymorphic lexical entries.,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .']"
CC925,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC926,J97-4003,On Expressing Lexical Generalizations in HPSG,a computational treatment of hpsg lexical rules as covariation in lexical entries,"['Detmar Meurers', 'Guido Minnen']",introduction,We describe a compiler which translates a set of HPSG lexical rules and their interaction into definite relations used to constrain lexical entries. The compiler ensures automatic transfer of properties unchanged by a lexical rule. Thus an operational semantics for the full lexical rule mechanism as used in HPSG linguistics is provided. Program transformation techniques are used to advance the resulting encoding. The final output constitutes a computational counterpart of the linguistic generalizations captured by lexical rules and allows ``on the fly'' application.,"Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .","['Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .', 'We developed a compiler that takes as its input a set of lexical rules, deduces the nec- essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'Each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'The definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries.']",4,"['Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .']"
CC927,J97-4003,On Expressing Lexical Generalizations in HPSG,the representation of lexical semantic information cognitive science research paper csrp 280,['Ann Copestake'],related work,,"This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .","['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC928,J97-4003,On Expressing Lexical Generalizations in HPSG,on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars,['Detmar Meurers'],introduction,"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.","However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .","['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']",4,"['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']"
CC929,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .","['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']",0,"['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .']"
CC930,J97-4003,On Expressing Lexical Generalizations in HPSG,featurebased inheritance networks for computational lexicons,"['Hans-Ulrich Krieger', 'John Nerbonne']",related work,"The virtues of viewing the lexicon as an inheritance network are its succinctness and its tendency to highlight significant clusters of linguistic properties. From its succinctness follow two practical advantages, namely its ease of maintenance and modification. In this paper we present a feature-based foundation for lexical inheritance. We argue that the feature-based foundation is both more economical and expressively more powerful than non-feature-based systems. It is more economical because it employs only mechanisms already assumed to be present elsewhere in the grammar (viz., in the feature system), and it is more expressive because feature systems are more expressive than other mechanisms used in expressing lexical inheritance (cf. DATR). The lexicon furthermore allows the use of default unification, based on the ideas of default unification, defined by Bouma. These claims are buttressed in sections sketching the opportunities for lexical description in feature-based lexicons in two central lexical topics, inflection and derivation. Briefly, we argue that the central notion of paradigm may be defined in feature structures, and that it may be more satisfactorily (in fact, immediately) linked to the syntactic information in this fashion. Our discussion of derivation is more programmatic; but here, too, we argue that feature structures of a suitably rich sort provide a foundation for the definition of lexical rules. We illustrate theoretical claims in application to German lexis. This work is currently under implementation in a natural language understanding effort (DISCO) at the German Artiffical Intelligence Center (Deutsches Forschungszentrum fur Kunstliche Intelligenz).","In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).","['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC931,J97-4003,On Expressing Lexical Generalizations in HPSG,of csli lecture notes center for the study of language and information,"['Carl Pollard', 'Ivan Sag']",introduction,,"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .","['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']",1,"['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .']"
CC932,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,"Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )","['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']",0,"['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']"
CC933,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,design challenges and misconceptions in named entity recognition,"['L Ratinov', 'D Roth']",related work,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .","['The task of mention detection is closely related to Named Entity Recognition (NER).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'However, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .', 'NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g.', 'Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004).', 'The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities.']",0,"['#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .']"
CC934,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,understanding the value of features for coreference resolution,"['E Bengtson', 'D Roth']",,"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.","For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #AUTHOR_TAG .","['Here, y u,v = 1 iff mentions u, v are directly linked.', 'Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred.', 'For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in #AUTHOR_TAG .']",5,"['For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in #AUTHOR_TAG .']"
CC935,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,understanding the value of features for coreference resolution,"['E Bengtson', 'D Roth']",introduction,"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.","In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .","['Here, phrases in the brackets are mentions and the underlined simple phrases are mention heads.', 'Moreover, mention boundaries can be nested (the boundary of a mention is inside the boundary of another mention), but mention heads never overlap.', 'This property also simplifies the problem of mention head candidate generation.', 'In the example above, the first ""they"" refers to ""Multinational companies investing in China"" and the second ""They"" refers to ""Domestic manufacturers, who are also suffering"".', 'In both cases, the mention heads are sufficient to support the decisions: ""they"" refers to ""companies"", and ""They"" refers to ""manufacturers"".', 'In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .']",0,"['In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .']"
CC936,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a joint model for entity analysis coreference typing and linking,"['G Durrett', 'D Klein']",experiments,"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.","For Berkeley system , we use the reported results from #AUTHOR_TAG .","['The latest scorer is version v8.01, but MUC, B , CEAF e and CoNLL average scores are not changed.', 'For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12', 'We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input.', 'Results for HOTCoref are slightly different from the results reported in Björkelund and Kuhn (2014).', 'For Berkeley system , we use the reported results from #AUTHOR_TAG .']",1,"['The latest scorer is version v8.01, but MUC, B , CEAF e and CoNLL average scores are not changed.', 'For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12', 'We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input.', 'Results for HOTCoref are slightly different from the results reported in Bjorkelund and Kuhn (2014).', 'For Berkeley system , we use the reported results from #AUTHOR_TAG .']"
CC937,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a joint model for entity analysis coreference typing and linking,"['G Durrett', 'D Klein']",experiments,"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.","Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .","['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .', 'Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']",1,"['Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .']"
CC938,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,understanding the value of features for coreference resolution,"['E Bengtson', 'D Roth']",experiments,"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.","We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .","['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']",5,"['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.']"
CC939,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,headdriven statistical models for natural language parsing,['M Collins'],,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .","['Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples.', 'We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples.', 'Specifically, after mention head candidate generation (described in Sec.', '3), we train on a set of candidates with precision larger than 50%.', 'We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .', 'When these extracted heads do not overlap with gold mention heads, we treat them as negative examples.']",5,"['We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .']"
CC940,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",introduction,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.","Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .","['Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception).', 'Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .', 'However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1.', 'Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions.', 'These performance gaps are worrisome, since the real goal of NLP systems is to process raw data.', '1: Performance gaps between using gold mentions and predicted mentions for three state-of-the-art coreference resolution systems.', 'Performance gaps are always larger than 10%.', ""Illinois's system (Chang et al., 2013) is evaluated on CoNLL (2012CoNLL ( , 2011) Shared Task and ACE-2004 datasets."", 'It reports an average F1 score of MUC, B and CEAF e metrics using CoNLL v7.0 scorer.', ""Berkeley's system (Durrett and Klein, 2013) reports the same average score on the CoNLL-2011 Shared Task dataset."", ""Results of Stanford's system (Lee et al., 2011) are for B 3 metric on ACE-2004 dataset."", 'This paper focuses on improving end-to-end coreference performance.', 'We do this by: 1) Developing a new ILP-based joint learning and inference formulation for coreference and mention head detection.', '2) Developing a better mention head candidate generation algorithm.', 'Importantly, we focus on heads rather than mention boundaries since those can be identified more robustly and used effectively in an end-to-end system.', 'As we show, this results in a dramatic improvement in the quality of the MD component and, consequently, a significant reduction in the performance gap between coreference on gold mentions and coreference on raw data.']",0,"['Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .']"
CC941,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,design challenges and misconceptions in named entity recognition,"['L Ratinov', 'D Roth']",,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .","['Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks.', 'The problem is then transformed into a simple, but constrained, 5-class classification problem.']",4,"['Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks.']"
CC942,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,conll2012 shared task modeling multilingual unrestricted coreference in ontonotes,"['S Pradhan', 'A Moschitti', 'N Xue', 'O Uryupina', 'Y Zhang']",experiments,"The CoNLL-2012 shared task involved predicting coreference in three languages -- English, Chinese and Arabic -- using OntoNotes data. It was a follow-on to the English-only task organized in 2011. Until the creation of the OntoNotes corpus, resources in this subfield of language processing have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ACE entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types and covering multiple languages. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, evaluation criteria, and presents and discusses the results achieved by the participating systems. Being a task that has a complex evaluation history, and multiple evalation conditions, it has, in the past, been difficult to judge the improvement in new algorithms over previously reported results. Having a standard test set and evaluation parameters, all based on a resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.","The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .","['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008).', 'The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']",5,"['The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .']"
CC943,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",experiments,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.","Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .","['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Björkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']",5,"['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.']"
CC944,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",related work,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.","In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .","['Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013;Björkelund and Kuhn, 2014;Song et al., 2012).', 'Many of the early rule-based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'The early designs were easy to understand and the rules were designed manually.', 'Machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'The introduction of ILP methods has influenced the coreference area too Denis and Baldridge, 2007).', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .']",5,"['Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013;Bjorkelund and Kuhn, 2014;Song et al., 2012).', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .']"
CC945,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,firstorder probabilistic models for coreference resolution,"['A Culotta', 'M Wick', 'A McCallum']",experiments,"Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases. In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference. We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases. This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently.","We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .","['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']",5,"['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.']"
CC946,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.",More details can be found in #AUTHOR_TAG et al. (2013).,"['More details can be found in #AUTHOR_TAG et al. (2013).', 'The difference here is that we also consider the validity of mention heads using �(u),�(m)']",0,['More details can be found in #AUTHOR_TAG et al. (2013).']
CC947,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a joint model for entity analysis coreference typing and linking,"['G Durrett', 'D Klein']",related work,"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.",Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .,"['Several recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .', 'The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different.']",0,['Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .']
CC948,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,headdriven statistical models for natural language parsing,['M Collins'],experiments,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .","['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Björkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']",5,"['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.']"
CC949,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.",Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .,"['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.', 'The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'This joint framework aims to improve performance on both mention head detection and on coreference.']",5,"['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.']"
CC950,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,ontonotes the 90 solution,"['E Hovy', 'M Marcus', 'M Palmer', 'L Ramshaw', 'R Weischedel']",experiments,"We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.","We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .","['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).', 'Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.']",5,"['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).']"
CC951,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",method,,We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .,"['For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross-linguistically-plausible categories.', 'To formalize the notion of what it means for a category to be more ""plausible"", we extend the category generator of our previous work, which we will call P CAT .', 'We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C:']",0,"['We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in SS5) with probability p del , or a standard CCG category C:']"
CC952,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a generative constituentcontext model for improved grammar induction,"['Dan Klein', 'Christopher D Manning']",related work,,#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) .,"[""#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', 'While CCM is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels.']",0,"[""#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.']"
CC953,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,bayesian inference for pcfgs via markov chain monte carlo,"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']",introduction,"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.","In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .","['In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', 'However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.', 'Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios.']",5,"['In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', 'However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.']"
CC954,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,building a large annotated corpus of english the penn treebank,"['Mitchell P Marcus', 'Beatrice Santorini', 'Mary Ann Marcinkiewicz']",experiments,"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.","We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .","['In our evaluation we compared our supertagcontext approach to (our reimplementation of) the best-performing model of our previous work (Garrette et al., 2015), which SCM extends.', 'We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .']",5,"['We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .']"
CC955,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",method,,The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG .,"[""The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG ."", ""Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label.""]",1,"[""The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG ."", ""Thus, the right-side context prior mean th RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label.""]"
CC956,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",experiments,,We use the same splits as #AUTHOR_TAG .,"['Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set.', 'We use the same splits as #AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.', 'In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999).', 'For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k).', 'For Chinese and Italian, for training efficiency, we used only raw sentences that were 50 words or fewer (note that we did not drop tag dictionary set or test set sentences).']",5,"['Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set.', 'We use the same splits as #AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.']"
CC957,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,online learning of relaxed ccg grammars for parsing to logical form,"['Luke S Zettlemoyer', 'Michael Collins']",experiments,"We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar--for example allowing flexible word order, or insertion of lexical items-- with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).","This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .","['When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be ""wrong"".', 'Thus, it is important that we make a best effort to find a parse.', 'To accomplish this, we implemented a parsing backoff strategy.', 'The parser first tries to find a valid parse that has either s dcl or np at its root.', 'If that fails, then it searches for a parse with any root.', 'If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without).', 'This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D →u for every potential supertag u in the tree.', 'Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t→ D , v and t→ v, D .', 'Recall that in §3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.', 'Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents.']",1,"['When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be ""wrong"".', 'Thus, it is important that we make a best effort to find a parse.', 'This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D -u for every potential supertag u in the tree.', 'Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t- D , v and t- v, D .', 'Recall that in SS3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.']"
CC958,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",introduction,,We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .,"['Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.', 'In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015).', 'Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable.', 'We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .']",2,['We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .']
CC959,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,typesupervised hidden markov models for partofspeech tagging with incomplete tag dictionaries,"['Dan Garrette', 'Jason Baldridge']",method,"Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the min-greedy algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to performance over the original min-greedy algorithm for both English and Italian data.","We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4","['We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4', 'This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'These counts are then combined with estimates of the �openness� of each tag in order to assess its likelihood of appearing with new words.']",5,"['We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4']"
CC960,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a generative constituentcontext model for improved grammar induction,"['Dan Klein', 'Christopher D Manning']",method,,"Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .","['Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .']",4,"['Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .']"
CC961,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,widecoverage efficient statistical parsing with ccg and loglinear models,"['Stephen Clark', 'James R Curran']",,,We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #AUTHOR_TAG .,"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of #AUTHOR_TAG .']",5,"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of #AUTHOR_TAG .']"
CC962,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,bayesian inference for pcfgs via markov chain monte carlo,"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']",,"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.","To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .","['To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .', 'For a sentence w, the strategy is to use the Inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non-terminal position spanning words w i through w j−1 and category t, going ""up"" the tree, the probability of generating w i , . . .', ', w j−1 via any arrangement of productions that is rooted by y ij = t.']",5,"['To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .']"
CC963,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,bayesian inference for pcfgs via markov chain monte carlo,"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']",,"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.",Our strategy is based on the approach presented by #AUTHOR_TAG .,"['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']",5,"['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']"
CC964,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a generative constituentcontext model for improved grammar induction,"['Dan Klein', 'Christopher D Manning']",introduction,,"One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .","['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.', 'This DET-VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN.', 'From this, we might deduce that DET-VERB is a likely context for a noun phrase.', 'CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability.', 'However, since there is nothing intrinsic about the POS pair DET-VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data.', 'Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000;Steedman and Baldridge, 2011) categories reflect universal grammatical properties.', 'CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'For example, a category might encode that ""this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence"" instead of simply VERB.', 'CCG has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical-expansion algorithms (Bisk and Hockenmaier, 2012;2013) or encoding that information as priors within a Bayesian framework (Garrette et al., 2015).']",0,"['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .']"
CC965,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a ccg parsing with a supertagfactored model,"['Mike Lewis', 'Mark Steedman']",,,"We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .","['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007).']",5,"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .']"
CC966,N01-1003,SPoT,discriminative reranking for natural language parsing,['Michael Collins'],,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .","['Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones.', 'In total, we used 3,291 features in training the SPR.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .', 'The motivation for the features was to capture declaratively decisions made by the randomized SPG.', 'We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times.']",1,"['Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .']"
CC967,N01-1003,SPoT,clause aggregation using linguistic knowledge,['James Shaw'],related work,"By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar.",Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .,"['Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.', 'Presumably, this is the reason why dialog systems to date have not used this kind of sentence planning.']",0,"['Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.']"
CC968,N01-1003,SPoT,discriminative reranking for natural language parsing,['Michael Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .","['Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into sentences. 1', 'For example, consider the required capabilities of a sentence planner for a mixed-initiative spoken dialog system for travel planning: (D1) System1: Welcome.... What airport would you like to fly out of?', 'User2: I need to go to Dallas.', 'System3: Flying to Dallas.', 'What departure airport was that?', 'User4: from Newark on September the 1st.', 'System5: What time would you like to travel on September the 1st to Dallas from Newark?', ""Utterance System1 requests information about the caller's departure airport, but in User2, the caller takes the initiative to provide information about her destination."", ""In System3, the system's goal is to implicitly confirm the destination (because of the possibility of error in the speech recognition component), and request information (for the second time) of the caller's departure airport."", 'In User4, the caller provides this information but also provides the month and day of travel.', ""Given the system's dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has provided so far, i.e. the departure and destination cities and the month and day information, as well as to request information about the time of travel."", ""The system's representation of its communicative goals for utterance System5 is in Figure 1."", 'The job of the sentence planner is to decide among the large number of potential realizations of these communicative goals.', 'Some example alternative realizations are in Figure 2. 2 implicit-confirm(orig-city:NEWARK) implicit-confirm(dest-city:DALLAS) implicit-confirm(month:9) implicit-confirm(day-number:1) request(depart-time) In this paper, we present SPoT, for ""Sentence Planner, Trainable"".', 'We also present a new methodology for automatically training SPoT on the basis of feedback provided by human judges.', 'In order to train SPoT, we reconceptualize its task as consisting of two distinct phases.', 'In the first phase, the sentence-plan-generator (SPG) generates a potentially large sample of possible sentence plans for a given text-plan input.', 'In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer.', 'Our primary contribution is a method for training the SPR.', 'The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .']",1,"['The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .']"
CC969,N01-1003,SPoT,clause aggregation using linguistic knowledge,['James Shaw'],,"By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar.","These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .","['As already mentioned, we divide the sentence planning task into two phases.', 'In the first phase, the sentenceplan-generator (SPG) generates 12-20 possible sentence plans for a given input text plan.', ""Each speech act is assigned a canonical lexico-structural representation (called a DSyntS -Deep Syntactic Structure (Mel'čuk, 1988))."", 'The sentence plan is a tree recording how these elementary DSyntS are combined into larger DSyntSs; the DSyntS for the entire input text plan is associated with the root node of the tree.', 'In the second phase, the sentence plan ranker (SPR) ranks sentence plans generated by the SPG, and then selects the top-ranked output as input to the surface realizer, RealPro (Lavoie and Rambow, 1997)  The research presented here is primarily concerned with creating a trainable SPR.', 'A strength of our approach is the ability to use a very simple SPG, as we explain below.', 'The basis of our SPG is a set of clausecombining operations that incrementally transform a list of elementary predicate-argument representations (the DSyntSs corresponding to elementary speech acts, in our case) into a single lexico-structural representation, by combining these representations using the following combining operations.', 'Examples can be found in Figure  ADJECTIVE.', 'This transforms a predicative use of an adjective into an adnominal construction.', 'PERIOD.', 'Joins two complete clauses with a period.', 'These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .']",0,"['These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .']"
CC970,N01-1003,SPoT,sentence planning as description using tree adjoining grammar,"['Matthew Stone', 'Christine Doran']",,"We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices.","The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .","['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The interior nodes are associated with DSyntSs by executing their clausecombing operation on their two daughter nodes.', '(A PE-RIOD node results in a DSyntS headed by a period and whose daughters are the two daughter DSyntSs.)', 'If a clause combination fails, the sp-tree is discarded (for example, if we try to create a relative clause of a structure which already contains a period).', 'As a result, the DSyntS for the entire turn is associated with the root node.', 'This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes).', 'The SPG is designed in such a way that if a DSyntS is associated with the root node, it is a valid structure which can be realized.', 'Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- 3 The sp-tree is inspired by (Lavoie and Rambow, 1998).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the origin and destination cities. Figure 8 illustrates the relationship between the sp-tree and the DSyntS for alternative 8.', 'The labels and arrows show the DSyntSs associated with each node in the sp-tree (in Figure 7), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'In our approach, we do not need to encode such constraints.', 'Rather, we generate a random sample of possible sentence plans for each text plan, up to a pre-specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution. 4']",0,"['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']"
CC971,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],method,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.","2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .","['2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .']",5,"['2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .']"
CC972,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.","The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .","['The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .']",1,"['The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .']"
CC973,N01-1006,Transformation Based Learning in the Fast Lane,classifier combination for improved lexical disambiguation,"['E Brill', 'J Wu']",experiments,"One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees..","The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .","['The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .']",5,"['The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .']"
CC974,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],experiments,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.","â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #AUTHOR_TAG ) .","[""â\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm ( #AUTHOR_TAG ) .""]",1,"[""â\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm ( #AUTHOR_TAG ) .""]"
CC975,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.",The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .,['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .'],0,['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']
CC976,N01-1010,Tree-cut and a lexicon based on systematic polysemy,automatic extraction of systematic polysemy using treecut,['N Tomuro'],experiments,"This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins.","Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .","['Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .', 'In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method.']",2,"['Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .']"
CC977,N01-1010,Tree-cut and a lexicon based on systematic polysemy,automatic extraction of systematic polysemy using treecut,['N Tomuro'],introduction,"This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins.","In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .","['In this paper, we describes a lexicon organized around systematic polysemy.', 'The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).', 'In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).', 'The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data.']",2,"['In this paper, we describes a lexicon organized around systematic polysemy.', 'The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).', 'In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.']"
CC978,N01-1024,Knowledge-free induction of inflectional morphologies,knowledgefree induction of morphology using latent semantic analysis,"['P Schone', 'D Jurafsky']",method,"Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stem-and-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (""ally"" stemming to ""all""). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plus-affix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.","In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .","['In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .', 'Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix.', 'The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1.', ""The matrix is structured such that for a given word w's row, the first N columns denote words that -NCS (µ,1)""]",2,"['In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .']"
CC979,N12-1010,A user modeling-based performance analysis of a wizarded uncertainty-adaptive dialogue system corpus,examining the impacts of dialogue content and system automation on affect models in a spoken tutorial dialogue system,"['J Drummond', 'D Litman']",,"Many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users ' affect. Previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models. Thus, we wish to investigate how more minor differences, like small dialogue content changes and switching from a wizarded system to a fully automated system, influence the performance of our affect detection models. We perform a post-hoc experiment where we use various data sets to train multiple models, and compare against a test set from the most recent version of our dialogue system. Analyzing these results strongly suggests that these differences do impact these models ' performance","Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .","['To train our DISE model, we first extracted the set of speech and dialogue features shown in Figure 2 from the user turns in our corpus.', 'As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\'s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (""incorrect runs"").', 'We also included two user-based features, gender and pretest score.', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .', 'To date, however, these features have only decreased the crossvalidation performance of our models.', '8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of ""response appropriateness"" in other domains, while pretest score corresponds to the general notion of domain expertise).', 'Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed.', 'To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users.']",2,"['To train our DISE model, we first extracted the set of speech and dialogue features shown in Figure 2 from the user turns in our corpus.', 'As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\'s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (""incorrect runs"").', 'We also included two user-based features, gender and pretest score.', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .']"
CC980,P00-1004,Translation with Cascaded Finite State Transducers,a polynomialtime algorithm for statistical machine translation,['D Wu'],introduction,,"Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .","['Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .']",0,"['Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .']"
CC981,P00-1004,Translation with Cascaded Finite State Transducers,a polynomialtime algorithm for statistical machine translation,['D Wu'],,,It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .,['It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .'],5,['It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .']
CC982,P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,a maximum entropy approach to natural language processing,"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']",method,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .,['It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .'],4,['It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .']
CC983,P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,a maximum entropy approach to natural language processing,"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .,['#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .'],0,['#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .']
CC984,P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,a maximum entropy approach to natural language processing,"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']",introduction,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .,['A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .'],5,['A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .']
CC985,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,memorybased shallow parsing,"['Walter Daelemans', 'Sabine Buchholz', 'Jorn Veenstra']",conclusion,,"As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .","['As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .']",3,"['As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .']"
CC986,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,statistical decisiontree models for parsing,['David M Magerman'],,,"The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .","['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .']",1,"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .']"
CC987,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,three generative lexicalised models for statistical parsing,['M Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .","['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']",0,"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']"
CC988,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,structural ambiguity and lexical relations,"['D Hindle', 'M Rooth']",conclusion,"We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.","It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .","['It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .']",1,"['It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .']"
CC989,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a maximumentropy partial parser for unrestricted text,"['W Skut', 'T Brants']",introduction,,Another approach for partial parsing was presented by #AUTHOR_TAG .,['Another approach for partial parsing was presented by #AUTHOR_TAG .'],0,['Another approach for partial parsing was presented by #AUTHOR_TAG .']
CC990,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],introduction,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .","['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .']",0,"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .']"
CC991,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,three generative lexicalised models for statistical parsing,['M Collins'],,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .","['Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .']",1,"['Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .']"
CC992,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,cascaded grammatical relation assignment,"['S Buchholz', 'J Veenstra', 'W Daelemans']",conclusion,"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.","In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .","['In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .']",3,"['In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .']"
CC993,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,statistical decisiontree models for parsing,['David M Magerman'],introduction,,"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .","['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']",0,"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']"
CC994,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .","['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .']",1,"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .']"
CC995,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,three generative lexicalised models for statistical parsing,['M Collins'],,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .","['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .']",1,"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .']"
CC996,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,cascaded grammatical relation assignment,"['S Buchholz', 'J Veenstra', 'W Daelemans']",introduction,"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.","One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .","['One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .']",0,"['One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .']"
CC997,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a maximumentropy partial parser for unrestricted text,"['W Skut', 'T Brants']",conclusion,,"In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .","['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']",3,"['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']"
CC998,P00-1012,The order of prenominal adjectives in natural language generation,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",experiments,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.","To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .","['Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second.', 'The first ordered pairs are more frequent, as are the individual adjectives involved.', 'To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a → b is:']",0,"['Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second.', 'The first ordered pairs are more frequent, as are the individual adjectives involved.', 'To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a - b is:']"
CC999,P00-1012,The order of prenominal adjectives in natural language generation,distributional clustering of english words,"['Fernando Pereira', 'Naftali Tishby', 'Lilian Lee']",conclusion,"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.","More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .","['While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'Future work will pursue at least two directions for improving the results.', 'First, while semantic information is not available for all adjectives, it is clearly available for some.', 'Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .', 'Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results.']",3,"['More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .']"
CC1000,P00-1012,The order of prenominal adjectives in natural language generation,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",experiments,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.",The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .,"['The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .', 'To order the pair {a, b}, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often.']",0,"['The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .', 'To order the pair {a, b}, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often.']"
CC1001,P00-1012,The order of prenominal adjectives in natural language generation,boosting applied to tagging and pp attachment,"['Steven Abney', 'Robert E Schapire', 'Yoram Singer']",conclusion,Boosting is a machine learning algorithm that is not well known in computational linguistics. We apply it to part-of-speech tagging and prepositional phrase attachment. Performance is very encouraging. We also show how to improve data quality by using boosting to identify annotation errors.,"In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .","['The second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data.', 'It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .']",3,"['In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .']"
CC1002,P00-1012,The order of prenominal adjectives in natural language generation,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",experiments,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.",#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .,"['One way to think of the direct evidence method is to see that it defines a relation ≺ on the set of English adjectives.', 'Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a ≺ b.', 'If the re-verse is true, and b, a is found more often than a, b , then b ≺ a.', 'If neither order appears in the training data, then neither a ≺ b nor b ≺ a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', 'That is, if a ≺ c and c ≺ b, we can conclude that a ≺ b.', 'To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'However, the pairs large, new and new, green occur fairly frequently.', 'Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.']",0,"['#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', 'That is, if a  c and c  b, we can conclude that a  b.', 'To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'However, the pairs large, new and new, green occur fairly frequently.', 'Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.']"
CC1003,P00-1012,The order of prenominal adjectives in natural language generation,generation that exploits corpusbased statistical knowledge,"['Irene Langkilde', 'Kevin Knight']",method,"We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.","One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .","['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.', 'To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood.']",5,"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .""]"
CC1004,P02-1001,Parameter estimation for probabilistic finite-state transducers,translation with finitestate devices,"['Kevin Knight', 'Yaser Al-Onaizan']",introduction,,"Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) .","['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']",0,"[""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC1005,P02-1001,Parameter estimation for probabilistic finite-state transducers,a rational design for a weighted finitestate transducer library,"['M Mohri', 'F Pereira', 'M Riley']",,,"fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )","['10 Traditionally log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )']",0,"['10 Traditionally log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )']"
CC1006,P02-1001,Parameter estimation for probabilistic finite-state transducers,rational series and their languages,"['Jean Berstel', 'Christophe Reutenauer']",,"I. Rational Series.- 1 Semirings.- 2 Formal Series.- 3 The Topology of K""X"".- 4 Rational Series.- 5 Recognizable Series.- 6 The Fundamental Theorem.- Exercises for Chapter I.- Notes to Chapter I.- II. Minimization.- 1 Syntactic Ideals.- 2 Reduced Linear Representations.- 3 The Reduction Algorithm.- Exercises for Chapter II.- Notes to Chapter II.- III. Series and Languages.- 1 The Theorem of Kleene.- 2 Series and Rational Languages.- 3 Supports.- 4 Iteration.- 5 Complementation.- Exercises for Chapter III.- Notes to Chapter III.- IV. Rational Series in One Variable.- 1 Rational Functions.- 2 The Exponential Polynomial.- 3 A Theorem of Polya.- 4 A Theorem of Skolem, Mahler and Lech.- Notes to Chapter IV.- V. Changing the Semiring.- 1 Rational Series over a Principal Ring.- 2 Positive Rational Series.- 3 Fatou Extensions.- Exercises for Chapter V.- Notes to Chapter V.- VI. Decidability.- 1 Problems of Supports.- 2 Growth.- Exercises for Chapter VI.- Notes to Chapter VI.- VII. Noncommutative Polynomials.- 1 The Weak Algorithm.- 2 Continuant Polynomials.- 3 Inertia.- 4 Gauss's Lemma.- Exercises for Chapter VII.- Notes to Chapter VII.- VIII. Codes and Formal Series.- 1 Codes.- 2 Completeness.- 3 The Degree of a Code.- 4 Factorization.- Exercises for Chapter VIII.- Notes to Chapter VIII.- References.",#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .,"['The denominator of equation ( 1) is the total probability of all accepting paths in x i • f • y i .', 'But while computing this, we will also compute the numerator.', 'The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'We will enforce an invariant: the weight of any pathset Π must be ( π∈Π P (π), π∈Π P (π) val(π)) ∈ R ≥0 × V , from which (1) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', 'Multiplication and addition are replaced by binary operations ⊗ and ⊕ on K. Thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to combine the weights of alternative paths.', 'To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = ∞ i=0 k i .', 'The usual finite-state algorithms work if (K, ⊕, ⊗, * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring (R ≥0 , +, ×, * ). 16', 'Our novel weights fall in a novel 14 Formal derivation of (1):']",5,['#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .']
CC1007,P02-1001,Parameter estimation for probabilistic finite-state transducers,an inequality and associated maximization technique in statistical estimation of probabilistic functions of a markov process,['L E Baum'],,,"For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .","['• In many cases of interest, T i is an acyclic graph. 20', ""hen Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of ⊕ and ⊗ operations."", 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .', 'But notice that it has no backward pass.', 'In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.', 'This is slower because our ⊕ and ⊗ are vector operations, and the vectors rapidly lose sparsity as they are added together.']",0,"['* In many cases of interest, T i is an acyclic graph. 20', ""hen Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of  and  operations."", 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .', 'But notice that it has no backward pass.', 'In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.', 'This is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together.']"
CC1008,P02-1001,Parameter estimation for probabilistic finite-state transducers,a rational design for a weighted finitestate transducer library,"['M Mohri', 'F Pereira', 'M Riley']",introduction,,"The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .","['The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', 'Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).', 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']",0,"['The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', 'Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).', 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC1009,P02-1001,Parameter estimation for probabilistic finite-state transducers,learning string edit distance,"['E Ristad', 'P Yianilos']",introduction,"In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other. In this report, we provide a stochastic model for string-edit distance. Our stochastic model allows us to learn a string-edit distance function from a corpus of examples. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. In this application, we learn a string-edit distance with nearly one-fifth the error rate of the untrained Levenshtein distance. Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes.","For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .","['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .']",0,"['For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .']"
CC1010,P02-1001,Parameter estimation for probabilistic finite-state transducers,iterative methods for solving linear systems,['Anne Greenbaum'],,"bthkhady`  withiithamchamsamhrabhaaphlechlyrabbechingesnmii`yuudwykan 2 aebbaihy + khuue` withiithamcham`yaangningkabwithiipriphuumiy`yaikhrl`f bthkhwaamwichaakaarnii`phipraayaenwkhidthawaipaelaethkhnikhphuuenthaankh`ngwithiithamcham`yaangning aidaek withiicchaaokhbii withiiekaas-aichedl aelawithiiph`nprnekinsuuebenuue`ng n`kcchaaknanyangphicchaarnaawithiithamchamthiiphathnaat`y`dcchaakwithiidangklaaw aidaek withiiph`nprnekinaebberng withiithamchamthiimiithaancchaakekrediiynt eaelawithiithamchamkamlangs`ngn`ysud samhrabwithiipriphuumiy`yaikhrl`fnanmiitnaebbmaacchaakwithiikh`ncchuuektekrediiynt withiidangklaawcchasraangthaanhlakechingtangchaakkh`ngpriphuumiaebbyukhlidcchaakemthrikchsamprasiththiodyphicchaarnaacchaakekrediiyntkh`ngfangkchankamlangs`ngthiis`dkhl`ng thaanhlakdangklaawprak`bdwyewket`rthiimiithisthaangthiithamaihphlechlykhaapramaanekhaaaiklphlechlycchringaiderwthiisud klaawodysrupaidwaa withiithamcham`yaangning 4 withiiaerkthiiklaawmaanancchakaarantiikaarluuekhaakh`nglamdabkh`ngphlechlyodypramaansuuphlechlycchringemuue`aichkabrabbthiimiiemthrikchsamprasiththi`yuuainruupaebbechphaaa echn emthrikchaenwthaeyngmumkhmaeth emthrikchldth`naimaid aelaemthrikchaebbae`l odyt`ngkamhndtawaepresrimthiiehmaaasm swnwithiithamchamthiimiithaancchaakekrediiyntaelawithiithamchamkamlangs`ngn`ysudaichaidkabrabbthiimiiemthrikchsamprasiththimiikhaalamdabchanetm samhrabwithiikh`ncchuuektekrediiyntaichaidkabrabbthiiemthrikchsamprasiththiepnemthrikchsmmaatrthiiepnbwkaenn`n   khamsamkhay: rabbechingesn withiiph`nprnekinsuuebenuue`ng  withiiph`nprnekinaebberng withiithamchamthiimiithaancchaakekrediiynt  withiikh`ncchuuektekrediiynt     ABSTRACT There are two major types of iterative methods for solving linear systems, namely, stationary iterative methods and Krylov subspace methods. This survey article discusses general ideas and elementary techniques for stationary iterative methods such as Jacobi method, Gauss-Seidel method, and the successive over-relaxation method. Moreover, we investigate further developed methods, namely, the accelerated over-relaxation method, the gradient based iterative method, and the least squares iterative method. On the other hand, Krylov subspace methods have prototypes from the conjugate gradient method. The latter method constructs an orthogonal basis for the Euclidean space from the gradient of the associated quadratic function. Such basis consists of vectors in directions so that the approximated solutions fastest approach to the exact solution. In conclusions, all 1st-4th mentioned stationary iterative methods guarantee the convergence of the sequence of approximated solutions to the exact solution when applying to the system with specific coefficient matrices such as strictly diagonally dominant matrices, irreducible matrices, and L-matrices. Here, the parameters in the methods must be appropriate. The gradient based iterative method and the least squares iterative method can be applied to systems with full-column rank coefficient matrices. The conjugate gradient method is applicable for the system whose coefficient matrix is a positive definite symmetric matrix.Keywords: linear system, successive over-relaxation method, accelerated over-relaxation method, gradient based iterative method, conjugate gradient metho","The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .","['We therefore reintroduce a backward pass that lets us avoid ⊕ and ⊗ when computing t i (so they are needed only to construct T i ).', 'This speedup also works for cyclic graphs and for any V .', 'Write w jk as (p jk , v jk ), and let w 1 jk = (p 1 jk , v 1 jk ) denote the weight of the edge from j to k. 19 Then it can be shown that w 0n = (p 0n , j,k p 0j v 1 jk p kn ).', 'The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .']",0,"['The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .']"
CC1011,P02-1001,Parameter estimation for probabilistic finite-state transducers,rational series and their languages,"['Jean Berstel', 'Christophe Reutenauer']",introduction,"I. Rational Series.- 1 Semirings.- 2 Formal Series.- 3 The Topology of K""X"".- 4 Rational Series.- 5 Recognizable Series.- 6 The Fundamental Theorem.- Exercises for Chapter I.- Notes to Chapter I.- II. Minimization.- 1 Syntactic Ideals.- 2 Reduced Linear Representations.- 3 The Reduction Algorithm.- Exercises for Chapter II.- Notes to Chapter II.- III. Series and Languages.- 1 The Theorem of Kleene.- 2 Series and Rational Languages.- 3 Supports.- 4 Iteration.- 5 Complementation.- Exercises for Chapter III.- Notes to Chapter III.- IV. Rational Series in One Variable.- 1 Rational Functions.- 2 The Exponential Polynomial.- 3 A Theorem of Polya.- 4 A Theorem of Skolem, Mahler and Lech.- Notes to Chapter IV.- V. Changing the Semiring.- 1 Rational Series over a Principal Ring.- 2 Positive Rational Series.- 3 Fatou Extensions.- Exercises for Chapter V.- Notes to Chapter V.- VI. Decidability.- 1 Problems of Supports.- 2 Growth.- Exercises for Chapter VI.- Notes to Chapter VI.- VII. Noncommutative Polynomials.- 1 The Weak Algorithm.- 2 Continuant Polynomials.- 3 Inertia.- 4 Gauss's Lemma.- Exercises for Chapter VII.- Notes to Chapter VII.- VIII. Codes and Formal Series.- 1 Codes.- 2 Completeness.- 3 The Degree of a Code.- 4 Factorization.- Exercises for Chapter VIII.- Notes to Chapter VIII.- References.","4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .","['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .', 'A full proof is straightforward, as are proofs of (3)_(2), (2)_(1).']",5,"['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .']"
CC1012,P02-1001,Parameter estimation for probabilistic finite-state transducers,a systolic array algorithm for the algebraic path problem shortest paths matrix inversion,['G¨unter Rote'],,"It is shown how the Gauss-Jordan Elimination algorithm for the Algebraic Path Problem can be implemented on a hexagonal systolic array of a quadratic number of simple processors in linear time. Special instances of this general algorithm include parallelizations of the Warshall-Floyd Algorithm, which computes the shortest distances in a graph or the transitive closure of a relation, and of the Gauss-Jordan Elimination algorithm for computing the inverse of a real matrix.ZusammenfassungEs wird dargestellt, wie man den gaus-Jordanschen Eliminationsalgorithmus fur das algebraische Wegproblem auf einem hexagonalen systolischen Feld (systolic array) mit einer quadratischen Anzahl einfacher Prozessoren in linearer Zeit ausfuhren kann. Zu den Anwendungsbeispielen dieses allgemeinen Algorithmus gehort der Warshall-Floyd-Algorithmus zur Berechnung der kurzesten Wegen in einem Graphen oder zur Bestimmung der transitiven Hulle einer Relation sowie der Gauss-Jordansche Eliminationsalgorithmus zur Inversion reeller Matrizen.",Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .,"['Division and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ).', 'Division is commonly used in defining f θ (for normalization). 19', 'Multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .']",3,"['Division and subtraction are also possible: -(p, v) = (-p, -v) and (p, v) -1 = (p -1 , -p -1 vp -1 ).', 'Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .']"
CC1013,P02-1001,Parameter estimation for probabilistic finite-state transducers,expectation semirings flexible em for finitestate transducers,['Jason Eisner'],,,"Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .","['13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a ∈ Σ; their weights are normalized to sum to 1.', 'Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a.', 'Then the predicted vector given θ is j,a dj,a • (expected feature counts on a randomly chosen arc in Dj,a).', 'Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .', 'The difficult case is global conditional normalization.', 'It arises, for example, when training a joint model of the form']",1,"['13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a  S; their weights are normalized to sum to 1.', 'Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .', 'The difficult case is global conditional normalization.']"
CC1014,P02-1001,Parameter estimation for probabilistic finite-state transducers,maximum likelihood from incomplete data via the em algorithm,"['A P Dempster', 'N M Laird', 'D B Rubin']",,,The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .,"['The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .', 'Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f θ , which FST paths stand a chance of having been the path used?', '(Guessing the path also guesses the exact input and output.)', 'The M step updates θ to make those paths more likely.', 'EM alternates these steps and converges to a local optimum.', ""The M step's form depends on the parameterization and the E step serves the M step's needs.""]",5,"['The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .', 'Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f th , which FST paths stand a chance of having been the path used?', '(Guessing the path also guesses the exact input and output.)', 'The M step updates th to make those paths more likely.', 'EM alternates these steps and converges to a local optimum.', ""The M step's form depends on the parameterization and the E step serves the M step's needs.""]"
CC1015,P02-1001,Parameter estimation for probabilistic finite-state transducers,compilation of weighted finitestate transducers from decision trees,"['Richard Sproat', 'Michael Riley']",introduction,"We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996).","For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .","['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']",0,"['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']"
CC1016,P02-1001,Parameter estimation for probabilistic finite-state transducers,algebraic structures for transitive closure,['D J Lehmann'],,"AbstractClosed semi-rings and the closure of matrices over closed semi-rings are defined and studied. Closed semi-rings are structures weaker than the structures studied by Conway [3] and Aho, Hopcroft and Ullman [1]. Examples of closed semi-rings and closure operations are given, including the case of semi-rings on which the closure of an element is not always defined. Two algorithms are proved to compute the closure of a matrix over any closed semi-ring; the first one based on Gauss-Jordan elimination is a generalization of algorithms by Warshall, Floyd and Kleene; the second one based on Gauss elimination has been studied by Tarjan [11, 12], from the complexity point of view in a slightly different framework. Simple semi-rings, where the closure operation for elements is trivial, are defined and it is shown that the closure of an n x n-matrix over a simple semi-ring is the sum of its powers of degree less than n. Dijkstra semi-rings are defined and it is shown that the rows of the closure of a matrix over a Dijkstra semi-ring, can be computed by a generalized version of Dijkstra's algorithm","Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).","['Now for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', 'It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.', 'If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph Ti, Tarjan (1981b) shows how to partition into �hard� subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.', 'The overhead of partitioning and recombining is essentially only O(m).']",0,"['Now for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', 'It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.', 'If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph Ti, Tarjan (1981b) shows how to partition into hard subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.']"
CC1017,P02-1001,Parameter estimation for probabilistic finite-state transducers,conditional random fields probabilistic models for segmenting and labeling sequence data,"['J Lafferty', 'A McCallum', 'F Pereira']",introduction,"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.","Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .","['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .']",0,"['Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .']"
CC1018,P02-1001,Parameter estimation for probabilistic finite-state transducers,a gaussian prior for smoothing maximum entropy models,"['Stanley F Chen', 'Ronald Rosenfeld']",,"Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.","In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .","['Maximum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .']",5,"['Maximum-posterior estimation tries to maximize P (th) * i f th (x i , y i ) where P (th) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .']"
CC1019,P02-1001,Parameter estimation for probabilistic finite-state transducers,hidden markov models with finite state supervision in,['E Ristad'],,"In this chapter we provide a supervised training paradigm for hidden Markov models (HMMs). Unlike popular ad-hoc approaches, our paradigm is completely general, need not make any simplifying assumptions about independence, and can take better advantage of the information contained in the training corpus.","For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11","['As training data we are given a set of observed (input, output) pairs, (xi, yi).', 'These are assumed to be independent random samples from a joint dis- tribution of the form f_�(x, y); the goal is to recover the true _�.', 'Samples need not be fully observed (partly supervised training): thus xi _ __, yi _ �_ may be given as regular sets in which input and output were observed to fall.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11']",0,"['For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11']"
CC1020,P02-1001,Parameter estimation for probabilistic finite-state transducers,speech recognition by composition of weighted finite automata,"['Fernando C N Pereira', 'Michael Riley']",introduction,"We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.","Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) .","['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']",0,"[""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC1021,P02-1001,Parameter estimation for probabilistic finite-state transducers,generic epsilonremoval and input epsilonnormalization algorithms for weighted transducers,['M Mohri'],,,"It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .","['• Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a).', 'Let T i = x i •f •y i .', 'Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted).', 'It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', 'If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph T i , Tarjan (1981b) shows how to partition into ""hard"" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.', 'The overhead of partitioning and recombining is essentially only O(m).']",0,"['* Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a).', 'It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', 'If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).']"
CC1022,P02-1001,Parameter estimation for probabilistic finite-state transducers,conditional random fields probabilistic models for segmenting and labeling sequence data,"['J Lafferty', 'A McCallum', 'F Pereira']",introduction,"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.","Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .","['• An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .', 'Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since ""dead ends"" leak probability mass). 8', ' A better-founded approach is global normalization, which simply divides each f (x, y) by']",0,"['* An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .']"
CC1023,P02-1001,Parameter estimation for probabilistic finite-state transducers,expectation semirings flexible em for finitestate transducers,['Jason Eisner'],introduction,,"A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .","['A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .', 'A leisurely journal-length version with more details has been prepared and is available.']",2,"['A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .']"
CC1024,P02-1001,Parameter estimation for probabilistic finite-state transducers,regular approximation of contextfree grammars through transformation,"['M Mohri', 'M-J Nederhof']",introduction,We present an algorithm for approximating context-free languages with regular languages. The algorithm is based on a simple transformation that applies to any context-free grammar and guarantees that the result can be compiled into a finite automaton. The resulting grammar contains at most one new nonterminal for any nonterminal symbol of the input grammar. The result thus remains readable and if necessary modifiable. We extend the approximation algorithm to the case of weighted context-free grammars. We also report experiments with several grammars showing that the size of the minimal deterministic automata accepting the resulting approximations is of practical use for applications such as speech recognition.,"A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .","['w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7', 'here are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.""]",0,"['A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .']"
CC1025,P02-1001,Parameter estimation for probabilistic finite-state transducers,an efficient compiler for weighted rewrite rules,"['Mehryar Mohri', 'Richard Sproat']",introduction,"Context-dependent rewrite rules are used in many areas of natural language and speech processing. Work in computational phonology has demonstrated that, given certain conditions, such rewrite rules can be represented as finite-state transducers (FSTs). We describe a new algorithm for compiling rewrite rules into FSTs. We show the algorithm to be simpler and more efficient than existing algorithms. Further, many of our applications demand the ability to compile weighted rules into weighted FSTs, transducers generalized by providing transitions with weights. We have extended the algorithm to allow for this.","For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .","['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']",0,"['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']"
CC1026,P02-1001,Parameter estimation for probabilistic finite-state transducers,maximum entropy markov models for information extraction and segmentation,"['A McCallum', 'D Freitag', 'F Pereira']",introduction,"Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ's.","Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .","['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .']",0,"['Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .']"
CC1027,P02-1001,Parameter estimation for probabilistic finite-state transducers,practical experiments with regular approximation of contextfree languages,['Mark-Jan Nederhof'],introduction,,"A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .","['w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7', 'here are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.""]",0,"['A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .']"
CC1028,P02-1001,Parameter estimation for probabilistic finite-state transducers,an inequality and associated maximization technique in statistical estimation of probabilistic functions of a markov process,['L E Baum'],introduction,,"For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .","['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .']",0,"['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .']"
CC1029,P02-1001,Parameter estimation for probabilistic finite-state transducers,expectation semirings flexible em for finitestate transducers,['Jason Eisner'],introduction,,"Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .","['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']",0,"['Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"
CC1030,P02-1001,Parameter estimation for probabilistic finite-state transducers,inducing features of random fields,"['S Della Pietra', 'V Della Pietra', 'J Lafferty']",,"We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.","The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and","['• If arc probabilities (or even λ, ν, µ, ρ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f θ whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']",5,"['The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (S * ,  * ).']"
CC1031,P02-1001,Parameter estimation for probabilistic finite-state transducers,speech recognition by composition of weighted finite automata,"['Fernando C N Pereira', 'Michael Riley']",introduction,"We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.","A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .","['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ). 4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)⇒( 2), ( 2)⇒(1).']",0,"['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .']"
CC1032,P05-3005,Dynamically generating a protein entity dictionary using online resources,dr introducing refseq and locuslink curated human genome resources at the ncbi trends genet,"['Pruitt KD', 'Katz KS', 'H Sicotte', 'Maglott']",,,"The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and","['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].', 'The following provides a brief description of each of the database.']",5,"['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and']"
CC1033,P05-3005,Dynamically generating a protein entity dictionary using online resources,sgd saccharomyces genome database nucleic acids res,"['Cherry JM', 'C Adler', 'C Ball', 'Chervitz SA', 'Dwight SS', 'Hester ET', 'Y Jia', 'G Juvik', 'T Roe', 'M Schroeder']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1034,P05-3005,Dynamically generating a protein entity dictionary using online resources,boddy wj et al the mouse genome database mgd integrating biology with the genome nucleic acids res,"['Bult CJ', 'Blake JA', 'Richardson JE', 'Kadin JA', 'Eppig JT', 'Baldarelli RM', 'K Barsanti', 'M Baya', 'Beal JS']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1035,P05-3005,Dynamically generating a protein entity dictionary using online resources,al wormbase a multispecies resource for nematode biology and genomics nucleic acids res,"['Harris TW', 'N Chen', 'F Cunningham', 'M TelloRuiz', 'I Antoshechkin', 'C Bastiani', 'T Bieri', 'D Blasiar', 'K Bradnam', 'Chan J et']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1036,P05-3005,Dynamically generating a protein entity dictionary using online resources,the unified medical language system umls integrating biomedical terminology,['O Bodenreider'],,"The Unified Medical Language System (http://umlsks.nlm.nih.gov) is a repository of biomedical vocabularies developed by the US National Library of Medicine. The UMLS integrates over 2 million names for some 900,000 concepts from more than 60 families of biomedical vocabularies, as well as 12 million relations among these concepts. Vocabularies integrated in the UMLS Metathesaurus include the NCBI taxonomy, Gene Ontology, the Medical Subject Headings (MeSH), OMIM and the Digital Anatomist Symbolic Knowledge Base. UMLS concepts are not only inter-related, but may also be linked to external resources such as GenBank. In addition to data, the UMLS includes tools for customizing the Metathesaurus (MetamorphoSys), for generating lexical variants of concept names (lvg) and for extracting UMLS concepts from text (MetaMap). The UMLS knowledge sources are updated quarterly. All vocabularies are available at no fee for research purposes within an institution, but UMLS users are required to sign a license agreement. The UMLS knowledge sources are distributed on CD-ROM and by FTP.",The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .,"['The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .', 'It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network.', 'The META provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'The SPECIALIST lexicon contains syntactic information for many terms, component words, and English words, including verbs, which do not appear in the META.', 'The Semantic Network contains information about the types or categories (e.g., ""Disease or Syndrome"", ""Virus"") to which all META concepts have been assigned.']",0,['The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .']
CC1037,P05-3005,Dynamically generating a protein entity dictionary using online resources,mining the biomedical literature in the genomic era an overview,"['H Shatkay', 'R Feldman']",introduction,"The past decade has seen a tremendous growth in the amount of experimental and computational biomedical data, specifically in the areas of genomics and proteomics. This growth is accompanied by an accelerated increase in the number of biomedical publications discussing the findings. In the last few years, there has been a lot of interest within the scientific community in literature-mining tools to help sort through this abundance of literature and find the nuggets of information most relevant and useful for specific analysis tasks. This paper provides a road map to the various literature-mining methods, both in general and within bioinformatics. It surveys the disciplines involved in unstructured-text analysis, categorizes current work in biomedical literature mining with respect to these disciplines, and provides examples of text analysis methods applied towards meeting some of the current challenges in bioinformatics.","With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .","['With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.', 'Such task is called biological entity tagging.', 'Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases).']",0,"['With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.', 'Such task is called biological entity tagging.', 'Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases).']"
CC1038,P05-3005,Dynamically generating a protein entity dictionary using online resources,online mendelian inheritance in man omim a knowledgebase of human genes and genetic disorders nucleic acids res,"['A Hamosh', 'Scott AF', 'Amberger JS', 'Bocchini CA', 'McKusick VA']",,"Online Mendelian Inheritance in Man (OMIM) is a comprehensive, authoritative and timely knowledgebase of human genes and genetic disorders compiled to support human genetics research and education and the practice of clinical genetics. Started by Dr Victor A. McKusick as the definitive reference Mendelian Inheritance in Man, OMIM (http://www.ncbi.nlm.nih.gov/omim/) is now distributed electronically by the National Center for Biotechnology Information, where it is integrated with the Entrez suite of databases. Derived from the biomedical literature, OMIM is written and edited at Johns Hopkins University with input from scientists and physicians around the world. Each OMIM entry has a full-text summary of a genetically determined phenotype and/or gene and has numerous links to other genetic databases such as DNA and protein sequence, PubMed references, general and locus-specific mutation databases, HUGO nomenclature, MapViewer, GeneTests, patient support groups and many others. OMIM is an easy and straightforward portal to the burgeoning information in human genetics.","Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1039,P05-3005,Dynamically generating a protein entity dictionary using online resources,godzik a clustering of highly homologous sequences to reduce the size of large protein databases bioinformatics,"['W Li', 'L Jaroszewski']",,"We present a fast and flexible program for clustering large protein databases at different sequence identity levels. It takes less than 2 h for the all-against-all sequence comparison and clustering of the non-redundant protein database of over 560,000 sequences on a high-end PC. The output database, including only the representative sequences, can be used for more efficient and sensitive database searches.","Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .","['PIR Resources -There are three databases in PIR: the Protein Sequence Database (PSD), iProClass, and PIR-NREF.', 'PSD database includes functionally annotated protein sequences.', 'The iProClass database is a central point for exploration of protein information, which provides summary descriptions of protein family, function and structure for all protein sequences from PIR, Swiss-Prot, and TrEMBL (now UniProt).', 'Additionally, it links to over 70 biological databases in the world.', 'The PIR-NREF database is a comprehensive database for sequence searching and protein identification.', 'It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB.', 'Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .', 'NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE.', 'GenPept entries are those translated from the GenBanknucleotide sequence database.', 'RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms.', ""Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI's Map Viewer."", 'It records gene names, symbols, and many other attributes associated with genes and the products they encode.']",5,"['Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .']"
CC1040,P05-3005,Dynamically generating a protein entity dictionary using online resources,kwitek a et al rat genome database rgd mapping disease onto the genome nucleic acids res,"['S Twigger', 'J Lu', 'M Shimoyama', 'D Chen', 'D Pasko', 'H Long', 'J Ginster', 'Chen CF', 'R Nigam']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1041,P05-3005,Dynamically generating a protein entity dictionary using online resources,suzek be et al the protein information resource nucleic acids res,"['Wu CH', 'Yeh LS', 'H Huang', 'L Arminski', 'J Castro-Alvear', 'Y Chen', 'Z Hu', 'P Kourtesis', 'Ledley RS']",,,"The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and","['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].', 'The following provides a brief description of each of the database.']",5,"['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and']"
CC1042,P05-3005,Dynamically generating a protein entity dictionary using online resources,the flybase database of the drosophila genome projects and community literature nucleic acids res,['F Consortium'],,"FlyBase (http://flybase.bio.indiana.edu/) provides an integrated view of the fundamental genomic and genetic data on the major genetic model Drosophila melanogaster and related species. FlyBase has primary responsibility for the continual reannotation of the D. melanogaster genome. The ultimate goal of the reannotation effort is to decorate the euchromatic sequence of the genome with as much biological information as is available from the community and from the major genome project centers. A complete revision of the annotations of the now-finished euchromatic genomic sequence has been completed. There are many points of entry to the genome within FlyBase, most notably through maps, gene products and ontologies, structured phenotypic and gene expression data, and anatomy.","Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1043,P05-3005,Dynamically generating a protein entity dictionary using online resources,enzyme nomenclature functional or structural rna,['P Gegenheimer'],,"Altman and colleagues (this issue) call attention to the inability of current standardized enzyme nomenclature to distinguish between enzymatic activities that reside in nonhomologous macromolecules+ This issue is highlighted by the fact that the pre-tRNA 59-maturation activities of bacteria and plant chloroplasts present the first instance (of which I am aware) of two naturally occurring enzymes that cannot be evolutionarily related, but which catalyze an identical reaction+ (In the classic example of convergent evolution between the trypsin family and subtilisin, the enzymes do not have an identical substrate specificity+) Altman and colleagues propose that a single trivial name be used only for members of a family of homologous macromolecules; in other words, that different trivial names be given to enzymes that catalyze the same precursor-product conversion but do so with different catalytic mechanisms, or which are not members of a single family of homologous macromolecules+ I am not convinced that there is a problem needing solution+ The current proposal seems to run counter to the rationale behind current EC nomenclature, and could create more confusion than it would alleviate+ One can distinguish between a function-based nomenclature based on the biochemical reaction catalyzed--the substrate-product conversion--and a structure-based nomenclature based on the physical nature of the catalyst+ For a classical enzymologist, the reaction type being catalyzed is paramount: It is the reaction that one uses to purify the enzyme+ One identifies the enzyme based on its activity,whereas its physical structure may initially be of secondary importance+ The value of function-based nomenclature is precisely that it allows the biochemical reaction (the substrate- product conversion) to be described, specified, and studied concomitant with continuing purification and analysis of the corresponding enzyme+ Further, as more is learned about the enzyme's structure and catalytic mechanism, it is not necessary to rename it+ Indeed, the utility of function-based nomenclature is exemplified by the history of bacterial RNase P purification and characterization+","Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .']"
CC1044,P07-1007,Estimating class priors in domain adaptation for word sense disambiguation,an empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation,"['Y K Lee', 'H T Ng']",,"In this paper, we evaluate a variety  of knowledge sources and supervised  learning algorithms for word sense  disambiguation on SENSEVAL-2 and  SENSEVAL-1 data. Our knowledge  sources include the part-of-speech of  neighboring words, single words in the  surrounding context, local collocations,  and syntactic relations. The learning algorithms  evaluated include Support Vector  Machines (SVM), Naive Bayes, AdaBoost,  and decision tree algorithms. We  present empirical results showing the relative  contribution of the component knowledge  sources and the different learning  algorithms. In particular, using all of  these knowledge sources and SVM (i.e.,  a single learning algorithm) achieves accuracy  higher than the best official scores  on both SENSEVAL-2 and SENSEVAL-1  test data",These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .,"['For our experiments, we use naive Bayes as the learning algorithm.', 'The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .']",2,['These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .']
CC1045,P07-1068,Advanced Machine Learning Models for Coreference Resolution,the nonutility of predicateargument frequencies for pronoun interpretation,"['A Kehler', 'D Appelt', 'L Taylor', 'A Simma']",introduction,,"While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']",0,"['While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .']"
CC1046,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['X Luo', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'S Roukos']",introduction,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,"More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.","['Many ACE participants have also adopted a corpus-based approach to SC deter- mination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)).', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Un- like them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowl- edge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC clas- sifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and an- notated with their SCs.', 'This provides us with a train- ing set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']",1,"['More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']"
CC1047,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a corpusbased evaluation of centering and pronoun resolution,['J Tetreault'],introduction,,"In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.']"
CC1048,P07-1068,Advanced Machine Learning Models for Coreference Resolution,bbn pronoun coreference and entity type corpus linguistica data consortium,"['R Weischedel', 'A Brunstein']",introduction,,"Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.","['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)).', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']",5,"['Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.']"
CC1049,P07-1068,Advanced Machine Learning Models for Coreference Resolution,anaphora resolution,['R Mitkov'],introduction,"In anaphora resolution for English, animacy identification can play an integral role in the application of agreement restrictions between pronouns and candidates, and as a result, can improve the accuracy of anaphora resolution systems. In this paper, two methods for animacy identification are proposed and evaluated using intrinsic and extrinsic measures. The first method is a rule-based one which uses information about the unique beginners in WordNet to classify NPs on the basis of their animacy. The second method relies on a machine learning algorithm which exploits a WordNet enriched with animacy information for each sense. The effect of word sense disambiguation on the two methods is also assessed. The intrinsic evaluation reveals that the machine learning method reaches human levels of performance. The extrinsic evaluation demonstrates that animacy identification can be beneficial in anaphora resolution, especially in the cases where animate entities are identified with high precision","While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']",0,"['While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .']"
CC1050,P07-1068,Advanced Machine Learning Models for Coreference Resolution,libsvm a library for support vector machines software available at httpwwwcsientuedutw∼cjlinlibsvm,"['C-C Chang', 'C-J Lin']",introduction,,"Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.","['In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.�s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.', 'Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.', 'More specifically, let L = i li\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).', 'To represent i, we generate one feature from each non-empty subset of Li.']",5,"['In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.', 'Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.', 'More specifically, let L = i li\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).', 'To represent i, we generate one feature from each non-empty subset of Li.']"
CC1051,P07-1068,Advanced Machine Learning Models for Coreference Resolution,an algorithm that learns what’s in a name,"['D M Bikel', 'R Schwartz', 'R M Weischedel']",introduction,"In this paper, we present IdentiFinderTM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder's performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.","( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ .","[""( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", 'If NPi is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type.', 'However, if NPi is determined to be a LOCATION, we create a feature with value GPE (because most of the MUC LOCA- TION NEs are ACE GPE NEs).', 'Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types).']",5,"[""( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", 'If NPi is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type.']"
CC1052,P07-1068,Advanced Machine Learning Models for Coreference Resolution,comparing knowledge sources for nominal anaphora resolution,"['K Markert', 'M Nissim']",introduction,"We compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphora  and definite noun phrase coreference. Specifically, we compare an algorithm that relies on links  encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora  by means of shallow lexico-semantic patterns. As corpora we use the British National  Corpus (BNC), as well as the Web, which has not been previously used for this task. Our  results show that (a) the knowledge encoded in WordNet is often insufficient, especially for  anaphor-antecedent relations that exploit subjective or context-dependent knowledge; (b) for  other-anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite  NP coreference, the Web-based method yields results comparable to those obtained using  WordNet over the whole dataset and outperforms the WordNet-based method on subsets of the  dataset; (d) in both case studies, the BNC-based method is worse than the other methods because  of data sparseness. Thus, in our studies, the Web-based method alleviated the lexical knowledge  gap often encountered in anaphora resolution, and handled examples with context-dependent relations  between anaphor and antecedent. Because it is inexpensive and needs no hand-modelling  of lexical knowledge, it is a promising knowledge source to integrate in anaphora resolution systems","However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .","['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]",0,"['However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .']"
CC1053,P07-1068,Advanced Machine Learning Models for Coreference Resolution,unsupervised models for named entity classification,"['M Collins', 'Y Singer']",introduction,"This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of la-beled examples should be required to train a classi-fier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple ""seed "" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).","We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .","['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']",5,"['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .']"
CC1054,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a modeltheoretic coreference scoring scheme,"['M Vilain', 'J Burger', 'J Aberdeen', 'D Connolly', 'L Hirschman']",,,"We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .","['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition.', 'In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.']",5,"['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .']"
CC1055,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'D Lim']",,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.","['After training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj.']",4,"['After training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj.']"
CC1056,P07-1068,Advanced Machine Learning Models for Coreference Resolution,factorizing complex models a case study in mention detection,"['R Florian', 'H Jing', 'N Kambhatla', 'I Zitouni']",introduction,"As natural language understanding research advances towards deeper knowledge modeling, the tasks become more and more complex: we are interested in more nuanced word characteristics, more linguistic properties, deeper semantic and syntactic features. One such example, explored in this article, is the mention detection and recognition task in the Automatic Content Extraction project, with the goal of identifying named, nominal or pronominal references to real-world entities---mentions---and labeling them with three types of information: entity type, entity subtype and mention type. In this article, we investigate three methods of assigning these related tags and compare them on several data sets. A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation.","Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .","['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']",1,"['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .']"
CC1057,P07-1068,Advanced Machine Learning Models for Coreference Resolution,coreference resolution using competitive learning approach,"['X Yang', 'G Zhou', 'J Su', 'C L Tan']",,"In this paper we propose a competition learning approach to coreference resolution. Traditionally, supervised machine learning approaches adopt the single-candidate model. Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. By contrast, our approach adopts a twin-candidate learning model. Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model.","Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .","['Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .']",1,"['Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .', 'Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.']"
CC1058,P07-1068,Advanced Machine Learning Models for Coreference Resolution,c45 programs for machine learning,['J R Quinlan'],,,Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .,"['Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , . .', '., NP j−1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']",5,['Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .']
CC1059,P07-1068,Advanced Machine Learning Models for Coreference Resolution,improving machine learning approaches to coreference resolution,"['V Ng', 'C Cardie']",,"We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.","Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.","['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj.', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.']",1,"['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.']"
CC1060,P07-1068,Advanced Machine Learning Models for Coreference Resolution,exploiting semantic role labeling wordnet and wikipedia for coreference resolution,"['S P Ponzetto', 'M Strube']",,"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.","Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .","['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al., 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .', 'In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.']",5,"['Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .']"
CC1061,P07-1068,Advanced Machine Learning Models for Coreference Resolution,unsupervised learning of contextual role knowledge for coreference resolution,"['D Bean', 'E Riloff']",introduction,"We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.","As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .']"
CC1062,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'D Lim']",introduction,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .","['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]",0,"['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]"
CC1063,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'D Lim']",,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .","['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', '., NP j−1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']",5,"['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .']"
CC1064,P07-1068,Advanced Machine Learning Models for Coreference Resolution,unsupervised word sense disambiguation rivaling supervised methods,['D Yarowsky'],introduction,"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%","We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .","['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']",4,"['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .']"
CC1065,P07-1068,Advanced Machine Learning Models for Coreference Resolution,exploiting semantic role labeling wordnet and wikipedia for coreference resolution,"['S P Ponzetto', 'M Strube']",introduction,"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.","As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']"
CC1066,P07-1068,Advanced Machine Learning Models for Coreference Resolution,using semantic relations to refine coreference decisions,"['H Ji', 'D Westbrook', 'R Grishman']",introduction,We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses. Experiments show that this new framework can improve performance on two quite different languages - English and Chinese.,"As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']"
CC1067,P07-1068,Advanced Machine Learning Models for Coreference Resolution,automatic retrieval and clustering of similar words,['D Lin'],introduction,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .,"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs. 2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)).', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]",4,['( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .']
CC1068,P07-1068,Advanced Machine Learning Models for Coreference Resolution,automatic acquisition of hyponyms from large text corpora,['M Hearst'],introduction,"We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction  Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects..","These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .","['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]",4,"['These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.']"
CC1069,P10-2059,Classification of Feedback Expressions in Multimodal Data,contextual recognition of head gestures,"['Louis-Philippe Morency', 'Candace Sidner', 'Christopher Lee', 'Trevor Darrell']",introduction,"Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people. We investigate how dialog context from an embodied conversational agent (ECA) can improve visual recognition of user gestures. We present a recogntion framework which (1) extracts contextual features from an ECA's dialog manager, (2) computes a predicition of head nod and head shakes, and (3) integrates the contextual predictions with the visual observation of a vision-based head gesture recognizer. We found a subset of lexical, punctuation and timing features that are easily available in most ECA architectures and can be used to learn how to predict user feedback. Using a discriminative approach to contextual prediction and multi-modal integration, we were able to improve the performancae of head gesture detection even when the topic of the test set was significantly different than the training set","Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .']"
CC1070,P10-2059,Classification of Feedback Expressions in Multimodal Data,the mumin coding scheme for the annotation of feedback turn management and sequencing multimodal corpora for modelling human multimodal behaviour,"['Jens Allwood', 'Loredana Cerrato', 'Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']",,,All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .,"['All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .', 'The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication.', 'In this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'Therefore, only a subset of the MUMIN attributes has been used, i.e.', 'Smile, Laughter, Scowl, FaceOther for facial expressions, and Nod, Jerk, Tilt, SideTurn, Shake, Waggle, Other for head movements.']",5,"['All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .', 'The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication.']"
CC1071,P10-2059,Classification of Feedback Expressions in Multimodal Data,turnyielding cues in taskoriented dialogue,"['Agustin Gravano', 'Julia Hirschberg']",introduction,,"Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.']"
CC1072,P10-2059,Classification of Feedback Expressions in Multimodal Data,a conversation robot using head gesture recognition as paralinguistic information,"['Shinya Fujie', 'Y Ejiri', 'K Nakajima', 'Y Matsusaka', 'Tetsunor Kobayashi']",introduction,"A conversation robot that recognizes user's head gestures and uses its results as para-linguistic information is developed. In the conversation, humans exchange linguistic information, which can be obtained by transcription of the utterance, and para-linguistic information, which helps the transmission of linguistic information. Para-linguistic information brings a nuance that cannot be transmitted by linguistic information, and the natural and effective conversation is realized. We recognize user's head gestures as the para-linguistic information in the visual channel. We use the optical flow over the head region as the feature and model them using HMM for the recognition. In actual conversation, while the user performs a gesture, the robot may perform a gesture, too. In this situation, the image sequence captured by the camera mounted on the eyes of the robot includes sways caused by the movement of the camera. To solve this problem, we introduced two artifices. One is for the feature extraction: the optical flow of the body area is used to compensate the swayed images. The other is for the probability models: mode-dependent models are prepared by the MLLR model adaptation technique, and the models are switched according to the motion mode of the robot. Experimental results show the effectiveness of these techniques.","Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .']"
CC1073,P10-2059,Classification of Feedback Expressions in Multimodal Data,linguistic functions of head movements in the context of speech,['Evelyn McClave'],introduction,,Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']",0,['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .']
CC1074,P10-2059,Classification of Feedback Expressions in Multimodal Data,coefficient kappa some uses misuses and alternatives,"['Robert L Brennan', 'Dale J Prediger']",introduction,"This paper considers some appropriate and inappropriate uses of coefficient kappa and alternative kappa-like statistics. Discussion is restricted to the descriptive characteristics of these statistics for measuring agreement with categorical data in studies of reliability and validity. Special consideration is given to assumptions about whether marginals are fixed a priori, or free to vary. In reliability studies, when marginals are fixed, coefficient kappa is found to be appropriate. When either or both of the marginals are free to vary, however, it is suggested that the ""chance"" term in kappa be replaced by 1/n, where n is the number of categories. In validity studies, we suggest considering whether one wants an index of improvement beyond ""chance"" or beyond the best a priori strategy employing base rates. In the former case, considerations are similar to those in reliability studies with the marginals for the criterion measure considered as fixed. In the latter case, it is suggested that the largest marginal proportion for the criterion measure be used in place of the ""chance"" term in kappa. Similarities and differences among these statistics are discussed and illustrated with synthetic data.","Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 .","['In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.', 'A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 ."", 'Anvil divides the annotations in slices and compares each slice.', 'We used slices of 0.04 seconds.', 'The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.']",5,"[""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 .""]"
CC1075,P10-2059,Classification of Feedback Expressions in Multimodal Data,the hcrc map task corpus language and speech,"['Anne H Anderson', 'Miles Bader', 'Ellen Gurman Bard', 'Elizabeth Boyle', 'Gwyneth Doherty', 'Simon Garrod', 'Stephen Isard', 'Jacqueline Kowtko', 'Jan McAllister', 'Jim Miller', 'Catherine Sotillo', 'Henry S Thompson', 'Regina Weinert']",introduction,,"Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .']"
CC1076,P10-2059,Classification of Feedback Expressions in Multimodal Data,data mining practical machine learning tools and techniques,"['Ian H Witten', 'Eibe Frank']",,"As with any burgeoning technology that enjoys commercial attention, the use of data mining is surrounded by a great deal of hype. Exaggerated reports tell of secrets that can be uncovered by setting algorithms loose on oceans of data. But there is no magic in machine learning, no hidden power, no alchemy. Instead there is an identifiable body of practical techniques that can extract useful information from raw data. This book describes these techniques and shows how they work. The book is a major revision of the first edition that appeared in 1999. While the basic core remains the sam","These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .","['The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005).', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']",5,"['These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005).', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']"
CC1077,P10-2059,Classification of Feedback Expressions in Multimodal Data,combining lexical syntactic and prosodic cues for improved online dialog act tagging,"['Vivek Kumar Rangarajan Sridhar', 'Srinivas Bangaloreb', 'Shrikanth Narayanan']",introduction,,"#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .']"
CC1078,P10-2059,Classification of Feedback Expressions in Multimodal Data,a coefficient of agreement for nominal scales,['Jacob Cohen'],introduction,"CONSIDER Table 1. It represents in its formal characteristics a situation which arises in the clinical-social-personality areas of psychology, where it frequently occurs that the only useful level of measurement obtainable is nominal scaling (Stevens, 1951, pp. 2526), i.e. placement in a set of k unordered categories. Because the categorizing of the units is a consequence of some complex judgment process performed by a &dquo;two-legged meter&dquo; (Stevens, 1958), it becomes important to determine the extent to which these judgments are reproducible, i.e., reliable. The procedure which suggests itself is that of having two (or more) judges independently categorize a sample of units and determine the degree, significance, and","Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 .","['In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.', 'A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 ."", 'Anvil divides the annotations in slices and compares each slice.', 'We used slices of 0.04 seconds.', 'The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.']",5,"[""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 .""]"
CC1079,P10-2059,Classification of Feedback Expressions in Multimodal Data,head gestures for perceptual interfaces the role of context in improving recognition,"['Louis-Philippe Morency', 'Candace Sidner', 'Christopher Lee', 'Trevor Darrell']",introduction,"AbstractHead pose and gesture offer several conversational grounding cues and are used extensively in face-to-face interaction among people. To accurately recognize visual feedback, humans often use contextual knowledge from previous and current events to anticipate when feedback is most likely to occur. In this paper we describe how contextual information can be used to predict visual feedback and improve recognition of head gestures in human-computer interfaces. Lexical, prosodic, timing, and gesture features can be used to predict a user's visual feedback during conversational dialog with a robotic or virtual agent. In non-conversational interfaces, context features based on user-interface system events can improve detection of head gestures for dialog box confirmation or document browsing. Our user study with prototype gesture-based components indicate quantitative and qualitative benefits of gesture-based confirmation over conventional alternatives. Using a discriminative approach to contextual prediction and multi-modal integration, performance of head gesture detection was improved with context features even when the topic of the test set was significantly different than the training set","Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .']"
CC1080,P10-2059,Classification of Feedback Expressions in Multimodal Data,intercoder agreement for computational linguistics,"['Ron Artstein', 'Massimo Poesio']",introduction,,"Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971).","['It should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element.']",0,"[""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971).""]"
CC1081,P10-2059,Classification of Feedback Expressions in Multimodal Data,detecting action meetings in meetings,"['Gabriel Murray', 'Steve Renals']",introduction,,Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .']",0,['Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .']
CC1082,P10-2059,Classification of Feedback Expressions in Multimodal Data,clustering experiments on the communicative prop erties of gaze and gestures,"['Kristiina Jokinen', 'Anton Ragni']",introduction,,"For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .","['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']",0,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .']"
CC1083,P10-2059,Classification of Feedback Expressions in Multimodal Data,distinguishing the communicative functions of gestures,"['Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']",introduction,"This paper deals with the results of a machine learning experiment conducted on annotated gesture data from two case studies (Danish and Estonian). The data concern mainly facial displays, that are annotated with attributes relating to shape and dynamics, as well as communicative function. The results of the experiments show that the granularity of the attributes used seems appropriate for the task of distinguishing the desired communicative functions. This is a promising result in view of a future automation of the annotation task.","For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .","['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi- modal corpus.']",0,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .']"
CC1084,P10-2059,Classification of Feedback Expressions in Multimodal Data,measuring nominal scale agreement among many raters,['Joseph L Fleiss'],introduction,,"Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) .","['It should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) ."", 'Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element.']",0,"[""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) .""]"
CC1085,P10-2059,Classification of Feedback Expressions in Multimodal Data,distinguishing the communicative functions of gestures,"['Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']",,"This paper deals with the results of a machine learning experiment conducted on annotated gesture data from two case studies (Danish and Estonian). The data concern mainly facial displays, that are annotated with attributes relating to shape and dynamics, as well as communicative function. The results of the experiments show that the granularity of the attributes used seems appropriate for the task of distinguishing the desired communicative functions. This is a promising result in view of a future automation of the annotation task.","These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.","['The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions.', 'In the case of gestures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.']",1,"['The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions.', 'In the case of gestures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.']"
CC1086,P10-2059,Classification of Feedback Expressions in Multimodal Data,feedback in head gesture and speech to appear in,"['Patrizia Paggio', 'Costanza Navarretta']",introduction,,"The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",1,"['The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .']"
CC1087,P10-2059,Classification of Feedback Expressions in Multimodal Data,hidden naive bayes,"['Harry Zhang', 'Liangxiao Jiang', 'Jiang Su']",,,The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .,"['The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (Witten and Frank, 2005).', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']",5,['The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .']
CC1088,P10-2059,Classification of Feedback Expressions in Multimodal Data,gesture generation by imitation  from human behavior to computer character animation,['Michael Kipp'],introduction,"In an e ort to extend traditional human-computer interfaces research has introduced embodied agents to utilize the modalities of everyday human-human communication, like facial expression, gestures and body postures. However, giving computer agents a human-like body introduces new challenges. Since human users are very sensitive and critical concerning bodily behavior the agents must act naturally and individually in order to be believable. This dissertation focuses on conversational gestures. It shows how to generate conversational gestures for an animated embodied agent based on annotated text input. The central idea is to imitate the gestural behavior of a human individual. Using TV show recordings as empirical data, gestural key parameters are extracted for the generation of natural and individual gestures. The gesture generation task is solved in three stages: observation, modeling and generation. For each stage, a software module was developed. For observation, the video annotation research tool ANVIL was created. It allows the eAEcient transcription of gesture, speech and other modalities on multiple layers. ANVIL is application-independent by allowing users to de ne their own annotation schemes, it provides various import/export facilities and it is extensible via its plugin interface. Therefore, the tool is suitable for a wide variety of research elds. For this work, selected clips of the TV talk show  Das Literarische Quartett"" were transcribed and analyzed, arriving at a total of 1,056 gestures. For the modeling stage, the NOVALIS module was created to compute individual gesture pro les from these transcriptions with statistical methods. A gesture pro le models the aspects handedness, timing and function of gestures for a single human individual using estimated conditional probabilities. The pro les are based on a shared lexicon of 68 gestures, assembled from the data. Finally, for generation, the NOVA generator was devised to create gestures based on gesture pro les in an overgenerate-andlter approach. Annotated text input is processed in a graph-based representation in multiple stages where semantic data is added, the location of potential gestures is determined by heuristic rules, and gestures are added and ltered based on a gesture pro le. NOVA outputs a linear, player-independent action script in XML.",Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .,"['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.', 'Finally, the two turn management categories Turn-Take and TurnElicit were also coded.']",5,"['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.']"
CC1089,P10-2059,Classification of Feedback Expressions in Multimodal Data,praat doing phonetics by computer retrieved,"['Paul Boersma', 'David Weenink']",introduction,,The Praat tool was used ( #AUTHOR_TAG ) .,"['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']",5,['The Praat tool was used ( #AUTHOR_TAG ) .']
CC1090,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,linking semantic and knowledge representations in a multidomain dialogue system,"['Myroslava O Dzikovska', 'James F Allen', 'Mary D Swift']",experiments,"We describe a two-layer architecture for supporting semantic interpretation and domain reasoning in dialogue systems. Building system that supports both semantic interpretation and domain reasoning in a transparent and well-integrated manner is an unresolved problem because of the diverging requirements of the semantic representations used in contextual interpretation versus the knowledge representations used in domain reasoning. We propose an architecture that provides both portability and efficiency in natural language interpretation by maintaining separate semantic and domain knowledge representations, and integrating them via an ontology mapping procedure. The ontology mapping is used to obtain representations of utterances in a form most suitable for domain reasoners and to automatically specialize the lexicon. The use of a linguistically motivated parser for producing semantic representations for complex natural language sentences facilitates building portable semantic interpretation components as well as connections with domain reasoners. Two evaluations demonstrate the effectiveness of our approach: we show that a small number of mapping rules are sufficient for customizing the generic semantic representation to a new domain, and that our automatic lexicon specialization technique improves parser speed and accuracy.","The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .","['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."", 'At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008).']",2,"[""The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .""]"
CC1091,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,developing pedagogically effective tutorial dialogue tactics experiments and a testbed,"['Kurt VanLehn', 'Pamela Jordan', 'Diane Litman']",introduction,"Although effective tutorial dialogue strategies are well understood, tutorial tactics that govern brief episodes of tutoring, such as a single step, are not. Because better tactics seem to be crucial for further improving pedagogical effectiveness, we have begun investigating the effects of varying tutorial tactics. In this paper we describe two planned experiments and the testbed we have created to support this experimentation.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1092,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,controlling content realization with functional unification grammars in,"['Michael Elhadad', 'Jacques Robin']",experiments,"Standard Functional Unification Grammars (FUGs) provide a structurally guided top-down control regime for sentence generation. When using FUGs to perform content realization as a whole, including lexical choice, this regime is no longer appropriate for two reasons: (1) the unification of non-lexicalized semantic input with an integrated lexico-grammar requires mapping ""floating"" semantic elements which can trigger extensive backtracking and (2) lexical choice requires accessing external constraint sources on demand to preserve the modularity between conceptual and linguistic knowledge.","The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .","[""The strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer (e.g., part of the answer to confirm), is passed to content planning and generation."", 'The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .', 'Templates are used to generate some stock phrases such as ""When you are ready, go on to the next slide.""']",5,"['The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .']"
CC1093,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,cohesion and learning in a tutorial spoken dialog system,"['Arthur Ward', 'Diane Litman']",conclusion,"Two measures of lexical cohesion were developed and applied to a corpus of human-computer tutoring dialogs. For both measures, the amount of cohesion in the tutoring dialog was found to be significantly correlated to learning for students with below-mean pretest scores, but not for those with above-mean pre-test scores, even though both groups had similar amounts of cohesion. We also find that only cohesion between tutor and student is significant: the cohesiveness of tutor, or of student, utterances is not. These results are discussed in light of previous work in textual cohesion and recall.",Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .,"['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']",3,['Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .']
CC1094,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,a natural language tutorial dialogue system for physics,"['Pamela Jordan', 'Maxim Makatchev', 'Umarani Pappuswamy', 'Kurt VanLehn', 'Patricia Albacete']",introduction,Abstract : We describe the WHY2-ATLAS intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue. We focus on the issue of analyzing and responding to multi-sentential explanations. We explore approaches for achieving a deeper understanding of these explanations and dialogue management approaches and strategies for providing appropriate feedback on them.,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1095,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,information state and dialogue management in the trindi dialogue move engine toolkit,"['Staffan Larsson', 'David Traum']",experiments,"We introduce an architecture and toolkit for building dialogue managers currently being developed in the TRINDI project, based on the notions of information state and dialogue move engine. The aim is to provide a framework for experimenting with implementations of different theories of information state, information state update and dialogue control. A number of dialogue managers are currently being built using the toolkit, and we present overviews of two of them. We believe that this framework will make implementation of dialogue processing theories easier, also facilitating comparison of different types of dialogue systems, thus helping to achieve a prerequisite for arriving at a best practice for the development of the dialogue management component of a spoken dialogue system.",Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .,"['Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .', 'The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer.', 'Once the complete answer has been accumulated, the system accepts it and moves on.', 'Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student.']",5,['Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .']
CC1096,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,contentlearning correlations in spoken tutoring dialogs at word turn and discourse levels,"['Amruta Purandare', 'Diane Litman']",introduction,"We study correlations between dialog content and learning in a corpus of human-computer tutoring dialogs. Using an online encyclopedia, we first extract domainspecific concepts discussed in our dialogs. We then extend previously studied shallow dialog metrics by incorporating content at three levels of granularity (word, turn and discourse) and also by distinguishing between students' spoken and written contributions. In all experiments, our content metrics show strong correlations with learning, and outperform the corresponding shallow baselines. Our word-level results show that although verbosity in student writings is highly associated with learning, verbosity in their spoken turns is not. On the other hand, we notice that content along with conciseness in spoken dialogs is strongly correlated with learning. At the turn-level, we find that effective tutoring dialogs have more content-rich turns, but not necessarily more or longer turns. Our discourse-level analysis computes the distribution of content across larger dialog units and shows high correlations when student contributions are rich but unevenly distributed across dialog segments. Copyright (c) 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .']"
CC1097,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,the impact of interpretation problems on tutorial dialogue,"['Myroslava O Dzikovska', 'Johanna D Moore', 'Natalie Steinhauser', 'Gwendolyn Campbell']",conclusion,"Supporting natural language input may improve learning in intelligent tutoring systems. However, interpretation errors are unavoidable and require an effective recovery policy. We describe an evaluation of an error recovery policy in the BEETLE II tutorial dialogue system and discuss how different types of interpretation problems affect learning gain and user satisfaction. In particular, the problems arising from student use of non-standard terminology appear to have negative consequences. We argue that existing strategies for dealing with terminology problems are insufficient and that improving such strategies is important in future ITS research.","The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) .","['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']",3,"['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']"
CC1098,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,learning to assess lowlevel conceptual understanding,"['Rodney D Nielsen', 'Wayne Ward', 'James H Martin']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1099,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,generalizing tutorial dialogue results,"['Diane Litman', 'Johanna Moore', 'Myroslava Dzikovska', 'Elaine Farrow']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1100,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,dealing with interpretation errors in tutorial dialogue,"['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow', 'Johanna D Moore', 'Natalie B Steinhauser', 'Gwendolyn C Campbell']",experiments,"We describe an approach to dealing with interpretation errors in a tutorial dialogue system. Allowing students to provide explanations and generate contentful talk can be helpful for learning, but the language that can be understood by a computer system is limited by the current technology. Techniques for dealing with understanding problems have been developed primarily for spoken dialogue systems in informationseeking domains, and are not always appropriate for tutorial dialogue. We present a classification of interpretation errors and our approach for dealing with them within an implemented tutorial dialogue system.","At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .","['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue.', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']",5,"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .']"
CC1101,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,resolving pronominal reference to abstract entities,['Donna K Byron'],experiments,"This paper describes PHORA, a technique for resolving pronominal reference to either individual or abstract entities. It defines processes for evoking abstract referents from discourse and for resolving both demonstrative and personal pronouns. It successfully interprets 72% of test pronouns, compared to 37% for a leading technique without these features.","The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output .","['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output ."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']",1,"[""The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output .""]"
CC1102,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,the beetle and beediff tutoring systems,"['Charles B Callaway', 'Myroslava Dzikovska', 'Elaine Farrow', 'Manuel Marques-Pita', 'Colin Matheson', 'Johanna D Moore']",introduction,"We describe two tutorial dialogue systems that adapt techniques from task-oriented dialogue systems to tutorial dialogue. Both systems employ the same reusable deep natural language understanding and generation components to interpret students' written utterances and to automatically generate adaptive tutorial responses, with separate domain reasoners to provide the necessary knowledge about the correctness of student answers and hinting strategies. We focus on integrating the domain-independent language processing components with domain-specific reasoning and tutorial components in order to improve the dialogue interaction, and present a preliminary analysis of BeeDiff's evaluation. Index Terms: tutoring systems, dialogue, deep processing",The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .,"['The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .', 'It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'This allows the system to consistently apply the same tutorial policy across a range of questions.', 'To some extent, this comes at the expense of being able to address individual student misconceptions.', ""However, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning.""]",0,['The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .']
CC1103,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,learning to assess lowlevel conceptual understanding,"['Rodney D Nielsen', 'Wayne Ward', 'James H Martin']",experiments,,"At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .","['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer."", 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .']",3,"['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer."", 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .']"
CC1104,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,simulated tutors in immersive learning environments empiricallyderived design principles,"['N B Steinhauser', 'L A Butler', 'G E Campbell']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .']"
CC1105,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,towards tutorial dialog to support selfexplanation adding natural language understanding to a cognitive tutor,"['V Aleven', 'O Popescu', 'K R Koedinger']",introduction,"Self-explanation is an effective metacognitive strategy, as a number of cognitive science studies have shown. In a previous study we showed that self-explanation can be supported effectively in a cognitive tutor for geometry problem solving. In that study, students explained their own problem-solving steps by selecting from a menu the name of a problem-solving principle that justifies the step. They learned with greater understanding, as compared to students who did not explain their reasoning. Currently, we are working toward testing the hypothesis that students will learn even better when they provide explanations in their own words rather than selecting them from a menu. We have implemented a prototype of a cognitive tutor that understands students' explanations and provides feedback. The tutor uses a knowledge-based approach to natural language understanding. We are entering a phase of pilot testing, both for the purpose of assessing the coverage of the natural language understanding component and for gaining insight into the kinds of dialog strategies that are needed.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1106,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,linking semantic and knowledge representations in a multidomain dialogue system,"['Myroslava O Dzikovska', 'James F Allen', 'Mary D Swift']",experiments,"We describe a two-layer architecture for supporting semantic interpretation and domain reasoning in dialogue systems. Building system that supports both semantic interpretation and domain reasoning in a transparent and well-integrated manner is an unresolved problem because of the diverging requirements of the semantic representations used in contextual interpretation versus the knowledge representations used in domain reasoning. We propose an architecture that provides both portability and efficiency in natural language interpretation by maintaining separate semantic and domain knowledge representations, and integrating them via an ontology mapping procedure. The ontology mapping is used to obtain representations of utterances in a form most suitable for domain reasoners and to automatically specialize the lexicon. The use of a linguistically motivated parser for producing semantic representations for complex natural language sentences facilitates building portable semantic interpretation components as well as connections with domain reasoners. Two evaluations demonstrate the effectiveness of our approach: we show that a small number of mapping rules are sufficient for customizing the generic semantic representation to a new domain, and that our automatic lexicon specialization technique improves parser speed and accuracy.","The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output .","['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output ."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']",5,"['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output .""]"
CC1107,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,the beetle and beediff tutoring systems,"['Charles B Callaway', 'Myroslava Dzikovska', 'Elaine Farrow', 'Manuel Marques-Pita', 'Colin Matheson', 'Johanna D Moore']",experiments,"We describe two tutorial dialogue systems that adapt techniques from task-oriented dialogue systems to tutorial dialogue. Both systems employ the same reusable deep natural language understanding and generation components to interpret students' written utterances and to automatically generate adaptive tutorial responses, with separate domain reasoners to provide the necessary knowledge about the correctness of student answers and hinting strategies. We focus on integrating the domain-independent language processing components with domain-specific reasoning and tutorial components in order to improve the dialogue interaction, and present a preliminary analysis of BeeDiff's evaluation. Index Terms: tutoring systems, dialogue, deep processing",Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .,['Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .'],3,['Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .']
CC1108,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,autotutor a simulation of a human tutor cognitive systems research,"['A C Graesser', 'P Wiemer-Hastings', 'P WiemerHastings', 'R Kreuz']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1109,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,towards modelling and using common ground in tutorial dialogue,"['Mark Buckley', 'Magdalena Wolska']",introduction,"In order to avoid miscommunication par-ticipants in dialogue continuously attempt to align their mutual knowledge (the ""common ground""). A setting that is per-haps most prone to misalignment is tu-toring. We propose a model of common ground in tutoring dialogues which explic-itly models the truth and falsity of do-main level contributions and show how it can be used to detect and repair stu-dents ' false conjectures and facilitate stu-dent modelling.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1110,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,targeted help for spoken dialogue systems intelligent feedback improves naive users’ performance,"['Beth Ann Hockey', 'Oliver Lemon', 'Ellen Campana', 'Laura Hiatt', 'Gregory Aist', 'James Hieronymus', 'Alexander Gruenstein', 'John Dowding']",experiments,"We present experimental evidence that providing naive users of a spoken dialogue system with immediate help messages related to their out-of-coverage utterances improves their success in using the system. A grammar-based recognizer and a Statistical Language Model (SLM) recognizer are run simultaneously. If the grammar-based recognizer suceeds, the less accurate SLM recognizer hypothesis is not used. When the grammar-based recognizer fails and the SLM recognizer produces a recognition hypothesis, this result is used by the Targeted Help agent to give the user feedback on what was recognized, a diagnosis of what was problematic about the utterance, and a related in-coverage example. The in-coverage example is intended to encourage alignment between user inputs and the language model of the system. We report on controlled experiments on a spoken dialogue system for command and control of a simulated robotic helicopter.",Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .,"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']",2,"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.']"
CC1111,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,deep linguistic processing for spoken dialogue systems,"['James Allen', 'Myroslava Dzikovska', 'Mehdi Manshadi', 'Mary Swift']",experiments,"We describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems. The goal is to create domaingeneral processing techniques that can be shared across all domains and dialogue tasks, combined with domain-specific optimization based on an ontology mapping from the generic LF to the application ontology. This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning.",We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .,"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']",5,['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .']
CC1112,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,interpretation and generation in a knowledgebased tutorial system,"['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow']",experiments,,"The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .","['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']",5,"['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .']"
CC1113,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,tree edit distance for textual entailment,"['Milen Kouleykov', 'Bernardo Magnini']",introduction,,"All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .","['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']",0,"['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']"
CC1114,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,statistical phrasebased translation,"['Philipp Koehn', 'Franz Josef Och', 'Daniel Marcu']",,"This work summarizes a comparison between two ap-proaches to Statistical Machine Translation (SMT), namely Ngram-based and Phrase-based SMT. In both approaches, the translation process is based on bilingual units related by word-to-word alignments (pairs of source and target words), while the main differences are based on the extraction process of these units and the sta-tistical modeling of the translation context. The study has been carried out on two different translation tasks (in terms of translation difculty and amount of available training data), and allowing for distortion (reordering) in the decoding pro-cess. Thus it extends a previous work were both approaches were compared under monotone conditions. We nally report comparative results in terms of trans-lation accuracy, computation time and memory size. Re-sults show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1",They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .,"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']",0,"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.']"
CC1115,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,dirt  discovery of inference rules from text,"['Dekang Lin', 'Patrick Pantel']",introduction,"In this paper, we propose an unsupervised method for discovering inference rules from text, such as ""X is author of Y  X wrote Y"", ""X solved Y  X found a solution to Y"", and ""X caused Y  Y is triggered by X"". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus.","ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .","['ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are fre- quently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']",0,"['ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.']"
CC1116,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,a semantic approach to recognizing textual entailment,['Marta Tatu andDan Moldovan'],introduction,"Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system. 1 Recognizing Textual Entailment While communicating, humans use different expressions to convey the same meaning. Therefore, numerous NLP applications, such as, Question Answering, Information Extraction, or Summarization require computational models of language that recognize if two texts semantically overlap. Trying to capture the major inferences needed to understand equivalent semantic expressions, the PASCAL Network proposed the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2005). Given two text fragments, the task is to determine if the meaning of one text (the entailed hypothesis, H) can be inferred from the meaning of the other text (the entailing text, T). Given the wide applicability of this task, there is an increased interest in creating systems which detect the semantic entailment between two texts. The systems that participated in the Pascal RTE challenge competition exploit various inference elements which, later, they combine within statistical models, scoring methods, or machine learning frameworks. Several systems (Bos and Markert, 2005; Herrera et al., 2005; Jijkoun and de Rijke, 2005; Kouylekov and Magnini, 2005; Newman et al., 2005) measured the word overlap between the two text strings. Using either statistical or Word-Net's relations, almost all systems considered lexical relationships that indicate entailment. The degree of similarity between the syntactic parse trees of the two texts was also used as a clue for entailment by several systems (Herrera et al., 2005; Kouylekov and Magnini, 2005; de Salvo Braz et al., 2005; Raina et al., 2005). Several groups used logic provers to show the entailment between T and H (Bayer e","All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .","['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']",0,"['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']"
CC1117,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,creating a bilingual entailment corpus through translations with mechanical turk 100 for a 10day rush,"['Matteo Negri', 'Yashar Mehdad']",experiments,"This paper reports on experiments in the creation of a bi-lingual Textual Entailment corpus, using non-experts' workforce under strict cost and time limitations ($100, 10 days). To this aim workers have been hired for translation and validation tasks, through the Crowd-Flower channel to Amazon Mechanical Turk. As a result, an accurate and reliable corpus of 426 English/Spanish entailment pairs has been produced in a more cost-effective way compared to other methods for the acquisition of translations based on crowdsourcing. Focusing on two orthogonal dimensions (i.e. reliability of annotations made by non experts, and overall corpus creation costs), we summarize the methodology we adopted, the achieved results, the main problems encountered, and the lessons learned.","Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .","['The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.', 'It consists of 1600 pairs derived from the RTE3 development and test sets (800+800).', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .', ""The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce."", 'Translation jobs return one Spanish version for each hypothesis.', 'Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.', 'At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.', 'Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus.', 'The validation, carried out by a Spanish native speaker on 100 randomly selected pairs after two translation-validation cycles, showed the good quality of the collected material, with only 3 minor ""errors"" consisting in controversial but substantially acceptable translations reflecting regional Spanish variations.']",5,"['The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .']"
CC1118,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",introduction,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .","['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'For instance, the reliance of cur- rent monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal ex- pressions recognizers and normalizers) has to con- front, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the ex- isting ones, and the burden of integrating language- specific components into the same cross-lingual ar- chitecture.']",0,"['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.']"
CC1119,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",introduction,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :","['This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques.', 'Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour.', 'Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system.', 'Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :']",1,"['Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :']"
CC1120,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .","['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']",5,"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .']"
CC1121,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,tracking and summarizing news on a daily basis with columbias newsblaster,"['Kathleen R McKeown', 'Regina Barzilay', 'David Evans', 'Vasileios Hatzivassiloglou', 'Judith L Klavans', 'Ani Nenkova', 'Carl Sable', 'Barry Schiffman', 'Sergey Sigelman']",,"Recently, there have been significant advances in several areas of language technology, including clustering, text categorization, and summarization. However, efforts to combine technology from these areas in a practical system for information access have been limited. In this paper, we present Columbia's Newsblaster system for online news summarization. Many of the tools developed at Columbia over the years are combined together to produce a system that crawls the web for news articles, clusters them on specific topics and produces multidocument summaries for each cluster.","They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .']"
CC1122,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,paraphrasing with bilingual parallel corpora,"['Colin Bannard', 'Chris Callison-Burch']",introduction,"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.","Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .","['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']",0,"['Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']"
CC1123,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,making largescale support vector machine learning practical,['Thorsten Joachims'],,"Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.","To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .","['To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .']",5,"['To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .']"
CC1124,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,lexical selection and paraphrase in a meaning text generation model,"['Lidija Iordanskaja', 'Richard Kittredge', 'Alain Polg re']",,"We introduce a computationally tractable model for language generation based on the Meaning-Text Theory of Mel'cuk et al., in which the lexicon plays a central role. To illustrate the descriptive scope and paraphrase capabilities of the model, we show how the lexicon influences the set of choices at four different points during the multi-stage realization process: (1) semantic net simplification, (2) determination of root lexical node for the deep syntactic dependency tree, (3) possible application of deep paraphrase rules using lexical functions, and (4) surface syntactic realization. We also show some of the ways in which the theme/rheme specifications within the semantic net influence lexical and syntactic choices during realization. Examples are taken primarily from an implemented system which generates paragraph-length reports about the usage of operating systems.","They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"
CC1125,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,verbocean mining the web for finegrained semantic verb relations,"['Timothy Chklovski', 'Patrick Pantel']",introduction,"Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/.","These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .","['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']",0,"['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .']"
CC1126,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,multiwordnet developing and aligned multilingual database,"['Emanuele Pianta', 'Luisa Bentivogli', 'Christian Girardi']",introduction,,"Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .","['Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources.', 'As regards the first issue, it�s worth noting that in the monolingual scenario simple �bag of words� (or �bag of n- grams�) approaches are per se sufficient to achieve results above baseline.', 'In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lex- ical matches between texts and hypotheses in different languages.', 'This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English.', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .', 'As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE.', 'As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.', 'However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.', 'In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g bilingual dictionaries).']",0,"['Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .']"
CC1127,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,inference rules and their application to recognizing textual entailment,"['Georgiana Dinu', 'Rui Wang']",,"In this paper, we explore ways of improv-ing an inference rule collection and its ap-plication to the task of recognizing textual entailment. For this purpose, we start with an automatically acquired collection and we propose methods to refine it and ob-tain more rules using a hand-crafted lex-ical resource. Following this, we derive a dependency-based structure representa-tion from texts, which aims to provide a proper base for the inference rule appli-cation. The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible im-provements.","They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"
CC1128,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,paraphrasing with bilingual parallel corpora,"['Colin Bannard', 'Chris Callison-Burch']",,"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.",One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",0,"['One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.']"
CC1129,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",introduction,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .","['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']",0,"['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .']"
CC1130,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,probabilistic textual entailment generic applied modeling of language variability,"['Ido Dagan', 'Oren Glickman']",introduction,,"Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .","['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'For instance, the reliance of cur- rent monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognizers and normalizers) has to confront, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating language- specific components into the same cross-lingual architecture.']",0,"['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.']"
CC1131,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,the berkeley framenet project,"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']",introduction,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work","These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .","['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']",0,"['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.']"
CC1132,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,fluency adequacy or hter exploring different human judgments with a tunable mt metric,"['Matthew Snover', 'Nitin Madnani', 'Bonnie Dorr', 'Richard Schwartz']",,"Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, mea-sure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT met-ric: TER-Plus, which extends the Transla-tion Edit Rate evaluation metric with tun-able parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest aver-age rank in terms of Pearson and Spear-man correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating sig-nificant differences between the types of human judgments.","After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']",0,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']"
CC1133,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,extending the meteor machine translation evaluation metric to the phrase level,"['Michael Denkowski', 'Alon Lavie']",,"This paper presents METEOR-NEXT, an ex-tended version of the METEOR metric de-signed to have high correlation with post-editing measures of machine translation qual-ity. We describe changes made to the met-ric's sentence aligner and scoring scheme as well as a method for tuning the metric's pa-rameters to optimize correlation with human-targeted Translation Edit Rate (HTER). We then show that METEOR-NEXT improves cor-relation with HTER over baseline metrics, in-cluding earlier versions of METEOR, and ap-proaches the correlation level of a state-of-the-art metric, TER-plus (TERp).","They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .']"
CC1134,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,extracting paraphrase patterns from bilingual parallel corpora,"['Shiqi Zhao', 'Haifeng Wang', 'Ting Liu', 'Sheng Li']",introduction,"Paraphrase patterns are semantically equivalent patterns, which are useful in both paraphrase recognition and generation. This paper presents a pivot approach for extracting paraphrase patterns from bilingual parallel corpora, whereby the paraphrase patterns in English are extracted using the patterns in another language as pivots. We make use of log-linear models for computing the paraphrase likelihood between pattern pairs and exploit feature functions based on maximum likelihood estimation (MLE), lexical weighting (LW), and monolingual word alignment (MWA). Using the presented method, we extract more than 1 million pairs of paraphrase patterns from about 2 million pairs of bilingual parallel sentences. The precision of the extracted paraphrase patterns is above 78%. Experimental results show that the presented method significantly outperforms a well-known method called discovery of inference rules from text (DIRT). Additionally, the log-linear model with the proposed feature functions are effective. The extracted paraphrase patterns are fully analyzed. Especially, we found that the extracted paraphrase patterns can be classified into five types, which are useful in multiple natural language processing (NLP) applications.","Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .","['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']",0,"['Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']"
CC1135,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",experiments,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) .","[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']",1,"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"
CC1136,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,a syntaxbased statistical translation model,"['Kenji Yamada', 'Kevin Knight']",conclusion,,"One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .","['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', 'patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.', '1343']",3,"['One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']"
CC1137,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,jlsi a tool for latent semantic indexing software available at httptccitcitresearchtextectoolsresourcesjlsihtml,['Claudio Giuliano'],experiments,,We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .,"['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']",5,['We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .']
CC1138,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",experiments,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .","['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.', 'This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', 'In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.', ""Finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%)."", 'This demonstrates that phrase tables can successfully replace MT systems in the CLTE task.']",1,"['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']"
CC1139,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,moses open source toolkit for statistical machine translation,"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen']",,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.","Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .","['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']",5,"['Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']"
CC1140,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,recognizing textual entailment rational evaluation and approaches,"['Ido Dagan', 'Bill Dolan', 'Bernardo Magnini', 'Dan Roth']",introduction,"The goal of identifying textual entailment - whether one piece of text can be plausibly inferred  from another - has emerged in recent years as a generic core problem in natural language  understanding. Work in this area has been largely driven by the PASCAL Recognizing  Textual Entailment (RTE) challenges, which are a series of annual competitive meetings.  The current work exhibits strong ties to some earlier lines of research, particularly automatic  acquisition of paraphrases and lexical semantic relationships and unsupervised inference in  applications such as question answering, information extraction and summarization. It has  also opened the way to newer lines of research on more involved inference methods, on  knowledge representations needed to support this natural language understanding challenge  and on the use of learning methods in this context. RTE has fostered an active and growing  community of researchers focused on the problem of applied entailment. This special issue  of the JNLE provides an opportunity to showcase some of the most important work in this  emerging area","Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .","['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']",0,"['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.']"
CC1141,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphology and meaning in the english mental lexicon,"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']",introduction,"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect",Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .,"['Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments).']",0,"['Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments).']"
CC1142,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",the measurement of interrater agreement statistical methods for rates and proportions2212–236,"['Joseph L Fleiss', 'Bruce Levin', 'Myunghee Cho Paik']",method,,We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .,"['Organization and Processing of Compound Verbs in the Mental Lexicon Compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'The verb V1 is known as pole and V2 is called as vector.', 'For example, ""ওঠে পড়া "" (getting up) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'However, not all V1+V2 combinations are CVs.', 'For example, expressions like, ""নিঠে য়াও ""(take and then go) and "" নিঠে আঠ ়া"" (return back) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as CV.', 'The key question linguists are trying to identify for a long time and debating a lot is whether to consider CVs as a single lexical units or consider them as two separate units.', 'Since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'A clear understanding about these phenomena may help us to classify or extract actual CVs from other verb sequences.', 'In order to do so, presently we have applied three different techniques to collect user data.', 'In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'We asked the experts to classify the verb sequences into three classes namely, CV, not a CV and not sure.', 'Each linguist has received 2000 verb pairs along with their respective example sentences.', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of κ=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', 'This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'We found a significant correlation between the expert annotation and the compositionality score.', 'We observe verb sequences that are annotated as CVs (like, খেঠে খিল , ওঠে পড , কঠে খি ) have got low compositionality score (average score ranges between 1-4) on the other hand high compositional values are in general tagged as not a cv (নিঠে য়া (come and get), নিঠে আে (return back), তু ঠল খেঠেনি (kept), গনিঠে পিল (roll on floor)).', 'This reflects that verb sequences which are not CV shows high degree of compositionality.', 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences.', 'We followed the same experimental procedure as discussed in (Taft, 2004) for English polymorphemic words.', 'However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.', 'The reaction time (RT) of each subject is recorded.', 'Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.', 'This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']",5,['We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .']
CC1143,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",serial verb construction in marathiquot,['R Pandharipande'],related work,,"Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .","['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .']"
CC1144,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphological decomposition and the reverse base frequency effect,['M Taft'],method,"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.",We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .,"['Organization and Processing of Compound Verbs in the Mental Lexicon Compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'The verb V1 is known as pole and V2 is called as vector.', 'For example, ""ওঠে পড়া "" (getting up) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'However, not all V1+V2 combinations are CVs.', 'For example, expressions like, ""নিঠে য়াও ""(take and then go) and "" নিঠে আঠ ়া"" (return back) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as CV.', 'The key question linguists are trying to identify for a long time and debating a lot is whether to consider CVs as a single lexical units or consider them as two separate units.', 'Since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'A clear understanding about these phenomena may help us to classify or extract actual CVs from other verb sequences.', 'In order to do so, presently we have applied three different techniques to collect user data.', 'In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'We asked the experts to classify the verb sequences into three classes namely, CV, not a CV and not sure.', 'Each linguist has received 2000 verb pairs along with their respective example sentences.', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We measure the inter annotator agreement using the Fleiss Kappa (Fleiss et al., 1981) measure (κ) where the agreement lies around 0.79.', 'Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of κ=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', 'This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'We found a significant correlation between the expert annotation and the compositionality score.', 'We observe verb sequences that are annotated as CVs (like, খেঠে খিল , ওঠে পড , কঠে খি ) have got low compositionality score (average score ranges between 1-4) on the other hand high compositional values are in general tagged as not a cv (নিঠে য়া (come and get), নিঠে আে (return back), তু ঠল খেঠেনি (kept), গনিঠে পিল (roll on floor)).', 'This reflects that verb sequences which are not CV shows high degree of compositionality.', 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences.', 'We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .', 'However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.', 'The reaction time (RT) of each subject is recorded.', 'Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.', 'This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']",5,['We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .']
CC1145,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",causal chains and compound verbsquot,['E Bashir'],related work,,"#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".","['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.']"
CC1146,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphological decomposition and the reverse base frequency effect,['M Taft'],related work,"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.","Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .","['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']",0,"['Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .']"
CC1147,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",lexical representation of derivational relation,['D Bradley'],related work,,"The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .","['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988).']",0,"['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988).']"
CC1148,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",masked morphological priming in visual word recognition,"['J Grainger', 'P Cole', 'J Segui']",introduction,"Masked priming studies with adult readers have provided evidence for a form-based morpho-orthographic segmentation mechanism that ""blindly"" decomposes any word with the appearance of morphological complexity. The present studies investigated whether evidence for structural morphological decomposition can be obtained with developing readers. We used a masked primed lexical decision design first adopted by Rastle, Davis, and New (2004), comparing truly suffixed (golden-GOLD) and pseudosuffixed (mother-MOTH) prime-target pairs with nonsuffixed controls (spinach-SPIN). Experiment 1 tested adult readers, showing that priming from both pseudo- and truly suffixed primes could be obtained using our own set of high-frequency word materials. Experiment 2 assessed a group of Year 3 and Year 5 children, but priming only occurred when prime and target shared a true morphological relationship, and not when the relationship was pseudomorphological. This pattern of results indicates that morpho-orthographic decomposition mechanisms do not become automatized until a relatively late stage in reading development.21 page(s","There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .']"
CC1149,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",conscious choice and some light verbs in urduquot,['M Butt'],related work,,#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.']"
CC1150,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphology and meaning in the english mental lexicon,"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']",introduction,"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect","With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .","['With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', 'Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated.', 'These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes.', 'Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words.', 'Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix.']",5,"['With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .']"
CC1151,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",repetition priming and frequency attenuation in lexical access,"['K I Forster', 'C Davis']",method,,"We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .","['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'The same target word is again probed but with a different audio or visual probe called the control word.', 'The control shows no relationship with the target.', 'For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).']",5,"['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .']"
CC1152,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",what can we learn from the morphology of hebrew a maskedpriming investigation of morphological representation,"['R Frost', 'K I Forster', 'A Deutsch']",introduction,"All Hebrew words are composed of 2 interwoven morphemes: a triconsonantal root and a phonological word pattern. the lexical representations of these morphemic units were examined using masked priming. When primes and targets shared an identical word pattern, neither lexical decision nor naming of targets was facilitated. In contrast root primes facilitated both lexical decisions and naming of target words that were derived from these roots. This priming effect proved to be independent of meaning similarity because no priming effects were found when primes and targets were semantically but not morphologically related. These results suggest that Hebrew roots are lexical units whereas word patterns are not. A working model of lexical organization in Hebrew is offered on the basis of these results.","There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.']"
CC1153,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphology and meaning in the english mental lexicon,"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']",method,"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect","We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .","['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'The same target word is again probed but with a different audio or visual probe called the control word.', 'The control shows no relationship with the target.', 'For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).']",5,"['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .']"
CC1154,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",wordnet an electronic lexical database,['C Fellbaum'],introduction,"Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.","Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .","['A clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language.', 'Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications.', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .']",0,"['Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .']"
CC1155,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphological decomposition and the reverse base frequency effect,['M Taft'],introduction,"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.","On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English, Hebrew, Italian, French, Dutch, and few other languages (Marslen-Wilson et al., 2008;Frost et al., 1997;Grainger, et al., 1991;Drews and Zwitserlood, 1995).', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .']"
CC1156,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",lexical access and inflectional morphology,"['A Caramazza', 'A Laudanna', 'C Romani']",related work,,"For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .","['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon (Bradley, 1980;Butterworth, 1983).', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .']",0,"['For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .']"
CC1157,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",hindi structures intermediate levelquot michigan papers on south and,['P E Hook'],related work,,#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC1158,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",lexical representation of derivational relation,['D Bradley'],related work,,"Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .","['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']",0,"['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.']"
CC1159,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",and orthographic similarity in visual word recognition,"['E Drews', 'P Zwitserlood']",introduction,,"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .']"
