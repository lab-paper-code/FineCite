{"id": "087922a2-e3ce-415a-8149-d146175ee6de", "citing_paper": {"title": "USST's System for AutoSimTrans 2022", "year": 2022, "authors": ["Jiahui Zhu", "Jun Yu"]}, "text": ["Simultaneous", "translation", "(Gu et al., 2017, Ma et al., 2018)", "consists", "in", "generating", "a", "translation", "before", "the", "source", "speaker", "finishes", "speaking.", "It", "is", "widely", "used", "in", "many", "real-time", "scenarios", "such", "as", "international", "conferences,", "business", "negotiations", "and", "legal", "proceedings.", "The", "challenge", "of", "Simultaneous", "machine", "translation", "is", "to", "find", "a", "read-write", "policy", "that", "balances", "translation", "quality", "and", "latency.", "The", "translation", "quality", "will", "decline", "if", "the", "machine", "translation", "system", "reads", "insufficient", "source", "information.", "When", "reading", "wider", "source", "text,", "latency", "will", "increase."], "cited_papers": [{"title": null, "year": "2018", "authors": ["Mingbo Ma", "Liang Huang", "Hao Xiong", "Renjie Zheng", "Kaibo Liu", "Baigong Zheng", "Chuanqiang Zhang", "Zhongjun He", "Hairong Liu", "Xing Li"]}, {"title": "Learning to translate in real-time with neural machine translation", "year": "2017", "authors": ["Jiatao Gu", "Graham Neubig", "Kyunghyun Cho", "O Victor", "unk Li"]}], "target_citation_location": 2, "citation_locations": [2], "citation_type": "group", "annotations": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "08d9477e-5f02-460b-b714-8ad20a125568", "citing_paper": {"title": "Estimating word co-occurrence probabilities from pretrained static embeddings using a log-bilinear model", "year": 2022, "authors": ["Richard Futrell"]}, "text": ["Word", "co-occurrence", "probabilities", "are", "a", "key", "ingredient", "in", "usage-based", "cognitive", "models", "of", "language.", "By", "word", "co-occurrence", "probabilities,", "I", "mean", "the", "probability", "of", "a", "word", "w", "given", "some", "other", "single", "word", "c,", "p(w", "|", "c),", "where", "words", "w", "and", "c", "have", "some", "specific", "relationship,", "for", "example", "adjectives", "that", "attributively", "modify", "nouns", "or", "nouns", "serving", "as", "direct", "objects", "of", "verbs", "(Gries and Durrant, 2020)."], "cited_papers": [{"title": "Analyzing co-occurrence data", "year": "2020", "authors": ["Stefan Th", "Gries unk", "Philip Durrant"]}], "target_citation_location": 58, "citation_locations": [58], "citation_type": "single", "annotations": [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "0906419b-3006-4f1b-a13b-312845a3ba6f", "citing_paper": {"title": "Non-Contiguous Tree Parsing", "year": 2004, "authors": ["Mark Dras", "Chung-Hye Han"]}, "text": ["In", "this", "Spanish-English", "example,", "the", "clitic", "can", "climb", "over", "an", "unlimited", "number", "of", "'trigger", "verbs'", "(Aissen and Perlmutter, 1976)", "(indicated", "by", "the", "ellipses", "in", "the", "example),", "and", "for", "certain", "TAG", "grammars", "this", "can", "correspond", "to", "a", "pair", "of", "derivation", "trees", "as", "in", "Figure", "2.", "In", "this", "pair", "of", "trees,", "his", "corresponds", "to", "both", "los", "and", "the", "clitic", "le.", "Both", "his", "and", "los", "are", "fixed", "in", "relation", "to", "the", "root", "of", "the", "tree,", "but", "le", "is", "an", "unbounded", "distance", "from", "it,", "so", "it", "is", "not", "possible", "to", "form", "a", "gCN", "in", "the", "Spanish", "tree", "for", "pairing", "without", "the", "unbounded", "and", "unrelated", "recursively-inserted", "verbs,", "hence", "requiring", "infinitely", "many", "transfer", "rules."], "cited_papers": [{"title": "Clause Reduction in Spanish", "year": "1976", "authors": ["J Aissen", "D Perlmutter"]}], "target_citation_location": 15, "citation_locations": [15], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "0a3cdb38-7e4a-457c-af09-dc5cd15126f2", "citing_paper": {"title": "Situated Dialogue Learning through Procedural Environment Generation", "year": 2022, "authors": ["Prithviraj Ammanabrolu", "Renee Jia", "Mark Riedl", "Sanmit Narvekar", "Bei Peng", "Matteo Leonetti", "Jivko Sinapov", "Matthew Taylor", "Peter Stone", "Olivier Pietquin", "Matthieu Geist", "Senthilkumar Chan", "Shrimai Prabhumoye", "Margaret Li", "Jack Urbanek", "Sebastien Racaniere", "Andrew Lampinen", "Adam Santoro", "David Reichert", "Vlad Firoiu", "Tim- Othy Lillicrap", "Stephen Roller", "Emily Dinan", "Naman Goyal", "Da Ju", "Mary Williamson", "Yinhan Liu", "Jing Xu", "Myle Ott", "Kurt Shuster", "Mikayel Samvelyan", "Robert Kirk", "Vitaly Kurin", "Jack Parker-Holder", "Minqi Jiang", "Eric Hambro", "Fabio Petroni", "Heinrich Kuttler", "Edward Grefenstette", "Tim Rockt\u00e4schel", "unk Minihack", "Satinder Singh", "Michael Kearns", "Diane Litman", "Marilyn Walker", "unk Reinforcement", "Sainbayar Sukhbaatar", "Zeming Lin", "Ilya Kostrikov", "Gabriel Synnaeve", "Angela Fan", "Siddharth Karamcheti", "Saachi Jain", "Samuel Humeau", "Douwe Kiela", "Arthur Szlam", "Yinfei Yang", "Steve Yuan", "Daniel Cer", "Sheng-Yi Kong", "Noah Constant", "Petr Pilar", "Heming Ge"]}, "text": ["Sampled", "Curriculums.", "Inspired", "by", "Chawla et al. (2002),", "Graves", "et", "al.", "(2017),", "we", "explore", "an", "alternate", "method", "of", "creating", "curriculums", "by", "simply", "oversampling", "the", "same", "rare", "quests", "found", "in", "the", "tails", "of", "the", "distributions."], "cited_papers": [{"title": "Smote: synthetic minority over-sampling technique", "year": "2002", "authors": ["V Nitesh", "Kevin Chawla", "Lawrence Bowyer", "W Philip Hall", "unk Kegelmeyer"]}], "target_citation_location": 4, "citation_locations": [4], "citation_type": "single", "annotations": [[0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "0a49f3d6-b7c3-427d-9b43-ca9b3427cddf", "citing_paper": {"title": "Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts", "year": 2022, "authors": ["Wenhao Yu", "Chenguang Zhu", "Lianhui Qin", "Zhihan Zhang", "Tong Zhao", "Meng Jiang"]}, "text": ["The", "quality", "is", "measured", "by", "standard", "N-gram", "based", "metrics,", "including", "the", "BLEU", "score", "(Papineni et al., 2002)", "and", "the", "ROUGE", "score", "(Lin, 2004).", "This", "measures", "the", "highest", "accuracy", "comparing", "the", "best", "hypothesis", "among", "the", "top-K", "with", "the", "target", "(Vijayakumar et al., 2018).", "Concretely,", "we", "generate", "hypotheses", "{\u0176", "(1),", "\u0176", "(K)}", "from", "each", "source", "X", "and", "keep", "the", "hypothesis", "\u0176", "best", "that", "achieves", "the", "best", "sentencelevel", "metric", "with", "the", "target", "Y.", "Then", "we", "calculate", "a", "corpus-level", "metric", "with", "the", "greedily-selected", "hypotheses", "{Y", "(i),best}", "N", "i=1", "and", "references", "{Y", "(i)}", "N", "i=1.", "The", "diversity", "of", "evaluated", "by", "three", "aspects:", "concept,", "pairwise", "and", "corpus", "diversity."], "cited_papers": [{"title": "Rouge: A package for automatic evaluation of summaries", "year": "2004", "authors": ["Chin-Yew Lin"]}], "target_citation_location": 18, "citation_locations": [13, 18, 34], "citation_type": "single", "annotations": [[0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "0bac0ead-4c41-4b6b-b157-3853a8050936", "citing_paper": {"title": "MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages", "year": 2022, "authors": ["Karthik Gokul", "Abhishek Kumar", "Gehlot Singh", "Shaji Sahal", "Karthik Mullappilly", "unk Nandakumar"]}, "text": ["We", "use", "AI4Bharat's", "IndicTrans", "2", "(Ramesh et al., 2021)", "for", "translation,", "which", "is", "a", "Transformer-4X", "model", "trained", "on", "Samanantar", "dataset", "(Ramesh et al., 2021).", "In", "IndicTrans,", "translation", "can", "be", "done", "from", "Indian", "languages", "to", "English", "and", "vice", "versa.", "Available", "Indian", "languages", "include", "Assamese,", "Bengali,", "Gujarati,", "Hindi,", "Kannada,", "Malayalam,", "Marathi,", "Oriya,", "Punjabi,", "Tamil,", "and", "Telugu.", "At", "first,", "we", "translate", "the", "ChAII", "dataset", "from", "Hindi", "and", "Tamil", "to", "English", "and", "then", "to", "Bengali,", "Marathi,", "Malayalam,", "and", "Telugu.", "In", "the", "FLORES", "devset", "benchmark", "(Goyal et al., 2021),", "the", "BLEU", "scores", "of", "IndicTrans", "for", "translating", "Hindi", "and", "Tamil", "to", "English", "are", "37.9", "and", "28.6,", "respectively.", "The", "scores", "(Radford et al., 2021)", "for", "translating", "English", "to", "Bengali,", "Marathi,", "Malayalam, and Telugu are 20.3, 16.1, 16.3, and 22", ".0,", "respectively.", "We", "were", "not", "able", "to", "translate", "nearly", "500", "of", "the", "ChAII", "instances", "to", "English", "as", "the", "automatic", "search", "for", "the", "translated", "answers", "in", "the", "translated", "contexts", "failed.", "This", "happened", "because", "the", "same", "word", "got", "translated", "differently", "in", "the", "context", "and", "the", "answer.", "For", "the", "same", "reason,", "we", "lost", "nearly", "another", "200", "instances", "when", "translating", "from", "English", "to", "other", "Indian", "languages."], "cited_papers": [{"title": "Learning transferable visual models from natural language supervision", "year": "2021", "authors": ["Alec Radford", "Jong Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark"]}], "target_citation_location": 94, "citation_locations": [5, 17, 74, 94, 101], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "0c6d5458-1e92-4403-9f91-e713d999af9f", "citing_paper": {"title": "Text-based NP Enrichment", "year": 2022, "authors": ["Yanai Elazar", "Victoria Basmov", "Yoav Goldberg", "Reut Tsarfaty"]}, "text": ["We", "chose", "to", "use", "prepositions", "as", "relation", "labels,", "despite", "this", "ambiguity.", "This", "follows", "a", "line", "of", "annotation", "work", "that", "aims", "to", "express", "semantic", "relations", "using", "natural", "language", "(FitzGerald et al., 2018, Roit et al., 2020, Klein et al., 2020, Pyatkin et al., 2020),", "as", "opposed", "to", "works", "that", "used", "formal", "linguistic", "terms,", "traditionally", "relying", "on", "expert-defined", "taxonomies", "of", "semantic", "roles", "and", "discourse", "relations.", "The", "aforementioned", "works", "label", "predicateargument", "relations", "using", "restricted", "questions.", "In", "the", "same", "vein,", "we", "label", "nominal", "relations", "using", "prepositions."], "cited_papers": [{"title": "Controlled crowdsourcing for high-quality QA-SRL annotation", "year": "2020", "authors": ["Paul Roit", "Ayal Klein", "Daniela Stepanov", "Jonathan Mamou", "Julian Michael", "Gabriel Stanovsky", "Luke Zettlemoyer", "Ido Dagan"]}, {"title": "Large-scale QA-SRL parsing", "year": "2018", "authors": ["Nicholas Fitzgerald", "Julian Michael", "Luheng He", "Luke Zettlemoyer"]}, {"title": null, "year": null, "authors": ["unknown"]}], "target_citation_location": 27, "citation_locations": [27], "citation_type": "group", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "0ed5f699-3e87-4e59-b43a-0a2d38c6865f", "citing_paper": {"title": "Coreference Reasoning in Machine Reading Comprehension", "year": 2021, "authors": ["Mingzhu Wu", "Nafise Moosavi", "Dan Roth", "Iryna Gurevych"]}, "text": ["There", "are", "two", "general", "approaches", "to", "tackle", "such", "artifacts:", "(1)", "adversarial", "filtering", "of", "biased", "examples,", "i.e.,", "examples", "that", "contain", "artifacts,", "and", "(2)", "debiasing", "methods.", "In", "the", "first", "approach,", "potentially", "biased", "examples", "are", "discarded", "from", "the", "dataset,", "either", "after", "the", "dataset", "creation", "(Zellers et al., 2018, Yang et al., 2018a, Le Bras et al., 2020, Bartolo et al., 2020),", "or", "while", "creating", "the", "dataset", "(Dua et al., 2019, Chen et al., 2019, Nie et al., 2020)."], "cited_papers": [{"title": "Adversarial filters of dataset biases", "year": "2020", "authors": ["Swabha Ronan Le Bras", "Chandra Swayamdipta", "Rowan Bhagavatula", "Matthew Zellers", "Ashish Peters", "Yejin Sabharwal", "unk Choi"]}, {"title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering", "year": "2018", "authors": ["Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher Manning"]}, {"title": null, "year": null, "authors": ["unknown"]}], "target_citation_location": 41, "citation_locations": [41, 47], "citation_type": "group", "annotations": [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 0, 3, 3, 3, 3, 3]]}
{"id": "1004ac9f-126d-4e87-ab1d-6684e27aa93a", "citing_paper": {"title": "Memory-efficient Transformers via Top-k Attention", "year": 2021, "authors": ["Ankit Gupta", "Guy Dar", "Shaya Goodman", "David Ciprut", "Jonathan Berant", "Ibm Research"]}, "text": ["Feed-forward", "layer", "While", "considerable", "effort", "has", "been", "dedicated", "to", "devising", "efficient", "models", "for", "long", "contexts,", "a", "large", "feed-forward", "dimension", "is", "useful", "for", "knowledge-intensive", "tasks", "such", "as", "opendomain", "QA", "(Roberts et al., 2020, Brown et al., 2020),", "and", "efforts", "have", "been", "made", "to", "reduce", "its", "complexity", "(Fedus et al., 2021).", "We", "benchmark", "the", "resource", "usage", "of", "top-k", "attention", "at", "a", "single", "feed-forward", "layer", "for", "different", "feed-forward", "dimensions", "using", "batch", "size", "512", "and", "input", "length", "512,", "which", "results", "in", "2", "18", "queries", "per", "batch."], "cited_papers": [{"title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "year": "2021", "authors": ["W Fedus", "Barret Zoph", "Noam Shazeer"]}], "target_citation_location": 38, "citation_locations": [28, 38], "citation_type": "single", "annotations": [[0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "13d638f7-eee6-432f-88e8-783d7fee010a", "citing_paper": {"title": "BRIO: Bringing Order to Abstractive Summarization", "year": 2022, "authors": ["Yixin Liu", "Pengfei Liu", "Dragomir Radev", "Graham Neubig"]}, "text": ["We", "use", "the", "standard", "ROUGE", "(Lin, 2004)", "Perl", "package", "15", "for", "evaluation.", "The", "command", "line", "parameters", "are", "'-c", "95", "-r", "1000", "-n", "2", "-m'.", "Before", "the", "Datasets", "\u03bb", "(Eq.", "8)", "\u03b1", "(Eq.", "9)", "\u03b3", "(Eq.", "10)", "CNNDM", "0.001", "2.0", "100", "XSum", "0.1", "0.6", "100", "NYT", "0.001", "2.0", "100"], "cited_papers": [{"title": "ROUGE: A package for automatic evaluation of summaries", "year": "2004", "authors": ["Chin-Yew Lin"]}], "target_citation_location": 5, "citation_locations": [5], "citation_type": "single", "annotations": [[2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "1430bd67-9452-4e66-b50b-386a57dbf0e4", "citing_paper": {"title": "Comparison of post-editing productivity between professional translators and lay users", "year": 2014, "authors": ["Nora Aranberri", "Gorka Labaka"]}, "text": ["Finally,", "to", "test", "whether", "the", "MT", "engine", "was", "better", "prepared", "to", "address", "Text", "A", "or", "Text", "B,", "we", "calculated", "perplexity", "and", "out-of-vocabulary", "(OOV)", "words.", "Perplexity", "is", "used", "as", "a", "mea-surement", "of", "how", "well", "the", "language", "model", "predicts", "the", "reference", "translations.", "The", "smaller", "the", "perplexity,", "the", "more", "and", "longer", "overlap", "exists", "between", "the", "reference", "and", "the", "language", "model", "in", "the", "MT", "system.", "This", "measurement", "shows", "that", "the", "MT", "engine", "is", "better", "suited", "to", "output", "a", "correct", "version", "for", "Text", "A", "than", "for", "Text", "B", "(see", "Table", "10).", "Note", "that", "the", "high", "perplexity", "values,", "calculated", "per", "word,", "are", "in", "line", "with", "those", "reported", "for", "morphologically", "rich", "languages", "(see", "Popel and Mare\u010dek, 2010).", "Table", "10.", "Perplexity", "calculated", "on", "5-grams."], "cited_papers": [{"title": "Perplexity of n-Gram and Dependency Language Models", "year": "2010", "authors": ["M Popel", "D Mare\u010dek"]}], "target_citation_location": 106, "citation_locations": [106], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3]]}
{"id": "1982fc86-d8b3-4338-91a5-ad9ea59447ff", "citing_paper": {"title": "USST's System for AutoSimTrans 2022", "year": 2022, "authors": ["Jiahui Zhu", "Jun Yu"]}, "text": ["Byte-pair-encoding", "(BPE)", "(Sennrich et al., 2016)", "6:", "For", "both", "the", "Chinese", "and", "English", "sides,", "we", "use", "BPE", "with", "32K", "operations."], "cited_papers": [{"title": "Neural machine translation of rare words with subword units", "year": "2016", "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"]}], "target_citation_location": 2, "citation_locations": [2], "citation_type": "single", "annotations": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1], [1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1]]}
{"id": "1c34f6c2-bcc3-48d1-bf3e-17831ef4572f", "citing_paper": {"title": "Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts", "year": 2022, "authors": ["Wenhao Yu", "Chenguang Zhu", "Lianhui Qin", "Zhihan Zhang", "Tong Zhao", "Meng Jiang"]}, "text": ["All", "baseline", "methods", "were", "built", "on", "the", "Transformer", "architecture", "with", "6-layer", "encoder", "and", "decoder,", "and", "initialized", "with", "pre-trained", "parameters", "from", "BARTbase", "(Lewis et al., 2020),", "which", "is", "one", "of", "the", "stateof-the-art", "pre-trained", "Transformer", "models", "for", "natural", "language", "generation", "(Gehrmann et al., 2021).", "In", "our", "MoKGE,", "the", "Transformer", "parameters", "were", "also", "initialized", "by", "BART-base,", "in", "order", "to", "make", "fair", "comparison", "with", "all", "baseline", "methods.", "The", "R-GCN", "parameters", "were", "random", "initialized."], "cited_papers": [{"title": "The gem benchmark: Natural language generation, its evaluation and metrics", "year": "2021", "authors": ["Sebastian Gehrmann", "Tosin Adewumi", "Karmanya Aggarwal", "Pawan Sasanka Ammanamanchi", "Anuoluwapo Aremu", "Antoine Bosselut", "Miruna-Adriana Khyathi Raghavi Chandu", "Dipanjan Clinciu", "Kaustubh Das", "unk Dhole"]}], "target_citation_location": 35, "citation_locations": [21, 35], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "1ffab4bb-2188-4f90-8029-fdc2f472e1d8", "citing_paper": {"title": "Multitasking Framework for Unsupervised Simple Definition Generation", "year": 2022, "authors": ["Cunliang Kong", "Yun Chen", "Hengyuan Zhang", "Liner Yang", "Erhong Yang"]}, "text": ["The", "goal", "of", "SDG", "task", "is", "to", "generate", "simple", "definitions", "for", "languages", "that", "lack", "learner's", "dictionary.", "For", "example,", "Chinese", "as", "Second", "Language", "(CSL)", "learners", "do", "not", "have", "suitable", "dictionaries.", "As", "Zhang (2011)", "pointed", "out,", "since", "the", "difficulty", "of", "definitions", "is", "not", "considered,", "the", "existing", "dictionary", "cannot", "meet", "CSL", "learner's", "needs."], "cited_papers": [{"title": "Discussion on the Definitions in Chinese Learner's Dictionaries: Comparative Study of Domestic and Foreign Learner Dictionaries (Translated from Chinese)", "year": "2011", "authors": ["Yihua Zhang"]}], "target_citation_location": 30, "citation_locations": [30], "citation_type": "single", "annotations": [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "22c0a872-f553-438b-a298-1c649989c712", "citing_paper": {"title": "Modeling Users and Online Communities for Abuse Detection: A Position on Ethics and Explainability", "year": 2021, "authors": ["Pushkar Mishra", "Helen Yannakoudakis", "Ekaterina Shutova"]}, "text": ["Consequently,", "explainability", "has", "a", "bigger", "role", "to", "play", "here", "than", "simply", "being", "a", "tool", "that", "provides", "interpretability", "to", "designers", "or", "offers", "justifications", "to", "users.", "Operationalizing", "explainability", "in", "a", "manner", "that", "spreads", "awareness", "about", "existing", "stereotypes", "and", "fills", "the", "information", "gap", "can", "be", "very", "effective", "(Miller, 2018, Sap et al., 2020).", "One", "way", "to", "achieve", "this", "is", "by", "having", "generative", "explanations", "in", "conjunction", "with", "information", "retrieval", "techniques", "that", "fulfill", "the", "property", "of", "elucidating", "stereotypes", "in", "a", "human-understandable", "way", "(Gilpin et al., 2018)", "while", "offering", "references", "to", "reliable", "sources", "on", "the", "stereotypes.", "In", "fact,", "such", "an", "operationalization", "that", "elucidates", "stereotypes", "or", "frames", "of", "bias", "(Sap et al., 2020)", "in", "abusive", "comments", "at", "a", "community", "level,", "while", "providing", "information", "to", "debunk", "the", "stereotypes", "themselves,", "can", "offer", "validation", "to", "the", "victims", "of", "abuse", "by", "communities,", "e.g.,", "minority", "groups,", "and", "help", "them", "feel", "safer", "on", "the", "platform."], "cited_papers": [{"title": "Explaining explanations: An overview of interpretability of machine learning", "year": "2018", "authors": ["L Gilpin", "D Bau", "B Yuan", "A Bajwa", "M Specter", "L Kagal"]}], "target_citation_location": 72, "citation_locations": [44, 72, 94], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "23f5ee5c-e55e-45ec-a317-47416da47104", "citing_paper": {"title": "Interpretable Entity Representations through Large-Scale Typing", "year": 2020, "authors": ["Yasumasa Onoe", "Greg Durrett"]}, "text": ["We", "use", "the", "entity", "typing", "model", "trained", "on", "the", "Wiki-Context", "data", "(see", "Section", "4)", "to", "get", "the", "mention", "and", "context", "representation", "t.", "In", "the", "CoNLL-YAGO", "setting,", "similar", "to", "past", "work", "(Onoe and Durrett, 2019, F\u00e9vry et al., 2020),", "we", "prepend", "the", "document", "title", "and", "the", "first", "sentence", "to", "the", "input", "to", "enrich", "the", "context", "information.", "To", "obtain", "the", "candidate", "representations", "{c", "1,", "c", "2,", "...,", "c", "j,", "...},", "we", "use", "the", "model", "trained", "on", "the", "Wiki-Description", "data,", "which", "is", "specialized", "for", "entity", "descriptions", "(see", "Section", "4)", "similar", "to", "Gillick et al. (2019).", "We", "choose", "Wikipedia", "datasets", "here", "because", "UFET", "does", "not", "support", "entity", "descriptions.", "We", "rank", "the", "candidate", "entities", "based", "on", "cosine", "similarity", "between", "t", "and", "c", "j,", "and", "the", "entity", "with", "the", "highest", "score", "is", "our", "model's", "prediction."], "cited_papers": [{"title": null, "year": null, "authors": ["unknown"]}, {"title": "Learning to Denoise Distantly-Labeled Data for Entity Typing", "year": "2019", "authors": ["Yasumasa Onoe", "Greg Durrett"]}], "target_citation_location": 30, "citation_locations": [30, 81], "citation_type": "group", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "2849b2a0-f4e6-4eb9-80c4-a193ca0af6a2", "citing_paper": {"title": "Factored Neural Machine Translation Architectures", "year": 2016, "authors": ["Mercedes Garc\u00eda-Mart\u00ednez", "Lo\u00efc Barrault", "Fethi Bougares"]}, "text": ["The", "first", "assumption", "we", "made", "is", "highly", "dependent", "on", "the", "design", "of", "the", "considered", "factors,", "i.e.", "the", "lemmas", "are", "the", "most", "informative", "factors", "among", "all.", "Then,", "we", "tried", "using", "only", "the", "lemma", "embedding", "as", "feedback", "(see", "equation", "5).", "fb(y", "t\u22121)", "=", "y", "L", "t\u22121", "(5)", "where", "y", "L", "t\u22121", "is", "the", "embedding", "of", "the", "lemma", "generated", "at", "previous", "timestep."], "cited_papers": [{"title": "On using very large target vocabulary for neural machine translation", "year": null, "authors": ["S Jean", "K Cho", "R Memisevic", "Y Bengio"]}], "target_citation_location": 44, "citation_locations": [44], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "299df407-c7f9-4850-8c77-9be4383b31a2", "citing_paper": {"title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering", "year": 2021, "authors": ["Yingqi Qu", "Yuchen Ding", "Jing Liu", "Kai Liu", "Ruiyang Ren", "Wayne Zhao", "Daxiang Dong", "Hua Wu", "Haifeng Wang"]}, "text": ["Cross-batch", "Negatives", "When", "training", "the", "dualencoder,", "the", "trick", "of", "in-batch", "negatives", "has", "been", "widely", "used", "in", "previous", "work", "(Henderson et al., 2017, Gillick et al., 2019, Wu et al., 2020, Karpukhin et al., 2020, Luan et al., 2020).", "Assume", "that", "there", "are", "B", "questions", "in", "a", "mini-batch", "on", "a", "single", "GPU,", "and", "each", "question", "has", "one", "positive", "passage.", "With", "the", "in-batch", "negative", "trick,", "each", "question", "can", "be", "further", "paired", "with", "B", "\u2212", "1", "negatives", "(i.e.,", "positive", "passages", "of", "the", "rest", "questions)", "without", "sampling", "additional", "negatives.", "In-batch", "negative", "training", "is", "a", "memory-efficient", "way", "to", "reuse", "the", "examples", "already", "loaded", "in", "a", "mini-batch", "rather", "than", "sampling", "new", "negatives,", "which", "increases", "the", "number", "of", "negatives", "for", "each", "question.", "As", "illustrated", "at", "the", "top", "of", "Figure", "2,", "we", "present", "an", "example", "for", "in-batch", "negatives", "when", "training", "on", "A", "GPUs", "in", "a", "data", "parallel", "way.", "To", "further", "optimize", "the", "training", "with", "more", "negatives,", "we", "propose", "to", "use", "cross-batch", "negatives", "when", "training", "on", "multiple", "GPUs,", "as", "illustrated", "at", "the", "bottom", "of", "Figure", "2.", "Specifically,", "we", "first", "compute", "the", "passage", "embeddings", "within", "each", "single", "GPU,", "and", "then", "share", "these", "passage", "embeddings", "among", "all", "the", "GPUs.", "Besides", "the", "in-batch", "negatives,", "we", "collect", "all", "passages", "(i.e.,", "their", "dense", "representations)", "from", "other", "GPUs", "as", "the", "additional", "negatives", "for", "each", "question.", "Hence,", "with", "A", "GPUs", "(or", "mini-batches)", "2,", "we", "can", "indeed", "obtain", "A\u00d7B", "\u22121", "negatives", "for", "a", "given", "question,", "which", "is", "approximately", "A", "times", "as", "many", "as", "the", "original", "number", "of", "in-batch", "negatives.", "In", "this", "way,", "we", "can", "use", "more", "negatives", "in", "the", "training", "objective", "of", "Equation", "2,", "so", "that", "the", "results", "are", "expected", "to", "be", "improved.", "Denoised", "Hard", "Negatives", "Although", "the", "above", "strategy", "can", "increase", "the", "number", "of", "negatives,", "most", "of", "negatives", "are", "easy", "ones,", "which", "can", "be", "easily", "discriminated.", "While,", "hard", "negatives", "are", "shown", "to", "be", "important", "to", "train", "a", "dual-encoder", "(Gillick et al., 2019, Wu et al., 2020, Karpukhin et al., 2020, Luan et al., 2020, Xiong et al., 2020).", "To", "obtain", "hard", "negatives,", "a", "straightforward", "method", "is", "to", "select", "the", "top-ranked", "passages", "(excluding", "the", "labeled", "positive", "passages)", "as", "negative", "samples.", "However,", "it", "is", "likely", "to", "bring", "false", "negatives", "(i.e.,", "unlabeled", "positives),", "since", "the", "annotators", "can", "only", "annotate", "a", "few", "top-retrieved", "passages", "(as", "discussed", "in", "Section", "1).", "Another", "note", "is", "that", "previous", "work", "mainly", "focuses", "on", "factoid", "questions,", "to", "which", "the", "answers", "are", "short", "and", "concise.", "Hence,", "it", "is", "not", "challenging", "to", "filter", "false", "negatives", "by", "using", "the", "short", "answers", "(Karpukhin et al., 2020).", "However,", "it", "cannot", "apply", "to", "non-factoid", "questions.", "In", "this", "paper,", "we", "aim", "to", "learn", "dense", "passage", "retrieval", "for", "both", "factoid", "questions", "and", "non-factoid", "questions,", "which", "needs", "a", "more", "effective", "way", "for", "denoising", "hard", "negatives."], "cited_papers": [{"title": "Sparse, dense, and attentional representations for text retrieval. CoRR, abs", "year": null, "authors": ["Yi Luan", "Jacob Eisenstein", "Kristina Toutanova", "Michael Collins"]}, {"title": "Learning dense representations for entity retrieval", "year": "2019", "authors": ["Daniel Gillick", "Sayali Kulkarni", "Larry Lansing", "Alessandro Presta", "Jason Baldridge", "Eugene Ie", "Diego Garc\u00eda-Olano"]}, {"title": null, "year": null, "authors": ["unknown"]}], "target_citation_location": 283, "citation_locations": [18, 283, 364], "citation_type": "group", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "2c0bd06e-dc90-4200-8aa4-cc6bfeabe833", "citing_paper": {"title": "On the Usability of Transformers-based models for a French Question-Answering task", "year": 2021, "authors": ["Oralie Cattan", "Christophe Servan", "Sophie Rosset"]}, "text": ["Other", "architectural", "improvements", "highlighted", "with", "the", "introduction", "of", "the", "ALBERT", "model", "(Lan et al., 2020)", "such", "as", "the", "factorization", "of", "the", "attention", "matrix", "or", "parameter", "sharing.", "Indeed,", "the", "most", "time-consuming", "and", "memory-intensive", "operations", "concerns", "the", "forward", "propagation", "and", "attention", "computation", "operations.", "The", "self-attention", "layer", "of", "BERT", "pretrained", "models", "grows", "quadratically", "in", "respect", "to", "the", "input", "sequence", "length.", "One", "common", "approach", "to", "this", "issue", "consists", "of", "approximating", "the", "dot-product", "attention", "for", "example", "by", "using", "hashing", "techniques", "(Kitaev et al., 2020)", "to", "accelerate", "the", "training", "and", "inference", "phases", "when", "long", "sequence", "lengths", "are", "used.", "However", "these", "solutions", "have", "demonstrated", "they", "suffer", "from", "important", "computational", "overheads", "for", "tasks", "with", "smaller", "lengths,", "such", "as", "question-answering."], "cited_papers": [{"title": "Reformer: The efficient transformer", "year": "2020", "authors": ["Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya"]}], "target_citation_location": 72, "citation_locations": [11, 72], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3]]}
{"id": "3106ddaf-ae84-415b-85ab-2c5a462e567d", "citing_paper": {"title": "MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages", "year": 2022, "authors": ["Karthik Gokul", "Abhishek Kumar", "Gehlot Singh", "Shaji Sahal", "Karthik Mullappilly", "unk Nandakumar"]}, "text": ["The", "choice", "of", "languages", "used", "for", "translation", "and", "transliteration", "is", "critical.", "Kudugunta et al. (2019)", "showed", "that", "languages", "under", "the", "same", "family", "have", "similar", "representations", "in", "multilingual", "models.", "Hence,", "we", "put", "together", "translations", "and", "transliterations", "from", "related", "languages", "within", "the", "same", "language", "family", "to", "achieve", "better", "performance.", "This", "will", "also", "help", "with", "better", "use", "of", "the", "vo-cabulary", "corpora", "from", "the", "low-resource", "languages.", "We", "also", "study", "the", "impact", "of", "translation", "and", "transliteration", "on", "languages", "outside", "the", "family", "of", "the", "target", "language.", "Since", "the", "cross-family", "language", "transfer", "degraded", "the", "QA", "performance,", "we", "introduce", "a", "contrastive", "loss", "(Radford et al., 2021)", "between", "the", "translated", "pairs", "to", "help", "retain", "or", "improve", "the", "original", "performance", "by", "encouraging", "the", "embeddings", "from", "all", "languages", "to", "be", "similar", "regardless", "of", "the", "family", "group.", "Thus,", "the", "contributions", "of", "the", "paper", "are", "three-fold:"], "cited_papers": [{"title": "Learning transferable visual models from natural language supervision", "year": "2021", "authors": ["Alec Radford", "Jong Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark"]}], "target_citation_location": 91, "citation_locations": [11, 91], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "31968bed-8257-4056-b1ea-a0b5773db033", "citing_paper": {"title": "Text-based NP Enrichment", "year": 2022, "authors": ["Yanai Elazar", "Victoria Basmov", "Yoav Goldberg", "Reut Tsarfaty"]}, "text": ["While", "reading", "comprehension", "(RC)", "and", "question", "answering", "(QA)", "are", "often", "used", "interchangeably", "in", "the", "literature,", "measuring", "the", "reading", "comprehension", "capacity", "of", "models", "via", "question", "answering,", "as", "implemented", "in", "benchmarks", "such", "as", "SQuAD", "(Rajpurkar et al., 2016),", "BoolQ", "(Clark et al., 2019)", "and", "others,", "has", "several", "well-documented", "problems", "(Dunietz et al., 2020).", "We", "argue", "that", "the", "TNE", "task", "we", "propose", "herein", "has", "properties", "that", "make", "it", "appealing", "for", "assessing", "RC,", "more", "than", "QA", "is.", "First,", "benchmarks", "for", "extractive", "(span-marking)", "QA", "are", "sensitive", "to", "the", "span-boundary", "selection,", "on", "the", "other", "hand,", "benchmarks", "for", "yes/no,", "multiple", "choice,", "or", "generative", "questions", "can", "in", "principle", "be", "answered", "in", "a", "way", "that", "is", "completely", "divorced", "from", "the", "text.", "On", "a", "more", "fundamental", "level,", "all", "QA", "benchmarks", "are", "very", "sensitive", "to", "lexical", "choices", "in", "the", "question", "and", "its", "similarity", "to", "the", "text.", "Furthermore,", "QA", "benchmarks", "rely", "on", "human", "authored", "questions", "that", "are", "easy", "to", "solve", "based", "on", "surface", "artifacts.", "Finally,", "in", "many", "cases,", "the", "existence", "of", "the", "question", "itself", "provides", "a", "huge", "hint", "towards", "the", "answer", "(Kaushik and Lipton, 2018)."], "cited_papers": [{"title": "To test machine comprehension, start by defining comprehension", "year": "2020", "authors": ["Jesse Dunietz", "Greg Burnham", "Akash Bharadwaj", "Owen Rambow", "Jennifer Chu-Carroll", "Dave Ferrucci"]}], "target_citation_location": 41, "citation_locations": [32, 34, 41, 160], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "349ee356-3282-4903-94ac-2c836daa2aab", "citing_paper": {"title": "Unsupervised Representation Disentanglement of Text: An Evaluation on Synthetic Datasets", "year": 2021, "authors": ["Lan Zhang", "Victor Prokhorov", "Ehsan Shareghi"]}, "text": ["As", "demonstrated", "in", "(1)", "the", "positive", "correlation", "of", "C", "with", "AU", "which", "intuitively", "means", "the", "increase", "of", "channel", "capacity", "demands", "more", "dimensions", "of", "the", "representation", "to", "carry", "information", "which", "then", "translates", "into", "having", "a", "better", "reconstruction", "of", "data,", "(2)", "the", "negative", "correlation", "between", "the", "increase", "of", "\u03b2", "and", "decrease", "of", "reconstruction", "loss,", "(3)", "the", "best", "Rec.", "and", "AU", "are", "achieved", "by", "AE", "and", "MAT-VAE", "whereas", "the", "worst", "one", "is", "achieved", "by", "the", "(collapsed)", "vanilla-VAE,", "(4)", "the", "MAT-VAE", "(\u03b2", "=", "0.01,", "\u03bb", "=", "0.1)", "model", "which", "induces", "more", "sparse", "representations", "6", "performs", "the", "best", "on", "both", "datasets,", "indicating", "the", "positive", "impact", "of", "representation", "sparsity", "as", "an", "inductive", "bias.", "As", "illustrated", "in", "Figure", "1,", "the", "difference", "between", "means", "of", "each", "disentanglement", "score", "on", "various", "models", "is", "relatively", "small,", "and", "due", "to", "large", "standard", "deviation", "on", "metrics,", "it", "is", "difficult", "to", "single", "out", "a", "superior", "model.", "This", "verifies", "findings", "of", "Lo-6", "Sparsity", "is", "measured", "using", "Hoyer", "(Hurley and Rickard, 2009).", "In", "this", "paper", "we", "report", "this", "as", "the", "average", "Hoyer", "over", "data", "points'", "posterior", "means.", "Hoyer", "for", "data", "point", "xi", "with", "posterior", "mean", "\u00b5i", "is", "calculated", "as\u221a", "d\u2212||", "\u03bci", "||", "1", "/||", "\u03bci", "||", "2", "\u221a", "d\u22121,", "where", "d", "is", "the", "dimensionality", "of", "the", "representations", "and", "\u03bci", "=", "\u00b5i/\u03c3(\u00b5),", "where", "\u00b5", "=", "{\u00b51,", "...,", "\u00b5n},", "and", "\u03c3(.)", "is", "the", "standard", "deviation.", "catello", "et", "al.", "(2019)", "on", "image", "domain.", "In", "Table", "3", "(Top-3", "column)", "we", "report", "the", "number", "of", "appearances", "of", "a", "model", "among", "the", "top", "3", "highest", "scoring", "models", "on", "at", "least", "one", "disentanglement", "metric.", "The", "ranking", "suggests", "that", "\u03b2-VAE", "with", "smaller", "\u03b2", "values", "reach", "better", "disentangled", "representations,", "and", "MAT-VAE", "performing", "superior", "on", "YNOC", "and", "poorly", "on", "POS,", "highlighting", "its", "more", "challenging", "nature.", "For", "MAT-VAE", "we", "also", "observe", "an", "interesting", "correlation", "between", "sparsity", "and", "disentanglement:", "for", "instance", "on", "YNOC,", "MAT-VAE", "(\u03b2", "=", "0.01,", "\u03bb", "=", "0.1)", "achieves", "the", "highest", "Hoyer", "(See", "Table", "4)", "and", "occurs", "7", "times", "among", "Top-3", "(see", "Table", "3).", "Interestingly,", "the", "success", "of", "MAT-VAE", "does", "not", "translate", "to", "POS", "dataset,", "where", "it", "underperforms", "AE.", "These", "two", "observations", "suggest", "that", "sparsity", "could", "be", "a", "facilitator", "for", "disentanglement,", "but", "achieving", "a", "stable", "level", "of", "sparsity", "remains", "as", "a", "challenge.", "The", "more", "recent", "development", "in", "the", "direction", "of", "sparsity,", "HSVAE", "(Prokhorov et al., 2020),", "addresses", "the", "stability", "issue", "of", "MAT-VAE", "but", "we", "leave", "its", "exploration", "to", "future", "work."], "cited_papers": [{"title": "Hierarchical sparse variational autoencoder for text encoding", "year": "2020", "authors": ["Victor Prokhorov", "Yingzhen Li", "Ehsan Shareghi", "Nigel Collier"]}], "target_citation_location": 364, "citation_locations": [153, 364], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "37d5b13b-af3c-43ce-abcb-48797993be90", "citing_paper": {"title": "Generalizable and Explainable Dialogue Generation via Explicit Action Learning", "year": 2020, "authors": ["Xinting Huang", "Jianzhong Qi", "Yu Sun", "Rui Zhang"]}, "text": ["We", "use", "a", "three-layer", "transformer", "(Vaswani et al., 2017)", "with", "a", "hidden", "size", "of", "128", "and", "4", "heads", "as", "our", "base", "model", "for", "content", "planning", "and", "response", "generation,", "i.e.,", "p", "l", "(a|c)", "and", "p", "r", "(a,", "c),", "respectively.", "We", "use", "grid", "search", "to", "find", "the", "best", "hyperparameters", "for", "the", "models", "based", "on", "validation", "performance,", "which", "we", "use", "a", "combination", "of", "Inform,", "Success", "and", "BLEU", "scores", "to", "measure.", "We", "choose", "the", "embedding", "dimensionality", "d", "among", "{50,", "75,", "100,", "150,", "200},", "the", "hyperparameters", "\u03b1", "and", "\u03b2", "in", "[0.01,", "1.0]."], "cited_papers": [{"title": "Attention is all you need", "year": "2017", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"]}], "target_citation_location": 5, "citation_locations": [5], "citation_type": "single", "annotations": [[2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "3a0eace8-ed7b-4f21-8907-47cc4c813de0", "citing_paper": {"title": "Effects of Word Alignment Visualization on Post-Editing Quality & Speed \u2020", "year": 2015, "authors": ["Lane Schwartz", "Isabel Lacruz", "Tatyana Bystrova"]}, "text": ["We", "examine", "the", "effect", "that", "alignment", "link", "visualization", "has", "on", "each", "bilingual", "post-editor", "in", "Figure", "5", "on", "the", "next", "page.", "In", "the", "Russian-English", "condition,", "where", "overall", "MT", "quality", "is", "poor,", "we", "observe", "that", "post-editing", "quality", "varies", "widely", "between", "post-editors", "(with", "PE2", "and", "PE3", "performing", "best).", "For", "all", "six", "bilingual", "post-editors,", "we", "observe", "higher", "mean", "adequacy", "scores", "when", "alignment", "links", "were", "presented", "than", "when", "they", "were", "omitted", "from", "the", "post-editing", "tool.", "We", "also", "note", "that", "when", "alignment", "links", "were", "absent,", "one", "bilingual", "post-editor", "(PE5)", "performed", "worse", "than", "the", "monolingual", "post-editor", "(PE0)", "from", "Schwartz et al. (2014).", "On", "the", "other", "hand,", "in", "the", "Spanish-English", "condition,", "where", "overall", "MT", "quality", "is", "good,", "we", "observe", "relatively", "little", "variation", "in", "quality", "between", "the", "ten", "post-editors.", "When", "compared", "to", "the", "unedited", "machine", "trans-PE0", "PE1", "PE2", "PE3", "PE4", "PE5", "PE6", "lations,", "post-editing", "resulted", "in", "improved", "mean", "adequacy", "for", "all", "post-editors,", "both", "bilingual", "and", "monolingual."], "cited_papers": [{"title": "Machine translation and monolingual postediting: The AFRL WMT-14 system", "year": "2014", "authors": ["L Schwartz", "T Anderson", "J Gwinnup", "K Young"]}], "target_citation_location": 91, "citation_locations": [91], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "3b1198ad-371f-4d9f-acc3-fccfac15a4d2", "citing_paper": {"title": "Memory-efficient Transformers via Top-k Attention", "year": 2021, "authors": ["Ankit Gupta", "Guy Dar", "Shaya Goodman", "David Ciprut", "Jonathan Berant", "Ibm Research"]}, "text": ["First,", "we", "compare", "the", "performance", "of", "UNI-FIEDQA", "(Khashabi et al., 2020)", "before", "and", "after", "replacing", "its", "feed-forward", "layers", "with", "our", "implementation", "of", "top-k", "attention", "and", "directly", "performing", "inference", "on", "12", "different", "question", "answering", "(QA)", "datasets", "without", "any", "training.", "UNIFIEDQA", "is", "a", "T5-based", "(Raffel et al., 2020)", "model", "with", "11B", "parameters", "(Raffel et al., 2020),", "fine-tuned", "on", "a", "weighted", "mixture", "of", "QA", "datasets.", "The", "12", "datasets", "include", "diverse", "domains,", "such", "as", "science", "questions,", "factoid", "questions", "over", "Wikipedia,", "commonsense", "questions,", "etc.", "Details", "regarding", "the", "datasets", "and", "metrics", "can", "be", "found", "in", "\u00a7A.2."], "cited_papers": [{"title": "UNIFIEDQA: Crossing format boundaries with a single QA system", "year": "2020", "authors": ["Daniel Khashabi", "Sewon Min", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Clark", "Hannaneh Hajishirzi"]}], "target_citation_location": 7, "citation_locations": [7, 39, 44], "citation_type": "single", "annotations": [[0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "3bdcf8da-ac96-44d2-bdef-7a0f626588d1", "citing_paper": {"title": "Associating semantic components with intersective Levin classes", "year": 1997, "authors": ["Hoa Dang", "Joseph Rosenzweig", "Martha Palmer"]}, "text": ["This", "paper", "examines", "the", "question", "of", "differences", "between", "a", "traditional", "interlingua", "approach", "and", "a", "transferbased", "approach", "that", "uses", "cross-linguistic", "semantic", "features", "to", "generalize", "its", "transfer", "lexicon", "entries,", "and", "concludes", "that", "the", "two", "approaches", "share", "a", "common", "interest", "in", "lexical", "classifications", "that", "can", "be", "distinguished", "by", "cross-linguistic", "semantic", "features.", "The", "paper", "goes", "on", "to", "discuss", "current", "approaches", "to", "English", "classification,", "Levin", "classes", "[8]", "and", "WordNet", "[9].", "We", "present", "a", "refinement", "of", "Levin", "classes", "-Intersective", "Classes", "-that", "shows", "interesting", "correlations", "to", "WordNet", "and", "that", "makes", "more", "explicit", "the", "semantic", "components", "that", "serve", "to", "distinguish", "different", "classes."], "cited_papers": [{"title": "Five papers on wordnet", "year": "1990", "authors": ["G Miller", "R Beckwith", "C Fellbaum", "D Gross", "K Miller"]}], "target_citation_location": 64, "citation_locations": [61, 64], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "42961dcc-aa43-4df6-9dd7-c1c5416b5712", "citing_paper": {"title": "End-to-end ASR to jointly predict transcriptions and linguistic annotations", "year": 2021, "authors": ["Motoi Omachi", "Yuya Fujita", "Shinji Watanabe", "Matthew Wiesner"]}, "text": ["In", "O2M", "models", "shown", "in", "Fig.", "1(a),", "a", "multitask", "objective", "is", "used", "in", "which", "an", "extra", "branch", "is", "tasked", "with", "estimating", "the", "secondary", "label", "sequence.", "For", "example,", "in", "(Kubo and Bacchiani, 2020),", "the", "phonemic", "transcript", "is", "produced", "in", "addition", "to", "the", "graphemic", "transcript.", "The", "O2M", "model", "can", "estimate", "each", "sequence", "more", "accurately", "than", "separate", "models", "responsible", "for", "producing", "phonemic", "and", "graphemic", "transcripts", "independently.", "We", "can", "implement", "this", "approach", "with", "less", "effort", "by", "attaching", "multiple", "loss", "functions", "to", "the", "base", "architecture.", "However,", "this", "O2M", "model", "does", "not", "explicitly", "consider", "dependencies", "between", "phonemic", "and", "graphemic", "transcripts.", "Furthermore,", "aligning", "phoneme", "and", "grapheme", "sub-sequences", "requires", "additional", "post-processing", "based", "on", "time", "alignment", "or", "alignment", "across", "the", "multiple", "sequences", "during", "inference.", "Performance", "of", "downstream", "NLP", "tasks", "built", "on", "top", "of", "ASR", "outputs", "will", "suffer", "if", "this", "post-processing", "fails", "to", "generate", "alignment.", "Fig. 1(b)", "shows", "an", "O2O", "model", "with", "a", "conditional", "chain", "mapping.", "This", "method", "for", "multiple", "sequence", "modeling", "has", "been", "applied", "to", "dialog", "modeling", "(Liang et al., 2020),", "speaker", "diarization", "(Fujita et al., 2020a),", "and", "multi-speaker", "ASR", "(Shi et al., 2020).", "Unlike", "the", "O2M", "model,", "this", "model", "can", "predict", "a", "variable", "number", "of", "output", "sequences", "while", "explicitly", "considering", "dependencies", "between", "the", "multiple", "sequences", "based", "on", "the", "probabilistic", "chain", "rule.", "However,", "modeling", "these", "inter-sequence", "dependencies", "requires", "more", "complicated", "neural", "architectures,", "and", "alignment", "of", "the", "sequences", "still", "requires", "post-processing", "during", "inference."], "cited_papers": [{"title": "Neural speaker diarization with speaker-wise chain rule", "year": "2020", "authors": ["Y Fujita", "S Watanabe", "S Horiguchi", "Y Xue", "J Shi", "K Nagamatsu"]}], "target_citation_location": 157, "citation_locations": [28, 132, 154, 157, 161], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "44888512-408c-4b10-b33c-2727c485a682", "citing_paper": {"title": "Corpora and Machine Translation", "year": 1993, "authors": ["Yorick Wilks"]}, "text": ["An", "important", "note", "before", "continuing:", "when", "I", "refer", "to", "IBM", "machine", "translation", "I", "mean", "only", "the", "systems", "referred", "to", "at", "the", "end", "by", "Brown", "et", "al.", "IBM", "as", "a", "whole", "supports", "many", "approaches", "to", "MT,", "including", "McCord's (1989)", "Prolog-based", "symbolic", "approach,", "as", "well", "as", "symbolic", "systems", "in", "Germany", "and", "Japan."], "cited_papers": [{"title": "A new version of the machine translation system LMT", "year": "1989", "authors": ["M Mccord"]}], "target_citation_location": 36, "citation_locations": [36], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "46562612-2587-482e-a6a4-b1fa0e605ef7", "citing_paper": {"title": "Harmonised large-scale syntactic/semantic lexicons: a European multilingual infrastructure", "year": 1999, "authors": ["Nicoletta Calzolari", "Antonio Zampolli"]}, "text": ["Among", "the", "EC", "projects", "working", "in", "this", "direction", "we", "mention", "LE", "SPARKLE", "(Shallow", "PARsing", "and", "Knowledge", "extraction", "for", "Language", "Engineering", "2", "),", "combining", "shallow", "parsing", "and", "lexical", "acquisition", "techniques", "capable", "of", "learning", "(from", "large", "corpora)", "aspects", "of", "word", "knowledge", "required", "for", "LE", "applications", "(Federici et al. 1998).", "The", "project", "(http://www.ilc.pi.cnr.it/sparkle.html)", "is", "positioned", "as", "research", "on", "the", "development", "of", "methodologies", "and", "techniques", "for", "application-or", "domaindependent", "lexical", "resources", "to", "be", "acquired", "(semi)automatically", "from", "texts,", "an", "area", "which", "is", "crucial", "to", "most", "NLP", "applications.", "Economically", "feasible", "development", "of", "language", "models", "and", "of", "substantial", "lexical", "resources", "for", "real-world", "NLP", "applications", "needs", "to", "be", "based", "on", "substantially", "(semi)-automated", "techniques", "and", "flexible", "tools", "for", "analysing", "and", "extracting", "lexical", "information", "from", "textual", "corpora,", "otherwise", "coverage", "and/or", "accuracy", "will", "remain", "inadequate."], "cited_papers": [{"title": "Analogy-based Extraction of Lexical Knowledge from Corpora: The SPARKLE Experience", "year": "1998", "authors": ["S Federici", "S Montemagni", "V Pirrelli", "N Calzolari"]}], "target_citation_location": 43, "citation_locations": [43], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "484fd32f-9ef0-4263-9fe2-4c51601a1246", "citing_paper": {"title": "Coreference Reasoning in Machine Reading Comprehension", "year": 2021, "authors": ["Mingzhu Wu", "Nafise Moosavi", "Dan Roth", "Iryna Gurevych"]}, "text": ["There", "are", "a", "few", "studies", "on", "the", "joint", "understanding", "of", "coreference", "relations", "and", "reading", "comprehension.", "Wu et al. (2020b)", "propose", "to", "formulate", "coreference", "resolution", "as", "a", "span-prediction", "task", "by", "generating", "a", "query", "for", "each", "mention", "using", "the", "surrounding", "context,", "thus", "converting", "coreference", "resolution", "to", "a", "reading", "comprehension", "problem.", "They", "leverage", "the", "plethora", "of", "existing", "MRC", "datasets", "for", "data", "augmentation", "and", "improve", "the", "generalization", "of", "the", "coreference", "model.", "In", "parallel", "to", "Wu et al. (2020b),", "Aralikatte et al. (2019)", "also", "cast", "ellipsis", "and", "coreference", "resolution", "as", "reading", "comprehension", "tasks.", "They", "leverage", "the", "existing", "neural", "archi-tectures", "designed", "for", "MRC", "for", "ellipsis", "resolution", "and", "outperform", "the", "previous", "best", "results.", "In", "a", "similar", "direction,", "Hou (2020)", "propose", "to", "cast", "bridging", "anaphora", "resolution", "as", "question", "answering", "and", "present", "a", "question", "answering", "framework", "for", "this", "task.", "However,", "none", "of", "the", "above", "works", "investigate", "the", "impact", "of", "using", "coreference", "data", "on", "QA.", "Dua et al. (2020)", "use", "Amazon", "Mechanical", "Turkers", "to", "annotate", "the", "corresponding", "coreference", "chains", "of", "the", "answers", "in", "the", "passages", "of", "Quoref", "for", "2,000", "QA", "pairs.", "They", "then", "use", "this", "additional", "coreference", "annotation", "for", "training", "a", "model", "on", "Quoref.", "They", "show", "that", "including", "these", "additional", "coreference", "annotations", "improves", "the", "overall", "performance", "on", "Quoref.", "The", "proposed", "method", "by", "Dua et al. (2020)", "requires", "annotating", "additional", "coreference", "relations", "on", "every", "new", "coreference-aware", "QA", "dataset.", "Contrary", "to", "this,", "our", "approach", "uses", "existing", "coreference", "resolution", "datasets,", "and", "therefore,", "applies", "to", "any", "new", "QA", "dataset", "without", "introducing", "any", "additional", "cost."], "cited_papers": [{"title": "CorefQA: Coreference resolution as query-based span prediction", "year": "2020", "authors": ["Wei Wu", "Fei Wang", "Arianna Yuan", "Fei Wu", "Jiwei Li"]}], "target_citation_location": 15, "citation_locations": [15, 67, 68, 101, 135, 189], "citation_type": "single", "annotations": [[0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "48ae5a79-6288-4d1b-bdf5-1f95f38e6f9c", "citing_paper": {"title": "DRS Parsing as Sequence Labeling", "year": 2022, "authors": ["Minxing Shen", "Kilian Evang"]}, "text": ["Our", "parser", "also", "consistently", "fails", "to", "recognize", "generic", "you", "as", "opposed", "to", "deictic", "you", "(cf.", "Appendix,", "Figure", "8),", "which", "points", "to", "the", "importance", "of", "discourse", "context", "for", "understanding", "the", "(speaker)", "meaning", "of", "even", "a", "single", "word,", "and", "perhaps", "to", "something", "that", "all", "current", "DRS", "parsers", "lack:", "an", "explicit", "distinction", "between", "sentence", "meaning", "and", "speaker", "meaning", "(cf.", "Bender et al., 2015)."], "cited_papers": [{"title": "Layers of interpretation: On grammar and compositionality", "year": "2015", "authors": ["Emily Bender", "Dan Flickinger", "Stephan Oepen", "Woodley Packard", "Ann Copestake"]}], "target_citation_location": 56, "citation_locations": [56], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "4b74881a-d12e-4073-9a33-229b6c09ad85", "citing_paper": {"title": "DEVELOPING AND EVALUATING A PROBABILISTIC LR PARSER OF PART-OF-SPEECH AND PUNCTUATION LABELS*", "year": 1995, "authors": ["Ted Briscoe", "John Carroll"]}, "text": ["In", "order", "to", "assess", "the", "contribution", "of", "punctuation", "to", "the", "selection", "of", "the", "correct", "analysis,", "w\ufffd", "applied", "the", "same", "trained", "version", "of", "the", "integrated", "grammar", "to", "the", "106", "sentences", "from", "the", "test", "set", "which", "contain", "internal", "punctuation,", "both", "with", "and", "without", "the", "punctuation", "marks", "in", "the", "input.", "A", "comparison", "of", "the", "GEIG", "evaluation", "metrics", "for", "this", "set", "of", "sentences", "punctuated", "and", "unpunctuated", "gives", "a", "measure", "of", "the", "contribution", "of", "punctuation", "to", "parse", "selection", "on", "this", "data.", "(The", "results", "for", "the", "unpunctuated", "set", "were", "computed", "against", "a", "version", "of", "the", "Susanne", "tree", "bank", "from", "which", "punctuation", "had", "also", "been", "\ufffd", "emoved.)", "As", "table", "3", "shows,", "recall", "declines", "by", "10%,", "precision", "by", "5%", "and", "there", "are", "an", "average", "of", "1.27", "more", "crossing", "brackets", "per", "sentence.", "6", "Conclusions", "Briscoe and Carroll (1993)", "and", "Carroll (1993)", "showed", "that", "the", "LR", "model,", "combined", "with", "a", "gram", "mar", "exploiting", "subcategorisation", "constraints,", "could", "achieve", "good", "parse", "selection", "accuracy", "bu\ufffd", "at", "the", "expense", "of", "poor", "coverage", "of", "free", "text.", "The", "results", "reported", "here", "suggest", "that", "improved", "coverage", "of", "heterogeneous", "text", "can", "be", "achieved", "by", "exploiting", "textual", "and", "grammatical", "con", "straints", "on", "PoS", "and", "punctuation", "sequences.", "The", "experiments", "show", "that", "grammatical", "coverage", "can", "be", "greatly", "increased", "by", "relaxing", "subcategorisation", "constraints,", "and", "that", "text", "grammatical", "or", "punctuation-cued", "constraints", "can", "reduce", "ambiguity", "and", "increase", "coverage", "during", "parsing."], "cited_papers": [{"title": "Generalised probabilistic LR parsing for unification-based grammars", "year": "1993", "authors": ["E Briscoe", "J Carroll"]}], "target_citation_location": 125, "citation_locations": [125, 127], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "4db24651-9033-430b-9445-cdb55aec81fc", "citing_paper": {"title": "On the weak link between importance and prunability of attention heads", "year": 2020, "authors": ["Aakriti Budhraja", "Madhura Pande", "Preksha Nema", "Pratyush Kumar", "Mitesh Khapra"]}, "text": ["i.e.", "we", "prune", "all", "the", "attention", "heads", "of", "particular", "layers.", "When", "all", "the", "self-attention", "heads", "of", "a", "layer", "l", "are", "pruned,", "only", "the", "feed-forward", "network", "of", "that", "layer", "will", "be", "active", "whose", "input", "will", "just", "be", "the", "output", "from", "the", "previous", "layer", "l-1.", "Bottom", "layers", "of", "BERT", "have", "been", "identified", "to", "model", "word", "morphology", "(Liu et al., 2019, Belinkov et al., 2017)", "and", "are", "considered", "to", "be", "important", "(Sajjad et al., 2020).", "Further,", "recent", "work", "has", "identified", "high", "cosine-similarity", "between", "output", "vectors", "of", "the", "top", "layers,", "indicating", "reduced", "importance", "of", "top", "layers", "(Goyal et al., 2020).", "We", "relate", "these", "studies", "to", "pruning", "by", "comparing", "the", "pruning", "of", "the", "same", "number", "of", "top", "and", "bottom", "layers", "(rows", "2-9", "in", "Table", "5).", "Amongst", "the", "four", "cases,", "two", "cases", "each", "favor", "pruning", "top", "layers", "and", "bottom", "layers,", "revealing", "no", "preference", "in", "pruning."], "cited_papers": [{"title": "Poor man's bert: Smaller and faster transformer models", "year": "2020", "authors": ["Hassan Sajjad", "Fahim Dalvi", "Nadir Durrani", "Preslav Nakov"]}], "target_citation_location": 61, "citation_locations": [54, 61, 82], "citation_type": "single", "annotations": [[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "4ec4d2d2-b35b-4d10-97c3-f1dd7ebf28ef", "citing_paper": {"title": "Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data", "year": 2022, "authors": ["Colin Leong", "Daniel Whitenack"]}, "text": ["Borrowing", "from", "techniques", "used", "on", "languages", "that", "do", "not", "indicate", "word", "boundaries", "by", "the", "use", "of", "whitespace,", "we", "address", "the", "problem", "by", "removing", "all", "whitespace", "from", "our", "data", "sets", "after", "phone", "transliteration.", "We", "train", "character-based", "language", "models", "over", "the", "resulting", "data.", "Character-based", "models", "such", "as", "CharFormer", "(Tay et al., 2021)", "or", "ByT5", "(Xue et al., 2021)", "have", "shown", "promise", "in", "recent", "years", "for", "language", "modeling,", "even", "if", "this", "approach", "is", "known", "to", "have", "some", "trade", "offs", "related", "to", "shorter", "context", "windows."], "cited_papers": [{"title": "Charformer: Fast character transformers via gradientbased subword tokenization", "year": "2021", "authors": ["Yi Tay", "Vinh Tran", "Sebastian Ruder", "Jai Gupta", "unk Hyung Won", "Dara Chung", "Zhen Bahri", "Simon Qin", "Cong Baumgartner", "Donald Yu", "unk Metzler"]}], "target_citation_location": 46, "citation_locations": [46, 49], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "4f94e045-70a9-4713-bf5c-7d10a84a0150", "citing_paper": {"title": "Effects of Word Alignment Visualization on Post-Editing Quality & Speed \u2020", "year": 2015, "authors": ["Lane Schwartz", "Isabel Lacruz", "Tatyana Bystrova"]}, "text": ["For", "the", "Russian-English", "portion", "of", "this", "study,", "we", "selected", "as", "source", "texts", "a", "subset", "of", "the", "texts", "from", "the", "2014", "Workshop", "on", "Statistical", "Machine", "Translation", "(WMT-14)", "shared", "translation", "task", "(Bojar et al., 2014).", "Source", "texts", "were", "news", "articles", "covering", "world", "news", "events", "in", "late", "2013."], "cited_papers": [{"title": "Findings of the 2014 workshop on statistical machine translation", "year": "2014", "authors": ["O Bojar", "C Buck", "C Federmann", "B Haddow", "P Koehn", "J Leveling", "C Monz", "P Pecina", "M Post", "H Saint-Amand", "R Soricut", "L Specia", "A Tamchyna"]}], "target_citation_location": 29, "citation_locations": [29], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "5088259d-1bf8-4197-b79a-0289c2f545be", "citing_paper": {"title": "On the Usability of Transformers-based models for a French Question-Answering task", "year": 2021, "authors": ["Oralie Cattan", "Christophe Servan", "Sophie Rosset"]}, "text": ["ALBERT", "is", "based", "on", "parameter", "sharing/reduction", "techniques", "that", "allows", "to", "reduce", "the", "computational", "complexity", "and", "speed", "up", "training", "and", "inference", "phases.", "Compared", "to", "previous", "compact", "models", "such", "as", "DistilBERT", "(Sanh et al., 2019),", "Q-BERT", "(Shen et al., 2020)", "or", "TernaryBERT", "(Zhang et al., 2020),", "ALBERT", "is", "to", "the", "date", "the", "smallest", "pre-trained", "models", "with", "12", "million", "parameters", "and", "&lt,50", "megabyte", "(MB)", "model", "size."], "cited_papers": [{"title": "Ternary-BERT: Distillation-aware ultra-low bit BERT", "year": "2020", "authors": ["Wei Zhang", "Lu Hou", "Yichun Yin", "Lifeng Shang", "Xiao Chen", "Xin Jiang", "Qun Liu"]}], "target_citation_location": 34, "citation_locations": [29, 31, 34], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "545da3ef-a749-4bf7-8a57-23b32d90f71f", "citing_paper": {"title": "Effects of Word Alignment Visualization on Post-Editing Quality & Speed \u2020", "year": 2015, "authors": ["Lane Schwartz", "Isabel Lacruz", "Tatyana Bystrova"]}, "text": ["Post-editing", "is", "the", "process", "whereby", "a", "human", "user", "corrects", "the", "output", "of", "a", "machine", "translation", "system.", "The", "use", "of", "basic", "post-editing", "tools", "by", "bilingual", "human", "translators", "has", "been", "shown", "to", "yield", "substantial", "increases", "in", "terms", "of", "productivity", "(Plitt and Masselot, 2010)", "as", "well", "as", "improvements", "in", "translation", "quality", "(Green et al., 2013)", "when", "compared", "to", "bilingual", "human", "translators", "working", "without", "assistance", "from", "machine", "translation", "and", "post-editing", "tools.", "More", "sophisticated", "interactive", "interfaces", "(Langlais et al., 2000, Barrachina et al., 2009, Koehn, 2009b, Denkowski and Lavie, 2012)", "may", "also", "provide", "benefit", "(Koehn, 2009a)."], "cited_papers": [{"title": "A process study of computer aided translation", "year": "2009", "authors": ["P Koehn"]}], "target_citation_location": 70, "citation_locations": [37, 45, 65, 70], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 1, 1, 1]]}
{"id": "5a178e8d-62f9-4440-9c59-984c867ab001", "citing_paper": {"title": "Harmonised large-scale syntactic/semantic lexicons: a European multilingual infrastructure", "year": 1999, "authors": ["Nicoletta Calzolari", "Antonio Zampolli"]}, "text": ["A", "coherent", "development", "of", "semantic", "lexical", "resources", "must", "be", "guided", "by", "an", "underlying", "theoretical", "framework", "for", "structuring", "word", "meaning", "and", "generating", "concepts", "which", "satisfies", "both", "ontological", "considerations", "as", "well", "as", "the", "need", "to", "capture", "linguistic", "generalisations.", "The", "SIMPLE", "model", "is", "a", "concrete", "major", "step", "towards", "this", "objective.", "It", "is", "based", "on", "EAGLES", "Lexicon/Semantics", "Working", "Group", "recommendations", "(Sanfilippo et al. 1999)", "and", "on", "extensions", "of", "Generative", "Lexicon", "(GL)", "theory", "(Pustejovsky 1998", ").", "An", "essential", "characteristicswhich", "makes", "it", "basically", "different", "from", "EuroWordNet", "(where", "the", "main", "structuring", "semantic", "relations", "are", "synon-ymy", "and", "hyponymy)", "-is", "its", "ability", "to", "capture", "the", "various", "dimensions", "of", "word", "meaning", "which", "are", "equally", "important", "in", "language", "and", "therefore", "in", "the", "development", "of", "a", "computational", "lexicon.", "The", "basic", "vocabulary", "relies", "on", "an", "extension", "of", "\"qualia", "structure\"", "for", "structuring", "the", "semantic/conceptual", "types,", "which", "is", "understood", "as", "a", "representational", "tool", "for", "expressing", "the", "componential", "multidimensional", "aspect", "of", "word", "meaning", "(Pustejovsky 1991 (Pustejovsky , 1995,, Calzolari 1991)."], "cited_papers": [{"title": "The generative lexicon", "year": "1991", "authors": ["J Pustejovsky"]}, {"title": "The Generative Lexicon. Cambridge", "year": "1995", "authors": ["J Pustejovsky"]}, {"title": "Acquiring and Representing Semantic Information in a Lexical Knowledge Base", "year": "1991", "authors": ["N Calzolari"]}], "target_citation_location": 143, "citation_locations": [56, 65, 143], "citation_type": "group", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "5a8d81f9-5820-4c24-9239-f61a6b8a36a6", "citing_paper": {"title": "Situation-Specific Multimodal Feature Adaptation", "year": 2021, "authors": ["\u00d6zge Alac"]}, "text": ["In", "many", "cases,", "even", "the", "object", "names", "are", "verbally", "omitted", "in", "spoken", "utterances.", "This", "kind", "of", "implicit", "commands", "like", "\"I", "prefer", "red,", "can", "you", "open", "a", "bottle", "and", "bring", "it", "to", "me?\",", "require", "the", "hearer", "to", "reconstruct", "the", "underlying", "intention", "\"Open", "a", "bottle", "of", "red", "wine,", "and", "bring", "the", "bottle\"", "(Gundel et al., 2012).", "Alternatively,", "depending", "on", "the", "spatial", "arrangements", "of", "the", "agents", "and", "the", "objects", "in", "the", "room,", "the", "intention", "of", "the", "speaker", "might", "be", "slightly", "different", "and", "more", "complex.", "For", "example,", "when", "the", "empty", "glasses", "are", "closer", "to", "the", "listener", "than", "the", "speaker,", "the", "interpretation", "might", "be:", "\"Open", "a", "bottle", "of", "red", "wine,", "pour", "the", "wine", "into", "one", "of", "the", "empty", "wine", "glasses", "and", "bring", "the", "glass", "of", "wine", "to", "me\".", "Expressing", "this", "intention", "explicitly", "most", "often", "results", "in", "unwieldy", "utterances."], "cited_papers": [{"title": "Underspecification of cognitive status in reference production: Some empirical predictions", "year": "2012", "authors": ["Jeanette Gundel", "Nancy Hedberg", "Ron Zacharski"]}], "target_citation_location": 50, "citation_locations": [50], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "5acbdf1e-89de-4a5f-89de-2184a4337623", "citing_paper": {"title": "Memory-efficient Transformers via Top-k Attention", "year": 2021, "authors": ["Ankit Gupta", "Guy Dar", "Shaya Goodman", "David Ciprut", "Jonathan Berant", "Ibm Research"]}, "text": ["Top-k", "attention", "also", "reduces", "memory", "consumption", "in", "Transformer", "feed-forward", "layers,", "by", "casting", "this", "layer", "into", "the", "familiar", "query-key-value", "framework", "using", "ReLU", "instead", "of", "the", "row-wise", "softmax", "(Sukhbaatar et al., 2019).", "This", "is", "specifically", "appealing", "in", "models", "such", "as", "T5", "(Raffel et al., 2020)", "and", "GPT-3", "(Brown et al., 2020),", "where", "for", "short", "inputs,", "the", "memory", "consumption", "is", "dominated", "by", "the", "feed-forward", "layers,", "as", "the", "number", "of", "keys,", "corresponding", "to", "the", "feedforward", "hidden", "dimension", "size,", "is", "as", "large", "as", "65K.", "Conversely,", "methods", "that", "rely", "on", "random", "feature", "approximations", "of", "attention,", "such", "as", "Performer", "(Choromanski et al., 2021)", "and", "RFA", "(Peng et al., 2021)", "do", "not", "admit", "an", "efficient", "approximation", "for", "the", "ReLU", "activation", "(Yehudai and Shamir, 2019)."], "cited_papers": [{"title": "On the power and limitations of random features for understanding neural networks", "year": "2019", "authors": ["Gilad Yehudai", "Ohad Shamir"]}], "target_citation_location": 97, "citation_locations": [26, 36, 39, 83, 86, 97], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "5b8676d6-a422-4307-8bde-f035eefc3119", "citing_paper": {"title": "Public Apologies in India -Semantics, Sentiment and Emotion", "year": 2018, "authors": ["Sangeeta Shukla", "Rajita Shukla"]}, "text": ["Sorry", "(adjective)", "-is", "a", "kind", "of", "feeling,", "with", "a", "high", "negative", "score", "and", "emotion", "label", "of", "regret-sorrow.", "This", "keyword", "can", "be", "effective", "in", "situations", "where", "emotions", "and", "sentiments", "are", "strongly", "involved.", "Its", "use", "can", "also", "make", "the", "communication", "sound", "like", "a", "heartfelt", "apology.", "Also,", "to", "be", "noted", "is", "the", "fact", "that", "though", "the", "adjective", "sorry", "is", "found", "to", "be", "the", "most", "commonly-used", "form", "in", "different", "spoken", "corpora.", "(Harrison, 2013", "),", "yet", "in", "our", "data,", "the", "word", "sorry", "has", "a", "higher", "occurrence", "in", "written", "apologies", "given", "by", "individuals-in-a", "role", "and", "organizations."], "cited_papers": [{"title": null, "year": null, "authors": ["unknown"]}], "target_citation_location": 67, "citation_locations": [67], "citation_type": "single", "annotations": [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]]}
{"id": "60abda72-59f6-450f-a0cf-255eac9bfca8", "citing_paper": {"title": "Multitasking Framework for Unsupervised Simple Definition Generation", "year": 2022, "authors": ["Cunliang Kong", "Yun Chen", "Hengyuan Zhang", "Liner Yang", "Erhong Yang"]}, "text": ["Baselines", "We", "compare", "the", "SimpDefiner", "with", "generation-simplification", "pipelines.", "We", "first", "employ", "LOG-CaD", "(Ishiwatari et al., 2019)", "and", "MASS", "(Song et al., 2019)", "models", "to", "generate", "definitions,", "and", "then", "employ", "ACCESS", "(Martin et al., 2019)", "and", "MUSS", "(Martin et al., 2020)", "models", "to", "simplify", "them.", "Thus,", "we", "have", "four", "different", "pipeline", "baselines.", "Since", "these", "models", "are", "not", "available", "in", "Chinese,", "we", "only", "apply", "these", "pipelines", "to", "English", "datasets.", "For", "the", "Chinese", "SDG", "task,", "we", "specially", "pretrained", "a", "MASS-ZH", "model", "from", "scratch", "using", "the", "Chinese", "Gigaword", "Fifth", "Edition", "3", "corpus.", "Note", "that", "we", "set", "the", "learning", "rate", "to", "3e-4,", "warmup", "steps", "to", "500", "when", "fine-tuning", "both", "MASS", "and", "MASS-ZH."], "cited_papers": [{"title": "Controllable sentence simplification. CoRR, abs", "year": "1910", "authors": ["Louis Martin", "Beno\u00eet Sagot"]}], "target_citation_location": 24, "citation_locations": [12, 15, 24, 27], "citation_type": "single", "annotations": [[0, 3, 3, 3, 3, 3, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 3, 3, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, 3, 3, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "64b186ee-8965-47e8-a244-f0c237228eec", "citing_paper": {"title": "NEW TABULAR ALGORITHMS FOR LIG PARSING", "year": 2000, "authors": ["Mi G Uel Alonso Jor G E Grana", "Manuel Vilares"]}, "text": ["Parsing", "schemata", "are", "closely", "related", "to", "grammatical", "deduction", "systems", "[14],", "where", "items", "are", "called", "formula", "schemata,", "deduction", "steps", "are", "inference", "rules,", "hypothesis", "are", "axioms", "and", "final", "items", "are", "goal", "formulas."], "cited_papers": [{"title": "Principles and implementation of deductive parsing", "year": "1995", "authors": ["S Shieber", "Y Schabes", "F Pereira"]}], "target_citation_location": 9, "citation_locations": [9], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "69496a23-0c09-45fb-80db-03c33b2816f1", "citing_paper": {"title": "On the Usability of Transformers-based models for a French Question-Answering task", "year": 2021, "authors": ["Oralie Cattan", "Christophe Servan", "Sophie Rosset"]}, "text": ["The", "Transformer", "architecture", "(Vaswani et al., 2017)", "is", "based", "on", "a", "stack", "of", "encoder-decoder", "blocks,", "composed", "at", "a", "high", "level", "of", "forward", "propagation", "networks", "and", "multi-headed", "self-attention", "operations."], "cited_papers": [{"title": "Attention is all you need", "year": "2017", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"]}], "target_citation_location": 3, "citation_locations": [3], "citation_type": "single", "annotations": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "6bdc9ad7-e9a6-43fb-8d13-3644f54f8271", "citing_paper": {"title": "USST's System for AutoSimTrans 2022", "year": 2022, "authors": ["Jiahui Zhu", "Jun Yu"]}, "text": ["In", "this", "paper", "we", "describe", "our", "Chinese-to-English", "simultaneous", "translation", "system,", "which", "uses", "a", "deep", "Transformer", "to", "improve", "translation", "quality", "and", "adopts", "wait-k", "policy", "(Ma et al., 2018)", "to", "reduce", "latency.", "Besides,", "for", "better", "domain", "adaption,", "we", "combined", "mixed", "fine-tuning", "(Chu et al., 2017)", "with", "in-domain", "data", "filtering", "(Moore", "and", "Lewis,", "2010,", "Ng et al., 2019)", "and", "proposed", "a", "new", "domain", "adaption", "method", "called", "\"in-domain", "mixed", "fine-tuning\",", "which", "is", "empirically", "more", "effective", "than", "fine-tuning", "and", "mixed", "fine-tuning."], "cited_papers": [{"title": "An empirical comparison of domain adaptation methods for neural machine translation", "year": "2017", "authors": ["Chenhui Chu", "Raj Dabre", "Sadao Kurohashi"]}], "target_citation_location": 36, "citation_locations": [23, 36, 45], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "72f9e739-6b8b-4216-b7fb-3e7fbef45a94", "citing_paper": {"title": "A Fine-Grained Analysis of BERTScore", "year": 2021, "authors": ["Michael Hanna", "Ond\u0159ej Bojar"]}, "text": ["What,", "then,", "is", "known", "about", "BERT,", "and", "its", "syntactic", "and", "semantic", "capabilities?", "Of", "the", "two,", "it", "is", "syntax", "that", "BERT", "is", "most", "widely", "claimed", "to", "capture", "within", "its", "internal", "representations:", "Hewitt and Manning (2019)", "use", "structural", "probing", "to", "find", "dependency", "trees", "in", "BERT's", "vector", "geometry,", "while", "Tenney et al. (2019)", "use", "probing", "to", "find", "part", "of", "speech", "tags", "and", "dependency", "arc", "labels,", "among", "other", "types", "of", "syntactic", "information.", "Analysis", "of", "BERT's", "attention", "has", "shown", "that", "certain", "heads", "attend", "to", "not", "only", "relevant", "linguistic", "units", "such", "as", "determiners", "of", "nouns", "and", "coreferent", "mentions", "(Clark et al., 2019),", "but", "also", "dependency", "relations", "(Htut et al., 2019)."], "cited_papers": [{"title": "Do attention heads in bert track syntactic dependencies? ArXiv, abs", "year": "1911", "authors": ["Jason Phu Mon Htut", "Shikha Phang", "Samuel Bordia", "unk Bowman"]}], "target_citation_location": 91, "citation_locations": [30, 43, 86, 91], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1]]}
{"id": "74828d16-0dbc-4e66-a58e-df907b875e27", "citing_paper": {"title": "Embed More Ignore Less (EMIL): Exploiting Enriched Representations for Arabic NLP", "year": 2020, "authors": ["Ahmed Younes", "Julie Weeds"]}, "text": ["As", "discussed", "elsewhere,", "e.g.,", "by", "Farghaly and Shaalan (2009),", "Arabic", "is", "rich", "morphology", "language", "with", "complex", "grammar", "structure", "which", "poses", "extra", "challenges", "to", "systems", "when", "considering", "Arabic", "text", "as", "input.", "Habash (2010)", "discussed", "the", "script", "differences", "such", "as", "letter", "shaping,", "script", "direction", "(right", "to", "left)", "and", "obligatory", "ligatures.", "Also", "Habash et al. (2013)", "discussed", "the", "lack", "of", "standard", "orthographies:", "e.g.,", "and", "both", "mean", "(gram).", "One", "of", "the", "major", "challenges", "in", "Arabic", "NER", "is", "the", "lack", "of", "capitalization", "(Shaalan, 2014, Benajiba et al., 2008a).", "Shaalan (2014)", "also", "discussed", "the", "agglutinative", "nature", "of", "the", "Arabic", "language", "where", "new", "words", "and", "sometimes", "even", "sentences", "can", "be", "derived", "by", "adding", "affixes", "and", "clitics", "to", "Arabic", "words,", "making", "Arabic", "a", "morphologically", "rich", "language."], "cited_papers": [{"title": "A survey of arabic named entity recognition and classification", "year": "2014", "authors": ["Khaled Shaalan"]}], "target_citation_location": 71, "citation_locations": [5, 27, 45, 70, 71], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "74eadcdf-52d9-4017-a121-fa0f4bcb8a4a", "citing_paper": {"title": "Decoding Part-of-Speech from Human EEG Signals", "year": 2022, "authors": ["Alex Murphy", "Bernd Bohnet", "Ryan Mcdonald", "Uta Noppeney"]}, "text": ["The", "decoding", "analyses", "used", "linear", "support", "vector", "machines", "(SVM)", "(Chang and Lin, 2011)", "and", "Transformers,", "which", "can", "capture", "complex", "interactions", "of", "EEG", "data", "across", "time", "points.", "All", "classifiers", "were", "trained", "on", "the", "EEG", "training", "data,", "assessed", "on", "the", "dev", "set", "and", "scored", "on", "the", "independent", "test", "set.", "Hyperparameters", "and", "early", "stopping", "were", "selected", "based", "on", "the", "dev", "set.", "We", "assessed", "linear", "SVMs", "and", "Transformers", "on", "the", "dev", "set", "using", "10", "different", "random", "seed", "points.", "We", "show", "mean", "classification", "accuracy", "with", "68%", "confidence", "intervals", "(CI)", "over", "those", "10", "replications", "on", "the", "dev", "(Table", "4,", "Figure", "3)", "resp.", "test", "set", "(Figure", "2,", "4,", "5).", "We", "compute", "statistics", "on", "test", "set", "classification", "responses", "from", "the", "model", "that", "scored", "the", "highest", "on", "the", "dev", "set", "(e.g.", "binomial", "or", "Wilcoxon", "signed", "rank", "tests)."], "cited_papers": [{"title": "LIBSVM: A library for support vector machines", "year": "2011", "authors": ["Chih-Chung Chang", "Chih-Jen Lin"]}], "target_citation_location": 9, "citation_locations": [9], "citation_type": "single", "annotations": [[2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "75495dd6-da3f-4e54-9af8-66e4f201c6f6", "citing_paper": {"title": "Decoding Part-of-Speech from Human EEG Signals", "year": 2022, "authors": ["Alex Murphy", "Bernd Bohnet", "Ryan Mcdonald", "Uta Noppeney"]}, "text": ["We", "performed", "decoding", "based", "on", "(i)", "EEG", "for", "single-trials", "(i.e.", "no", "averaging),", "(ii)", "EEG", "averaged", "across", "three", "and", "(iii)", "ten", "trials.", "Averaging", "EEG", "signals", "across", "trials", "increases", "the", "signal", "to", "noise", "ratio", "of", "the", "'samples'", "(Grootswagers et al., 2017, Guggenmos et al., 2018, Roy et al., 2019, Tuckute et al., 2019),", "but", "ignores", "true", "variability", "across", "EEG", "data", "from", "different", "words", "of", "the", "same", "category", "(M\u00fcnte et al., 2001).", "For", "training", "(resp.", "dev)", "set", "we", "generated", "the", "same", "number", "of", "samples", "for", "3", "and", "10", "trial", "averages", "as", "for", "the", "single", "trial", "test", "(resp.", "dev)", "sets", "via", "boostrapping.", "For", "the", "test", "set,", "we", "averaged", "data", "without", "replacement,", "so", "that", "examples", "can", "be", "entered", "as", "independent", "data", "points", "in", "statistical", "tests.", "Hence,", "the", "number", "of", "samples", "in", "the", "test", "set", "(but", "not", "in", "the", "training", "or", "dev", "sets)", "is", "smaller", "for", "3", "and", "10", "trial", "averages", "than", "single", "trials", "(Table", "3)."], "cited_papers": [{"title": "Differences in brain potentials to open and closed class words: class and frequency effects", "year": "2001", "authors": ["unk Thomas F M\u00fcnte", "M Bernardina", "Helga Wieringa", "Andras Weyerts", "Mike Szentkuti", "S\u00f6nke Matzke", "unk Johannes"]}], "target_citation_location": 50, "citation_locations": [35, 50], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "76f07cfc-82b7-40a5-8a9b-0a6229571ee3", "citing_paper": {"title": "Classifying Argumentative Relations Using Logical Mechanisms and Argumentation Schemes", "year": 2021, "authors": ["Yohan Jo", "Seojin Bang", "Chris Reed", "Eduard Hovy"]}, "text": ["We", "first", "compile", "rules", "that", "specify", "evidence", "for", "the", "support", "and", "attack", "relations", "between", "claim", "C", "and", "statement", "S", "(Table", "1).", "2", "These", "rules", "are", "combined", "via", "PSL", "(Bach et al., 2017)", "to", "estimate", "the", "optimal", "relation", "between", "C", "and", "S.", "3", "We", "will", "describe", "individual", "rules", "in", "four", "categories:", "factual", "consistency,", "sentiment", "coherence,", "causal", "relation,", "and", "normative", "relation,", "followed", "by", "additional", "chain", "rules."], "cited_papers": [{"title": "Hinge-Loss Markov random fields and probabilistic soft logic", "year": "2017", "authors": ["H Stephen", "Matthias Bach", "Bert Broecheler", "Lise Huang", "unk Getoor"]}], "target_citation_location": 28, "citation_locations": [28], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "7883a3d7-869f-4a49-bea3-d25a6ea43a6a", "citing_paper": {"title": "Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data", "year": 2022, "authors": ["Colin Leong", "Daniel Whitenack"]}, "text": ["Our", "approach", "is", "inspired", "by", "the", "fact", "that", "many", "languages", "are", "primarily", "oral,", "with", "writing", "systems", "that", "represent", "spoken", "sounds.", "We", "convert", "both", "text", "and", "audio", "into", "single", "common", "representation", "of", "sounds,", "or", "\"phones,\"", "represented", "using", "the", "International", "Phonetic", "Alphabet,", "or", "IPA.", "Then,", "we", "perform", "both", "language", "model", "pre-training", "and", "the", "training", "of", "models", "for", "downstream", "tasks", "in", "this", "phonetic", "representation.", "Well-tested", "architectures,", "such", "as", "BERT-style", "transformer", "models", "(Vaswani et al., 2017),", "are", "thus", "flexibly", "extended", "to", "either", "speech", "or", "audio", "data."], "cited_papers": [{"title": null, "year": "2017", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan Gomez", "Lukasz Kaiser", "Illia Polosukhin"]}], "target_citation_location": 68, "citation_locations": [68], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "836b2436-e729-4470-8a35-8272c2525557", "citing_paper": {"title": "Codenames as a Game of Co-occurrence Counting", "year": 2022, "authors": ["R\u00e9ka Cserh\u00e1ti", "Istv\u00e1n Koll\u00e1th", "Andr\u00e1s Kicsi", "G\u00e1bor Berend"]}, "text": ["Among", "the", "similarity", "measures", "of", "Koyyalagunta et al. (2021),", "generally", "FastText", "seems", "to", "be", "the", "best", "model.", "So,", "following", "the", "cited", "work,", "we", "create", "a", "relatedness", "matrix", "based", "on", "the", "cosine", "similarity", "of", "FastText", "vectors.", "That", "is,", "if", "v", "i,", "v", "j", "are", "vectors", "corresponding", "to", "words", "w", "i,", "w", "j,", "thens", "F", "(w", "i,", "w", "j)", "=", "cos(v", "i,", "v", "j", ").For", "comparability", "with", "the", "other", "methods,", "we", "train", "our", "FastText", "models", "on", "the", "above", "corpora", "for", "English", "and", "Hungarian", "in", "300", "dimensions,", "using", "window", "size", "10."], "cited_papers": [{"title": "Playing codenames with language graphs and word embeddings", "year": "2021", "authors": ["Divya Koyyalagunta", "Anna Sun", "Rachel Draelos", "Cynthia Rudin"]}], "target_citation_location": 5, "citation_locations": [5], "citation_type": "single", "annotations": [[2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "85111c6c-80fe-427f-9bf8-53684b3edb0d", "citing_paper": {"title": "TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation", "year": 2021, "authors": ["Hansi Hettiarachchi", "Tharindu Ranasinghe"]}, "text": ["i", "[CLS]", "Strategy", "-This", "is", "the", "default", "sentence", "pair", "classification", "architecture", "with", "transformers", "(Devlin et al., 2019)", "where", "the", "two", "sentences", "are", "concatenated", "with", "a", "[SEP]", "token", "and", "passed", "through", "a", "transformer", "model.", "Then", "the", "output", "of", "the", "[CLS]", "token", "is", "fed", "into", "a", "softmax", "layer", "to", "predict", "the", "labels", "(Figure", "1)."], "cited_papers": [{"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "year": "2019", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"]}], "target_citation_location": 13, "citation_locations": [13, 35], "citation_type": "single", "annotations": [[0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "8512c562-6e19-44ac-80db-6318b1dd292b", "citing_paper": {"title": "Word Formation in Computational Linguistics", "year": 2002, "authors": ["Pius Ten Hacken"]}, "text": ["The", "central", "position", "in", "the", "bow-tie", "model", "is", "taken", "by", "the", "lexeme.", "The", "notion", "of", "lexeme", "in", "WM", "is", "similar", "to", "the", "one", "adopted", "by", "Matthews (1974),", "Aronoff (1994)", "and", "others.", "A", "lexeme", "is", "a", "word", "considered", "as", "an", "inflectional", "paradigm.", "Fig.", "1", "highlights", "two", "mappings", "involving", "the", "lexeme:"], "cited_papers": [{"title": null, "year": "1994", "authors": ["Mark Aronoff"]}], "target_citation_location": 26, "citation_locations": [25, 26], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "8b4e05f4-a98c-46df-a9e9-e123868ea8d1", "citing_paper": {"title": "Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models", "year": 2022, "authors": ["Tianlu Wang", "Rohit Sridhar", "Diyi Yang", "Xuezhi Wang"]}, "text": ["Models.", "We", "use", "the", "attention", "scores", "over", "BERT", "(Devlin et al., 2019)", "based", "classification", "models", "as", "they", "have", "achieved", "the", "state-of-art", "performance.", "Note", "that", "our", "proposed", "framework", "can", "also", "be", "easily", "extended", "to", "models", "with", "different", "architectures."], "cited_papers": [{"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "year": "2019", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"]}], "target_citation_location": 8, "citation_locations": [8], "citation_type": "single", "annotations": [[0, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "8d948e21-1df7-4913-a73f-050de2c7758a", "citing_paper": {"title": "Unsupervised Representation Disentanglement of Text: An Evaluation on Synthetic Datasets", "year": 2021, "authors": ["Lan Zhang", "Victor Prokhorov", "Ehsan Shareghi"]}, "text": ["\u03b2-VAE", "(Higgins et al., 2017)", "adds", "a", "hyperparameter", "\u03b2", "to", "control", "the", "regularisation", "from", "the", "KL-term", "via", "the", "following", "objective", "function:E", "q", "\u03c6", "(z|x)", "log", "p", "\u03b8", "(x|z)", "\u2212", "\u03b2D", "KL", "(q", "\u03c6", "(z|x),", "p(z))Reconstructing", "under", "\u03b2-VAE", "(with", "the", "right", "value", "of", "\u03b2)", "framework", "encourages", "encoding", "data", "points", "on", "a", "set", "of", "representational", "axes", "on", "which", "nearby", "points", "along", "those", "dimensions", "are", "also", "close", "in", "original", "data", "space", "(Burgess et al., 2018).", "(Burgess et al., 2018)", "extends", "\u03b2-VAE", "via", "constraint", "optimisation:"], "cited_papers": [{"title": "Understanding disentangling in \u03b2-vae", "year": "2018", "authors": ["Christopher Burgess", "Irina Higgins", "Arka Pal", "Lo\u00efc Matthey", "Nick Watters", "Guillaume Desjardins", "Alexander Lerchner"]}], "target_citation_location": 65, "citation_locations": [1, 65, 66], "citation_type": "single", "annotations": [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}
{"id": "95562879-f0de-43b1-a980-2e65636dba80", "citing_paper": {"title": "The DCU Machine Translation Systems for IWSLT 2011", "year": 2011, "authors": ["Pratyush Banerjee", "Hala Almaghout", "Sudip Naskar", "Johann Roturier", "Jie Jiang", "Andy Way", "Josef Van Genabith"]}, "text": ["First,", "each", "target-side", "sentence", "from", "the", "parallel", "corpus", "is", "supertagged", "by", "assigning", "the", "best", "sequence", "of", "CCG", "supertags", "to", "its", "words.", "Next,", "phrase", "pairs", "are", "extracted", "from", "the", "parallel", "corpus", "according", "to", "the", "PBSMT", "phrase", "extraction", "method", "[2].", "Then,", "each", "phrase", "pair", "is", "assigned", "a", "CCG", "category", "that", "results", "from", "combining", "the", "supertags", "of", "the", "words", "of", "the", "target-side", "phrase", "using", "CCG", "combinatory", "operators.", "In", "case", "phrase", "parsing", "fails", "to", "find", "a", "single", "CCG", "category", "for", "the", "phrase,", "a", "general", "X", "label", "is", "assigned", "to", "the", "phrase.", "Finally,", "hierarchical", "rules", "are", "extracted", "from", "sentencepairs", "according", "to", "the", "same", "basic", "HPB", "SMT", "rule", "extraction", "method", "[3]."], "cited_papers": [{"title": "Statistical phrasebased translation", "year": "2003", "authors": ["P Koehn", "F Och", "D Marcu"]}], "target_citation_location": 37, "citation_locations": [37, 104], "citation_type": "single", "annotations": [[0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "982fa80a-c8de-4a55-953f-e0f7c47a75b1", "citing_paper": {"title": "BRIO: Bringing Order to Abstractive Summarization", "year": 2022, "authors": ["Yixin Liu", "Pengfei Liu", "Dragomir Radev", "Graham Neubig"]}, "text": ["We", "further", "perform", "some", "in-depth", "analyses", "from", "diverse", "perspectives", "on", "the", "CNNDM", "dataset", "to", "gain", "more", "insights", "into", "our", "proposed", "method.", "3:", "Model", "performance", "with", "different", "\u03b3", "coefficients", "weighting", "the", "contrastive", "loss", "(Eq.", "10)", "on", "CNNDM.", "BRIO-Ctr", "is", "trained", "with", "the", "contrastive", "loss", "only,", "which", "no", "longer", "preserves", "its", "generation", "ability.", "We", "report", "its", "performance", "when", "it", "is", "used", "as", "an", "evaluation", "model", "to", "select", "from", "candidate", "summaries.", "R-1/2/L", "are", "the", "ROUGE-1/2/L", "F1", "scores.", "Coefficients", "of", "the", "Multi-Task", "Loss", "The", "multitask", "loss", "(Eq.", "10)", "used", "to", "train", "our", "model", "contains", "two", "parts:", "the", "cross-entropy", "loss", "and", "the", "contastive", "loss.", "As", "shown", "in", "Tab.", "3,", "as", "the", "weight", "of", "the", "contrastive", "loss", "(\u03b3)", "increases,", "the", "model's", "performance", "improves.", "However,", "the", "cross-entropy", "loss", "is", "still", "necessary", "to", "preserve", "the", "model's", "ability", "as", "a", "generation", "model.", "We", "argue", "that", "this", "is", "because", "the", "token", "level", "accuracy", "is", "still", "important", "during", "the", "autoregressive", "generation", "process,", "where", "the", "individual", "tokens", "are", "predicted", "sequentially.", "In", "addition,", "we", "also", "found", "that", "the", "model", "tends", "to", "achieve", "the", "best", "performance", "(w.r.t", "the", "ROUGE", "scores", "on", "the", "development", "set)", "faster", "with", "a", "higher", "\u03b3.", "Specifically,", "it", "requires", "less", "than", "one", "entire", "epoch", "to", "achieve", "the", "best", "performance", "on", "CNNDM,", "making", "our", "approach", "an", "efficient", "fine-tuning", "method.Coefficient", "(\u03b3)", "R-1", "R-2", "R-L", "0", "(Generation-Finetuning", "as", "a", "Loop", "Since", "the", "fine-tuned", "model", "(BRIO-Mul)", "is", "still", "able", "to", "gen-", "erate,", "we", "can", "use", "it", "to", "generate", "a", "new", "set", "of", "candidates", "in", "the", "same", "way", "as", "we", "used", "the", "pre-trained", "BART", "model,", "and", "continue", "fine-tuning", "it", "on", "this", "newly", "created", "set", "of", "candidates", "(Och, 2003).", "Fig.", "2", "illustrates", "this", "iterative", "process.", "The", "results", "shown", "in", "Tab.", "4", "illustrate", "that", "this", "new", "model", "(BRIO-Loop)", "outperforms", "BRIO-Mul.", "Besides,", "the", "model", "reached", "the", "best", "performance", "very", "quickly,", "showing", "the", "potential", "of", "adopting", "our", "method", "in", "an", "online", "framework", "where", "the", "new", "candidates", "are", "dynamically", "generated", "from", "the", "current", "model.", "We", "leave", "this", "direction", "for", "future", "work."], "cited_papers": [{"title": "Minimum error rate training in statistical machine translation", "year": "2003", "authors": ["F Och"]}], "target_citation_location": 260, "citation_locations": [260], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "993825f3-a8eb-427e-a7e4-b290611d5dca", "citing_paper": {"title": "Public Apologies in India -Semantics, Sentiment and Emotion", "year": 2018, "authors": ["Sangeeta Shukla", "Rajita Shukla"]}, "text": ["Of", "particular", "interest", "to", "us", "were", "the", "keywords", "apology", "(noun)", "and", "regret", "(verb).", "We", "compare", "the", "SentiWordNet", "scores", "and", "the", "WordNet-Affect", "labels", "of", "these", "two", "keywords.", "While", "emotion", "is", "defined", "as", "a", "relatively", "brief", "episode", "of", "response", "to", "the", "evaluation", "of", "an", "external", "or", "internal", "event", "as", "being", "of", "major", "significance.", "(such", "as", "angry,", "sad,", "joyful,", "fearful,", "ashamed,", "proud,", "elated,", "desperate),", "a", "sentiment", "is", "the", "positive", "or", "negative", "orientation", "that", "a", "person", "expresses", "toward", "some", "object", "or", "situation", "(Scherer, 2000).", "Thus,", "we", "can", "posit", "that", "the", "word", "apology", "which", "has", "no", "emotion", "label,", "has", "no", "or", "weak", "emotional", "connect,", "which", "also", "aligns", "with", "our", "conclusion", "about", "the", "keyword", "apologize.", "In", "contrast,", "the", "verb", "regret", "helps", "to", "effectively", "communicate", "the", "emotion", "of", "repentance.", "Looking", "at", "the", "sentiment", "associated", "with", "these", "words,", "we", "conclude", "that", "the", "mental", "attitude", "of", "the", "writer", "is", "more", "objective", "to", "the", "situation", "in", "using", "the", "verb", "regret", "while", "it", "is", "highly", "negative", "in", "the", "case", "of", "the", "usage", "of", "the", "word", "apology.", "This", "further", "implies", "that", "a", "high", "negative", "sentiment", "score", "means", "that", "the", "writer", "of", "the", "apology", "realizes", "the", "gravity", "of", "the", "transgression", "and", "to", "some", "extent", "admits", "to", "the", "wrong", "done.", "However,", "a", "high", "objective", "score", "implies", "the", "writer", "taking", "a", "neutral", "stance", "to", "the", "situation", "and", "not", "necessarily", "admitting", "to", "any", "wrongdoing."], "cited_papers": [{"title": "Psychological models of emotion. The neuropsychology of emotion", "year": "2000", "authors": ["R Klaus", "unk Scherer"]}], "target_citation_location": 78, "citation_locations": [78], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "9991da19-0609-4fa7-966e-fa092df74c41", "citing_paper": {"title": "Embed More Ignore Less (EMIL): Exploiting Enriched Representations for Arabic NLP", "year": 2020, "authors": ["Ahmed Younes", "Julie Weeds"]}, "text": ["Similar", "to", "other", "NLP", "applications,", "the", "most", "recent", "attention", "in", "this", "area", "has", "been", "on", "neural", "approaches.", "Wang et al. (2015)", "demonstrated", "an", "effective", "way", "of", "applying", "a", "Bi-LSTM", "to", "the", "POS", "tagging", "task,", "achieving", "97.4%", "on", "the", "English", "Penn", "Treebank.", "Darwish et al. (2017)", "used", "a", "Bi-LSTM", "in", "their", "work", "on", "Arabic", "POS", "tagging,", "achieving", "95.50%.", "Alrajhi and ELAffendi (2019)", "used", "the", "LSTM-RNN", "model", "on", "the", "Quranic", "Arabic", "Corpus", "(QAC).", "They", "reported", "accuracy", "of", "99.76%", "at", "the", "word", "level", "and", "99.18%", "at", "the", "morpheme", "level.", "They", "also", "compared", "their", "system", "against", "the", "Word2Vec", "POS", "tagger,", "for", "which", "they", "reported", "accuracy", "levels", "of", "97.33%", "and", "99.55%", "for", "words", "and", "morphemes", "respectively."], "cited_papers": [{"title": "Part-of-speech tagging with bidirectional long short-term memory recurrent neural network", "year": "2015", "authors": ["Peilu Wang", "Yao Qian", "K Frank", "Lei Soong", "Hai He", "unk Zhao"]}], "target_citation_location": 17, "citation_locations": [17, 38, 51], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "9f9b269e-0777-498f-90d8-6fb8618b6127", "citing_paper": {"title": "Translation of Multiword Expressions Using Parallel Suffix Arrays", "year": 2006, "authors": ["Paul Mcnamee", "James Mayfield"]}, "text": ["To", "score", "translation", "candidates", "we", "will", "compute", "global", "and", "local", "frequencies", "of", "occurrence", "of", "substrings", "using", "the", "method", "described", "by", "Yamamoto and Church (2001)", "which", "is", "based", "on", "suffix", "arrays", "and", "longest", "common", "prefix", "arrays."], "cited_papers": [{"title": "Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus", "year": "2001", "authors": ["M Yamamoto", "K Church"]}], "target_citation_location": 20, "citation_locations": [20], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "9fb94bb8-ea71-467f-8b4d-5c0c747db6ab", "citing_paper": {"title": "Codenames as a Game of Co-occurrence Counting", "year": 2022, "authors": ["R\u00e9ka Cserh\u00e1ti", "Istv\u00e1n Koll\u00e1th", "Andr\u00e1s Kicsi", "G\u00e1bor Berend"]}, "text": ["One", "can", "create", "a", "graph", "(Bel-Enguix,", "2014),", "and", "its", "transformation", "to", "a", "word", "embedding", "model", "(Bel-Enguix et al., 2019),", "specifically", "for", "modeling", "associations,", "but", "these", "require", "difficult-to-obtain", "association", "data.", "This", "would", "be", "a", "high", "resource", "requirement", "and", "would", "make", "it", "difficult", "to", "apply", "such", "methods", "in", "various", "languages."], "cited_papers": [{"title": null, "year": "2019", "authors": ["Gemma Bel-Enguix", "Helena G\u00f3mez-Adorno", "Jorge Reyes-Maga\u00f1a", "Gerardo Sierra"]}], "target_citation_location": 15, "citation_locations": [15], "citation_type": "single", "annotations": [[2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "a1020816-710e-453b-8815-1353eec5dce3", "citing_paper": {"title": "A Semi-Supervised Approach to Detect Toxic Comments", "year": 2021, "authors": ["Ghivvago Saraiva", "Rafael Anchi\u00eata", "Francisco Neto", "Raimundo Moura"]}, "text": ["We", "modeled", "toxic", "comments", "detection", "as", "a", "heterogeneous", "network", "since", "this", "network", "type", "contains", "abundant", "information", "with", "structural", "relations", "(edges)", "among", "multi-typed", "nodes", "as", "well", "as", "unstructured", "content", "associated", "with", "each", "node", "(Zhang et al., 2019).", "Graph", "structures", "have", "been", "used", "for", "several", "tasks,", "such", "as:", "topic", "model,", "name", "disambiguation,", "scientific", "impact", "measurement,", "and", "others,", "obtaining", "good", "results", "(King et al., 2014).", "We", "defined", "a", "undirected", "and", "weighted", "graph", "as", "G", "=", "(V,", "E,", "W", "),", "where", "V", "is", "a", "set", "of", "vertices", "V", "=", "{v", "1,", "...,", "v", "n", "},", "E", "indicates", "a", "set", "of", "edges", "E", "=", "{e", "1,", "...,", "e", "n", "},", "and", "W", "is", "a", "weighted", "adjacency", "matrix,", "in", "which", "W", "i,j", "denotes", "the", "weight", "of", "an", "edge", "between", "nodes", "i", "and", "j.", "We", "defined", "two", "node", "types:", "token", "and", "sentence", "and", "two", "constraints", "not", "allowing", "link", "among", "tokens", "nodes", "or", "among", "sentences", "nodes."], "cited_papers": [{"title": "Heterogeneous graph neural network", "year": "2019", "authors": ["Chuxu Zhang", "Dongjin Song", "Chao Huang", "Ananthram Swami", "Nitesh Chawla"]}], "target_citation_location": 32, "citation_locations": [32, 55], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "a4c86e92-5840-4f85-83fd-9f7d187e0e95", "citing_paper": {"title": "Harmonised large-scale syntactic/semantic lexicons: a European multilingual infrastructure", "year": 1999, "authors": ["Nicoletta Calzolari", "Antonio Zampolli"]}, "text": ["Encoding:", "All", "the", "information", "explicitly", "represented", "in", "the", "source", "texts", "is", "encoded", "following", "essentially", "the", "CES", "(Corpus", "Encoding", "Standard)", "designed", "by", "EAGLES,", "on", "the", "basis", "of", "the", "Text", "Encoding", "Initiative", "(TEI)", "guidelines", "(Ide et al. 1996).", "250,000", "running", "words", "are", "tagged", "at", "the", "morphosyntactic", "level,", "following", "the", "EAGLES", "guidelines", "(Leech, Wilson 1996 : Monachini, Calzolari 1996),", "instantiated", "by", "each", "PAROLE", "partner", "for", "his", "own", "language."], "cited_papers": [{"title": "Morphosyntactic annotation", "year": "1996", "authors": ["G Leech", "A Wilson"]}, {"title": "Synopsis and comparison of morphosyntactic phenomena encoded in lexicons and corpora. A common proposal and applications to European languages", "year": "1996", "authors": ["M Monachini", "N Calzolari"]}], "target_citation_location": 46, "citation_locations": [32, 46], "citation_type": "group", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "abc5c219-9a85-44b7-9201-b68b00b40c41", "citing_paper": {"title": "TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration", "year": 2021, "authors": ["Mokanarangan Thayaparan", "Marco Valentino", "Peter Jansen", "Dmitry Ustalov"]}, "text": ["Similarly", "to", "the", "previous", "editions", "of", "the", "shared", "task", "(Jansen and Ustalov, 2019, 2020)", "The", "percentage", "of", "overlap", "is", "computed", "by", "dividing", "the", "number", "of", "shared", "terms", "between", "question-answer", "pair", "and", "a", "fact", "by", "the", "total", "number", "of", "unique", "terms.", "To", "evaluate", "the", "systems", "in", "the", "most", "challenging", "setting,", "we", "gradually", "decrease", "the", "value", "of", "T", "down", "to", "0."], "cited_papers": [{"title": null, "year": null, "authors": ["unknown"]}, {"title": "A Study of Automatically Acquiring Explanatory Inference Patterns from Corpora of Explanations: Lessons from Elementary Science Exams", "year": "2017", "authors": ["Peter Jansen"]}], "target_citation_location": 9, "citation_locations": [9], "citation_type": "group", "annotations": [[2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "ae7944d0-f7a8-48a5-8943-b7b6eb97ba78", "citing_paper": {"title": "Toward High Performance Machine Translation: Preliminary Results from Massively Parallel Memory-Based Translation on SNAP*", "year": 1991, "authors": ["Hiroaki Kitano", "Dan Moldovan", "Seungho Cha"]}, "text": ["Structural", "ambiguity", "is", "resolved", "by", "the", "cost-based", "ambiguity", "resolution", "method", "[Kitano et al., 1989", "].", "The", "cost-based", "ambiguity", "resolution", "takes", "into", "account", "various", "psycholinguistic", "studies", "such", "as", "[Crain and Steedman, 1985]", "and", "[Ford et al., 1981]."], "cited_papers": [{"title": "On not being let up with the garden path: the use of context by the psychological syntax processor", "year": "1985", "authors": ["S Steedman ", " Crain", "M Steedman"]}], "target_citation_location": 24, "citation_locations": [10, 24, 26], "citation_type": "single", "annotations": [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 3, 3]]}
{"id": "b34020a5-aac4-4f26-b33f-0f616137ded0", "citing_paper": {"title": "The DCU Machine Translation Systems for IWSLT 2011", "year": 2011, "authors": ["Pratyush Banerjee", "Hala Almaghout", "Sudip Naskar", "Johann Roturier", "Jie Jiang", "Andy Way", "Josef Van Genabith"]}, "text": [")where", "k", "is", "the", "number", "of", "language", "models", "which", "are", "being", "interpolated,", "\u00b5", "i", "the", "interpolation", "weights", "and", "V", "is", "the", "vocabulary", "of", "the", "specific", "language", "model.", "The", "interpolation", "weights", "are", "estimated", "using", "Expectation", "Maximization", "(EM)", "[10]", "over", "the", "log-likelihood", "in", "(6)", ":N", "t=1", "log", "k", "i=1", "\u00b5", "i", "(f", "*", "i", "(w", "t", "|h", "t)", "+", "\u03bb", "i", "(h", "t", ")P", "r", "mix", "(w", "t", "|", "ht", "))", "(6)where", "the", "index", "t", "scans", "over", "all", "the", "n-grams", "in", "the", "training", "corpora.", "This", "mixture", "model", "was", "used", "to", "combine", "the", "'in-domain'", "language", "model", "with", "an", "'out-of-domain'", "one,", "with", "the", "mixture", "weights", "being", "estimated", "on", "the", "'in-domain'", "training", "data", "by", "applying", "a", "cross-validation", "scheme.", "Further", "improvements", "on", "this", "mixture", "models", "were", "achieved", "using", "parameter", "tying", "to", "the", "most-recent", "context", "words", "[4]."], "cited_papers": [{"title": "Maximum likelihood from incomplete data via the EM algorithm", "year": "1977", "authors": ["A Dempster", "N Laird", "D Rubin"]}], "target_citation_location": 36, "citation_locations": [36, 41, 129], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "b425429a-debe-4e6b-b9ed-e1e5856718c8", "citing_paper": {"title": "KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation", "year": 2022, "authors": ["Yongfei Liu", "Chenfei Wu", "Shao-Yen Tseng", "Vasudev Lal", "Xuming He", "Nan Duan"]}, "text": ["Finally,", "denoting", "the", "mask", "set", "M", "=", "{m", "n}", "N", "n=1,", "we", "have", "the", "overall", "PRA", "loss", "function", "as", "follows:L", "PRA", "=", "E", "(I,D)\u223cX", "L", "PRA", "({\u03b1", "z,n}", "|P|,N", "z,n=1,", "M,", "P,", "V,", "W)", "(5)Masked", "Language", "Modeling", "(MLM)", "We", "take", "the", "same", "masking", "strategy", "(15%", "prob.", "to", "mask)", "as", "in", "BERT", "(Devlin et al., 2018)", "to", "randomly", "mask", "out", "the", "input", "word", "tokens.", "Here,", "MLM", "aims", "to", "predict", "the", "original", "word", "index", "in", "vocabulary", "space", "for", "each", "masked", "token", "based", "on", "the", "whole", "image", "and", "its", "surrounding", "language", "context", "via", "the", "Transformer.", "Hence", "a", "cross-entropy", "loss", "is", "adopted:L", "MLM", "=", "\u2212E", "(I,D)\u223cX", "logP", "(w", "j", "|V,", "W", "\\j)", "(6)Image-Text", "Matching", "(ITM)", "In", "ITM,", "the", "multilayer", "Transformer", "is", "trained", "to", "distinguish", "whether", "the", "input", "image-text", "pairs", "are", "semantically", "matched", "based", "on", "the", "final", "layer", "[cls]", "token", "representation", "h", "cls.", "To", "construct", "the", "training", "samples,", "we", "randomly", "replace", "the", "text", "for", "each", "image-text", "pair", "with", "another", "text", "from", "dataset", "with", "a", "probability", "of", "0.5.", "Thus,", "the", "output", "label", "can", "be", "defined", "as", "y", "\u2208", "{0,", "1}", "where", "y", "=", "1", "indicates", "matched", "pair.", "The", "training", "objective", "for", "the", "ITM", "task", "is", "to", "minimize", "binary", "cross-entropy", "loss:L", "ITM", "=", "\u2212E", "(I,D)\u223cX", "logP", "(y|V,", "W)", "(7)4", "Experiments"], "cited_papers": [{"title": null, "year": "2018", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"]}], "target_citation_location": 51, "citation_locations": [51], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "b8666443-5bac-4f56-aa9a-e89e7e2f0c9d", "citing_paper": {"title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering", "year": 2021, "authors": ["Yingqi Qu", "Yuchen Ding", "Jing Liu", "Kai Liu", "Ruiyang Ren", "Wayne Zhao", "Daxiang Dong", "Hua Wu", "Haifeng Wang"]}, "text": ["Natural", "Question", "(NQ)", "Kwiatkowski", "et", "al.", "(2019)", "introduces", "a", "large", "dataset", "for", "open-domain", "QA.", "The", "original", "dataset", "contains", "more", "than", "300,", "000", "questions", "collected", "from", "Google", "search", "logs.", "In", "Karpukhin et al. (2020),", "around", "62,", "000", "factoid", "questions", "are", "selected,", "and", "all", "the", "Wikipedia", "articles", "are", "processed", "as", "the", "collection", "of", "passages.", "There", "are", "more", "than", "21", "million", "passages", "in", "the", "corpus.", "In", "our", "experiments,", "we", "reuse", "the", "version", "of", "NQ", "created", "by", "Karpukhin et al. (2020).", "Note", "that", "the", "dataset", "used", "in", "DPR", "contains", "empty", "negatives,", "and", "we", "discarded", "the", "empty", "ones."], "cited_papers": [{"title": null, "year": null, "authors": ["unknown"]}], "target_citation_location": 29, "citation_locations": [29, 70], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "c31724d2-3dc4-4d08-bef1-722f91d30ac2", "citing_paper": {"title": "Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data", "year": 2022, "authors": ["Colin Leong", "Daniel Whitenack"]}, "text": ["Pre-trained", "language", "models", "are", "increasingly", "applied", "in", "ways", "that", "are", "agnostic", "to", "targeted", "downstream", "tasks", "(Brown et al., 2020).", "This", "usage", "has", "led", "to", "a", "proliferation", "of", "large", "language", "models", "trained", "on", "enormous", "amounts", "of", "data.", "For", "example,", "the", "recent", "Megatron-Turing", "NLG", "530B", "model", "was", "trained", "on", "the", "Pile,", "which", "includes", "800GB+", "of", "text", "(Gao et al., 2021),", "and", "other", "large", "language", "models", "utilize", "large", "portions", "of", "the", "200TB+", "common", "crawl", "data.", "1", "These", "large", "data", "sets", "include", "impressive", "amounts", "of", "text,", "but", "all", "languages", "are", "not", "represented", "equally", "(or", "at", "all)", "in", "that", "text.", "The", "reality", "is", "that", "only", "a", "negligible", "fraction", "of", "the", "7000+", "currently", "spoken", "languages", "(Eberhard et al., 2021)", "have", "sufficient", "text", "corpora", "to", "train", "state-of-theart", "language", "models.", "This", "data", "scarcity", "results", "in", "systematic", "inequalities", "in", "the", "performance", "of", "NLP", "tasks", "across", "the", "world's", "languages", "(Blasi et al., 2021)."], "cited_papers": [{"title": "Ethnologue: Languages of the World, twenty-fourth edition", "year": "2021", "authors": ["David Eberhard", "Gary Simons", "Charles Fennig"]}], "target_citation_location": 103, "citation_locations": [15, 51, 103, 130], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "c498a02a-8fc1-4de9-844d-827648f2d1e8", "citing_paper": {"title": "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements", "year": 2022, "authors": ["Conrad Borchers", "Dalia Sara Gala", "Benjamin Gilburt", "Eduard Oravkin", "Wilfried Bounsi", "Yuki Asano", "Hannah Kirk"]}, "text": ["n", "biased", "words", "n", "words", "Superlative", "Prevalence", "This", "measure", "is", "based", "on", "a", "correlation", "identified", "between", "\"standout\"", "words", "to", "describe", "a", "job", "candidate", "and", "research", "skill", "when", "describing", "that", "candidate", "(Schmader et al., 2007).", "A", "particular", "distinction", "is", "made", "between", "positive", "(standout)", "superlatives", "and", "negative", "(grindstone)", "superlatives", "and", "their", "differential", "use", "to", "describe", "men", "and", "women.", "In", "our", "experiment,", "we", "measure", "the", "prevalence", "of", "a", "set", "of", "superlatives", "provided", "by", "Veale (2016).", "The", "calculation", "is:", "Gender-Laden", "Scoring", "A", "previous", "study", "provides", "a", "list", "of", "2,311", "words,", "based", "on", "an", "analysis", "of", "32", "properties", "related", "to", "a", "set", "of", "norms", "(Sap et al., 2017).", "In", "this", "study,", "words", "are", "scored", "for", "their", "\"gender-ladenness\"", "and", "\"gender", "replication\".", "Our", "study", "takes", "a", "count", "of", "the", "former,", "measuring", "their", "unweighted", "prevalence", "to", "make", "it", "comparable", "to", "the", "other", "bias", "measures.", "The", "calculation", "is:n", "biased", "words", "n", "wordsConnotation", "Frames", "This", "measure", "is", "based", "on", "the", "concept", "of", "power", "and", "agency", "connotation", "frames", "(Sap et al., 2017).", "Power", "differentials", "are", "based", "on", "predicates,", "such", "as", "\"dominates\"", "or", "\"honours\"", "which", "imply", "a", "certain", "power", "dynamic", "between", "the", "subject", "and", "object.", "Agency", "is", "attributed", "to", "the", "agent", "of", "the", "verb.", "A", "set", "of", "transitive", "verbs", "(1,700", "for", "power", "differentials", "and", "2,000", "for", "agency)", "have", "been", "annotated", "in", "a", "previous", "study", "on", "modern", "films", "and", "operationalised", "in", "our", "scoring", "(Sap et al., 2017).", "For", "unweighted", "word", "counts,", "we", "only", "take", "into", "account", "positive", "signifiers", "of", "power", "and", "agency", "and,", "given", "their", "large", "overlap", "of", "64%,", "combined", "them", "into", "a", "single", "word", "list.", "The", "calculation", "is:"], "cited_papers": [{"title": "Round Up The Usual Suspects: Knowledge-Based Metaphor Generation", "year": "2016", "authors": ["Tony Veale"]}], "target_citation_location": 67, "citation_locations": [30, 67, 95, 150, 210], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "c59434c1-4177-4d2a-a49a-f7cd67eb63b4", "citing_paper": {"title": "Multitasking Framework for Unsupervised Simple Definition Generation", "year": 2022, "authors": ["Cunliang Kong", "Yun Chen", "Hengyuan Zhang", "Liner Yang", "Erhong Yang"]}, "text": ["Lately,", "many", "works", "built", "upon", "the", "Seq2Seq", "MT", "model", "(Sutskever et al., 2014)", "performed", "well.", "First", "attempted", "by", "Nisioi et al. (2017),", "the", "Seq2Seq", "models", "for", "this", "task", "are", "able", "to", "perform", "lexical", "simplification", "and", "content", "reduction", "simultaneously", "by", "training", "on", "complex-simple", "sentence", "pairs.", "This", "method", "was", "inherited", "and", "improved", "by", "many", "subsequent", "works,", "such", "as", "combining", "with", "the", "reinforcement", "learning", "method", "by", "setting", "a", "simplification", "reward", "(Zhang and Lapata, 2017),", "augmenting", "memory", "capacities", "(Vu et al., 2018)", "or", "training", "with", "multitasking", "on", "entailment", "and", "paraphrase", "generation", "(Guo et al., 2018).", "Martin et al. (2019)", "proposed", "to", "prepend", "additional", "prompt", "tokens", "to", "source", "sentences", "at", "train", "time,", "which", "enables", "the", "end-users", "to", "condition", "the", "simplifications", "returned", "by", "the", "model", "on", "attributes", "like", "length,", "lexical", "complexity,", "and", "syntactic", "complexity.", "This", "controllable", "simplification", "system", "(called", "ACCESS)", "and", "its", "improved", "version", "MUSS", "(Martin et al., 2020)", "achieved", "SOTA", "results", "on", "the", "Turk", "corpus", "in", "terms", "of", "the", "SARI", "metric", "(Xu et al., 2016)."], "cited_papers": [{"title": "Sequence to sequence learning with neural networks", "year": "2014", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"]}], "target_citation_location": 9, "citation_locations": [9, 15, 61, 65, 75, 76, 121, 135], "citation_type": "single", "annotations": [[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "c6e387bd-8212-45eb-8718-bb45b139473c", "citing_paper": {"title": "Generalizable and Explainable Dialogue Generation via Explicit Action Learning", "year": 2020, "authors": ["Xinting Huang", "Jianzhong Qi", "Yu Sun", "Rui Zhang"]}, "text": ["Let", "{d", "i", "|1", "\u2264", "i", "\u2264", "N}", "be", "a", "set", "of", "dialogues,", "and", "each", "dialogue", "contains", "n", "d", "turns:d", "i", "=", "{(c", "t,", "a", "t,", "x", "t", ")|1", "\u2264", "t", "\u2264", "n", "d", "},", "where", "c", "t", "is", "the", "context", "at", "turn", "t,", "and", "a", "t", "is", "the", "dialogue", "action", "of", "system", "utterance", "x", "t.", "The", "context", "c", "t", "=", "{u", "1,", "x", "1,", "...,", "u", "t}", "consists", "of", "the", "dialogue", "history", "of", "user", "utterances", "u", "and", "system", "utterances", "x.", "Conditioned", "response", "generation", "tackles", "the", "context-to-response", "generation", "problem", "p(x|c)", "via", "two", "consecutive", "steps:", "a", "content", "planning", "step", "decides", "a", "dialogue", "action", "to", "proceed", "the", "dialogues", "p", "l", "(a|c),", "and", "a", "surface", "realization", "step", "further", "trans-forms", "the", "decided", "action", "into", "naturally", "sound", "utterances", "p", "r", "(x|a,", "c).", "Using", "the", "two-step", "process,", "response", "generation", "could", "be", "optimized", "towards", "better", "task", "completion", "while", "maintaining", "high-quality", "language", "quality", "(Huang et al., 2020a, Zhao et al., 2019).", "The", "optimization", "process", "also", "consists", "of", "two", "parts.", "Firstly,", "context-action", "pairs", "are", "used", "to", "train", "the", "content", "planning", "model", "p", "l", "(a|c)", "using", "the", "cross-entropy", "loss.L", "act", "=", "d", "i", "t=1:n", "d", "\u2212", "log(a", "t", "p", "l", "(a|c", "t", "))", "(1)Then,", "the", "surface", "realization", "model", "p", "r", "(x|a,", "c)", "is", "optimized", "from", "the", "(c", "t,", "a", "t,", "x", "t)", "triples", "to", "maximize", "the", "likelihood", "of", "ground-truth", "responses"], "cited_papers": [{"title": "Rethinking action spaces for reinforcement learning in end-to-end dialog agents with latent variable models", "year": "2019", "authors": ["Tiancheng Zhao", "Kaige Xie", "Maxine Eskenazi"]}, {"title": "Mala: Cross-domain dialogue generation with action learning", "year": "2020", "authors": ["Xinting Huang", "Jianzhong Qi", "Yu Sun", "Rui Zhang"]}], "target_citation_location": 145, "citation_locations": [145], "citation_type": "group", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "c89d4888-3f9f-4a14-99cb-85aba4cfd78a", "citing_paper": {"title": "DRS Parsing as Sequence Labeling", "year": 2022, "authors": ["Minxing Shen", "Kilian Evang"]}, "text": ["BERT", "has", "12", "layers,", "each", "of", "which", "has", "a", "768-dimensional", "output", "embedding", "per", "wordpiece.", "There", "is", "some", "mixed", "information", "in", "the", "literature", "as", "to", "which", "layer's", "output", "is", "most", "suitable", "for", "seman-", "Evaluation", "We", "evaluate", "the", "performance", "of", "our", "parser", "using", "Counter", "(van Noord et al., 2018a),", "an", "extension", "of", "the", "Smatch", "evaluation", "metric", "(Cai and Knight, 2013).", "Counter", "approximates", "an", "optimal", "mapping", "between", "the", "referents", "in", "the", "gold", "DRS", "and", "the", "predicted", "DRS", "using", "hill-climbing,", "then", "outputs", "recall,", "precision,", "and", "f-score", "for", "the", "predicted", "clauses", "compared", "to", "the", "gold", "clauses."], "cited_papers": [{"title": "Evaluating scoped meaning representations", "year": "2018", "authors": ["Rik Van Noord", "Lasha Abzianidze", "Hessel Haagsma", "Johan Bos"]}], "target_citation_location": 42, "citation_locations": [42, 50], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "cac87218-24d4-4168-b206-ed770d23337d", "citing_paper": {"title": "Codenames as a Game of Co-occurrence Counting", "year": 2022, "authors": ["R\u00e9ka Cserh\u00e1ti", "Istv\u00e1n Koll\u00e1th", "Andr\u00e1s Kicsi", "G\u00e1bor Berend"]}, "text": ["We", "randomly", "create", "100", "boards,", "with", "each", "containing", "10", "good", "and", "10", "bad", "words.", "For", "each", "board,", "we", "generate", "clues", "with", "the", "32", "configurations", "detailed", "above.", "This", "results", "in", "1304", "distinct", "clues", "in", "English,", "and", "1399", "in", "Hungarian.", "For", "evaluation,", "we", "create", "an", "online", "game,", "where", "human", "players", "get", "a", "board", "with", "one", "of", "the", "corresponding", "clues", "randomly,", "and", "have", "to", "choose", "the", "given", "number", "of", "words", "from", "the", "board", "which", "they", "think", "the", "clue", "refers", "to.", "The", "players", "do", "not", "know", "how", "the", "agents", "work,", "and", "to", "avoid", "that", "through", "the", "game", "they", "learn", "it", "at", "the", "end", "of", "the", "round", "they", "only", "see", "the", "color", "of", "their", "chosen", "words.", "We", "collected", "443", "rounds", "played", "in", "English,", "and", "1365", "in", "Hungarian.", "This", "way,", "we", "have", "31.5", "rounds", "on", "average", "to", "evaluate", "English", "configurations,", "and", "64", "rounds", "for", "Hungarian.", "For", "one", "board,", "players", "on", "average", "spent", "39", "seconds", "on", "guessing", "in", "English,", "while", "37", "seconds", "in", "Hungarian.", "We", "note", "that", "the", "players", "of", "the", "Hungarian", "game", "were", "most", "likely", "Hungarian", "native", "speakers,", "while", "the", "same", "cannot", "be", "said", "about", "the", "English", "game,", "therefore", "we", "consider", "the", "Hungarian", "data", "more", "reliable.", "Similar", "to", "Koyyalagunta et al. (2021),", "we", "compute", "the", "precision", "of", "the", "agents", "asP@targets", "=", "|I", "n", "\u2229", "U", "|", "n", ",where", "I", "n", "is", "the", "set", "of", "the", "targeted", "words,", "and", "U", "is", "the", "set", "of", "words", "chosen", "by", "the", "players.", "However,", "the", "scoring", "functions", "optimize", "clue", "words", "to", "stay", "away", "from", "red", "words,", "but", "not", "from", "non-targeted", "blue", "words,", "which", "might", "be", "almost", "as", "related", "to", "the", "clue", "as", "the", "targeted", "ones.", "If", "the", "user", "chooses", "such", "an", "untargeted", "word,", "the", "agent", "still", "performs", "well.", "So", "we", "define", "P@all,P@all", "=", "|A", "\u2229", "U", "|", "n", ",where", "A", "is", "the", "set", "of", "all", "good", "(blue)", "words.", "In", "Table", "3", "and", "Table", "4,", "we", "show", "the", "mean", "precision", "of", "the", "players'", "guesses", "on", "the", "clues", "of", "each", "agent."], "cited_papers": [{"title": "Playing codenames with language graphs and word embeddings", "year": "2021", "authors": ["Divya Koyyalagunta", "Anna Sun", "Rachel Draelos", "Cynthia Rudin"]}], "target_citation_location": 192, "citation_locations": [192], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "cb8646a2-174e-4e9d-bab5-ac7cda05c591", "citing_paper": {"title": "Coreference Reasoning in Machine Reading Comprehension", "year": 2021, "authors": ["Mingzhu Wu", "Nafise Moosavi", "Dan Roth", "Iryna Gurevych"]}, "text": ["Contrast", "set:", "the", "evaluation", "set", "by", "Gardner et al. (2020)", "that", "is", "created", "based", "on", "the", "official", "Quoref", "test", "set.", "For", "creating", "this", "evaluation", "set,", "the", "authors", "manually", "performed", "small", "but", "meaningful", "perturbations", "to", "the", "test", "examples", "in", "a", "way", "that", "it", "changes", "the", "gold", "label.", "This", "dataset", "is", "constructed", "to", "evaluate", "whether", "models", "decision", "boundaries", "align", "to", "true", "decision", "boundaries", "when", "they", "are", "measured", "around", "the", "same", "point."], "cited_papers": [{"title": null, "year": null, "authors": ["unknown"]}], "target_citation_location": 6, "citation_locations": [6], "citation_type": "single", "annotations": [[0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "ccd79bd6-f7b3-45c8-9524-bcbec77c7e09", "citing_paper": {"title": "Can Semantic Role Labeling Improve SMT?", "year": 2009, "authors": ["Dekai Wu", "Pascale Fung"]}, "text": ["Systems", "that", "perform", "shallow", "semantic", "parsing", "on", "Chinese", "texts", "are", "likewise", "based", "on", "classifiers", "and", "trained", "on", "the", "Chinese", "PropBank", "and", "the", "bilingual", "Chinese-English", "Parallel", "PropBank", "(Sun and Jurafsky (2004),", "Xue (2006),", "Fung et al. (2006)", ").", "It", "is", "interesting", "to", "note", "that,", "despite", "the", "very", "different", "characteristics", "of", "Chinese", "verbs", "(Xue and Palmer, 2005)", "from", "those", "in", "English,", "the", "core", "algorithm", "of", "a", "shallow", "semantic", "parser", "remains", "the", "same.", "As", "was", "found", "to", "be", "the", "case", "in", "English,", "SVM", "classifiers", "have", "been", "found", "to", "outperform", "maximum", "entropy", "classifiers", "for", "this", "task", "(Fung et al., 2006).", "The", "primary", "difference", "lies", "in", "the", "feature", "set", "chosen", "to", "represent", "semantic", "information."], "cited_papers": [{"title": "Shallow semantic parsing of chinese", "year": "2004", "authors": ["Honglin Sun", "Daniel Jurafsky"]}], "target_citation_location": 26, "citation_locations": [26, 27, 28, 44, 82], "citation_type": "single", "annotations": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "cd29c8d8-9bd1-4ca2-b9c1-66ae748afa0b", "citing_paper": {"title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection", "year": 2021, "authors": ["Henry Weld", "Guanghao Huang", "Jean Lee", "Tongshu Zhang", "Kunze Wang", "Xinghong Guo", "Siqu Long", "Josiah Soyeon", "Caren Han"]}, "text": ["Toxicity", "Datasets", "in", "Online", "Games", "In", "multiplayer", "online", "games,", "prior", "research", "focused", "on", "analysis", "of", "anti-social", "or", "disruptive", "behavior,", "socalled", "toxic", "behavior", "(Blackburn and Kwak, 2014, de Mesquita Neto and Becker, 2018)", "including", "cyberbullying", "(Kwak et al., 2015)", "and", "griefing", "(Murnion et al., 2018).", "Although", "these", "terms", "contain", "similar", "elements,", "a", "single", "definition", "of", "toxic", "behavior", "is", "yet", "to", "emerge.", "Some", "studies", "have", "conducted", "data", "annotation", "using", "pre-defined", "lexicon", "categories", "(M\u00e4rtens et al., 2015)", "or", "toxic", "player", "information", "(Stoop et al., 2019).", "These", "annotation", "methods", "are", "not", "robust", "enough", "to", "handle", "unlabelled", "toxicity", "words", "or", "unreported", "toxic", "players."], "cited_papers": [{"title": "STFU NOOB! Predicting crowdsourced decisions on toxic behavior in online games", "year": "2014", "authors": ["Jeremy Blackburn", "Haewoon Kwak"]}, {"title": "Relating conversational topics and toxic behavior effects in a moba game", "year": "2018", "authors": ["Joaquim Alvino De Mesquita Neto", "Karin Becker"]}], "target_citation_location": 22, "citation_locations": [22, 25, 28, 55, 60], "citation_type": "group", "annotations": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "cde5ca45-b309-4a6e-836e-1f77ce208959", "citing_paper": {"title": "Unsupervised Representation Disentanglement of Text: An Evaluation on Synthetic Datasets", "year": 2021, "authors": ["Lan Zhang", "Victor Prokhorov", "Ehsan Shareghi"]}, "text": ["A", "confounding", "factor", "which", "could", "pollute", "this", "analysis", "is", "the", "role", "of", "strong", "auto-regressive", "decoding", "of", "VAEs", "and", "the", "type", "of", "information", "captured", "by", "the", "decoder", "in", "such", "scenario.", "While", "a", "preliminary", "analysis", "has", "been", "provided", "recently", "(Bosc and Vincent, 2020),", "this", "has", "been", "vastly", "underexplored", "and", "requires", "more", "explicit", "attempts.", "We", "leave", "deeper", "investigation", "of", "this", "to", "future", "work."], "cited_papers": [{"title": "Do sequence-tosequence VAEs learn global features of sentences?", "year": "2020", "authors": ["Tom Bosc", "Pascal Vincent"]}], "target_citation_location": 37, "citation_locations": [37], "citation_type": "single", "annotations": [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "cfb94a5b-e884-4d58-b31d-60d4603fbc8d", "citing_paper": {"title": "Controlled Text Generation with Adversarial Learning", "year": 2020, "authors": ["Federico Betti", "Giorgia Ramponi", "Massimo Piccardi"]}, "text": ["The", "distribution", "over", "the", "vocabulary", "of", "the", "next", "word", "is", "evaluated", "using", "the", "memory", "output", "o", "t", "as", "in", "Eq.", "3", "with", "a", "feed-forward", "layer.", "Then,", "the", "next", "soft", "word,", "\u0177t,", "is", "sampled", "using", "the", "Gumbelsoftmax", "relaxation", "[11]", "with", "temperature", "T", "(Eq.", "4).", "The", "temperature", "value", "greatly", "influences", "the", "quality-diversity", "trade-off,", "more", "details", "on", "these", "parameters", "are", "provided", "in", "Appendix", "D."], "cited_papers": [{"title": "Categorical reparameterization with gumbel-softmax", "year": "2016", "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"]}], "target_citation_location": 37, "citation_locations": [37], "citation_type": "single", "annotations": [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "d036cda3-119f-468c-b5c9-a877833cebba", "citing_paper": {"title": "The DCU Machine Translation Systems for IWSLT 2011", "year": 2011, "authors": ["Pratyush Banerjee", "Hala Almaghout", "Sudip Naskar", "Johann Roturier", "Jie Jiang", "Andy Way", "Josef Van Genabith"]}, "text": ["Following", "the", "SAMT", "approach,", "CCG-augmented", "HPB", "SMT", "[12]", "uses", "CCG", "[5]", "to", "label", "non-terminals.", "CCG", "has", "distinct", "advantages", "over", "phrase-structure", "grammar", "in", "the", "general", "SMT", "context,", "particularly", "in", "extracting", "non-terminal", "labels", "in", "HPB", "SMT.", "This", "section", "gives", "a", "brief", "introduction", "to", "CCG", "followed", "by", "a", "description", "of", "the", "approach", "of", "extracting", "non-terminal", "labels", "using", "the", "same."], "cited_papers": [{"title": "CCG augmented hierarchical phrase-based machine translation", "year": "2010", "authors": ["H Almaghout", "J Jiang", "A Way"]}], "target_citation_location": 7, "citation_locations": [7, 10], "citation_type": "single", "annotations": [[2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "d81bb000-e526-40aa-a6a0-4eea1fbd01a3", "citing_paper": {"title": "Learning to Rank in the Age of Muppets: Effectiveness-Efficiency Tradeoffs in Multi-Stage Ranking", "year": 2021, "authors": ["Yue Zhang", "Chengcheng Hu", "Yuqi Liu", "Hui Fang", "Jimmy Lin"]}, "text": ["Table", "3", "shows", "the", "query", "latency", "breakdown", "for", "a", "few", "representative", "models.", "Note", "that", "latency", "is", "dominated", "by", "final-stage", "neural", "reranking", "latency,", "which", "scales", "linearly,", "so", "smaller", "N", "values", "(in", "Table", "2)", "are", "more", "desirable.", "However,", "this", "is", "balanced", "by", "the", "introduction", "of", "LTR", "overhead,", "both", "feature", "extraction", "as", "well", "as", "the", "prediction", "latency", "itself.", "Nevertheless,", "this", "is", "a", "worthwhile", "tradeoff", "as", "we", "observe", "large", "speedups", "overall.", "Since", "T5-base", "is", "faster", "than", "BERT-large,", "the", "effect", "of", "the", "LTR", "overhead", "is", "relatively", "larger", "and", "thus", "the", "speedup", "is", "lower.", "We", "can", "see", "that", "increasing", "the", "initial", "k", "0", "for", "BoW", "from", "1k", "to", "10k", "is", "acceptable", "as", "LTR", "overhead", "remains", "modest.", "dex", "for", "each", "document", "is", "the", "most", "expensive", "single", "step.", "Our", "experiments", "show", "that", "the", "number", "of", "features", "does", "not", "affect", "latency", "substantially", "because", "we", "only", "need", "to", "load", "the", "document", "once,", "once", "in", "cache,", "individual", "feature", "extraction", "is", "very", "fast.", "Note", "that", "we", "have", "not", "spent", "much", "effort", "optimizing", "feature", "extraction", "(which", "is", "relatively", "inefficient", "Java", "code)", "and", "that", "more", "engineering", "effort,", "for", "example,", "optimizations", "proposed", "by", "Asadi and Lin (2013),", "are", "likely", "to", "further", "increase", "speedups."], "cited_papers": [{"title": "Document vector representations for feature extraction in multistage document ranking", "year": "2013", "authors": ["Nima Asadi", "Jimmy Lin"]}], "target_citation_location": 178, "citation_locations": [178], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2]]}
{"id": "d92d28e8-1c99-4afc-9927-7dd0f928db1e", "citing_paper": {"title": "The DCU Machine Translation Systems for IWSLT 2011", "year": 2011, "authors": ["Pratyush Banerjee", "Hala Almaghout", "Sudip Naskar", "Johann Roturier", "Jie Jiang", "Andy Way", "Josef Van Genabith"]}, "text": ["where,", "h", "i", "(f,", "e)", "denotes", "the", "different", "components", "for", "translating", "the", "source", "sentence", "f", "into", "the", "target", "sentence", "e.", "K", "is", "the", "number", "of", "components", "(or", "features)", "used", "and", "\u03bb", "i", "are", "the", "corresponding", "weights", "of", "the", "components.", "The", "Moses", "SMT", "system", "[6],", "which", "implements", "this", "particular", "model,", "was", "used", "for", "all", "our", "PBSMT", "translation", "experiments.", "Different", "component", "weights", "(\u03bb", "i)", "were", "estimated", "using", "a", "discriminative", "training", "method", "known", "as", "Minimum", "Error", "Rate", "Training", "(MERT)", "[7],", "on", "a", "held", "out", "development", "set", "(devset)."], "cited_papers": [{"title": "Minimum error rate training in statistical machine translation", "year": "2003", "authors": ["F Och"]}], "target_citation_location": 76, "citation_locations": [43, 76], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2]]}
{"id": "dcb05e48-437b-4766-a9da-016242fbd9a7", "citing_paper": {"title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection", "year": 2021, "authors": ["Henry Weld", "Guanghao Huang", "Jean Lee", "Tongshu Zhang", "Kunze Wang", "Xinghong Guo", "Siqu Long", "Josiah Soyeon", "Caren Han"]}, "text": ["Toxicity", "Datasets", "in", "Online", "Community", "An", "extensive", "body", "of", "work", "has", "focused", "on", "datasets", "to", "detect", "toxicity", "including", "hate", "speech", "(Waseem and Hovy, 2016, Davidson et al., 2017, ElSherief et al., 2018)", "and", "abusive", "language", "(Nobata et al., 2016, Founta et al., 2018).", "However,", "the", "majority", "of", "toxicity", "datasets", "do", "not", "consider", "the", "context", "of", "a", "conversation,", "instead", "simply", "analysing", "a", "single", "utterance.", "Even", "if", "a", "model", "uses", "contextual", "information", "(Gao and Huang, 2017),", "it", "is", "limited", "to", "metainformation", "(e.g.", "news", "title", "or", "user", "name)", "which", "is", "not", "sufficient", "to", "understand", "a", "conversation.", "In", "our", "research,", "context", "is", "defined", "as", "linguistic", "contextual", "information,", "particularly", "previous", "single", "or", "multiple", "utterances.", "Along", "similar", "lines,", "recent", "studies", "have", "focused", "on", "conversation", "aiming", "to", "discover", "warning", "signals", "(Zhang et al., 2018),", "to", "generate", "intervention", "responses", "(Qian et al., 2019),", "or", "to", "measure", "the", "importance", "of", "context", "(Pavlopoulos et al., 2020).", "Existing", "toxicity", "datasets", "mainly", "focus", "on", "annotating", "at", "utterance-level,", "whereas", "ours", "conducts", "a", "dual-level", "annotation", "at", "utterance", "and", "token-level,", "while", "also", "providing", "a", "conversation", "history", "(see", "Table", "1).", "These", "extra", "features", "are", "what", "distinguish", "CONDA."], "cited_papers": [{"title": "Abusive language detection in online user content", "year": "2016", "authors": ["Chikashi Nobata", "Joel Tetreault", "Achint Thomas", "Yashar Mehdad", "Yi Chang"]}, {"title": "Large scale crowdsourcing and characterization of twitter abusive behavior", "year": "2018", "authors": ["Antigoni Founta", "Constantinos Djouvas", "Despoina Chatzakou", "Ilias Leontiadis", "Jeremy Blackburn", "Gianluca Stringhini", "Athena Vakali"]}], "target_citation_location": 24, "citation_locations": [20, 24, 52, 102, 107, 115], "citation_type": "group", "annotations": [[0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "e1dde46b-be53-4789-afd6-00dfff219692", "citing_paper": {"title": "A Semi-Supervised Approach to Detect Toxic Comments", "year": 2021, "authors": ["Ghivvago Saraiva", "Rafael Anchi\u00eata", "Francisco Neto", "Raimundo Moura"]}, "text": ["Most", "of", "the", "works", "that", "study", "this", "task", "commonly", "point", "first", "to", "surface-level", "features,", "such", "as", "bag", "of", "words", "and", "lexicon-based", "approaches,", "with", "negative", "words", "as", "features", "(Gitari et al., 2015, Waseem and Hovy, 2016, Waseem et al., 2017, Schmidt and Wiegand, 2017)."], "cited_papers": [{"title": "Understanding abuse: A typology of abusive language detection subtasks", "year": "2017", "authors": ["Zeerak Waseem", "Thomas Davidson", "Dana Warmsley", "Ingmar Weber"]}, {"title": "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter", "year": "2016", "authors": ["Zeerak Waseem", "Dirk Hovy"]}, {"title": "A lexicon-based approach for hate speech detection", "year": "2015", "authors": ["Njagi Dennis Gitari", "Zhang Zuping", "Hanyurwimfura Damien", "Jun Long"]}, {"title": "A survey on hate speech detection using natural language processing", "year": "2017", "authors": ["Anna Schmidt", "Michael Wiegand"]}], "target_citation_location": 27, "citation_locations": [27], "citation_type": "group", "annotations": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1]]}
{"id": "e2055f77-5542-4603-b5fe-f8349c3fd893", "citing_paper": {"title": "Dual-Channel Evidence Fusion for Fact Verification over Texts and Tables", "year": 2022, "authors": ["Nan Hu", "Zirui Wu", "Yuxuan Lai", "Xiao Liu", "Yansong Feng"]}, "text": ["Previous", "works", "on", "FEVEROUS", "generally", "convert", "all", "evidence", "pieces", "into", "either", "plain", "text", "(Aly et al., 2021, Saeed et al., 2021, Malon, 2021)", "or", "several", "tables", "(Bouziane et al., 2021).", "However,", "format", "conversions", "inevitably", "lose", "rich", "context", "information", "for", "the", "converted", "evidence,", "thus", "may", "mislead", "the", "subsequent", "encoding", "and", "interaction", "steps.", "For", "example,", "in", "Figure", "1,", "the", "entire", "top", "two", "rows", "are", "indispensable", "to", "understand", "the", "table", "cell", "Won.", "It", "is", "difficult", "to", "identify", "all", "related", "context", "cells", "and", "design", "a", "general", "conversion", "method", "to", "render", "them", "into", "sentences,", "but", "these", "connections", "can", "be", "easily", "caught", "by", "pre-trained", "table", "models", "(Herzig et al., 2020, Yin et al., 2020).", "On", "the", "other", "side,", "identifying/re-organizing", "crucial", "elements", "in", "a", "sentence", "to", "construct", "a", "table", "is", "also", "challenging.", "Simply", "inserting", "a", "whole", "sentence", "in", "a", "table", "cell", "(Bouziane et al., 2021)", "will", "make", "the", "new", "cells", "much", "larger", "(and", "unique)", "than", "normal", "ones,", "thus", "can", "not", "make", "the", "most", "of", "general", "pre-trained", "table", "models", "(Herzig et al., 2020, Yin et al., 2020)", "as", "we", "expect."], "cited_papers": [{"title": "Fabulous: Fact-checking based on understanding of language over unstructured and structured information", "year": "2021", "authors": ["Mostafa Bouziane", "Hugo Perrin", "Amine Sadeq", "Thanh Nguyen", "Aur\u00e9lien Cluzeau", "Julien Mardas"]}], "target_citation_location": 17, "citation_locations": [13, 17, 88, 115, 139], "citation_type": "single", "annotations": [[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 0, 1, 1, 1, 1, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "e2e03983-c35f-4895-9198-86e973a3cca5", "citing_paper": {"title": "Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models", "year": 2022, "authors": ["Tianlu Wang", "Rohit Sridhar", "Diyi Yang", "Xuezhi Wang"]}, "text": ["Pruthi", "et", "al.", "(2020)", "derived", "an", "occupation", "dataset", "to", "study", "the", "gender", "bias", "in", "NLP", "classification", "tasks.", "The", "task", "is", "framed", "as", "a", "binary", "classification", "task", "to", "distinguish", "between", "\"surgeons\"", "and", "\"physicians\".", "These", "two", "occupations", "are", "chosen", "because", "they", "share", "similar", "words", "in", "their", "biographies", "and", "a", "majority", "of", "surgeons", "are", "male.", "The", "dataset", "is", "further", "tuned", "-downsample", "minority", "classes", "(female", "surgeons", "and", "male", "physicians)", "by", "a", "factor", "of", "ten", "to", "encourage", "the", "model", "to", "rely", "on", "gendered", "words", "to", "make", "predictions.", "Pruthi et al. (2020)", "also", "provides", "a", "pre-specified", "list", "of", "impermissible", "tokens", "8", "that", "a", "robust", "model", "should", "assign", "low", "attention", "scores", "to.", "We", "instead", "treat", "this", "list", "of", "tokens", "as", "shortcuts", "and", "analyze", "the", "efficacy", "of", "our", "proposed", "framework", "on", "identifying", "these", "tokens.", "These", "impermissible", "tokens", "can", "be", "regarded", "as", "shortcuts", "because", "they", "only", "reflect", "the", "gender", "of", "the", "person,", "thus", "by", "definition", "should", "not", "affect", "the", "decision", "of", "a", "occupation", "classification", "model.", "Table", "6", "presents", "the", "result", "on", "identifying", "the", "list", "of", "impermissible", "tokens.", "Among", "the", "top", "ten", "tokens", "selected", "by", "our", "method,", "6", "of", "them", "are", "shortcuts.", "Furthermore,", "9", "out", "of", "12", "impermissible", "tokens", "are", "captured", "in", "the", "top", "50", "tokens", "selected", "by", "our", "method.", "This", "further", "demonstrates", "that", "our", "method", "can", "effectively", "find", "shortcuts", "in", "this", "occupation", "classification", "task,", "in", "a", "more", "automated", "way", "compared", "to", "existing", "approaches", "that", "rely", "on", "pre-defined", "lists."], "cited_papers": [{"title": "Learning to deceive with attention-based explanations", "year": "2020", "authors": ["Danish Pruthi", "Mansi Gupta", "Bhuwan Dhingra", "Graham Neubig", "Zachary Lipton"]}], "target_citation_location": 82, "citation_locations": [82], "citation_type": "single", "annotations": [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "e64b92f3-af62-4ec1-a9f1-e03ce08fb7fd", "citing_paper": {"title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering", "year": 2021, "authors": ["Yingqi Qu", "Yuchen Ding", "Jing Liu", "Kai Liu", "Ruiyang Ren", "Wayne Zhao", "Daxiang Dong", "Hua Wu", "Haifeng Wang"]}, "text": ["Previous", "experiments", "have", "shown", "the", "effectiveness", "of", "RocketQA", "on", "passage", "retrieval.", "Next,", "we", "verify", "whether", "the", "retrieval", "results", "of", "RocketQA", "can", "improve", "the", "performance", "of", "passage", "reading", "for", "extracting", "correct", "answers.", "We", "implement", "an", "end-to-end", "QA", "system", "in", "which", "we", "have", "an", "extractive", "reader", "stacked", "on", "our", "RocketQA", "retriever.", "For", "a", "fair", "comparison,", "we", "first", "re-use", "the", "released", "model", "6", "of", "the", "extractive", "reader", "in", "DPR", "(Karpukhin et al., 2020),", "and", "take", "100", "retrieved", "passages", "during", "inference", "(the", "same", "setting", "used", "in", "DPR).", "Besides,", "6", "https://github.com/facebookresearch/", "DPR", "Model", "EM", "BM25+BERT", "(Lee et al., 2019)", "26.5", "HardEM", "(Min et al., 2019a)", "28.1", "GraphRetriever", "(Min et al., 2019b) 34.5", "PathRetriever", "(Asai et al., 2020)", "32.6", "ORQA", "(Lee et al., 2019)", "33.3", "REALM", "(Guu et al., 2020)", "40.4", "DPR", "(Karpukhin et al., 2020)", "41.5", "GAR", "(Mao et al., 2020)", "41.6", "RocketQA", "+", "DPR", "reader", "42.0", "RocketQA", "+", "re-trained", "DPR", "reader", "42.8"], "cited_papers": [{"title": "REALM: retrievalaugmented language model pre-training", "year": "2002", "authors": ["Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Ming-Wei Chang"]}], "target_citation_location": 101, "citation_locations": [66, 87, 90, 93, 95, 98, 101, 104, 107], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]}
{"id": "e6e3f8ad-213a-4c65-8e5e-fee6618be60a", "citing_paper": {"title": "A Semi-Supervised Approach to Detect Toxic Comments", "year": 2021, "authors": ["Ghivvago Saraiva", "Rafael Anchi\u00eata", "Francisco Neto", "Raimundo Moura"]}, "text": ["We", "achieved", "the", "best", "result", "with", "the", "Gradient", "Boosting", "classifier", "5", "using", "only", "10%", "of", "the", "prelabeled", "nodes", "i.e.,", "the", "classification", "does", "not", "improve", "after", "this", "percentage.", "Table", "3", "Besides", "our", "approach,", "we", "evaluated", "other", "graph", "models", "of", "different", "structures.", "First,", "we", "used", "the", "network", "graph", "developed", "by", "Anchi\u00eata et al. (2020).", "That", "graph", "does", "not", "use", "weight", "between", "the", "nodes.", "Second,", "we", "used", "the", "Term", "Frequency-Inverse", "Document", "Frequency", "(TF-IDF)", "as", "weight", "instead", "of", "the", "average", "of", "embeddings.", "Third,", "we", "used", "bigrams", "and", "trigrams", "as", "nodes", "rather", "than", "token", "nodes.", "Finally,", "we", "used", "the", "Pointwise", "Mutual", "Information", "(PMI)", "measure", "(Church and Hanks, 1990)", "as", "the", "weight", "between", "the", "bi", "and", "trigrams", "nodes.", "For", "these", "approaches,", "we", "adopted", "the", "same", "regularization", "algorithm,", "ranging", "the", "pre-labeled", "nodes", "from", "5%", "to", "30%.", "In", "Table", "4,", "we", "present", "the", "bestachieved", "results.", "From", "this", "table,", "our", "graph", "modeling", "and", "the", "gradient", "boosting", "classifier", "achieved", "better", "results", "than", "these", "other", "graphs,", "as", "well", "as", "classifier", "variations.", "This,", "we", "think,", "is", "because", "of", "the", "embedding", "value", "among", "the", "graph", "nodes", "since", "it", "is", "able", "to", "capture", "morphological,", "syntactic,", "and", "semantic", "knowledge", "of", "a", "word.", "As", "we", "used", "the", "average", "word", "embedding", "value,", "it", "includes", "information", "from", "all", "of", "the", "individual", "vector", "values,", "working", "as", "an", "overall", "summary", "of", "all", "vector", "values."], "cited_papers": [{"title": "Word association norms, mutual information, and lexicography", "year": "1990", "authors": ["Kenneth Church", "Patrick Hanks"]}], "target_citation_location": 96, "citation_locations": [48, 96], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "e7bf43a4-01bd-4918-89b7-279ee02a9b0d", "citing_paper": {"title": "Control Image Captioning Spatially and Temporally", "year": 2021, "authors": ["Kun Yan", "Ji Lei", "Huaishao Luo", "Ming Zhou", "Nan Duan", "Shuai Ma"]}, "text": ["Caption", "Decoder", "Caption", "decoder", "combines", "vision", "and", "trace", "information", "using", "cross", "attention", "connected", "to", "the", "hidden", "states", "of", "Vision-Trace", "Encoder's", "last", "layer.", "Using", "a", "casual", "mask", "to", "encode", "generated", "token", "progressively,", "the", "transformer", "decoder", "ensures", "that", "the", "predictions", "for", "position", "i", "can", "depend", "only", "on", "the", "known", "outputs", "at", "positions", "less", "than", "i.", "During", "training,", "the", "ground", "truth", "caption", "tokens", "are", "shifted", "right,", "and", "a", "special", "token", "BOS", "(begin", "of", "the", "sentence)", "is", "inserted", "into", "the", "first", "position.", "A", "cross-entropy", "generation", "loss", "L", "gen", "is", "then", "computed", "with", "the", "logits", "transformed", "from", "the", "last", "decoder", "layer's", "hidden", "states", "and", "un-shifted", "ground", "truth", "caption", "token", "ids", "with", "a", "special", "token", "EOS", "(end", "of", "the", "sentence)", "appended.L", "gen", "=", "\u2212", "E", "\u0177i", "\u223c\u0177", "log", "p", "\u0177i", "|", "\u0177&lt,i,", "T,", "\u1e7c,", "\u03b8.", "(3)It", "is", "noted", "that", "\u0177", "is", "the", "masked", "version", "of", "the", "ground-truth", "caption", "y.", "To", "make", "a", "fair", "comparison", "with", "the", "baseline", "(Pont-Tuset", "et", "al.,", "2020),", "we", "apply", "the", "same", "setting", "and", "do", "not", "employ", "common", "techniques", "such", "as", "label", "smoothing", "(Szegedy et al., 2016)", "or", "self-critical", "training", "(Rennie et al., 2017)."], "cited_papers": [{"title": "Rethinking the inception architecture for computer vision", "year": "2016", "authors": ["C Szegedy", "V Vanhoucke", "S Ioffe", "J Shlens", "Z Wojna"]}], "target_citation_location": 170, "citation_locations": [170, 174], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 3, 3, 3, 3]]}
{"id": "e9e03e53-206d-40bd-b689-3bcd548d5168", "citing_paper": {"title": "Interactive multilingual text generation for a monolingual user", "year": 1992, "authors": ["Harold Somers"]}, "text": ["The", "second", "problem", "of", "coverage", "can", "be", "paraphrased", "as", "\"Are", "there", "any", "examples", "in", "the", "database", "which", "match", "this", "input", "at", "all?\",", "and", "is", "the", "basic", "problem", "with", "EBMT", "in", "that,", "in", "the", "extreme,", "EBMT", "ignores", "the", "relevance", "of", "linguistic", "generalities.", "It", "can", "be", "overcome", "by", "making", "the", "source", "text", "and", "target", "text", "composition", "process", "'recombinant'", "([18, 37]", ").", "This", "recombination", "of", "partially", "matching", "examples", "is", "significantly", "different", "from", "the", "corresponding", "technique", "of", "target", "text", "generation", "in", "standard", "MT", "systems,", "which", "is", "essentially", "rule-driven", "'direct", "replacement'", "(cf.", "[27],", "pp.", "200f)", "of", "an", "abstract", "representation", "with", "the", "corresponding", "text,", "based", "on", "pre-determined", "choices", "(cf.", "[15],", "pp.137f).", "In", "recombination", "of", "examples,", "it", "is", "necessary", "to", "locate", "the", "partially", "matched", "examples", "in", "a", "broader", "rhetorical", "structure:", "in", "order", "to", "accomplish", "this", "the", "representations", "and", "the", "contextual", "relation", "definitions", "of", "the", "examples,", "and", "the", "global", "intentional", "model", "all", "play", "their", "part.", "It", "is", "important", "to", "ascertain", "the", "grammatical,", "pragmatic", "and", "stylistic", "legality", "of", "recombining", "examples", "and", "to", "maintain", "textual", "cohesion.", "In", "some", "(extreme)", "cases,", "the", "system", "may", "require", "the", "user", "to", "rephrase", "the", "input", "so", "as", "to", "match", "more", "closely", "the", "expected", "input."], "cited_papers": [{"title": "An Introduction to Machine Translation", "year": "1992", "authors": ["W Hutchins", "H Somers"]}], "target_citation_location": 102, "citation_locations": [56, 86, 102], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "edf245db-d026-4dfc-b6d0-32acdb95c8a8", "citing_paper": {"title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection", "year": 2021, "authors": ["Henry Weld", "Guanghao Huang", "Jean Lee", "Tongshu Zhang", "Kunze Wang", "Xinghong Guo", "Siqu Long", "Josiah Soyeon", "Caren Han"]}, "text": ["In-game", "chat", "has", "similar", "characteristics", "to", "multi-turn", "dialogue", "in", "NLU.", "The", "approaches", "used", "in", "multi-turn", "dialogue", "analysis", "have", "not", "yet", "been", "observed", "in", "toxicity", "datasets.", "In", "NLU,", "generally,", "intent", "classification", "(IC)", "is", "treated", "as", "a", "semantic", "utterance", "classification", "task", "and", "slot", "filling", "(SF)", "is", "treated", "as", "a", "sequential", "token", "labelling", "task", "(Zhang and Wang, 2016)."], "cited_papers": [{"title": "A joint model of intent determination and slot filling for spoken language understanding", "year": "2016", "authors": ["Xiaodong Zhang", "Houfeng Wang"]}], "target_citation_location": 51, "citation_locations": [51], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "ef2c52e6-8135-470f-86a7-2e43e7dd9176", "citing_paper": {"title": "SemEval 2022 Task 12: Symlink Linking Mathematical Symbols to their Descriptions", "year": 2022, "authors": ["Viet Lai", "Amir Pouran", "Ben Veyseh", "Franck Dernoncourt", "Thien Nguyen"]}, "text": ["Early", "studies", "for", "scientific", "literature", "link", "formulae", "to", "Wikipedia", "page", "(Nghiem Quoc et al., 2010, Kristianto et al., 2016).", "Even", "though", "this", "can", "provide", "additional", "information", "regarding", "the", "mathematical", "expression,", "a", "reader", "might", "find", "it", "harder", "to", "understand", "the", "Wikipedia", "page", "as", "it", "is", "presented", "in", "many", "unrelated", "forms.", "Linking", "to", "the", "description", "in", "the", "same", "document", "is", "more", "practical", "(Kristianto et al., 2014, Alexeeva et al., 2020)", "as", "the", "descriptions", "are", "dedicated", "to", "the", "symbols", "and", "the", "context", "presented", "in", "the", "document."], "cited_papers": [{"title": "Mining coreference relations between formulas and text using Wikipedia", "year": "2010", "authors": ["Minh Nghiem Quoc", "Keisuke Yokoi"]}, {"title": "Entity linking for mathematical expressions in scientific documents", "year": "2016", "authors": ["Giovanni Kristianto", "Goran Topi\u0107", "Akiko Aizawa"]}], "target_citation_location": 10, "citation_locations": [10, 52], "citation_type": "group", "annotations": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "f4c66f82-ab58-4bce-b883-cc43eaa9b6cf", "citing_paper": {"title": "NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques", "year": 2021, "authors": ["Gunjan Chhablani", "Abheesht Sharma", "Harshit Pandey", "Yash Bhartia", "Shan Suthaharan"]}, "text": ["We", "use", "the", "BERT-based", "Span", "Prediction", "(Figure", "1c)", "models", "based", "on", "Extractive", "Question", "Answering", "systems", "similar", "to", "work", "on", "SQuAD", "(Rajpurkar et al., 2016)", "and", "MRQA", "(Fisch et al., 2019).", "In", "these", "systems,", "the", "output", "at", "each", "token", "is", "a", "start", "logit", "and", "an", "end", "logit", "denoting", "whether", "that", "token", "is", "a", "start", "token", "or", "an", "end", "token", "of", "the", "span,", "depending", "on", "the", "softmax", "value.", "Since", "the", "Toxic", "Spans", "text", "can", "have", "multiple", "toxic", "spans,", "we", "take", "different", "contiguous", "spans", "from", "the", "given", "offsets,", "and", "make", "several", "'samples'", "out", "of", "the", "example.", "Each", "span", "becomes", "an", "'answer'", "for", "the", "particular", "text", "sample.", "We", "use", "the", "word", "'offense'", "as", "a", "dummy", "question.", "Thus,", "each", "contiguous", "span", "leads", "to", "one", "'sample'", "for", "every", "example", "(Table", "1).", "We", "store", "the", "start", "index", "of", "the", "text,", "similar", "to", "the", "SQuAD", "(Rajpurkar et al., 2016)", "dataset,", "and", "process", "the", "data", "to", "provide", "start", "and", "end", "token", "positions", "during", "training.", "The", "classifier", "layer", "on", "top", "of", "the", "encoder", "embeddings", "performs", "a", "binary", "classification", "task", "for", "start", "and", "end", "positions.", "A", "span", "is", "scored", "using", "the", "sum", "of", "predicted", "start", "and", "end", "logits.", "From", "top-K", "start", "and", "end", "logits,", "valid", "predicted", "answer", "spans", "4", "are", "chosen", "during", "postprocessing.", "A", "union", "of", "all", "the", "corresponding", "offsets", "is", "taken", "to", "give", "the", "final", "prediction", "for", "the", "example.", "A", "threshold", "is", "learned", "on", "the", "span", "scores", "using", "the", "resulting", "dev", "set", "F", "1", "score", "on", "offsets,", "which", "is", "then", "used", "for", "test", "set", "prediction.", "All", "spans", "with", "score", "above", "threshold", "are", "considered", "to", "be", "toxic", "spans.BERT-based", "Model", "[EMB]", "you", "pathetic", "troll", "[SEP]", "[CLS]", "[EMB]", "[EMB]", "CLFR", "NT", "T", "[EMB]", "[EMB]", "1", "0", "CLFR", "NT", "T", "1", "0", "CLFR", "NT", "T", "0", "1", "CLFR", "NT", "T", "0", "1", "CLFR", "NT", "T", "X", "X", "(a)", "Token", "Classification", "BERT-based", "Model", "[EMB]", "[EMB]", "[EMB]", "CLFR", "S", "E", "[EMB]", "[EMB]", "0", "0", "CLFR", "0", "0", "CLFR", "1", "0", "CLFR", "0", "1", "CLFR", "0", "0", "S", "E", "S", "E", "S", "E", "S", "E", "CLFR", "NT", "T", "1", "0", "CLFR", "NT", "T", "1", "0", "CLFR", "NT", "T", "0", "1", "CLFR", "NT", "T", "0", "1", "CLFR", "NT", "T", "X", "X", "you", "pathetic", "troll", "[SEP]", "[CLS]", "(b)", "Span+Token", "BERT-based", "Model", "[EMB]", "[EMB]", "[EMB]", "CLFR", "S", "E", "[EMB]", "[EMB]", "0", "0", "CLFR", "0", "0", "CLFR", "1", "0", "CLFR", "0", "1", "CLFR", "0", "0", "S", "E", "S", "E", "S", "E", "S", "E", "you", "pathetic", "troll", "[SEP]", "[CLS]", "(c)", "Span", "Prediction", "BERT-based", "Model", "[EMB]", "CLFR", "1", "0", "S", "E", "[EMB]", "CLFR", "0", "1", "S", "E", "[EMB]", "CLFR", "0", "0", "S", "E", "[EMB]", "CLFR", "1", "0", "S", "E", "[EMB]", "CLFR", "0", "1", "S", "E", "[EMB]", "CLFR", "0", "0", "S", "E", "[EMB]", "CLFR", "0", "0", "S", "E", "[CLS]", "dumb", "ignorant", "boy", "pathetic", "troll", "[SEP]", "(d)", "Multi-Spans", "BERT-based", "Model", "[EMB]", "[EMB]", "[EMB]", "BiLSTM", "NT", "T", "[EMB]", "[EMB]", "0", "0", "BiLSTM", "BiLSTM", "BiLSTM", "BiLSTM", "CRF", "D", "1", "NT", "T", "1", "0", "0", "D", "NT", "T", "0", "1", "0", "D", "NT", "T", "0", "1", "0", "D", "NT", "T", "X", "X", "X", "D", "you", "pathetic", "troll", "[SEP]", "[CLS]", "(e)", "LSTM-CRF"], "cited_papers": [{"title": "MRQA 2019 shared task: Evaluating generalization in reading comprehension", "year": "2019", "authors": ["Adam Fisch", "Alon Talmor", "Robin Jia", "Minjoon Seo", "Eunsol Choi", "Danqi Chen"]}], "target_citation_location": 23, "citation_locations": [20, 23, 131], "citation_type": "single", "annotations": [[2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "f561ba37-4846-443c-97ba-9284e03bdb71", "citing_paper": {"title": "On the Usability of Transformers-based models for a French Question-Answering task", "year": 2021, "authors": ["Oralie Cattan", "Christophe Servan", "Sophie Rosset"]}, "text": ["FrALBERT", "Crawl", "projects", "such", "as", "OSCAR", "(Ortiz Su\u00e1rez et al., 2020)", "or", "CCNet", "(Wenzek et al., 2020)", "corpora,", "and", "because", "we", "focus", "on", "factual", "QA,", "we", "decide", "to", "use", "only", "Wikipedia", "as", "our", "primary", "source", "of", "knowledge.", "We", "used", "the", "same", "learning", "configuration", "as", "the", "original", "model", "with", "a", "batch", "size", "of", "128", "and", "a", "initial", "learning", "rate", "set", "to", "3.125", "\u00d7", "10", "-4."], "cited_papers": [{"title": null, "year": null, "authors": ["unknown"]}], "target_citation_location": 9, "citation_locations": [6, 9], "citation_type": "single", "annotations": [[1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [0, 0, 0, 3, 3, 3, 3, 3, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}
{"id": "f5bb6807-976e-432f-835f-192282a0f942", "citing_paper": {"title": "On the Usability of Transformers-based models for a French Question-Answering task", "year": 2021, "authors": ["Oralie Cattan", "Christophe Servan", "Sophie Rosset"]}, "text": ["French", "is", "a", "poorly", "endowed", "language", "since", "we", "do", "not", "have", "enough", "annotated", "data", "to", "train", "a", "deep", "learning", "model", "on", "QA", "tasks.", "Moreover,", "unlike", "the", "only", "two", "large", "monolingual", "French", "models:", "CamemBERT", "(Martin et al., 2020)", "and", "FlauBERT", "(Le et al., 2020),", "the", "English", "BERT", "model", "has", "become", "a", "branching", "point", "from", "which", "a", "growing", "number", "of", "large", "and", "compact", "English", "pre-trained", "models", "have", "emerged.", "These", "French", "monolingual", "models,", "although", "they", "provide", "good", "performances,", "do", "not", "reflect", "the", "rapid", "evolution", "of", "the", "field."], "cited_papers": [{"title": "FlauBERT: Unsupervised language model pre-training for French", "year": "2020", "authors": ["Hang Le", "Lo\u00efc Vial", "Jibril Frej", "Vincent Segonne", "Maximin Coavoux", "Benjamin Lecouteux", "Alexandre Allauzen", "Benoit Crabb\u00e9", "Laurent Besacier", "Didier Schwab"]}], "target_citation_location": 36, "citation_locations": [33, 36], "citation_type": "single", "annotations": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
{"id": "fed0077b-fbdb-45ad-b1bb-0ca1697e60a1", "citing_paper": {"title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection", "year": 2021, "authors": ["Henry Weld", "Guanghao Huang", "Jean Lee", "Tongshu Zhang", "Kunze Wang", "Xinghong Guo", "Siqu Long", "Josiah Soyeon", "Caren Han"]}, "text": ["RNN-NLU", "(Liu and Lane, 2016)", "is", "an", "attention-based", "bi-directional", "recurrent", "neural", "network", "model", "that", "jointly", "predicts", "the", "current", "slot", "and", "the", "intent", "at", "each", "time", "step", "using", "shared", "hidden", "states", "and", "attention."], "cited_papers": [{"title": "Attention-based recurrent neural network models for joint intent detection and slot filling", "year": "2016", "authors": ["Bing Liu", "Ian Lane"]}], "target_citation_location": 1, "citation_locations": [1], "citation_type": "single", "annotations": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
