token_context,word_context,seg_context,sent_cotext,label
"['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in ( #AUTHOR_TAG )']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in ( #AUTHOR_TAG )']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in ( #AUTHOR_TAG )']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in ( #AUTHOR_TAG )']",5
"['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",0
"['english weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we']","['model was also used to induce a translation lexicon from a 6200 - word corpus of french / english weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we']","['english weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']","['model was also used to induce a translation lexicon from a 6200 - word corpus of french / english weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']",0
"['##s ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a']","['- occurrence with the exception of (Fung, 1998 b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a ; Melamed, 1995).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997),']","['##s ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a']","['- occurrence with the exception of (Fung, 1998 b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a ; Melamed, 1995).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;']",0
"['. g. ( Macklovitch , 1994 ; Melamed , 1996 b ) ), concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ), computerassisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( Church & Hovy , 1993 ), certain machine - assisted translation tools ( e. g. ( Macklovitch , 1994 ; Melamed , 1996 b ) ), concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ), computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']","['. g. ( Macklovitch , 1994 ; Melamed , 1996 b ) ), concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ), computerassisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( Church & Hovy , 1993 ), certain machine - assisted translation tools ( e. g. ( Macklovitch , 1994 ; Melamed , 1996 b ) ), concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ), computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']",0
"[""the definition of a ` ` word'' to include multi - word lexical units ( #AUTHOR_TAG )."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy.', ""another interesting extension is to broaden the definition of a ` ` word'' to include multi - word lexical units ( #AUTHOR_TAG )."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a']","[""the definition of a ` ` word'' to include multi - word lexical units ( #AUTHOR_TAG )."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy.', ""another interesting extension is to broaden the definition of a ` ` word'' to include multi - word lexical units ( #AUTHOR_TAG )."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']",3
"['##s are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following ( #AUTHOR_TAG ) 2.', ""when the l ( u, v ) are re - estimated, the model's hidden""]","['that u and v can be mutual translations.', 'for each co - occurring pair of word types u and v, these likelihoods are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following ( #AUTHOR_TAG ) 2.', ""when the l ( u, v ) are re - estimated, the model's hidden""]","['translation model consists of the hidden parameters a + and a -, and likelihood ratios l ( u, v ).', 'the two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'l ( u, v ) represents the likelihood that u and v can be mutual translations.', 'for each co - occurring pair of word types u and v, these likelihoods are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following ( #AUTHOR_TAG ) 2.', ""when the l ( u, v ) are re - estimated, the model's hidden""]","['translation model consists of the hidden parameters a + and a -, and likelihood ratios l ( u, v ).', 'the two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'l ( u, v ) represents the likelihood that u and v can be mutual translations.', 'for each co - occurring pair of word types u and v, these likelihoods are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following ( #AUTHOR_TAG ) 2.', ""when the l ( u, v ) are re - estimated, the model's hidden parameters come into play""]",5
"['#AUTHOR_TAG ).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other (Gale & Church, 1991; Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997),']","['#AUTHOR_TAG ).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other (Gale & Church, 1991; Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997),']","['#AUTHOR_TAG ).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other (Gale & Church, 1991; Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997),']","['the exception of ( Fung , 1995 b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995 a ; #AUTHOR_TAG ).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other (Gale & Church, 1991; Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997)']",0
"['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",1
"[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm ( #AUTHOR_TAG ), they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr (']","[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm ( #AUTHOR_TAG ), they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr ( datalmodel ) in a non - monotonic fashion.', 'we have adopted the simple heuristic that the model "" has converged "" when this probability stops increasing']","[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm ( #AUTHOR_TAG ), they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr (']","[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm ( #AUTHOR_TAG ), they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr ( datalmodel ) in a non - monotonic fashion.', 'we have adopted the simple heuristic that the model "" has converged "" when this probability stops increasing']",0
"['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy ( #AUTHOR_TAG ).', 'another interesting extension is to broad']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy ( #AUTHOR_TAG ).', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units (Smadja, 1992).', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy ( #AUTHOR_TAG ).', 'another interesting extension is to broad']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy ( #AUTHOR_TAG ).', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units (Smadja, 1992).', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']",3
"['a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( #AUTHOR_TAG )']","['a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( #AUTHOR_TAG )']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( #AUTHOR_TAG )']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( #AUTHOR_TAG )']",0
"['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",1
"[""##we could just as easily use other symmetric ` ` association'' measures, such as 02 ( #AUTHOR_TAG ) or the dice coefficient ( Smadja , 1992 )."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk']","[""##we could just as easily use other symmetric ` ` association'' measures, such as 02 ( #AUTHOR_TAG ) or the dice coefficient ( Smadja , 1992 )."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk']","[""##we could just as easily use other symmetric ` ` association'' measures, such as 02 ( #AUTHOR_TAG ) or the dice coefficient ( Smadja , 1992 )."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk']","[""##we could just as easily use other symmetric ` ` association'' measures, such as 02 ( #AUTHOR_TAG ) or the dice coefficient ( Smadja , 1992 )."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates "" (Dagan et al., 1993).', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996 c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",1
"['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g.', '(Macklovitch, 1994;Melamed, 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993;,  computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']",0
"['be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ).', 'the']","['be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ).', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'the correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ).', 'the likelihoods in the word - to - word model remain unnormalized, so they do not']","['some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'the correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ).', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']",0
"['models ( #AUTHOR_TAG b ).', 'for']","['models ( #AUTHOR_TAG b ).', 'for']","['models ( #AUTHOR_TAG b ).', 'for']","['account for this difference, we can estimate separate values of x + and a - for different ranges of n ( u, v ).', 'similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'just as easily, we can model links that coincide with entries in a pre - existing translation lexicon separately from those that do not.', 'this method of incorporating dictionary information seems simpler than the method proposed by brown et al. for their models ( #AUTHOR_TAG b ).', 'for their models (Brown et al., 1993 b ).', 'when the hidden parameters are conditioned on different link classes, the estimation method does not change ; it is just repeated for each link class']",1
"['english weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we']","['model was also used to induce a translation lexicon from a 6200 - word corpus of french / english weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we']","['english weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']","['model was also used to induce a translation lexicon from a 6200 - word corpus of french / english weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']",1
"['the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from ( #AUTHOR_TAG ), we find that the performance of samt system is similar with the method of']","['the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from ( #AUTHOR_TAG ), we find that the performance of samt system is similar with the method of']","['the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from ( #AUTHOR_TAG ), we find that the performance of samt system is similar with the method of labeling scfg rules']","['the u - trees, we run the gibbs sampler for 1000 iterations on the whole corpus.', 'the sampler uses 1, 087s per iteration, on average, using a single core, 2. 3 ghz intel xeon machine.', 'for the hyperparameters, we set i to 0. 1 and p expand = 1 / 3 to give a preference to the rules with small fragments.', 'we built an s2t translation system with the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from ( #AUTHOR_TAG ), we find that the performance of samt system is similar with the method of labeling scfg rules with pos tags.', 'thus, to be convenient, we only conduct experiments with the samt system']",4
"['nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, #AUTHOR_TAG designed a sampler to infer an']","['nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, #AUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules 7 in terms of frontier nodes (Galley et al., 2004).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold italic nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, #AUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules 7 in terms of frontier nodes (Galley et al., 2004).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold italic nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, #AUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']",4
"['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system ( #AUTHOR_TAG ).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u']","['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system ( #AUTHOR_TAG ).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our']","['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system ( #AUTHOR_TAG ).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system ( #AUTHOR_TAG ).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['into the model more freely and effectively.', '#AUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scf']","['into the model more freely and effectively.', '#AUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scfg )']","['source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scf']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['as s2t ).', 'the system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['as s2t ).', 'the system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']",5
"['source sentences for syntactic pre - reordering.', 'our previous work ( #AUTHOR_TAG ) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u']","['source sentences for syntactic pre - reordering.', 'our previous work ( #AUTHOR_TAG ) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be']","['source sentences for syntactic pre - reordering.', 'our previous work ( #AUTHOR_TAG ) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scf']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work ( #AUTHOR_TAG ) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser (Petrov et al., 2006).', 'then, we binarize the english parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser (Petrov et al., 2006).', 'then, we binarize the english parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser (Petrov et al., 2006).', 'then, we binarize the english parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser (Petrov et al., 2006).', 'then, we binarize the english parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']",5
"['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011 b )']",0
"['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['##g production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow #AUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']","['base distribution 0 ( | ) p r n is designed to assign prior probabilities to the stsg production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow #AUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']","['base distribution 0 ( | ) p r n is designed to assign prior probabilities to the stsg production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow #AUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']","['base distribution 0 ( | ) p r n is designed to assign prior probabilities to the stsg production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow #AUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']",5
"['effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the scfg rules']","['effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the scfg rules']","['focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the scfg rules']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['nodes ( #AUTHOR_TAG ).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold']","['nodes ( #AUTHOR_TAG ).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold italic nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold italic nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']",5
"['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert (Och, 2004) to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach ( #AUTHOR_TAG )']","['evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert (Och, 2004) to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach ( #AUTHOR_TAG )']","['experiments are conducted on chinese - to - english translation.', 'the training data are the fbis corpus with approximately 7. 1 million chinese words and 9. 2 million english words.', 'we obtain the bidirectional word alignment with giza + +, and then adopt the grow - diag - final - and strategy to obtain the final symmetric alignment.', 'we train a 5gram language model on the xinhua portion of the english gigaword corpus and the english part of the training data.', 'for tuning and testing, we use the nist mt 2003 evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert (Och, 2004) to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach ( #AUTHOR_TAG )']","['experiments are conducted on chinese - to - english translation.', 'the training data are the fbis corpus with approximately 7. 1 million chinese words and 9. 2 million english words.', 'we obtain the bidirectional word alignment with giza + +, and then adopt the grow - diag - final - and strategy to obtain the final symmetric alignment.', 'we train a 5gram language model on the xinhua portion of the english gigaword corpus and the english part of the training data.', 'for tuning and testing, we use the nist mt 2003 evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert (Och, 2004) to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach ( #AUTHOR_TAG )']",5
"['as s2t ).', 'the system is implemented based on ( #AUTHOR_TAG ) and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on ( #AUTHOR_TAG ) and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['as s2t ).', 'the system is implemented based on ( #AUTHOR_TAG ) and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on ( #AUTHOR_TAG ) and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']",5
"['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', '#AUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u']","['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', '#AUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our']","['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', '#AUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', '#AUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do']","['are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do']","['are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['.', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules ( #AUTHOR_TAG ) here to']","['sampler might reinforce the frequent alignment errors ( ae ), which would harm the translation model ( tm ).', 'actually, the frequent aes also greatly impair the conventional tm.', 'besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent aes.', 'thus, compared with the conventional tms, we believe that our final tm would not be worse due to aes.', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules ( #AUTHOR_TAG ) here to']","['.', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules ( #AUTHOR_TAG ) here to']","['sampler might reinforce the frequent alignment errors ( ae ), which would harm the translation model ( tm ).', 'actually, the frequent aes also greatly impair the conventional tm.', 'besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent aes.', 'thus, compared with the conventional tms, we believe that our final tm would not be worse due to aes.', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler']",5
"['as s2t ).', 'the system is implemented based on (Galley et al., 2006) and ).', 'in the system, we extract both the minimal ghkm rules ( #AUTHOR_TAG ), and the rules of spmt model 1 ( Galley et al. , 2006 ) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on (Galley et al., 2006) and ).', 'in the system, we extract both the minimal ghkm rules ( #AUTHOR_TAG ), and the rules of spmt model 1 ( Galley et al. , 2006 ) with phrases up to length l = 5 on the source side']","['as s2t ).', 'the system is implemented based on (Galley et al., 2006) and ).', 'in the system, we extract both the minimal ghkm rules ( #AUTHOR_TAG ), and the rules of spmt model 1 ( Galley et al. , 2006 ) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on (Galley et al., 2006) and ).', 'in the system, we extract both the minimal ghkm rules ( #AUTHOR_TAG ), and the rules of spmt model 1 ( Galley et al. , 2006 ) with phrases up to length l = 5 on the source side']",5
"['##km algorithm ( #AUTHOR_TAG ), we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i.']","['obviously, towards an s - node for sampling, the two values of o would define two different u - trees.', 'using the ghkm algorithm ( #AUTHOR_TAG ), we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i. e., minimal ghkm translation rules ) of its own.', ""in the two derivations, the stsg rules defined by the two states include the one rooted at the s - node's lowest ancestor frontier node, and the one rooted at the s - node if it is a frontier node."", '']","[', the two values of o would define two different u - trees.', 'using the ghkm algorithm ( #AUTHOR_TAG ), we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i.']","['first gibbs operator, rotate, just works by sampling value of the oparameters, one at a time, and changing the u - tree accordingly.', 'for example, in figure 3 ( a ), the s - node is currently in the left vwdwho : hvdpsohwkhoriwklvqrghdqgli wkhvdpsohgydoxhriolvzhnhhswkhvwuxfwxuh unchanged, i. e., in the left state.', 'otherwise, we change its state to the right state o, and transform the u - tree to figure 3 obviously, towards an s - node for sampling, the two values of o would define two different u - trees.', 'using the ghkm algorithm ( #AUTHOR_TAG ), we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i. e., minimal ghkm translation rules ) of its own.', ""in the two derivations, the stsg rules defined by the two states include the one rooted at the s - node's lowest ancestor frontier node, and the one rooted at the s - node if it is a frontier node."", 'for instance, in figure 3 ( a ), as the s - node is not a frontier node, the left state ( o ) defines only one rule : using these stsg rules, the two derivations are evaluated as follows ( we use the value of o to denote the corresponding stsg derivation )']",5
"['; Quirk et al. , 2005 ; #AUTHOR_TAG, 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['; Quirk et al. , 2005 ; #AUTHOR_TAG, 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG, 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG, 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; #AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
['mt ( samt ) 11 system ( #AUTHOR_TAG ) respectively'],"['create the baseline system, we use the opensource joshua 4. 0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase - based ( hpb ) system, and a syntax - augmented mt ( samt ) 11 system ( #AUTHOR_TAG ) respectively']","['create the baseline system, we use the opensource joshua 4. 0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase - based ( hpb ) system, and a syntax - augmented mt ( samt ) 11 system ( #AUTHOR_TAG ) respectively']","['create the baseline system, we use the opensource joshua 4. 0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase - based ( hpb ) system, and a syntax - augmented mt ( samt ) 11 system ( #AUTHOR_TAG ) respectively']",5
"['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser ( #AUTHOR_TAG ).', 'then, we binarize the english parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser ( #AUTHOR_TAG ).', 'then, we binarize the english parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser ( #AUTHOR_TAG ).', 'then, we binarize the english parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser ( #AUTHOR_TAG ).', 'then, we binarize the english parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system']",5
"['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation ( #AUTHOR_TAG ; Zhang et al. , 2011 a )']","['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation ( #AUTHOR_TAG ; Zhang et al. , 2011 a )']","['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation ( #AUTHOR_TAG ; Zhang et al. , 2011 a )']","['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation ( #AUTHOR_TAG ; Zhang et al. , 2011 a )']",4
"['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG, 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG, 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG, 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG, 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models ( #AUTHOR_TAG )']","['training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models ( #AUTHOR_TAG )']","['training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models ( #AUTHOR_TAG )']","[', for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of tree - bank resources for training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models ( #AUTHOR_TAG )']",0
"['#AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['#AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['#AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( str | frag ) in equation ( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( str | frag ) in equation ( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar ( #AUTHOR_TAG ) 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']","['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar ( #AUTHOR_TAG ) 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']","['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar ( #AUTHOR_TAG ) 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']","['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar ( #AUTHOR_TAG ) 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']",1
"['effective unsupervised tree structures for tree - based translation models.', '#AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules']","['effective unsupervised tree structures for tree - based translation models.', '#AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules']","['focus on generating effective unsupervised tree structures for tree - based translation models.', '#AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', '#AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan (Kobayashi et al., 1992).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search ( #AUTHOR_TAG ) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan (Kobayashi et al., 1992).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search ( #AUTHOR_TAG ) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan (Kobayashi et al., 1992).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search ( #AUTHOR_TAG ) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan (Kobayashi et al., 1992).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search ( #AUTHOR_TAG ) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']",0
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ).', '']","['responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ).', '']","['have been several efforts aimed at developing a domain - independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ).', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no work on domain - independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which wit handles well.', 'therefore incorporating those techniques remains as a future work']","['have been several efforts aimed at developing a domain - independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ).', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no work on domain - independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which wit handles well.', 'therefore incorporating those techniques remains as a future work']",3
"['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG']","['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG']","['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG']","['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG']",1
"['.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan ( #AUTHOR_TAG ).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search (Hirasawa et al., 1998) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan ( #AUTHOR_TAG ).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search (Hirasawa et al., 1998) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan ( #AUTHOR_TAG ).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search (Hirasawa et al., 1998) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan ( #AUTHOR_TAG ).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search (Hirasawa et al., 1998) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']",5
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( #AUTHOR_TAG b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather']","['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( #AUTHOR_TAG b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( Dohsaka et al. , 2000 ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']","['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( #AUTHOR_TAG b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( Dohsaka et al. , 2000 ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( #AUTHOR_TAG b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( Dohsaka et al. , 2000 ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']",2
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ).', 'the language generation module']","['can be covered.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ).', 'the language generation module']","['.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ).', 'the language generation module features common lisp functions, so there is no limitation on the description.', 'some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).', 'it is also possible to build a simple finite - state - model - based dialogue system using wit.', '']","['previous finite - state - model - based toolkits place many severe restrictions on domain descriptions, wit has enough descriptive power to build a variety of dialogue systems.', 'although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.', 'for example, it is possible to represent a discourse stack whose depth is limited.', 'recording some dialogue history is also possible.', 'since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ).', 'the language generation module features common lisp functions, so there is no limitation on the description.', 'some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).', 'it is also possible to build a simple finite - state - model - based dialogue system using wit.', 'states can be represented by dialogue phases in wit']",3
"['is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997)']","['is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997)']","['the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997)']","['the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997)']",0
"['##miation system ( #AUTHOR_TAG ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre -']","['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( Nakano et al. , 1999 b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( #AUTHOR_TAG ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']","['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( Nakano et al. , 1999 b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( #AUTHOR_TAG ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( Nakano et al. , 1999 b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( #AUTHOR_TAG ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']",2
"['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method ( #AUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain -']","['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method ( #AUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain - dependent system specifications into internal knowledge sources so that building systems is easier.', 'although wit requires more domaindependent specifications than finite - state - modelbased toolkits, wit - based systems are capable of taking full advantage of language processing technology.', 'wit has been implemented and used to build several spoken dialogue systems']","['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method ( #AUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain - dependent system specifications into internal knowledge sources so that building systems is easier.', 'although wit requires more domaindependent specifications than finite - state - modelbased toolkits, wit - based systems are capable of taking full advantage of language processing technology.', 'wit has been implemented and used to build several spoken dialogue systems']","['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method ( #AUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain - dependent system specifications into internal knowledge sources so that building systems is easier.', 'although wit requires more domaindependent specifications than finite - state - modelbased toolkits, wit - based systems are capable of taking full advantage of language processing technology.', 'wit has been implemented and used to build several spoken dialogue systems']",5
"['knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ).', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ).', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ).', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ).', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']",5
"['in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']","['in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']","['to dcg (Pereira and Warren, 1980) rules ; they can include logical variables and these variables can be bound when these rules are applied.', 'it is possible to add to the rules constraints that stipulate relationships that must hold among variables ( nakano, 199 i ), but we do not explain these constraints in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']","['roles are similar to dcg (Pereira and Warren, 1980) rules ; they can include logical variables and these variables can be bound when these rules are applied.', 'it is possible to add to the rules constraints that stipulate relationships that must hold among variables ( nakano, 199 i ), but we do not explain these constraints in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']",5
"['this end, several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ).', 'one is the cslu toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']","['this end, several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ).', 'one is the cslu toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']","['this end, several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ).', 'one is the cslu toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']","['this end, several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ).', 'one is the cslu toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']",0
"['##s the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search ) ( #AUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental']","['from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search ) ( #AUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental']","['##s the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search ) ( #AUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental understanding of user utterances that are not segmented into sentences prior to pars - ing by incrementally finding the most plausible sequence of sentences ( or significant utterances in the isss terms ) out of the possible sentence sequences for the input word sequence.', 'isss also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time']","['language understanding : module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search ) ( #AUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental understanding of user utterances that are not segmented into sentences prior to pars - ing by incrementally finding the most plausible sequence of sentences ( or significant utterances in the isss terms ) out of the possible sentence sequences for the input word sequence.', 'isss also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time']",5
"['. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']",1
"['the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by']","['the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', 'the synonym approach is equivalent to changing']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', ""the synonym approach is equivalent to changing all non - zero translation probabilities p ( w ~ [ wy )'s to 1 in our retrieyal function."", 'even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations']",1
"['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test ( #AUTHOR_TAG ) at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']","['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test ( #AUTHOR_TAG ) at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']","['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test ( #AUTHOR_TAG ) at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']","['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test ( #AUTHOR_TAG ) at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']",5
"['##sm ), ( #AUTHOR_TAG ).', 'we believe our approach is computationally']","['third approach to cross - lingual retrieval is to map queries and documents to some intermediate representation, e. g latent semantic indexing ( lsi ) ( Littman et al , 1998 ), or the general vector space model ( gvsm ), ( #AUTHOR_TAG ).', 'we believe our approach is computationally']","['##sm ), ( #AUTHOR_TAG ).', 'we believe our approach is computationally less costly than ( lsi and']","['third approach to cross - lingual retrieval is to map queries and documents to some intermediate representation, e. g latent semantic indexing ( lsi ) ( Littman et al , 1998 ), or the general vector space model ( gvsm ), ( #AUTHOR_TAG ).', 'we believe our approach is computationally less costly than ( lsi and gvsm ) and assumes less resources ( wordnet in Diekema et al., 1999)']",1
['## the transition probability a is 0. 7 using the em algorithm ( #AUTHOR_TAG ) on the trec4 ad - hoc query set'],['## the transition probability a is 0. 7 using the em algorithm ( #AUTHOR_TAG ) on the trec4 ad - hoc query set'],['## the transition probability a is 0. 7 using the em algorithm ( #AUTHOR_TAG ) on the trec4 ad - hoc query set'],['## the transition probability a is 0. 7 using the em algorithm ( #AUTHOR_TAG ) on the trec4 ad - hoc query set'],5
"['.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono - lingual m']",0
"['.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de Jong , 1999 ; #AUTHOR_TAG.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de Jong , 1999 ; #AUTHOR_TAG.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de Jong , 1999 ; #AUTHOR_TAG.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de Jong , 1999 ; #AUTHOR_TAG.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']",0
"['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ).', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']","['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ).', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']","['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ).', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']","['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ).', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']",1
"['the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros,']","['the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros,']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros,']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', ""the synonym approach is equivalent to changing all non - zero translation probabilities p ( w ~ [ wy )'s to 1 in our retrieyal function."", 'even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations']",1
"['this section we compare our approach with two other approaches.', 'one approach is "" simple substitution "", i. e., replacing a query term with all its translations and treating the translated query as a bag of words in mono - lingual retrieval.', 'suppose we have a simple query q = ( a, b ), the translations for a are al, a2, a3, and the translations for b are bl, b2. the translated query would be ( at, a2, a3, b ~, b2 ).', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur ( #AUTHOR_TAG )']","['this section we compare our approach with two other approaches.', 'one approach is "" simple substitution "", i. e., replacing a query term with all its translations and treating the translated query as a bag of words in mono - lingual retrieval.', 'suppose we have a simple query q = ( a, b ), the translations for a are al, a2, a3, and the translations for b are bl, b2. the translated query would be ( at, a2, a3, b ~, b2 ).', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur ( #AUTHOR_TAG )']","['this section we compare our approach with two other approaches.', 'one approach is "" simple substitution "", i. e., replacing a query term with all its translations and treating the translated query as a bag of words in mono - lingual retrieval.', 'suppose we have a simple query q = ( a, b ), the translations for a are al, a2, a3, and the translations for b are bl, b2. the translated query would be ( at, a2, a3, b ~, b2 ).', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur ( #AUTHOR_TAG )']","['this section we compare our approach with two other approaches.', 'one approach is "" simple substitution "", i. e., replacing a query term with all its translations and treating the translated query as a bag of words in mono - lingual retrieval.', 'suppose we have a simple query q = ( a, b ), the translations for a are al, a2, a3, and the translations for b are bl, b2. the translated query would be ( at, a2, a3, b ~, b2 ).', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur ( #AUTHOR_TAG )']",0
"['.', 'each english word has around 1. 5 translations on average.', 'a cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem spanish words.', 'one difference from the treatment of']","['from the internet ( http : / / www. activa. arrakis. es )', 'containing around 22, 000 english words ( 16, 000 english stems ) and processed it similarly.', 'each english word has around 1. 5 translations on average.', 'a cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem spanish words.', 'one difference from the treatment of']","[', 000 english stems ) and processed it similarly.', 'each english word has around 1. 5 translations on average.', 'a cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem spanish words.', 'one difference from the treatment of']","['spanish, we downloaded a bilingual english - spanish lexicon from the internet ( http : / / www. activa. arrakis. es )', 'containing around 22, 000 english words ( 16, 000 english stems ) and processed it similarly.', 'each english word has around 1. 5 translations on average.', 'a cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem spanish words.', 'one difference from the treatment of chinese is to include the english word as one of its own translations in addition to its spanish translations in the lexicon.', 'this is useful for translating proper nouns, which often have identical spellings in english and spanish but are routinely excluded from a lexicon']",5
"['studies which view lr as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']",1
"['studies which view lr as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']",1
"['technique is automatic discovery of translations from parallel or non - parallel corpora ( #AUTHOR_TAG ).', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora ( #AUTHOR_TAG ).', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora ( #AUTHOR_TAG ).', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora ( #AUTHOR_TAG ).', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus - specific vocabularies']",0
"['#AUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']","['#AUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']","['#AUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']","['#AUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']",5
"['( #AUTHOR_TAG ; Charniak , 1997 a']","['full parsers ( #AUTHOR_TAG ; Charniak , 1997 a']","['full parsers ( #AUTHOR_TAG ; Charniak , 1997 a']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['first is the one used in the chunking competition in conll - 2000 ( tjong kim #AUTHOR_TAG ).', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of    different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are :']","['first is the one used in the chunking competition in conll - 2000 ( tjong kim #AUTHOR_TAG ).', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of    different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are : adjective phrase ( adjp ), adverb phrase ( advp ), conjunction phrase ( conjp ), interjection phrase ( intj ), list marker ( lst ), noun phrase ( np ), preposition phrase ( pp ), particle ( prt ), subordinated clause ( sbar ), unlike coordinated phrase ( ucp ), verb phrase ( vp ).', '( see details']","['first is the one used in the chunking competition in conll - 2000 ( tjong kim #AUTHOR_TAG ).', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of    different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are :']","['first is the one used in the chunking competition in conll - 2000 ( tjong kim #AUTHOR_TAG ).', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of    different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are : adjective phrase ( adjp ), adverb phrase ( advp ), conjunction phrase ( conjp ), interjection phrase ( intj ), list marker ( lst ), noun phrase ( np ), preposition phrase ( pp ), particle ( prt ), subordinated clause ( sbar ), unlike coordinated phrase ( ucp ), verb phrase ( vp ).', '( see details in (Tjong Kim Sang and Buchholz, 2000).']",5
"['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunk']","['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years,']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997 a ; Charniak, 1997 b ; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns - syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000)']",0
"['( Collins , 1997 ; #AUTHOR_TAG a']","['full parsers ( Collins , 1997 ; #AUTHOR_TAG a']","['full parsers ( Collins , 1997 ; #AUTHOR_TAG a']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAG a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['Charniak , 1997 a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 )']","['Charniak , 1997 a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 )']","['Charniak , 1997 a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( Charniak , 1997 b ; Charniak , 1997 a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 )']",0
"['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( Collins , 1996 ; #AUTHOR_TAG ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( Collins , 1996 ; #AUTHOR_TAG ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( Collins , 1996 ; #AUTHOR_TAG ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5 (Collins, 1997)']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( Collins , 1996 ; #AUTHOR_TAG ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5 (Collins, 1997)']",5
"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( #AUTHOR_TAG b']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( #AUTHOR_TAG b']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( #AUTHOR_TAG b']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( #AUTHOR_TAG b ; Charniak , 1997 a ; Collins , 1997 ; Ratnaparkhi , 1997 )']",0
"['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( #AUTHOR_TAG ; Roth , 1998 ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( #AUTHOR_TAG ; Roth , 1998 ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( #AUTHOR_TAG ; Roth , 1998 ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( #AUTHOR_TAG ; Roth , 1998 ) is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunk']","['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years,']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997 a ; Charniak, 1997 b ; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns - syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000)']",0
"['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunk']","['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years,']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997 a ; Charniak, 1997 b ; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns - syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000)']",0
"[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ).', 'second, while']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ).', 'second, while']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ).', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', '']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ).', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to produce this level of analysis.', 'this can be augmented later if more information is available.', 'finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low - sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes']",0
"['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim #AUTHOR_TAG ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim #AUTHOR_TAG ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim #AUTHOR_TAG ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim #AUTHOR_TAG ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",5
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['Charniak , 1997 a ; Collins , 1997 ; #AUTHOR_TAG )']","['Charniak , 1997 a ; Collins , 1997 ; #AUTHOR_TAG )']","['Charniak , 1997 a ; Collins , 1997 ; #AUTHOR_TAG )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( Charniak , 1997 b ; Charniak , 1997 a ; Collins , 1997 ; #AUTHOR_TAG )']",0
"['shallow parser used is the snow - based cscl parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim #AUTHOR_TAG ) terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']","['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim #AUTHOR_TAG ) terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']","['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim #AUTHOR_TAG ) terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']","['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim #AUTHOR_TAG ) terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']",5
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",2
"['by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further']","['by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further']","['on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further']","['on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint']",0
"['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( #AUTHOR_TAG ; Collins , 1997 ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( #AUTHOR_TAG ; Collins , 1997 ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( #AUTHOR_TAG ; Collins , 1997 ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5 (Collins, 1997)']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( #AUTHOR_TAG ; Collins , 1997 ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5 (Collins, 1997)']",5
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['shallow parser used is the snow - based cscl parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
['be chunked as follows ( tjong kim #AUTHOR_TAG ) : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow'],['be chunked as follows ( tjong kim #AUTHOR_TAG ) : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow'],"['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abneys work (Abney, 1991), who has suggested to chunk sentences to base level phrases.', 'for example, the sentence he reckons the current account deficit will narrow to only $ 1. 8 billion in september.', 'would be chunked as follows ( tjong kim #AUTHOR_TAG ) : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow ] [ pp to ] [ np only $ 1. 8 billion ] [ pp in ] [ np september ]']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abneys work (Abney, 1991), who has suggested to chunk sentences to base level phrases.', 'for example, the sentence he reckons the current account deficit will narrow to only $ 1. 8 billion in september.', 'would be chunked as follows ( tjong kim #AUTHOR_TAG ) : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow ] [ pp to ] [ np only $ 1. 8 billion ] [ pp in ] [ np september ]']",0
"[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ).', 'second, while']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ).', 'second, while']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ).', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', '']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ).', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to produce this level of analysis.', 'this can be augmented later if more information is available.', 'finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low - sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes']",0
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['Charniak , 1997 b ; #AUTHOR_TAG ), significant progress has been made on the use of statistical learning methods to']","['Charniak , 1997 b ; #AUTHOR_TAG ), significant progress has been made on the use of statistical learning methods to']","['Charniak , 1997 b ; #AUTHOR_TAG ), significant progress has been made on the use of statistical learning methods to']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; #AUTHOR_TAG ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['earlier versions of the snow based cscl were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",2
"['was done on the penn treebank ( #AUTHOR_TAG ) wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and Buchholz, 2000)']","['was done on the penn treebank ( #AUTHOR_TAG ) wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and Buchholz, 2000)']","['was done on the penn treebank ( #AUTHOR_TAG ) wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and Buchholz, 2000)']","['was done on the penn treebank ( #AUTHOR_TAG ) wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and Buchholz, 2000)']",5
"['hpsg ( #AUTHOR_TAG ).', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and']","['hpsg ( #AUTHOR_TAG ).', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equiva - lence between the original and obtained gram - mars.', 'other works (Kasper et al., 1995; Becker and Lopez, 2000) convert hpsg grammars']","['hpsg ( #AUTHOR_TAG ).', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and']","['##eisi et al. also translated ltag into hpsg ( #AUTHOR_TAG ).', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equiva - lence between the original and obtained gram - mars.', 'other works (Kasper et al., 1995; Becker and Lopez, 2000) convert hpsg grammars into ltag grammars.', 'however, given the greater ex - pressive power of hpsg, it is impossible to con - vert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capac - ity.', 'thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad - vantages']",1
"['have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ), and ones on programming / grammar - development environ - (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for']","['have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ), and ones on programming / grammar - development environ - (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for']","['have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ), and ones on programming / grammar - development environ - (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ), and ones on programming / grammar - development environ - (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['parser ( #AUTHOR_TAG ).', 'this result implies that parsing techniques for hp']","['Research Group, 2001), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser ( #AUTHOR_TAG ).', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the']","['Research Group, 2001), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser ( #AUTHOR_TAG ).', 'this result implies that parsing techniques for hp']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag Research Group, 2001), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser ( #AUTHOR_TAG ).', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",1
"['parser ( #AUTHOR_TAG ), c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord, 1994) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser ( #AUTHOR_TAG ), c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord, 1994) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser ( #AUTHOR_TAG ), c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord, 1994) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser ( #AUTHOR_TAG ), c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2 clearly shows that the hpsg parser is significantly faster than the ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper describes the detailed analysis on the factor of the difference of parsing performance']",1
"[') ( #AUTHOR_TAG ) by a method of grammar conversion.', 'the rental system automatically converts an fb - lt']","[') ( #AUTHOR_TAG ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a']","[') ( #AUTHOR_TAG ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hp']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( vijay - Shanker , 1987 ;  vijay - Shanker and Joshi , 1988 ) and head - driven phrase structure grammar ( hpsg ) ( #AUTHOR_TAG ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the lt']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english (']","['4 : adjunction tions called substitution and adjunction.', 'elementary trees are classified into two types, initial trees and auxiliary trees ( figure 2 ).', 'an elementary tree has at least one leaf node labeled with a terminal symbol called an anchor ( marked with  ).', 'in an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node ( marked with  ).', 'in an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes ( marked with ).', 'substitution replaces a substitution node with another initial tree ( figure 3 ).', 'adjunction grafts an auxiliary tree with the root node and foot node labeled u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag Research Group, 2001).', 'the xtag group (Doran et al., 2000) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and Candito, 2000) has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']","['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']","['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']","['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']",0
"['##1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and head - driven phrase structure grammar (']","['such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and head - driven phrase structure grammar (']","['##1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and head - driven phrase structure grammar (']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and head - driven phrase structure grammar ( hpsg ) ( Pollard and Sag , 1994 ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
['##ag ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera'],['##ag ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera'],['##ag ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera'],['##ag ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],0
"[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project ( #AUTHOR_TAG ).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project ( #AUTHOR_TAG ).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project ( #AUTHOR_TAG ).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project ( #AUTHOR_TAG ).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']",0
"[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( #AUTHOR_TAG ), which is used in a high - accuracy japanese dependency analyzer ( Kanayama et al. , 2000 )']","['online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( #AUTHOR_TAG ), which is used in a high - accuracy japanese dependency analyzer ( Kanayama et al. , 2000 )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( #AUTHOR_TAG ), which is used in a high - accuracy japanese dependency analyzer ( Kanayama et al. , 2000 )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( #AUTHOR_TAG ), which is used in a high - accuracy japanese dependency analyzer ( Kanayama et al. , 2000 )']",0
"['parse (Tateisi et al., 1998).', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert hp']","['into hpsg lexical entries.', 'the derivation translator module takes hpsg parse (Tateisi et al., 1998).', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some']","['lexical entries.', 'the derivation translator module takes hpsg parse (Tateisi et al., 1998).', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hp']","['1 depicts a brief sketch of the rental system.', 'the system consists of the following four modules : tree converter, type hierarchy extractor, lexicon converter and derivation translator.', 'the tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in section 3. the type hierarchy extractor module extracts the symbols of the node, features, and feature values from the ltag elementary tree templates and lexicon, and construct the type hierarchy from them.', 'the lexicon converter module converts ltag elementary tree templates into hpsg lexical entries.', 'the derivation translator module takes hpsg parse (Tateisi et al., 1998).', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', 'thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages']",1
"[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( Mitsuishi et al. , 1998 ), which is used in a high - accuracy japanese dependency analyzer ( #AUTHOR_TAG )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( Mitsuishi et al. , 1998 ), which is used in a high - accuracy japanese dependency analyzer ( #AUTHOR_TAG )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( Mitsuishi et al. , 1998 ), which is used in a high - accuracy japanese dependency analyzer ( #AUTHOR_TAG )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( Mitsuishi et al. , 1998 ), which is used in a high - accuracy japanese dependency analyzer ( #AUTHOR_TAG )']",0
"['and id grammar rules, each of which is described with typed feature structures ( #AUTHOR_TAG ).', 'a']","['and id grammar rules, each of which is described with typed feature structures ( #AUTHOR_TAG ).', 'a']","['hpsg grammar consists of lexical entries and id grammar rules, each of which is described with typed feature structures ( #AUTHOR_TAG ).', 'a lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'an id grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'figure 6 illustrates an example of bottom - up parsing with an hpsg grammar.', 'first, lexical entries for "" can "" and "" run "" are unified respectively with the daughter feature structures']","['hpsg grammar consists of lexical entries and id grammar rules, each of which is described with typed feature structures ( #AUTHOR_TAG ).', 'a lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'an id grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'figure 6 illustrates an example of bottom - up parsing with an hpsg grammar.', 'first, lexical entries for "" can "" and "" run "" are unified respectively with the daughter feature structures']",0
"['research #AUTHOR_TAG ), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hp']","['english grammar ( the xtag research #AUTHOR_TAG ), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag research #AUTHOR_TAG ), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hp']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag research #AUTHOR_TAG ), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",5
"['group ( #AUTHOR_TAG ) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', '']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fb - ltag ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag Research Group, 2001).', 'the xtag group ( #AUTHOR_TAG ) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', '']","['Research Group, 2001).', 'the xtag group ( #AUTHOR_TAG ) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', '']","['4 : adjunction tions called substitution and adjunction.', 'elementary trees are classified into two types, initial trees and auxiliary trees ( figure 2 ).', 'an elementary tree has at least one leaf node labeled with a terminal symbol called an anchor ( marked with  ).', 'in an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node ( marked with  ).', 'in an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes ( marked with ).', 'substitution replaces a substitution node with another initial tree ( figure 3 ).', 'adjunction grafts an auxiliary tree with the root node and foot node labeled u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fb - ltag ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag Research Group, 2001).', 'the xtag group ( #AUTHOR_TAG ) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and Candito, 2000) has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['grammar ( #AUTHOR_TAG ).', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the lt']","['1 ) ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) and head - driven phrase structure grammar ( hpsg ) (Pollard and Sag, 1994) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar ( #AUTHOR_TAG ).', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a']","[') (Pollard and Sag, 1994) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar ( #AUTHOR_TAG ).', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hp']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoin - ing grammar ( fb - ltag 1 ) ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) and head - driven phrase structure grammar ( hpsg ) (Pollard and Sag, 1994) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar ( #AUTHOR_TAG ).', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['parser ( #AUTHOR_TAG ), ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hp']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser ( #AUTHOR_TAG ), ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser ( #AUTHOR_TAG ), ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser ( #AUTHOR_TAG ), ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2 clearly shows that the hpsg parser is significantly faster than the ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper describes the detailed analysis on the factor of the difference of parsing performance']",1
"['( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the lt']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english (']","['4 : adjunction tions called substitution and adjunction.', 'elementary trees are classified into two types, initial trees and auxiliary trees ( figure 2 ).', 'an elementary tree has at least one leaf node labeled with a terminal symbol called an anchor ( marked with  ).', 'in an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node ( marked with  ).', 'in an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes ( marked with ).', 'substitution replaces a substitution node with another initial tree ( figure 3 ).', 'adjunction grafts an auxiliary tree with the root node and foot node labeled u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag Research Group, 2001).', 'the xtag group (Doran et al., 2000) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and Candito, 2000) has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['online ( lingo ) project ( #AUTHOR_TAG ).', 'in practical context,']","['online ( lingo ) project ( #AUTHOR_TAG ).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project ( #AUTHOR_TAG ).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project ( #AUTHOR_TAG ).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']",0
"['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['##sg parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ).', 'we applied our system to the']","['(Makino et al., 1998) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ).', 'we applied our system to the']","['##lfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ).', 'we applied our system to the xtag english grammar (']","['rental system is implemented in lil - fes (Makino et al., 1998) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ).', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus (Marcus et al., 1994) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",0
"['parsing.', 'another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']","['conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']","['are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord, 1994) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2 clearly shows that the hpsg parser is significantly faster than the ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']",0
"['and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus ( #AUTHOR_TAG ) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']","['and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus ( #AUTHOR_TAG ) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']","['and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus ( #AUTHOR_TAG ) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']","['rental system is implemented in lil - fes (Makino et al., 1998) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system (Nishida et al., 1999;.', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus ( #AUTHOR_TAG ) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",5
"['rental system is implemented in lilfes ( #AUTHOR_TAG ) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient']","['rental system is implemented in lilfes ( #AUTHOR_TAG ) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system (Nishida et al., 1999;.', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the']","['rental system is implemented in lilfes ( #AUTHOR_TAG ) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system (Nishida et al., 1999;.', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the']","['rental system is implemented in lilfes ( #AUTHOR_TAG ) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system (Nishida et al., 1999;.', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus (Marcus et al., 1994) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",5
"['##1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and head - driven phrase structure grammar (']","['such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and head - driven phrase structure grammar (']","['##1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and head - driven phrase structure grammar (']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and head - driven phrase structure grammar ( hpsg ) ( Pollard and Sag , 1994 ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ), and ones on programming / grammar - development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ), and ones on programming / grammar - development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ), and ones on programming / grammar - development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ), and ones on programming / grammar - development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
['##sg ( #AUTHOR_TAG ) is the core portion of the rental system'],['to hpsg ( #AUTHOR_TAG ) is the core portion of the rental system'],['##sg ( #AUTHOR_TAG ) is the core portion of the rental system'],['grammar conversion from ltag to hpsg ( #AUTHOR_TAG ) is the core portion of the rental system'],0
"['research #AUTHOR_TAG ).', 'the']","['english grammar, a large - scale grammar for english ( the xtag research #AUTHOR_TAG ).', 'the']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fb - ltag ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag research #AUTHOR_TAG ).', 'the xtag group (Doran et al., 2000) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', '']","['4 : adjunction tions called substitution and adjunction.', 'elementary trees are classified into two types, initial trees and auxiliary trees ( figure 2 ).', 'an elementary tree has at least one leaf node labeled with a terminal symbol called an anchor ( marked with  ).', 'in an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node ( marked with  ).', 'in an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes ( marked with ).', 'substitution replaces a substitution node with another initial tree ( figure 3 ).', 'adjunction grafts an auxiliary tree with the root node and foot node labeled u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fb - ltag ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag research #AUTHOR_TAG ).', 'the xtag group (Doran et al., 2000) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and Candito, 2000) has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['research #AUTHOR_TAG ) 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hp']","['english grammar ( the xtag research #AUTHOR_TAG ) 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the']","['on each computational framework and have never been shared among hpsg and ltag communities.', 'we applied our system to the xtag english grammar ( the xtag research #AUTHOR_TAG ) 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hp']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we applied our system to the xtag english grammar ( the xtag research #AUTHOR_TAG ) 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",5
"['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
"['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']","['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']",0
"['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance.', 'this has been reported for other languages,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998)']",0
"['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
"['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance.', 'this has been reported for other languages,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998)']",0
"['#AUTHOR_TAG ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', 'in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (']","['#AUTHOR_TAG ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", 'the']","['#AUTHOR_TAG ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', 'in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (']","['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( Choueka , 1990 ;  j [UNK] appinen and niemist [UNK] o, 1988 ; #AUTHOR_TAG ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", ""the goal is to map all occurring morphological variants to some canonical base forme. g.,'search'in the examples from above""]",0
"['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
"[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method']",1
"['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"['( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']","['( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']","['access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']","['has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']",0
"['html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model ( #AUTHOR_TAG ), with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']","['unbiased evaluation of our approach, we used a home - grown search engine ( implemented in the python script language ).', 'it crawls text / html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model ( #AUTHOR_TAG ), with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']","['html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model ( #AUTHOR_TAG ), with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']","['unbiased evaluation of our approach, we used a home - grown search engine ( implemented in the python script language ).', 'it crawls text / html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model ( #AUTHOR_TAG ), with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']",5
"['has been some controversy, at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
['- texts in an ir setting ( #AUTHOR_TAG )'],['is crucial for any attempt to cope adequately with medical free - texts in an ir setting ( #AUTHOR_TAG )'],"['. g., german ), often referred to as neo - classical compounding ( mc - Cray et al., 1988).', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting ( #AUTHOR_TAG )']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding ( mc - Cray et al., 1988).', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting ( #AUTHOR_TAG )']",0
"['thesaurus ( mesh, ( #AUTHOR_TAG )']","['thesaurus ( mesh, ( #AUTHOR_TAG )']","['of our synonym identifiers to a large medical thesaurus ( mesh, ( #AUTHOR_TAG )']","['##izing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'the gain is still not overwhelming.', ""with regard to orthographic normalization, we expected a higher performance benefit because of the well - known spelling problems for german medical terms of latin or greek origin ( such as in'zakum ','cakum ','zaekum ','caekum ','zaecum ','caecum')."", 'for our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'the same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided ( cf.', 'section 3 ).', 'in the layman queries, there were only few latin or greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'however, the experience with medical text retrieval ( especially on medical reports which exhibit a high rate of spelling variations ) shows that orthographic normalization is a desider - whereas the usefulness of subword indexing became evident, we could not provide sufficient evidence for synonym class indexing, so far.', 'however, synonym mapping is still incomplete in the current state of our subword dictionary.', 'a question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'we have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior ( for all layman and expert queries this method retrieved relevant documents, whereas word - based methods failed in 29. 6 % of the layman queries and 8 % of the expert queries, cf. figure 5 ).', 'it would be interesting to evaluate the retrieval effectiveness ( in terms of precision and recall ) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'this will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( mesh, ( #AUTHOR_TAG ) ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies (Hersh et al., 1995)']",3
"['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
"['Choueka , 1990 ; #AUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995']","['Choueka , 1990 ; #AUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995']","['Choueka , 1990 ; #AUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995']","['efforts required for performing morphologi - cal analysis vary from language to language.', 'for english, known for its limited number of inflection patterns, lexicon - free general - purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998)']",0
"['of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"['of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
['think of user - centered comparative studies ( #AUTHOR_TAG )'],['think of user - centered comparative studies ( #AUTHOR_TAG )'],"[', we may think of user - centered comparative studies ( #AUTHOR_TAG )']","['##izing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'the gain is still not overwhelming.', ""with regard to orthographic normalization, we expected a higher performance benefit because of the well - known spelling problems for german medical terms of latin or greek origin ( such as in'zakum ','cakum ','zaekum ','caekum ','zaecum ','caecum')."", 'for our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'the same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided ( cf.', 'section 3 ).', 'in the layman queries, there were only few latin or greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'however, the experience with medical text retrieval ( especially on medical reports which exhibit a high rate of spelling variations ) shows that orthographic normalization is a desider - whereas the usefulness of subword indexing became evident, we could not provide sufficient evidence for synonym class indexing, so far.', 'however, synonym mapping is still incomplete in the current state of our subword dictionary.', 'a question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'we have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior ( for all layman and expert queries this method retrieved relevant documents, whereas word - based methods failed in 29. 6 % of the layman queries and 8 % of the expert queries, cf. figure 5 ).', 'it would be interesting to evaluate the retrieval effectiveness ( in terms of precision and recall ) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'this will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( mesh, ( nlm, 2001 ) ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies ( #AUTHOR_TAG )']",3
"['( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']","['( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']","['access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']","['has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']",0
"['( #AUTHOR_TAG ), turns out to be infeasible, at least for german and related languages']","['( #AUTHOR_TAG ), turns out to be infeasible, at least for german and related languages']","['( #AUTHOR_TAG ), turns out to be infeasible, at least for german and related languages']","['one may argue that single - word compounds are quite rare in english ( which is not the case in the medical domain either ), this is certainly not true for german and other basically agglutinative languages known for excessive single - word nominal compounding.', 'this problem becomes even more pressing for technical sublanguages, such as medical german ( e. g., blut druck mess gera _ _ t translates to device for measuring blood pressure ).', 'the problem one faces from an ir point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'hence, enumerating morphological variants in a semi - automatically generated lexicon, such as proposed for french ( #AUTHOR_TAG ), turns out to be infeasible, at least for german and related languages']",0
"['Hedlund et al. , 2001 ; #AUTHOR_TAG ).', 'when it comes to a']","['Hedlund et al. , 2001 ; #AUTHOR_TAG ).', 'when it comes to a']","['Hedlund et al. , 2001 ; #AUTHOR_TAG ).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998)']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ;  ekmekc [UNK] ioglu et al., 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998)']",0
['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( #AUTHOR_TAG ; j [UNK]'],"['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( #AUTHOR_TAG ; j [UNK] appinen and niemist [UNK] o,']","['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( #AUTHOR_TAG ; j [UNK] appinen and niemist [UNK] o,']","['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( #AUTHOR_TAG ; j [UNK] appinen and niemist [UNK] o, 1988 ; Kraaij and Pohlmann , 1996 ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", ""the goal is to map all occurring morphological variants to some canonical base forme. g.,'search'in the examples from above""]",0
"['. g., german ), often referred to as neo - classical compounding ( #AUTHOR_TAG ).', 'while this is simply irrelevant for general']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding ( #AUTHOR_TAG ).', 'while this is simply irrelevant for general - purpose morphological']","['. g., german ), often referred to as neo - classical compounding ( #AUTHOR_TAG ).', 'while this is simply irrelevant for general']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding ( #AUTHOR_TAG ).', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting (Wolff, 1984)']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( #AUTHOR_TAG ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( #AUTHOR_TAG ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( #AUTHOR_TAG ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( #AUTHOR_TAG ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( #AUTHOR_TAG )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( #AUTHOR_TAG )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( #AUTHOR_TAG )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( #AUTHOR_TAG )']",0
"['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and']","['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation.', 'such correspondence is defined in a way that is able to handle some non - standard cases ( e. g.', 'non - projective correspondence )']","['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and']","['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation.', 'such correspondence is defined in a way that is able to handle some non - standard cases ( e. g.', 'non - projective correspondence )']",0
"['crossed dependencies ( #AUTHOR_TAG ).', 'crossed']","['crossed dependencies ( #AUTHOR_TAG ).', 'crossed']","['crossed dependencies ( #AUTHOR_TAG ).', 'crossed dependencies (Tang & Zaharin, 1995)']","['sstc is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'these features are very much desired in the design of an annotation scheme, in particular for the treatment of linguistic phenomena, which are non - standard, e. g. crossed dependencies ( #AUTHOR_TAG ).', 'crossed dependencies (Tang & Zaharin, 1995)']",0
"['string - tree correspondence ( sstc ) was introduced in #AUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is']","['string - tree correspondence ( sstc ) was introduced in #AUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is']","['string - tree correspondence ( sstc ) was introduced in #AUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is expressed by the sub - correspondences']","['this section, we stress on the fact that in order to describe natural language ( nl ) in a natural manner, three distinct components need to be expressed by the linguistic formalisms ; namely, the text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'actually, nl is not only a correspondence between different representation levels, as stressed by mtt postulates, but also a sub - correspondence between them.', 'for instance, between the string in a language and its representation tree structure, it is important to specify the sub - correspondences between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation in nlp.', 'it is well known that many linguistic constructions are not projective ( e. g.', 'scrambling, cross serial dependencies, etc. ).', 'hence, it is very much desired to define the correspondence in a way to be able to handle the non - standard cases ( e. g.', 'non - projective correspondence ), see figure 1.', 'towards this aim, a flexible annotation structure called structured string - tree correspondence ( sstc ) was introduced in #AUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is expressed by the sub - correspondences recorded as part of a sstc']",0
"['.', '#AUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","[', what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""the proposed s - sstc annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages'structures."", 's - sstc very well suited for the construction of a bkb, which is needed for the ebmt applications.', '#AUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","[', what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""the proposed s - sstc annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages'structures."", 's - sstc very well suited for the construction of a bkb, which is needed for the ebmt applications.', '#AUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","[', what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""the proposed s - sstc annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages'structures."", 's - sstc very well suited for the construction of a bkb, which is needed for the ebmt applications.', '#AUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( #AUTHOR_TAG ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( #AUTHOR_TAG ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( #AUTHOR_TAG ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( #AUTHOR_TAG ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( Shieber , 1994 ), ( #AUTHOR_TAG ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this']","['definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( Shieber , 1994 ), ( #AUTHOR_TAG ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this']","['##ille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( Shieber , 1994 ), ( #AUTHOR_TAG ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples (Shieber, 1994)']","['idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'the use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'for example, synchronous tree adjoining grammar ( s - tag ) can be used to relate tags for two different languages, for example, for the purpose of immediate structural translation in machine translation ( abeille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( Shieber , 1994 ), ( #AUTHOR_TAG ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples (Shieber, 1994)']",0
"[') is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ).', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']","[') is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ).', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']","[') is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ).', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']","['the meaning - text theory ( mtt ) 1 point of view, natural language ( nl ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ).', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( #AUTHOR_TAG ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( #AUTHOR_TAG ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( #AUTHOR_TAG ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( #AUTHOR_TAG ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g. #AUTHOR_TAG cases ).', 'Shieber (1994) cases ).', '']","['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g. #AUTHOR_TAG cases ).', 'Shieber (1994) cases ).', '']","['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g. #AUTHOR_TAG cases ).', 'Shieber (1994) cases ).', '']","['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g. #AUTHOR_TAG cases ).', 'Shieber (1994) cases ).', 'due to lack of space we will only brief on some of these non - standard cases without going into the details']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( #AUTHOR_TAG ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( #AUTHOR_TAG ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( #AUTHOR_TAG ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( #AUTHOR_TAG ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ), such as the relation between syntax and semantic']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ), such as the relation between syntax and semantic']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ), such as the relation between syntax and semantic']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ), such as the relation between syntax and semantic']",0
"[').', 'for instance, when building translation units in ebmt approaches ( Richardson et al. , 2001 ), ( Aramaki , 2001 ), ( al Adhaileh & Tang , 1999 ), ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural']","['can be used to license only the linguistically meaningful synchronous correspondences between the two sstcs of the s - sstc ( i. e. between the two languages ).', ""for instance, when building translation units in ebmt approaches ( Richardson et al. , 2001 ), ( Aramaki , 2001 ), ( al Adhaileh & Tang , 1999 ), ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules'extraction from parallel parsed""]","['between the two languages ).', ""for instance, when building translation units in ebmt approaches ( Richardson et al. , 2001 ), ( Aramaki , 2001 ), ( al Adhaileh & Tang , 1999 ), ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules'extraction from parallel parse""]","['are governed by the following constraints :.', 'this means allowing one - to - one, one - to - many and many - to - many, but the mappings do not overlap.', 'note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two sstcs of the s - sstc ( i. e. between the two languages ).', ""for instance, when building translation units in ebmt approaches ( Richardson et al. , 2001 ), ( Aramaki , 2001 ), ( al Adhaileh & Tang , 1999 ), ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules'extraction from parallel parsed""]",0
"['learning ( #AUTHOR_TAG ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( #AUTHOR_TAG ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e.', 'synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )']",0
"['. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see #AUTHOR_TAG']","['in figure 2, describes how the sstc structure treats some non - standard linguistic phenomena.', 'the particle "" up "" is featurised into the verb "" pick "" and in discontinuous manner ( e. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see #AUTHOR_TAG']","['. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see #AUTHOR_TAG']","['case depicted in figure 2, describes how the sstc structure treats some non - standard linguistic phenomena.', 'the particle "" up "" is featurised into the verb "" pick "" and in discontinuous manner ( e. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see #AUTHOR_TAG']",0
"['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( #AUTHOR_TAG )']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( #AUTHOR_TAG )']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( #AUTHOR_TAG )']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ).', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), ( al - Adhaileh & Tang , 1999 )']",0
"['languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( #AUTHOR_TAG ), ( Harbusch & Poller , 2000 ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this']","['example, for the purpose of immediate structural translation in machine translation ( abeille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( #AUTHOR_TAG ), ( Harbusch & Poller , 2000 ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this']","['##ille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( #AUTHOR_TAG ), ( Harbusch & Poller , 2000 ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples (Shieber, 1994)']","['idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'the use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'for example, synchronous tree adjoining grammar ( s - tag ) can be used to relate tags for two different languages, for example, for the purpose of immediate structural translation in machine translation ( abeille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( #AUTHOR_TAG ), ( Harbusch & Poller , 2000 ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples (Shieber, 1994)']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( #AUTHOR_TAG ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( #AUTHOR_TAG ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( #AUTHOR_TAG ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( #AUTHOR_TAG ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['#AUTHOR_TAG.', '']","['#AUTHOR_TAG.', '']","['#AUTHOR_TAG.', '']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
"['##aword corpus ( linguistic data #AUTHOR_TAG ).', 'recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text']","['the english gigaword corpus ( linguistic data #AUTHOR_TAG ).', 'recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","['the english gigaword corpus ( linguistic data #AUTHOR_TAG ).', 'recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus ( linguistic data #AUTHOR_TAG ).', 'recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['final interface we intend to implement is a collection of web services for nlp.', 'a web service provides a remote procedure that can be called using xml based encodings ( xmlrpc or soap ) of function names, arguments and results transmitted via internet protocols such as http.', 'systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with wsdl and uddi.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']",0
"['by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning ( Dagan and Engelson ,1995 ) and co - training ( #AUTHOR_TAG ).', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']","['by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning ( Dagan and Engelson ,1995 ) and co - training ( #AUTHOR_TAG ).', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']","['earlier, there are two main requirements of the system that are covered by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning ( Dagan and Engelson ,1995 ) and co - training ( #AUTHOR_TAG ).', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']","['discussed earlier, there are two main requirements of the system that are covered by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning ( Dagan and Engelson ,1995 ) and co - training ( #AUTHOR_TAG ).', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']",0
"[') for manual annotation ( e. g. general architecture for text engineering ( gate ) ( #AUTHOR_TAG ) and the alembic workbench ( Day et al. , 1997 ) ) as well as nlp tools and resources that can be manipulated from the']","[') for manual annotation ( e. g. general architecture for text engineering ( gate ) ( #AUTHOR_TAG ) and the alembic workbench ( Day et al. , 1997 ) ) as well as nlp tools and resources that can be manipulated from the']","[') for manual annotation ( e. g. general architecture for text engineering ( gate ) ( #AUTHOR_TAG ) and the alembic workbench ( Day et al. , 1997 ) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g. general architecture for text engineering ( gate ) ( #AUTHOR_TAG ) and the alembic workbench ( Day et al. , 1997 ) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['final interface we intend to implement is a collection of web services for nlp.', 'a web service provides a remote procedure that can be called using xml based encodings ( xmlrpc or soap ) of function names, arguments and results transmitted via internet protocols such as http.', 'systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with wsdl and uddi.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']",0
['an efficient version of the mxpost pos tagger ( #AUTHOR_TAG ) will simply involve composing and config'],"['an efficient version of the mxpost pos tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component,']","[', implementing an efficient version of the mxpost pos tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component,']","['generative programming approach to nlp infrastructure development will allow tools such as sentence boundary detectors, pos taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'for instance, implementing an efficient version of the mxpost pos tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component']",3
"['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt,']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt,']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster training times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster training times when we move to conjugate gradient methods']",4
"['implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ).', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t,']","['implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ).', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t,']","['implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ).', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t, the fastest existing p o s tagger.', 'these tools use a highly optimised g i s imple - mentation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster train - ing times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ).', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t, the fastest existing p o s tagger.', 'these tools use a highly optimised g i s imple - mentation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster train - ing times when we move to conjugate gradient methods']",4
"['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools (Mikheev et al., 1999;Grover et al., 2000) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools (Mikheev et al., 1999;Grover et al., 2000) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ).', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['##aword corpus (Linguistic Data Consortium, 2003).', 'recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","['. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"['will be manually annotated.', 'for example, 10 million words of the american national corpus ( Ide et al. , 2002 ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( #AUTHOR_TAG ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['will be manually annotated.', 'for example, 10 million words of the american national corpus ( Ide et al. , 2002 ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( #AUTHOR_TAG ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus ( Ide et al. , 2002 ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( #AUTHOR_TAG ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus ( Ide et al. , 2002 ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( #AUTHOR_TAG ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']",0
"['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp ( #AUTHOR_TAG )']","['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp ( #AUTHOR_TAG )']","['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp ( #AUTHOR_TAG )']","['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp ( #AUTHOR_TAG )']",2
"['. g. general architecture for text engineering ( gate ) ( Cunningham et al. , 1997 ) and the alembic workbench ( #AUTHOR_TAG ) ) as well as nlp tools and resources that can be manipulated from the']","[') for manual annotation ( e. g. general architecture for text engineering ( gate ) ( Cunningham et al. , 1997 ) and the alembic workbench ( #AUTHOR_TAG ) ) as well as nlp tools and resources that can be manipulated from the']","['. g. general architecture for text engineering ( gate ) ( Cunningham et al. , 1997 ) and the alembic workbench ( #AUTHOR_TAG ) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g. general architecture for text engineering ( gate ) ( Cunningham et al. , 1997 ) and the alembic workbench ( #AUTHOR_TAG ) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"[', gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly config']","[', gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly configurable and']","[', gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly config']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g.', 'general architecture for text engineering ( gate ) (Cunningham et al., 1997) and the alembic workbench (Day et al., 1997) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['engineering research on generative programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for']","['engineering research on generative programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for nlp will provide high performance 1 components inspired by generative programming principles']","['engineering research on generative programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for']","['engineering research on generative programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for nlp will provide high performance 1 components inspired by generative programming principles']",0
"['will be manually annotated.', 'for example, 10 million words of the american national corpus ( #AUTHOR_TAG ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( Marcus et al. , 1993 ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['will be manually annotated.', 'for example, 10 million words of the american national corpus ( #AUTHOR_TAG ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( Marcus et al. , 1993 ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus ( #AUTHOR_TAG ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( Marcus et al. , 1993 ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus ( #AUTHOR_TAG ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( Marcus et al. , 1993 ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']",0
"['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state,']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state,']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ), and for the dynamic version we will use configuration classes']","['infrastructure will be implemented in c / c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ), and for the dynamic version we will use configuration classes']","['c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ), and for the dynamic version we will use configuration classes']","['infrastructure will be implemented in c / c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ), and for the dynamic version we will use configuration classes']",5
"['##aword corpus (Linguistic Data Consortium, 2003).', 'recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","['. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state,']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state,']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['be interchangeable : transformation - based learning ( tbl ) ( #AUTHOR_TAG ) and memory - based learning ( mbl ) ( Daelemans et al. , 2002 ) have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka (Witten and Frank, 1999)']","['be interchangeable : transformation - based learning ( tbl ) ( #AUTHOR_TAG ) and memory - based learning ( mbl ) ( Daelemans et al. , 2002 ) have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka (Witten and Frank, 1999)']","['learning methods should be interchangeable : transformation - based learning ( tbl ) ( #AUTHOR_TAG ) and memory - based learning ( mbl ) ( Daelemans et al. , 2002 ) have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka (Witten and Frank, 1999)']","['learning methods should be interchangeable : transformation - based learning ( tbl ) ( #AUTHOR_TAG ) and memory - based learning ( mbl ) ( Daelemans et al. , 2002 ) have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka (Witten and Frank, 1999)']",4
"['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( #AUTHOR_TAG ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( #AUTHOR_TAG ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( #AUTHOR_TAG ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( #AUTHOR_TAG ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'an example of using the python tagger interface is shown in figure 1']",0
"['fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing ( #AUTHOR_TAG ).', 'we expect even faster training times when we move to conjugate gradient methods']","['fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing ( #AUTHOR_TAG ).', 'we expect even faster training times when we move to conjugate gradient methods']","['##an and.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing ( #AUTHOR_TAG ).', 'we expect even faster training times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging ( curran and.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing ( #AUTHOR_TAG ).', 'we expect even faster training times when we move to conjugate gradient methods']",5
"['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( #AUTHOR_TAG ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( #AUTHOR_TAG ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( #AUTHOR_TAG ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( #AUTHOR_TAG ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['##tk ) is a package of nlp components implemented in python ( #AUTHOR_TAG ).', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python ( #AUTHOR_TAG ).', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","['##tk ) is a package of nlp components implemented in python ( #AUTHOR_TAG ).', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python ( #AUTHOR_TAG ).', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']",0
"['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt,']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt,']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster training times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster training times when we move to conjugate gradient methods']",4
"['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'an example of using the python tagger interface is shown in figure 1']",0
"['very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure ( #AUTHOR_TAG ) which the gui is built on top of.', 'this allows components to be highly config']","['editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure ( #AUTHOR_TAG ) which the gui is built on top of.', 'this allows components to be highly configurable and']","[', gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure ( #AUTHOR_TAG ) which the gui is built on top of.', 'this allows components to be highly config']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g.', 'general architecture for text engineering ( gate ) (Cunningham et al., 1997) and the alembic workbench (Day et al., 1997) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure ( #AUTHOR_TAG ) which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"[', ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a head']","[', ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a head']","[', ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a head driven phrase structure grammar (']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance, ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
"['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).', 'techniques']",0
"['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","[', (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance, (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","[', (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance, (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['a metagrammar in the sense of ( #AUTHOR_TAG ).', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic']","['a metagrammar in the sense of ( #AUTHOR_TAG ).', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic']","['.', 'to address this problem, we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ).', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', '']","['intercategorial synonymic links.', ""a first investigation of anne abeille's tag for french suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward."", 'for instance, as figures 3, 4 and 5 show, the ftag trees assigned on syntactic grounds by anne abeille ftag to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ).', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', 'for instance, there will be a class novn1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'but additionnally there will be semantic classes such as, "" binary predicate of semantic type x "" which will be associated with the relevant syntactic classes for instance, novn1 ( the class of transitive verbs with nominal arguments ), binary npred ( the class of binary predicative nouns ), novsupnn1, the class of support verb constructions taking two nominal arguments.', 'by further associating semantic units ( e. g., "" cost "" ) with the appropriate semantic classes ( e. g., "" binary predicate of semantic type x "" ), we can in this way capture both intra and intercategorial paraphrasing links in a general way']",3
"['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","[', (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance, (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c. #AUTHOR_TAG ).', 'Johnson et al., 2002).', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c. #AUTHOR_TAG ).', 'Johnson et al., 2002).', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c. #AUTHOR_TAG ).', 'Johnson et al., 2002).', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c. #AUTHOR_TAG ).', 'Johnson et al., 2002).', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']",5
"[', ( #AUTHOR_TAG ) acquire two - argument']","[', ( #AUTHOR_TAG ) acquire two - argument']","[', ( #AUTHOR_TAG ) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance, ( #AUTHOR_TAG ) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ).', 'techniques']","['domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ).', 'techniques']",3
['test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG )'],['test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG )'],"['( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the par - seval lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG )']","['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? )', 'and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the par - seval lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases reflects the paraphrase typology.', 'in a first phase, we concentrate on simple, non - recursive predicate / argument structure.', 'given such a structure, the construction and annotation of a test item proceeds as follows']",1
"['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file']","['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? ) and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file']","['( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987)']","['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? ) and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987) ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998)']",0
"['language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']","['alternations are partially described in ( saint - Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']","['Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( saint - Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']",3
"['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction.', 'the meta']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. )']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. ) encoded in the grammar basic units.', 'this specification is then compiled to automatically produce a specific grammar']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. ) encoded in the grammar basic units.', 'this specification is then compiled to automatically produce a specific grammar']",5
['( #AUTHOR_TAG ) can furthermore be resort'],"['( #AUTHOR_TAG ) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( saint - Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables ( #AUTHOR_TAG ) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( saint - Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables ( #AUTHOR_TAG ) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']",3
"['( #AUTHOR_TAG ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typ']","['( #AUTHOR_TAG ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases']","['and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases']","['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? )', 'and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the par - seval lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases reflects the paraphrase typology.', 'in a first phase, we concentrate on simple, non - recursive predicate / argument structure.', 'given such a structure, the construction and annotation of a test item proceeds as follows']",0
"['domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ).', 'techniques']","['domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ).', 'techniques']",3
['phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip lexical functional grammar ( lfg )'],['phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip lexical functional grammar ( lfg )'],"[', ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip lexical functional grammar ( lfg )']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance, ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
['construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than - - as is more'],['construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than - - as is more'],['construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than - - as is more'],"['construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than - - as is more common in tag - - from the derivation tree.', 'this is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'the association between tree nodes and unification variables encodes the syntax / semantics interface - it specifies which node in the tree provides the value for which variable in the final semantic representation']",0
"[', ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a head']","[', ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a head']","[', ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a head driven phrase structure grammar (']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance, ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
"['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['shuffling paraphrases, french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs']","['shuffling paraphrases, french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']",0
"['arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the']","['arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root ""']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root "" and "" term "" interchangeably to refer to canonical forms obtained through this root extraction process']",4
"['arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the']","['arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root ""']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root "" and "" term "" interchangeably to refer to canonical forms obtained through this root extraction process']",4
"[') ( #AUTHOR_TAG ), minimum description length principal ( Lang , 1995 ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","[') ( #AUTHOR_TAG ), minimum description length principal ( Lang , 1995 ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","[') ( #AUTHOR_TAG ), minimum description length principal ( Lang , 1995 ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) ( #AUTHOR_TAG ), minimum description length principal ( Lang , 1995 ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['\' s categorizer "" (Sakhr, 2004).', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization (Mitchell, 1997).', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in ( #AUTHOR_TAG ), which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in (Yahyaoui, 2001) by experiment']","['arabic, one automatic categorizer has been reported to have been put under operational use to classify arabic documents ; it is referred to as "" sakhr\'s categorizer "" (Sakhr, 2004).', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization (Mitchell, 1997).', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in ( #AUTHOR_TAG ), which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest arabic site on the web : aljazeera. net']","['\' s categorizer "" (Sakhr, 2004).', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization (Mitchell, 1997).', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in ( #AUTHOR_TAG ), which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in (Yahyaoui, 2001) by experiment']","['arabic, one automatic categorizer has been reported to have been put under operational use to classify arabic documents ; it is referred to as "" sakhr\'s categorizer "" (Sakhr, 2004).', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization (Mitchell, 1997).', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in ( #AUTHOR_TAG ), which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest arabic site on the web : aljazeera. net']",2
"['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in ( #AUTHOR_TAG ).', 'in this work, the average accuracy over all categories is : 68. 78 % in cross validation and 62 % in evaluation set experiments.', 'the corresponding performances in (Yahyaoui, 2001) are 75. 6 % and 50 %, respectively.', 'thus, the overall performance ( including cross validation and evaluation set experiments ) in this work is comparable to that in (Yahyaoui, 2001).', 'this offers some indication that the performance of nb algorithm in classifying arabic documents is not sensitive to the arabic root extraction algorithm.', '']","['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in ( #AUTHOR_TAG ).', 'in this work, the average accuracy over all categories is : 68. 78 % in cross validation and 62 % in evaluation set experiments.', 'the corresponding performances in (Yahyaoui, 2001) are 75. 6 % and 50 %, respectively.', 'thus, the overall performance ( including cross validation and evaluation set experiments ) in this work is comparable to that in (Yahyaoui, 2001).', 'this offers some indication that the performance of nb algorithm in classifying arabic documents is not sensitive to the arabic root extraction algorithm.', '']","['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in ( #AUTHOR_TAG ).', 'in this work, the average accuracy over all categories is : 68. 78 % in cross validation and 62 % in evaluation set experiments.', 'the corresponding performances in (Yahyaoui, 2001) are 75. 6 % and 50 %, respectively.', 'thus, the overall performance ( including cross validation and evaluation set experiments ) in this work is comparable to that in (Yahyaoui, 2001).', 'this offers some indication that the performance of nb algorithm in classifying arabic documents is not sensitive to the arabic root extraction algorithm.', 'future work will be directed']","['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in ( #AUTHOR_TAG ).', 'in this work, the average accuracy over all categories is : 68. 78 % in cross validation and 62 % in evaluation set experiments.', 'the corresponding performances in (Yahyaoui, 2001) are 75. 6 % and 50 %, respectively.', 'thus, the overall performance ( including cross validation and evaluation set experiments ) in this work is comparable to that in (Yahyaoui, 2001).', 'this offers some indication that the performance of nb algorithm in classifying arabic documents is not sensitive to the arabic root extraction algorithm.', 'future work will be directed at experimenting with other root extraction algorithms.', ""further improvement of nb's performance may be effected by using unlabeled documents ; e. g., em has been used successfully for this purpose in ( nigam et al., 200 ), where em has increased the classification accuracy by 30 % for classifying english documents""]",1
"['good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ).', 'more recently, (Sebastiani, 2002) has performed a good survey of document categorization ; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004)']","['good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ).', 'more recently, (Sebastiani, 2002) has performed a good survey of document categorization ; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004)']","['good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ).', 'more recently, (Sebastiani, 2002) has performed a good survey of document categorization ; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004)']","['good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ).', 'more recently, (Sebastiani, 2002) has performed a good survey of document categorization ; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004)']",0
"['will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language ( #AUTHOR_TAG ), and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this']","['will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language ( #AUTHOR_TAG ), and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this']","['will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language ( #AUTHOR_TAG ), and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this processing is not straightforward : it necessitates expert knowledge in arabic language word morphology (']","['arabic, however, the use of stems will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language ( #AUTHOR_TAG ), and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this processing is not straightforward : it necessitates expert knowledge in arabic language word morphology ( al - Shalabi and Evens, 1998).', 'as an example, two close roots ( i. e., roots made of the same letters ), but semantically different, can yield the same infix form thus creating ambiguity']",0
"[') (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","[') (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']",0
"['as a way to cope with such a problem.', 'automatic text ( or document ) categorization attempts to replace and save human effort required in performing manual categorization.', 'it consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'as such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre - specified subject themes.', 'automatic text categorization has been used in search engines, digital library systems, and document management systems ( #AUTHOR_TAG ).', 'such applications have included electronic email filtering, newsgroups classification, and survey']","['as a way to cope with such a problem.', 'automatic text ( or document ) categorization attempts to replace and save human effort required in performing manual categorization.', 'it consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'as such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre - specified subject themes.', 'automatic text categorization has been used in search engines, digital library systems, and document management systems ( #AUTHOR_TAG ).', 'such applications have included electronic email filtering, newsgroups classification, and survey']","['the needs of different end users.', 'to this end, automatic text categorization has emerged as a way to cope with such a problem.', 'automatic text ( or document ) categorization attempts to replace and save human effort required in performing manual categorization.', 'it consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'as such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre - specified subject themes.', 'automatic text categorization has been used in search engines, digital library systems, and document management systems ( #AUTHOR_TAG ).', 'such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'barq']","['the explosive growth of text documents on the web, relevant information retrieval has become a crucial task to satisfy the needs of different end users.', 'to this end, automatic text categorization has emerged as a way to cope with such a problem.', 'automatic text ( or document ) categorization attempts to replace and save human effort required in performing manual categorization.', 'it consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'as such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre - specified subject themes.', 'automatic text categorization has been used in search engines, digital library systems, and document management systems ( #AUTHOR_TAG ).', 'such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).', 'in this paper, nb which is a statistical machine learning algorithm is used to learn to classify non - vocalized 1 arabic web text documents']",0
"[') (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in ( #AUTHOR_TAG ).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in ( #AUTHOR_TAG ).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","[') (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in ( #AUTHOR_TAG ).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in ( #AUTHOR_TAG ).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",1
"['n is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']","['n is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']","['n is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']","['the tf measurement concerns the importance of a term in a given document, idf seeks to measure the relative importance of a term in a collection of documents.', 'the importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'tf is given by tf d, t, and it denotes frequency of term t in document d. idf is given by idf t = log ( n / df t ), where n is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']",4
"[') ( Tzeras and Hartman , 1993 ), minimum description length principal ( #AUTHOR_TAG ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","[') ( Tzeras and Hartman , 1993 ), minimum description length principal ( #AUTHOR_TAG ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","[') ( Tzeras and Hartman , 1993 ), minimum description length principal ( #AUTHOR_TAG ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) ( Tzeras and Hartman , 1993 ), minimum description length principal ( #AUTHOR_TAG ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in ( #AUTHOR_TAG ), ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ), respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in ( #AUTHOR_TAG ), ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ), respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in ( #AUTHOR_TAG ), ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ), respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in ( #AUTHOR_TAG ), ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ), respectively']",0
"['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']",0
"['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example, ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example, ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example, ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example, ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']",0
"['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( Oueslati , 1999 ) or, more frequently, on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG,']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( Oueslati , 1999 ) or, more frequently, on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG,']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( Oueslati , 1999 ) or, more frequently, on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG,']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( Oueslati , 1999 ) or, more frequently, on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG, for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']",1
"['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles ( #AUTHOR_TAG ).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles ( #AUTHOR_TAG ).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles ( #AUTHOR_TAG ).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles ( #AUTHOR_TAG ).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']",5
"['number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG,']","['number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG,']","['number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG,']","['number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierentiated']",0
"['on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use, as a starting point, a number of identical characters']",0
"['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles (Lemay et al., 2004).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles (Lemay et al., 2004).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles (Lemay et al., 2004).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles (Lemay et al., 2004).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']",5
"['on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use, as a starting point, a number of identical characters']",0
"['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993;  kilgarri and Tugwell, 2001,  for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993;  kilgarri and Tugwell, 2001,  for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993;  kilgarri and Tugwell, 2001,  for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993;  kilgarri and Tugwell, 2001,  for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']",1
"['##res is based on a machine learning technique, inductive logic programming ( ilp ) ( #AUTHOR_TAG ), which infers general morpho - syntactic patterns from a set of examples ( this set is']","['##res is based on a machine learning technique, inductive logic programming ( ilp ) ( #AUTHOR_TAG ), which infers general morpho - syntactic patterns from a set of examples ( this set is']","['##res is based on a machine learning technique, inductive logic programming ( ilp ) ( #AUTHOR_TAG ), which infers general morpho - syntactic patterns from a set of examples ( this set is noted e + hereafter ) and counter -']","['##res is based on a machine learning technique, inductive logic programming ( ilp ) ( #AUTHOR_TAG ), which infers general morpho - syntactic patterns from a set of examples ( this set is noted e + hereafter ) and counter - examples ( e a ) of the elements one wants to acquire and their context.', 'the contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'the acquisition process can be summarized in 3 steps']",0
"[', most strategies are based on ` ` internal or ` ` external methods ( #AUTHOR_TAG ), i. e']","[', most strategies are based on ` ` internal or ` ` external methods ( #AUTHOR_TAG ), i. e. methods that rely on the form of terms or on the information gathered from contexts.', '( in some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process. )', 'the work reported here infers specific semantic relationships based on sets of examples and counterexamples']","[', most strategies are based on ` ` internal or ` ` external methods ( #AUTHOR_TAG ), i. e']","[', most strategies are based on ` ` internal or ` ` external methods ( #AUTHOR_TAG ), i. e. methods that rely on the form of terms or on the information gathered from contexts.', '( in some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process. )', 'the work reported here infers specific semantic relationships based on sets of examples and counterexamples']",1
['on computing that includes collocational information ( #AUTHOR_TAG )'],['on computing that includes collocational information ( #AUTHOR_TAG )'],['on computing that includes collocational information ( #AUTHOR_TAG )'],"['this paper, the method is applied to a french corpus on computing to and noun - verb combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG )']",4
"['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG )']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG )']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG )']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG )']",4
"['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']",0
"[', such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with wordnet relations']","[', such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with wordnet relations']","[', such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with wordnet relations']","['though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'indeed, such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with wordnet relations )']",1
"['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']",0
"[', contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see ( #AUTHOR_TAG )']","[', contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see ( #AUTHOR_TAG )']","[', contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see ( #AUTHOR_TAG )']","['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ), these patterns allow']",0
"['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in ( #AUTHOR_TAG ).', 'we simply give a short account of its basic principles']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in ( #AUTHOR_TAG ).', 'we simply give a short account of its basic principles herein']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in ( #AUTHOR_TAG ).', 'we simply give a short account of its basic principles']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in ( #AUTHOR_TAG ).', 'we simply give a short account of its basic principles herein']",5
"['number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996,  for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms,']","['number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996,  for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not differentiated']","['number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996,  for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not']","['number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996,  for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not differentiated']",0
"['structure that make arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 )']","['structure that make arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 )']","['', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 )']","['separate the edr task into two parts : a mention detection step, which identifies and classifies all the mentions in a text - and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'in its entirety, the edr task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non - named mentions ( nominal and pronominal ) and the requirement of grouping mentions into entities.', 'this is particularly true for arabic where nominals and pronouns are also attached to the word they modify.', 'in fact, most arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form arabic surface forms ( blank - delimited words ).', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 )']",0
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the fo - cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named (']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the fo - cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named (']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the fo - cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named (']","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the fo - cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g. john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",5
"[', similar to those used for chinese segmentation ( #AUTHOR_TAG ).', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']","['words, we also experimented with models based upon character n - grams, similar to those used for chinese segmentation ( #AUTHOR_TAG ).', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']","['with models based upon character n - grams, similar to those used for chinese segmentation ( #AUTHOR_TAG ).', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']","['addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n - grams, similar to those used for chinese segmentation ( #AUTHOR_TAG ).', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']",1
"['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the #AUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the #AUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the #AUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the #AUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']",5
"['', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard ( #AUTHOR_TAG )']","['', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard ( #AUTHOR_TAG )']","['2 ).', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard ( #AUTHOR_TAG )']","['2 ).', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard ( #AUTHOR_TAG )']",5
"['structure that make arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG )']","['structure that make arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG )']","['', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG )']","['separate the edr task into two parts : a mention detection step, which identifies and classifies all the mentions in a text - and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'in its entirety, the edr task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non - named mentions ( nominal and pronominal ) and the requirement of grouping mentions into entities.', 'this is particularly true for arabic where nominals and pronouns are also attached to the word they modify.', 'in fact, most arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form arabic surface forms ( blank - delimited words ).', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG )']",0
['coreference system system is similar to the bell tree algorithm as described by ( #AUTHOR_TAG )'],['coreference system system is similar to the bell tree algorithm as described by ( #AUTHOR_TAG )'],['coreference system system is similar to the bell tree algorithm as described by ( #AUTHOR_TAG )'],['coreference system system is similar to the bell tree algorithm as described by ( #AUTHOR_TAG )'],1
"['has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different (']","['has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']","['has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different (']","['has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']",0
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ).', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']","['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ).', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']","['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ).', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']","['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ).', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']",0
"['mention sub - type, which is a sub - category of the mention type ( #AUTHOR_TAG ) (']","['mention sub - type, which is a sub - category of the mention type ( #AUTHOR_TAG ) ( e. g. orggovernmental, facilitypath, etc. )']","['mention sub - type, which is a sub - category of the mention type ( #AUTHOR_TAG ) (']","['mention sub - type, which is a sub - category of the mention type ( #AUTHOR_TAG ) ( e. g. orggovernmental, facilitypath, etc. )']",5
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
['demonstrates a technique for segment'],"['demonstrates a technique for segmenting arabic text and uses it as a morphological processing step in machine translation.', 'a trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']","['demonstrates a technique for segmenting arabic text and uses it as a morphological processing step in machine translation.', 'a trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']","['demonstrates a technique for segmenting arabic text and uses it as a morphological processing step in machine translation.', 'a trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']",5
"['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in.', 'both systems are built around from the maximum - entropy technique ( #AUTHOR_TAG ).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in.', 'both systems are built around from the maximum - entropy technique ( #AUTHOR_TAG ).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in.', 'both systems are built around from the maximum - entropy technique ( #AUTHOR_TAG ).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in.', 'both systems are built around from the maximum - entropy technique ( #AUTHOR_TAG ).', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness']",5
"['are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classi cation problem.', '']","['are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classi cation problem.', '']","['are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classi cation problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classi cation problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the pre xes and su _ xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness']",1
"['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ).', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n  1 tokens in the history ( t in + 1,...', 't i1 ) and the forward token n - gram feature will contains the next n  1 tokens ( t i + 1,...', 't i + n1 )']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ).', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n  1 tokens in the history ( t in + 1,...', 't i1 ) and the forward token n - gram feature will contains the next n  1 tokens ( t i + 1,...', 't i + n1 )']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ).', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n  1 tokens in the history ( t in + 1,...', 't i1 ) and the forward token n - gram feature will contains the next n  1 tokens ( t i + 1,...', 't i + n1 )']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ).', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n  1 tokens in the history ( t in + 1,...', 't i1 ) and the forward token n - gram feature will contains the next n  1 tokens ( t i + 1,...', 't i + n1 )']",0
"['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ).', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ).', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ).', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ).', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']",5
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['to improve the dictionary based model.', 'as in ( #AUTHOR_TAG ), we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a']","['to improve the dictionary based model.', 'as in ( #AUTHOR_TAG ), we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a']","['to improve the dictionary based model.', 'as in ( #AUTHOR_TAG ), we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a portion of the arabic gigaword corpus.', 'from this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus']","[', an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'we seeked to exploit this ability to generalize to improve the dictionary based model.', 'as in ( #AUTHOR_TAG ), we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a portion of the arabic gigaword corpus.', 'from this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus']",5
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the #AUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the #AUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the #AUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the #AUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']",5
"['a kneser - ney ( #AUTHOR_TAG ) based backoff language model.', 'differing from (Lee et al., 2003), we have also introduced an explicit model for']","['a kneser - ney ( #AUTHOR_TAG ) based backoff language model.', 'differing from (Lee et al., 2003), we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']","['that accepts characters and produces identifiers corresponding to dictionary entries.', 'the final machine is a trigram language model, specifically a kneser - ney ( #AUTHOR_TAG ) based backoff language model.', 'differing from (Lee et al., 2003), we have also introduced an explicit model for']","['our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.', 'the first machine, illustrated in figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'the second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'the final machine is a trigram language model, specifically a kneser - ney ( #AUTHOR_TAG ) based backoff language model.', 'differing from (Lee et al., 2003), we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']",5
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['( #AUTHOR_TAG ) for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers']","['out.', 'we test the two systems on both "" true "" and system mentions of the devtest set.', 'true mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'we report results with two metrics : ecm - f and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx ( #AUTHOR_TAG ) for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers']","['this section, we present the coreference results on the devtest defined earlier.', 'first, to see the effect of stem matching features, we compare two coreference systems : one with the stem features, the other with - out.', 'we test the two systems on both "" true "" and system mentions of the devtest set.', 'true mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'we report results with two metrics : ecm - f and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx ( #AUTHOR_TAG ) for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers']","['this section, we present the coreference results on the devtest defined earlier.', 'first, to see the effect of stem matching features, we compare two coreference systems : one with the stem features, the other with - out.', 'we test the two systems on both "" true "" and system mentions of the devtest set.', 'true mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'we report results with two metrics : ecm - f and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx ( #AUTHOR_TAG ) for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers without stem features are listed under \\ base, "" and the results of the coreference system with stem features are listed under \\ base + stem.']",5
"['has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different (']","['has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']","['has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different (']","['has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']",0
"['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness']",1
"['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization,""]","['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization,""]","['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization,""]","['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization, etc ), a sub - type ( orgcommercial, orggovernmental, etc ), a mention level ( named, nominal, etc ), and a class ( specific, generic, etc )."", 'detecting the mention boundaries ( set of consecutive tokens ) and their main type is one of the important steps of our mention detection system.', 'the score that the ace community uses ( ace value ) attributes a higher importance ( outlined by its weight ) to the main type compared to other sub - tasks, such as the mention level and the class.', 'hence, to build our mention detection system we spent a lot of effort in improving the rst step : detecting the mention boundary and their main type.', 'in this paper, we report the results in terms of precision, recall, and f - measure3']",5
"['= 1 | e, mk, m ) is an exponential or maximum entropy model ( #AUTHOR_TAG )']","['= 1 | e, mk, m ) is an exponential or maximum entropy model ( #AUTHOR_TAG )']","['= 1 | e, mk, m ) is an exponential or maximum entropy model ( #AUTHOR_TAG )']","['mk is one mention in entity e, and the basic model building block pl ( l = 1 | e, mk, m ) is an exponential or maximum entropy model ( #AUTHOR_TAG )']",5
"['email to participate in the experiment.', 'thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annot']","['email to participate in the experiment.', 'thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', 'their results differed particularly in cases of antonymy or distributionally related pairs.', 'we created a manual']","['email to participate in the experiment.', 'thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', 'their results differed particularly in cases of antonymy or distributionally related pairs.', 'we created a manual']","['developed a web - based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'test subjects were invited via email to participate in the experiment.', 'thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', 'their results differed particularly in cases of antonymy or distributionally related pairs.', 'we created a manual with a detailed introduction to sr stressing the crucial points.', 'the manual was presented to the subjects before the experiment and could be re - accessed at any time.', 'during the experiment, one concept pair at a time was presented to the test subjects in random ordering.', 'subjects had to assign a discrete relatedness value { 0, 1, 2, 3, 4 } to each pair.', ""figure 2 shows the system's gui""]",4
"['', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts ( #AUTHOR_TAG ). 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts ( #AUTHOR_TAG ). 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'it is defined on different kinds of textual units, e. g.', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts ( #AUTHOR_TAG ). 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'it is defined on different kinds of textual units, e. g.', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts ( #AUTHOR_TAG ). 2 linguistic distance between words is inverse to their semantic similarity or relatedness']",0
"['im #AUTHOR_TAG ). table 1 : comparison of previous experiments.', 'r']","['shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im #AUTHOR_TAG ). table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']","['im #AUTHOR_TAG ). table 1 : comparison of previous experiments.', 'r']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im #AUTHOR_TAG ). table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['to #AUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']","['to #AUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']","['to #AUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']","['to #AUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']",0
"['.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and good']","['distribution of averaged human judgments on the whole test set ( see figure 3 ) is almost balanced with a slight underrepresentation of highly related concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and goodenough display an empty horizontal band that could be used to separate related and unrelated pairs.', 'this empty band is not observed here.', 'however, figure 4 shows the distribution of averaged judgments with the highest agreement between annotators ( standard deviation < 0. 8 ).', 'the']","['distribution of averaged human judgments on the whole test set ( see figure 3 ) is almost balanced with a slight underrepresentation of highly related concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and good']","['distribution of averaged human judgments on the whole test set ( see figure 3 ) is almost balanced with a slight underrepresentation of highly related concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and goodenough display an empty horizontal band that could be used to separate related and unrelated pairs.', 'this empty band is not observed here.', 'however, figure 4 shows the distribution of averaged judgments with the highest agreement between annotators ( standard deviation < 0. 8 ).', 'the plot clearly shows an empty horizontal band with no judgments.', 'the connection between averaged judgments and standard deviation is plotted in figure 5']",1
"['shown in #AUTHOR_TAG.', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['shown in #AUTHOR_TAG.', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']","['.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG.', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG.', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by #AUTHOR_TAG with 10 subjects.', 'table']","['38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by #AUTHOR_TAG with 10 subjects.', 'table']","['38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by #AUTHOR_TAG with 10 subjects.', 'table']","['the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by #AUTHOR_TAG with 10 subjects.', 'table']",0
"['by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir #AUTHOR_TAG ) systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]","['by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir #AUTHOR_TAG ) systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]","['extracted word pairs from three different domain - specific corpora ( see table 2 ).', 'this is motivated by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir #AUTHOR_TAG ) systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]","['extracted word pairs from three different domain - specific corpora ( see table 2 ).', 'this is motivated by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir #AUTHOR_TAG ) systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]",4
"['.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['( #AUTHOR_TAG ), the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for']","['implemented a set of filters for word pairs.', 'one group of filters removed unwanted word pairs.', 'word pairs are filtered if they contain at least one word that a ) has less than three letters b ) contains only uppercase letters ( mostly acronyms ) or c ) can be found in a stoplist.', 'another filter enforced a specified fraction of combinations of nouns ( n ), verbs ( v ) and adjectives ( a ) to be present in the result set.', 'we used the following parameters : were noun - noun pairs, 15 % noun - verb pairs and so on.', 'word pairs containing polysemous words are expanded to concept pairs using germanet ( #AUTHOR_TAG ), the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for german']","['.', 'word pairs containing polysemous words are expanded to concept pairs using germanet ( #AUTHOR_TAG ), the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for']","['implemented a set of filters for word pairs.', 'one group of filters removed unwanted word pairs.', 'word pairs are filtered if they contain at least one word that a ) has less than three letters b ) contains only uppercase letters ( mostly acronyms ) or c ) can be found in a stoplist.', 'another filter enforced a specified fraction of combinations of nouns ( n ), verbs ( v ) and adjectives ( a ) to be present in the result set.', 'we used the following parameters : were noun - noun pairs, 15 % noun - verb pairs and so on.', 'word pairs containing polysemous words are expanded to concept pairs using germanet ( #AUTHOR_TAG ), the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for german']",5
"['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric ( #AUTHOR_TAG ). 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']","['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric ( #AUTHOR_TAG ). 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']","['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric ( #AUTHOR_TAG ). 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']","['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric ( #AUTHOR_TAG ). 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']",0
"['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', '#AUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared']","['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', '#AUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared directly.', 'furthermore, we cannot']","['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', '#AUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared']","['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', '#AUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared directly.', 'furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low']",1
"['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'we used the revised experimental setup ( #AUTHOR_TAG ), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'we used the revised experimental setup ( #AUTHOR_TAG ), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'we used the revised experimental setup ( #AUTHOR_TAG ), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'we used the revised experimental setup ( #AUTHOR_TAG ), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']",5
"['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG.', 'we used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG.', 'we used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG.', 'we used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG.', 'we used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']",1
"['. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
['##p applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG )'],"['semantic relatedness ( sr ) is defined to cover any kind of lexical or functional association that may exist be - tween two words (Gurevych, 2005). 3', 'dissimilar words can be semantically related, e. g.', 'via functional relationships ( night - dark ) or when they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG )']","['semantic relatedness ( sr ) is defined to cover any kind of lexical or functional association that may exist be - tween two words (Gurevych, 2005). 3', 'dissimilar words can be semantically related, e. g.', 'via functional relationships ( night - dark ) or when they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG )']","['similarity is typically defined via the lexical relations of synonymy ( automobile - car ) and hypernymy ( vehicle - car ), while semantic relatedness ( sr ) is defined to cover any kind of lexical or functional association that may exist be - tween two words (Gurevych, 2005). 3', 'dissimilar words can be semantically related, e. g.', 'via functional relationships ( night - dark ) or when they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG )']",0
"['the seminal work by #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991)']","['the seminal work by #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991)']","['the seminal work by #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991)']","['the seminal work by #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by Resnik (1995) with 10 subjects.', 'table']",0
"['by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ). Lebart and Rajman (2000) argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ). Lebart and Rajman (2000) argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ). Lebart and Rajman (2000) argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ). Lebart and Rajman (2000) argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']",0
"['by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']",0
"['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger (Schmid, 1995).', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme ( #AUTHOR_TAG )""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger (Schmid, 1995).', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme ( #AUTHOR_TAG )""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger (Schmid, 1995).', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme ( #AUTHOR_TAG )""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger (Schmid, 1995).', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme ( #AUTHOR_TAG )""]",5
"['.', '#AUTHOR_TAG']","['not report inter - subject correlation for their larger dataset.', '#AUTHOR_TAG']","['. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter - subject correlation for their larger dataset.', '#AUTHOR_TAG']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter - subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend ( #AUTHOR_TAG ).', 'we removed words which had more than three senses']","['##et contains only a few conceptual glosses.', 'as they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e. g. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend ( #AUTHOR_TAG ).', 'we removed words which had more than three senses']","['. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend ( #AUTHOR_TAG ).', 'we removed words which had more than three senses']","['##et contains only a few conceptual glosses.', 'as they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e. g. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend ( #AUTHOR_TAG ).', 'we removed words which had more than three senses']",5
"['with pairs connected by all kinds of lexical - semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non - classical (']","['with pairs connected by all kinds of lexical - semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']","['with pairs connected by all kinds of lexical - semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non - classical (']","[', manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']",0
"['shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']","['.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger ( #AUTHOR_TAG ).', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme (Salton, 1989)""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger ( #AUTHOR_TAG ).', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme (Salton, 1989)""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger ( #AUTHOR_TAG ).', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme (Salton, 1989)""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger ( #AUTHOR_TAG ).', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme (Salton, 1989)""]",5
"['.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['.', 'dictionary - based ( #AUTHOR_TAG ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( #AUTHOR_TAG ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( #AUTHOR_TAG ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( #AUTHOR_TAG ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans ( #AUTHOR_TAG ). 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans ( #AUTHOR_TAG ). 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans ( #AUTHOR_TAG ). 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans ( #AUTHOR_TAG ). 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']",0
"['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced']","['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced']","['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced']","['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non - classical ( i. e.', 'other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']",0
"['dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter - subject correlation for their larger']","['dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter - subject correlation for their larger']","['. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', 'Gurevych (2006)']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', '#AUTHOR_TAG']","['. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', '#AUTHOR_TAG']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', '#AUTHOR_TAG']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', '#AUTHOR_TAG reported a correlation of r =. 9026. 10 the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter - subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"[') may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by #AUTHOR_TAG']","['coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations ( like cooccurrence of words ) and is thus a more complicated task.', 'judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'while most people may agree 10 note that resnik used the averaged correlation coefficient.', 'we computed the summarized correlation coefficient using a fisher z - value transformation.', 'that ( car - vehicle ) are highly related, a strong connection between ( parts - speech ) may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by #AUTHOR_TAG']","['coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations ( like cooccurrence of words ) and is thus a more complicated task.', 'judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'while most people may agree 10 note that resnik used the averaged correlation coefficient.', 'we computed the summarized correlation coefficient using a fisher z - value transformation.', 'that ( car - vehicle ) are highly related, a strong connection between ( parts - speech ) may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by #AUTHOR_TAG']","['coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations ( like cooccurrence of words ) and is thus a more complicated task.', 'judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'while most people may agree 10 note that resnik used the averaged correlation coefficient.', 'we computed the summarized correlation coefficient using a fisher z - value transformation.', 'that ( car - vehicle ) are highly related, a strong connection between ( parts - speech ) may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by #AUTHOR_TAG']",1
"['. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
['by #AUTHOR_TAG b ) and'],"['site based corpus annotation - in which the user can specify a web site to annotate  domain based corpus annotation - in which the user specifies a content domain ( with the use of keywords ) to annotate  crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages from a computational linguistic view, the framework will also need to take into account the granularity of the unit ( for example, pos tagging requires sentence - units, but anaphoric annotation needs paragraphs or larger ).', 'secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by #AUTHOR_TAG b ) and shingling techniques']",['by #AUTHOR_TAG b ) and'],"['site based corpus annotation - in which the user can specify a web site to annotate  domain based corpus annotation - in which the user specifies a content domain ( with the use of keywords ) to annotate  crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages from a computational linguistic view, the framework will also need to take into account the granularity of the unit ( for example, pos tagging requires sentence - units, but anaphoric annotation needs paragraphs or larger ).', 'secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by #AUTHOR_TAG b ) and shingling techniques described by Chakrabarti ( 2002 )']",3
"[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]","[""linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]","[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]",0
"['( Kennedy , 1998  : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ).', 'increasingly, corpus researchers are tapping the web to overcome the']","['( Kennedy , 1998  : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ).', 'increasingly, corpus researchers are tapping the web to overcome the']","['( Kennedy , 1998  : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologn']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible ( Kennedy , 1998  : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['for escience ( #AUTHOR_TAG ; Hughes et al , 2004 ).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more']","['of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for escience ( #AUTHOR_TAG ; Hughes et al , 2004 ).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more']","['for escience ( #AUTHOR_TAG ; Hughes et al , 2004 ).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more']","['the areas of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for escience ( #AUTHOR_TAG ; Hughes et al , 2004 ).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet (Shirky, 2001).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', 'examples include seti @ home ( looking for radio evidence of extraterrestrial life ), climateprediction. net ( studying climate change ), predictor @ home ( investigating protein - related diseases ) and einstein @ home ( searching for gravitational signals )']",0
"[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( Kilgarriff , 2003 ; #AUTHOR_TAG )""]","[""linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( Kilgarriff , 2003 ; #AUTHOR_TAG )""]","[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( Kilgarriff , 2003 ; #AUTHOR_TAG )""]","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( Kilgarriff , 2003 ; #AUTHOR_TAG )""]",0
"[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( #AUTHOR_TAG a ) and the linguist's search engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]","[""linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( #AUTHOR_TAG a ) and the linguist's search engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]","[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( #AUTHOR_TAG a ) and the linguist's search engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( #AUTHOR_TAG a ) and the linguist's search engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]",0
"['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web']","['example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ).', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to']","['is verifiability and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ).', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to']","['and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ).', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to']","['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ).', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to its database.', 'in contrast, little progress has been made toward annotating sizable sample corpora from the web']",0
"['restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web']","['example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two ap - proaches e. g. automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws4, amal - gam5, connexor6 ) are available, for example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguists search en - gine (Kilgarriff, 2003; Resnik and Elkiss, 2003)']",0
"['data are well documented ( Mair , 2005 ; #AUTHOR_TAG ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a']","['data are well documented ( Mair , 2005 ; #AUTHOR_TAG ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']","['in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998 : 56 ) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ).', 'increasingly, corpus researchers are tapping the web to overcome the']","['( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ).', 'increasingly, corpus researchers are tapping the web to overcome the']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologn']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( #AUTHOR_TAG ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( #AUTHOR_TAG ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( #AUTHOR_TAG ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( #AUTHOR_TAG ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ).', 'it is our intention to implement our distributed corpus annotation framework as a plug - in.', 'this will involve implementing new functionality and integrating this with our existing annotation tools ( such as claws11 ).', 'the development environment is also flexible enough to utilise the boinc platform, and such support will be built into it']",0
"['via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet ( #AUTHOR_TAG ).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', '']","['to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet ( #AUTHOR_TAG ).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', '']","['via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet ( #AUTHOR_TAG ).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', '']","['the areas of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for e - science (Carroll et al., 2005;Hughes et al, 2004).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet ( #AUTHOR_TAG ).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', 'examples include seti @ home ( looking for radio evidence of extraterrestrial life ), climateprediction. net ( studying climate change ), predictor @ home ( investigating protein - related diseases ) and einstein @ home ( searching for gravitational signals )']",0
"['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( Hughes and Walkerdine , 2005 ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( Hughes and Walkerdine , 2005 ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( Hughes and Walkerdine , 2005 ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( Hughes and Walkerdine , 2005 ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ).', 'it is our intention to implement our distributed corpus annotation framework as a plugin.', 'this will involve implementing new functionality and integrating this with our existing annotation tools ( such as claws 11 ).', 'the development environment is also flexible enough to utilise the boinc platform, and such support will be built into it']",0
"['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web']","['example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( #AUTHOR_TAG ).', 'studies have used several different methods to mine web']","['example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( #AUTHOR_TAG ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( #AUTHOR_TAG ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( #AUTHOR_TAG ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['##a - bte, tidy and parcels3 ( #AUTHOR_TAG )']","['with tools like hyppia - bte, tidy and parcels3 ( #AUTHOR_TAG )']","['##a - bte, tidy and parcels3 ( #AUTHOR_TAG )']","['key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'most web - derived corpora have exploited raw text or html pages, so efforts have focussed on boilerplate removal and cleanup of these formats with tools like hyppia - bte, tidy and parcels3 ( #AUTHOR_TAG )']",0
"['data problem ( #AUTHOR_TAG ).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bol']","['data problem ( #AUTHOR_TAG ).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of']","['(Kennedy, 1998 : 56 ) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem ( #AUTHOR_TAG ).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bol']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998 : 56 ) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem ( #AUTHOR_TAG ).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['the new collection onto a queue for the lse annotator to analyse.', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicograph']","['own collections from altavista search engine results.', 'the second method pushes the new collection onto a queue for the lse annotator to analyse.', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicographers to construct dictionary']","['the new collection onto a queue for the lse annotator to analyse.', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicograph']","['real - time "" linguistic analysis of web data at the syntactic level has been piloted by the linguist\'s search engine ( lse ).', 'using this tool, linguists can either perform syntactic searches via parse trees on a pre - analysed web collection of around three million sentences from the internet archive ( www. archive. org )', 'or build their own collections from altavista search engine results.', 'the second method pushes the new collection onto a queue for the lse annotator to analyse.', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicographers to construct dictionary entries requires large pre - annotated corpora.', ""a word sketch is an automatic one - page corpus - derived summary of a word's grammatical and collocational behaviour."", 'word sketches were first used to prepare the macmillan english dictionary for Advanced Learners (2002, edited by michael rundell ).', 'they have also served as the starting point for high - accuracy word sense disambiguation.', 'more recently, the sketch engine was used to develop the new edition of the oxford thesaurus of English (2004, edited by maurice waite )']",0
"['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']","['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']","['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']","['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']",0
"['over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a']","['over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a']","['over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for lingu']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g. automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws4, amal - gam5, connexor6 ) are available, for example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguists search en - gine (Kilgarriff, 2003; Resnik and Elkiss, 2003)']",0
"['.', '#AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by #AUTHOR_TAG']","['techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'the initial solution will be to store the resulting reference 11 http : / / www. comp. lancs. ac. uk / ucrel / claws / corpus in the sketch engine.', 'we will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web - based corpus studies based in general.', 'current practise elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by #AUTHOR_TAG']","['will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'the initial solution will be to store the resulting reference 11 http : / / www. comp. lancs. ac. uk / ucrel / claws / corpus in the sketch engine.', 'we will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web - based corpus studies based in general.', 'current practise elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by #AUTHOR_TAG']","['will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'the initial solution will be to store the resulting reference 11 http : / / www. comp. lancs. ac. uk / ucrel / claws / corpus in the sketch engine.', 'we will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web - based corpus studies based in general.', 'current practise elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by #AUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']",0
"['data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a']","['data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']","['in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998 : 56 ) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
[''],[''],[''],[''],0
"['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001)']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001)']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001)']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001)']",5
"['in academic writing with some success.', 'intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ).', 'storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""however, in our opinion storyspace is a product of its time and in fact it isn't a web application."", 'although it is possible to label links, it lacks a lot of features we need.', 'moreover, no hypertext writing tool']","['intermedia and storyspace.', 'in fact, they were used expecially in academic writing with some success.', 'intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ).', 'storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""however, in our opinion storyspace is a product of its time and in fact it isn't a web application."", 'although it is possible to label links, it lacks a lot of features we need.', 'moreover, no hypertext writing tool']","[', they were used expecially in academic writing with some success.', 'intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ).', 'storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""however, in our opinion storyspace is a product of its time and in fact it isn't a web application."", 'although it is possible to label links, it lacks a lot of features we need.', 'moreover, no hypertext writing tool']","['from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular intermedia and storyspace.', 'in fact, they were used expecially in academic writing with some success.', 'intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ).', 'storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""however, in our opinion storyspace is a product of its time and in fact it isn't a web application."", 'although it is possible to label links, it lacks a lot of features we need.', 'moreover, no hypertext writing tool available is released under an open source licence.', 'we hope that novelle will bridge this gap - we will choose the exact licence when our first public release is ready']",0
"[""the example of #AUTHOR_TAG, we will call the autonomous units of a hypertext lexias ( from ` lexicon'), a word coined by Roland Barthes ( 1970 )."", 'consequently, a hypertext is a set of lexias.', 'in hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'the main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992)']","[""the example of #AUTHOR_TAG, we will call the autonomous units of a hypertext lexias ( from ` lexicon'), a word coined by Roland Barthes ( 1970 )."", 'consequently, a hypertext is a set of lexias.', 'in hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'the main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992)']","[""the example of #AUTHOR_TAG, we will call the autonomous units of a hypertext lexias ( from ` lexicon'), a word coined by Roland Barthes ( 1970 )."", 'consequently, a hypertext is a set of lexias.', 'in hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'the main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992)']","[""the example of #AUTHOR_TAG, we will call the autonomous units of a hypertext lexias ( from ` lexicon'), a word coined by Roland Barthes ( 1970 )."", 'consequently, a hypertext is a set of lexias.', 'in hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'the main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992)']",5
"['the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a ` web page'is more similar to an infinite canvas than a written page ( #AUTHOR_TAG )."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an '""]","['. 1 hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a ` web page'is more similar to an infinite canvas than a written page ( #AUTHOR_TAG )."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco""]","['the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a ` web page'is more similar to an infinite canvas than a written page ( #AUTHOR_TAG )."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 196""]","['. 1 hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a ` web page'is more similar to an infinite canvas than a written page ( #AUTHOR_TAG )."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( mc Neill, 2005) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text?', 'which role is suitable for authors?', 'we have to analyse them before presenting the architecture of novelle']",0
"['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG )']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG )']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG )']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG )']",0
"[""in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author e. g. #AUTHOR_TAG.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind']","[""in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author e. g. #AUTHOR_TAG.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4']","[""in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author e. g. #AUTHOR_TAG.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( mc Neill, 2005).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', ""now they try to collect themselves in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author e. g. #AUTHOR_TAG.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4 for further aspects']",0
