---> Fisrt model running

export CUDA_VISIBLE_DEVICES=1
nohup bash -c 'for segment in token_mistral sentence_majo sentence_prio;
do
    for mode in scopes total;
    do
        python3 new_seq_tagger.py --model_name llm2vec_mistral --segment ${segment} --mode ${mode} --learning_rate 1e-04 --full_training;
    done;
done'> output/llm2vec_mistral/log.txt


export CUDA_VISIBLE_DEVICES=0
nohup bash -c 'for segment in token_llama sentence_majo sentence_prio;
do
    for mode in scopes total;
    do
        python3 new_seq_tagger.py --model_name llm2vec_llama3 --segment ${segment} --mode ${mode} --learning_rate 1e-04 --full_training;
    done;
done'> output/llm2vec_llama3/log.txt


export CUDA_VISIBLE_DEVICES=2
nohup bash -c 'for segment in token_scibert sentence_majo sentence_prio;
do
    for mode in scopes total;
    do
        python3 new_seq_tagger.py --model_name scibert --segment ${segment} --mode ${mode} --learning_rate 5e-05 --full_training;
    done;
done'> output/scibert/log.txt


====================================================================================================================================================================================
---> Token training


CUDA_VISIBLE_DEVICES=2 nohup bash -c 'for lr in 5e-05 1e-04 1e-04 ;
        do
            for bs in 0.0 0.05 0.1 0.15 0.2 ;
            do
                python3 -u new_seq_tagger.py --model_name llm2vec_mistral --segment token_mistral --mode scopes  --learning_rate ${lr} --dropout ${bs};
            done;
        done;
    done;
done'> output/llm2vec_mistral/log.txt


CUDA_VISIBLE_DEVICES=3 nohup bash -c 'for lr in 5e-05 1e-04 1e-04 ;
        do
            for bs in 0.0 0.05 0.1 0.15 0.2 ;
            do
                python3 -u new_seq_tagger.py --model_name llm2vec_llama3 --segment token_llama --mode scopes --learning_rate ${lr} --dropout ${bs};
            done;
        done;
    done;
done'> output/llm2vec_llama3/log.txt


CUDA_VISIBLE_DEVICES=1 nohup bash -c 'for lr in 3e-05 4e-05 6e-05 7e-05 ;
        do
            for dout in 0.03 0.04 0.05 0.06 0.07;
            do
            for bs in 1 2 4 8 16;
                do
                    python3 -u new_seq_tagger.py --model_name scibert --segment token_scibert --mode scopes --learning_rate ${lr} --dropout ${dout} --batch_size ${bs};
                done;
            done;
        done;
    done;
done'> output/scibert/log.txt


====================================================================================================================================================================================
--->full_training : batchsize: 2, lr: 5e-5, dropout: 0.1


CUDA_VISIBLE_DEVICES=1 python3 -u new_seq_tagger.py --model_name scibert --segment token_scibert --mode scopes --learning_rate 5e-05 --dropout 0.1 --batch_size 2 --full_training

CUDA_VISIBLE_DEVICES=1 python3 -u new_seq_tagger.py --model_name llm2vec_mistral --segment sentence_majo --mode scopes --learning_rate 5e-05 --dropout 0.1 --batch_size 2 --full_training

CUDA_VISIBLE_DEVICES=1 python3 -u new_seq_tagger.py --model_name llm2vec_llama3 --segment token_llama --mode scopes --learning_rate 5e-05 --dropout 0.1 --batch_size 2 --full_training



nohup bash -c 'for segment in token_scibert sentence_majo sentence_prio;
do
    for mode in scopes total;
    do
        python3 new_seq_tagger.py --model_name scibert --segment ${segment} --mode ${mode} --learning_rate 5e-05 --dropout 0.1 --batch_size 2 --full_training;
    done;
done'> output/scibert/log.txt


nohup bash -c 'for segment in token_mistral sentence_majo sentence_prio;
do
    for mode in scopes total;
    do
        python3 new_seq_tagger.py --model_name llm2vec_mistral --segment ${segment} --mode ${mode} --learning_rate 5e-05 --dropout 0.1 --batch_size 2 --full_training;
    done;
done'> output/llm2vec_mistral/log.txt


nohup bash -c 'for segment in token_llama sentence_majo sentence_prio;
do
    for mode in scopes total;
    do
        python3 new_seq_tagger.py --model_name llm2vec_llama3 --segment ${segment} --mode ${mode} --learning_rate 5e-05 --dropout 0.1 --batch_size 2 --full_training;
    done;
done'> output/llm2vec_llama3/log.txt

--> sentence_majo_scopes

====================================================================================================================================================================================
---> Hyperparameter tuning


CUDA_VISIBLE_DEVICES=2 nohup bash -c 'for lr in 2e-05 4e-5 5e-05 6e-05 8e-05 1e-04 1.2e-4 ;
        do
            for dout in 0.1 0.12 0.15 0.18 0.2 0.22 0.25;
            do
            for bs in 1 2 4 8;
                do
                    python3 -u new_seq_tagger.py --model_name llm2vec_mistral --segment token_mistral --mode scopes  --learning_rate ${lr} --dropout ${dout} --batch_size ${bs};
                done;
            done;
        done;
    done;
done'> output/llm2vec_mistral/log.txt


CUDA_VISIBLE_DEVICES=0 nohup bash -c 'for lr in 3e-05 5e-05 7e-05 1e-04 1.2e-04 ;
        do
            for dout in 0.0 0.05 0.1 0.15 0.2 ;
            do
            for bs in 1 2 4 8;
                do
                    python3 -u new_seq_tagger.py --model_name llm2vec_llama3 --segment token_llama --mode scopes --learning_rate ${lr} --dropout ${dout} --batch_size ${bs};
                done;
            done;
        done;
    done;
done'> output/llm2vec_llama3/log.txt


CUDA_VISIBLE_DEVICES=0 nohup bash -c 'for lr in 3e-05 4e-05 6e-05 7e-05 ;
        do
            for dout in 0.03 0.04 0.05 0.06 0.07 ;
            do
            for bs in 1 2 4 8 16;
                do
                    python3 -u new_seq_tagger.py --model_name llm2vec_llama3 --segment token_llama --mode scopes --learning_rate ${lr} --dropout ${dout} --batch_size ${bs};
                done;
            done;
        done;
    done;
done'> output/llm2vec_llama3/log.txt
'''