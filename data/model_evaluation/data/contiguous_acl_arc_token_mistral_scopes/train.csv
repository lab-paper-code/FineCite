CC,label
"LFS (Iordanskaja et al., 1992) , and JOYCE (Rambow and Korelsky, 1992) . The framework was originally developed for the realization of deep-syntactic structures in NLG ( #AUTHOR_TAG )  .</s>",0
"Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 )  , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #AUTHOR_TAG )  .</s>",2
"<s> --o II a failli pleuvoir. More details on how the structural divergences described in ( #AUTHOR_TAG )  can be accounted for using our formalism can be found in ( Nasr et al. , 1998 )  .</s>",0
"Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 )  , FoG ( Kittredge and Polguere , 1991 )  , JOYCE ( Rambow and #AUTHOR_TAG )  , and LFS ( Iordanskaja et al. , 1992 )  . Although it adopts the general principles",2
"The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 )  , LFS ( Iordanskaja et al. , 1992 )  , and JOYCE ( Rambow and #AUTHOR_TAG )  . The framework was originally developed for the realization of deep-syntactic structures in NLG .</s>",2
". However, knowledge of sentence boundaries is required by many NLP technologies. Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #AUTHOR_TAG )  ) and parsers generally aim to produce a tree spanning each sentence .</s>",0
"we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL ( #AUTHOR_TAG )  , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When",2
"and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #AUTHOR_TAG )  , ( Kennedy and Boguraev , 1996 )  ( Kameyama , 1997 )  ) . For example, CogNIAC (Baldwin, 1997) , a system based on seven ordered heuristics, generates high-precision resolution (over ",0
Details of the top performing heuristics of COCKTAIL were reported in ( #AUTHOR_TAG )  .,0
"For this research , we used a coreference resolution system ( ( #AUTHOR_TAG )  ) that implements different sets of heuristics corresponding to various forms of coreference . This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g",5
"the abbreviation component will specialize in identifying abbreviations, etc. Each component will return a confidence measure of the reliability of its prediction , c.f. ( #AUTHOR_TAG )  . The results from each component are evaluated to determine the final category of the word.",4
We use an in-house statistical tagger ( based on ( #AUTHOR_TAG )  ) to tag the text in which the,5
"Research that is more similar in goal to that outlined in this paper is Vosse ( #AUTHOR_TAG )  . Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not",1
"Corpus frequency : ( #AUTHOR_TAG )  differentiates between misspellings and neologisms ( new words ) in terms of their frequency . His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms. Our corpus frequency variable spec",5
"predict problematic situations. Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG )  . However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans. In addition, because that work was based on features",2
"-reprompts, percentconfirms, percent-subdials). The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG )  .</s>",4
"normalizer. We use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 )  . Specifically, let V = {v 1 , ...",5
"In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 )  , class n-grams ( #AUTHOR_TAG )  , grammatical features ( Amaya and Benedy , 2001 )  , etc ' .",3
"authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #AUTHOR_TAG )  , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline",4
"As shown in ( #AUTHOR_TAG )  , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results. No special effort was otherwise made in order to optimize the parameters of the classifiers.",4
"Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #AUTHOR_TAG  ] , which is still considered one of the best smoothing methods for n-gram language models . ",5
"The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG )  . When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.",2
"these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 )  and history-based parsing ( Nivre and McDonald , 2008 )  .",3
"these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG )  and history-based parsing ( Nivre and McDonald , 2008 )  .",3
"We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 )  and history-based parsing ( #AUTHOR_TAG )  .",3
"Donald, 2008) . We could also introduce new variables , e.g. , nonterminal refinements ( #AUTHOR_TAG )  , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ;  Buch-Kromann , 2006 ) ",3
". Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #AUTHOR_TAG )  , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses . When combined",2
"The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007) . The first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG )  which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) . This work, on the other hand, is in the orth",2
"can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #AUTHOR_TAG )  , and Machine Translation ( Boas 2002 ) . With the",0
"To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG  , Xue ( 2008 )  . Xue (2008) ",1
". Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG )  , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .",0
"word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word. The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG )  .</s>",5
"although it is more efficient. #AUTHOR_TAG  did very encouraging work on the feature calibration of semantic role labeling . They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings",0
"This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #AUTHOR_TAG )  was built , Xue and Palmer ( 2005 )  and Xue ( 2008 )  have produced more complete and systematic research on Chinese SRL . ",0
".e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( #AUTHOR_TAG  , Xue 2008 ) reassured these findings .</s>",4
#AUTHOR_TAG  has built a semantic role classifier exploiting the interdependence of semantic roles . It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features. Se- mantic context features indicates the features ex- tracted from the arguments around the current one. We can use window size to represent the scope of the context.,5
"Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained. #AUTHOR_TAG  has made the first attempt working on the single semantic role level to make further improvement . However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. What if we could extend the idea of hierarchical architecture to the",1
"on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG  , Xue and Palmer ( 2005 )  and Xue ( 2008 )  . Sun and Jurafsky (2004)",0
"To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 )  , #AUTHOR_TAG  . Xue (2008)  is the best SRL system until now and it has the same data setting with ours. The",1
"classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g. phrase type, are limited. be found in figure 2 , which is similar with that in #AUTHOR_TAG  .</s>",1
"+ , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #AUTHOR_TAG  .</s>",5
"RL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #AUTHOR_TAG )  . With the efforts of",0
"We use the same data setting with #AUTHOR_TAG  , however a bit different from Xue and Palmer ( 2005 )  .</s>",5
"in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 )  , Xue and Palmer ( 2005 )  and #AUTHOR_TAG  . Sun and Jurafsky (2004)",0
"and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( Xue and Palmer 2005 , #AUTHOR_TAG )  reassured these findings .</s>",0
"#AUTHOR_TAG  has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.",0
"on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 )  , #AUTHOR_TAG  and Xue ( 2008 )  . Sun and Jurafsky (2004)",0
"b_900 to chtb_931. We use the same data setting with Xue ( 2008 )  , however a bit different from #AUTHOR_TAG  .</s>",1
<s> The candidate feature templates include : Voice from #AUTHOR_TAG  .</s>,5
The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG )  . It is constituted of two parts.,5
"Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG  . The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing",0
"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 )  .",0
"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 )  .",0
"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG )  .",0
"Other representations use the link structure ( #AUTHOR_TAG )  or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 )  .",0
"Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG )  have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.</s>",0
"literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG )  .</s>",0
"2The WePS-1 corpus includes data from the Web03 testbed ( #AUTHOR_TAG )  which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .</s>",5
"document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG )  .",0
"literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 )  .</s>",0
"the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 )  and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 )  - .",0
"instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -. Other representations use the link structure ( Malin , 2005 )  or generate graph representations of the extracted features ( #AUTHOR_TAG )  .",0
"A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG )  . According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people ",0
". #AUTHOR_TAG  compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).</s>",0
"document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 )  .",0
"ser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc). It provides a fine grained NE recognition covering 100 different NE types ( #AUTHOR_TAG )  . Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex.",5
"identified as person names (Spink et al., 2004) . According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #AUTHOR_TAG )  . As the amount of information in the",0
"Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 )  have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.</s>",0
"ity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name. The Web People Search task , as defined in the first WePS evaluation campaign ( #AUTHOR_TAG )  , consists of grouping search results for a given name according to the different people that share it .</s>",0
"We have used the testbeds from WePS-1 ( #AUTHOR_TAG  , 2007)2 and WePS-2 (Artiles et al., 2009)  evaluation campaigns 3.</s>",5
"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 )  .",0
"The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 )  and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG )  . Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.",0
". In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG )  . Due",0
"(Bagga and Baldwin, 1998;Gooi and Allan, 2004) . Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #AUTHOR_TAG )  and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 )  - .",0
"the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 )  and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG )  - .",0
"WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG )  . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.",3
"WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 )  . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.",3
"ammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG )  for self training .",3
"ammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 )  for self training .",3
"anking approaches (Charniak and Johnson, 2005;Huang, 2008)  for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG )  , although training would be much slower compared to using generative models , as in our case .</s>",3
"<s> Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 )  or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )  .</s>",3
"<s> Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 )  or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )  .</s>",3
"<s> Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 )  or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )  .</s>",3
"<s> Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAG a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 )  or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )  .</s>",3
"When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #AUTHOR_TAG )  or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 )  to reduce required memory usage .",3
"The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 )  . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of",2
"These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #AUTHOR_TAG )  . However, UALIGN uses deep",1
"reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #AUTHOR_TAG  . Formally, this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the",5
"The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG )  . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of",2
"Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG  . Taking",5
"In our previous work ( #AUTHOR_TAG )  , we started an initial investigation on conversation entailment . We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the",2
"Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #AUTHOR_TAG )  and trained a logistic regression model to predict verb alignment based on the features in Table 1 .</s>",2
"Note that in our original work ( #AUTHOR_TAG )  , only development data were used to show some initial observations. Here we trained our mod- els on the development data and results shown are from the testing data.</s>",1
. This alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG )  .</s>,5
"To address this limitation , our previous work ( #AUTHOR_TAG )  has initiated an investigation on the problem of conversation entailment . The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example ",2
"In our previous work ( #AUTHOR_TAG )  , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of prob",2
". We use the structures previously used by #AUTHOR_TAG  , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.",5
structures perform the best. This revalidates the observation of #AUTHOR_TAG  that phrase structure representations and dependency representations add complimentary value to the learning task .,1
the PET and GR kernel perform similar : this is different from the results of ( #AUTHOR_TAG )  where GR performed much worse than</s>,1
"<s> Our results also confirm the insights gained by #AUTHOR_TAG  , who observed that in crossdomain polarity analysis adding more training data is not always beneficial . Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.</s>",1
"ially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG )  , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate",3
"ially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 )  , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate",3
"An implementation of the transition-based dependency parsing frame- work ( #AUTHOR_TAG )  using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in Zhang and Clark (2008)  with a beam size of 8. Beams with varying sizes can be used to produce k-best lists. The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word iden- tity of the syntactic head of the top word on the stack (if available); dependency arc label iden- tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the",5
"There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( Mann and McCallum , 2010 )  , posterior regularization ( Ganchev et al. , 2010 )  and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG )  . The work of Chang et al. (2007)  on constraint driven learning is perhaps the",0
"-factored parameterization (McDonald et al., 2005) . We use the non-projective k-best MST algorithm to generate k-best lists ( #AUTHOR_TAG )  , where k = 8 for the experiments in this paper .",5
"Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data. Consider, for example, the case of questions. #AUTHOR_TAG  observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where",1
1Our rules are similar to those from #AUTHOR_TAG  .</s>,1
"sers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering ( Wang et al. , 2007 )  , sentiment analysis ( #AUTHOR_TAG )  , MT reordering ( Xu et al. , 2009 )  , and many other tasks . In",0
An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #AUTHOR_TAG )  . We use the non-projective k-best M,5
", a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #AUTHOR_TAG )  , with the exception that the new parse data is targeted to produce accurate word reorderings . Our method differs as it does not statically fix a",1
"EOR scoring metric. Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #AUTHOR_TAG )  and is simpler to measure .",4
"sers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering ( Wang et al. , 2007 )  , sentiment analysis ( Nakagawa et al. , 2010 )  , MT reordering ( #AUTHOR_TAG )  , and many other tasks . In",0
"• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 )  using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG  with a beam size of 8 . Be",5
"to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007) , sentiment analysis (Nakagawa et al., 2010) , MT reordering (Xu et al., 2009) , and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 )  . But these accuracies are measured with respect to gold-standard out-of-domain parse",0
"In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #AUTHOR_TAG )  . We also make use of the Brown corpus, and the Question Treebank (QTB) (Judge et al., 2006 </s>",5
"to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007) , sentiment analysis (Nakagawa et al., 2010) , MT reordering (Xu et al., 2009) , and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 )  . But these accuracies are measured with respect to gold-standard out-of-domain parse",0
"The work that is most similar to ours is that of #AUTHOR_TAG  , who introduced the Constraint Driven Learning algorithm ( CODL ) . Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets). For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints. These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items. The augmented-loss algorithm can be viewed as an online version of this algorithm which performs",1
", we also evaluate the method on alternate extrinsic loss functions. #AUTHOR_TAG  presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we",1
"A recent study by #AUTHOR_TAG  also investigates the task of training parsers to improve MT reordering . In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006) , with the exception that the new parse data is targeted to produce accurate word reorderings. Our method differs as",1
"Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007)  and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG )  and textual entailment ( Berant et al. , 2010 )  .</s>",0
"Proof. Identical to the standard perceptron proof , e.g. , #AUTHOR_TAG  , by inserting in loss-separability for normal separability .</s>",0
". Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG )  and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009)  and textual entailment (Berant et al., 2010) .</s>",4
"to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007) , sentiment analysis (Nakagawa et al., 2010) , MT reordering (Xu et al., 2009) , and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 )  . But these accuracies are measured with respect to gold-standard out-of-domain parse",0
"One obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG )  . In such a setting, an auxiliary reranker is added in a pipeline following the parser. The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework). The reranker can then be trained to optimize for the downstream or extrinsic objective. While this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser.",0
"to do worse on longer dependencies (McDonald and Nivre, 2007)  and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 )  and textual entailment ( #AUTHOR_TAG )  .</s>",0
"There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( Mann and McCallum , 2010 )  , posterior regularization ( Ganchev et al. , 2010 )  and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 )  . The work of Chang et al. (2007)  on constraint driven learning is perhaps the",1
An implementation of the transition-based dependency parsing framework ( #AUTHOR_TAG )  using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 )  with a beam size of 8 . Beams with varying sizes can be,5
"There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( #AUTHOR_TAG )  , posterior regularization ( Ganchev et al. , 2010 )  and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 )  . The work of Chang et al. (2007) ",0
score. For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG )  .</s>,5
"to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007) , sentiment analysis (Nakagawa et al., 2010) , MT reordering (Xu et al., 2009) , and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG )  . But these accuracies are measured with respect to gold-standard out-of-domain parse",0
"sers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering ( #AUTHOR_TAG )  , sentiment analysis ( Nakagawa et al. , 2010 )  , MT reordering ( Xu et al. , 2009 )  , and many other tasks . In",0
<s> criteria and data used in our experiments are based on the work of #AUTHOR_TAG  .</s>,5
"In this paper , inspired by KNN-SVM ( #AUTHOR_TAG )  , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems . Compared with global training methods, such as",4
"the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here. Motivated by (#AUTHOR_TAG , 2003; Smith and Eisner, 2006) , we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−W",4
"<s>#AUTHOR_TAG  introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :</s>",0
"Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 )  . Instead of using translation examples to construct translation rules for enlarging the decoding space, we",1
"leneck. Actually , if we use LSH technique ( #AUTHOR_TAG )  in retrieval process , the local method can be easily scaled to a larger training data .",3
"Several works have proposed discriminative tech- niques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al., 2008)  used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG )  employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) ",1
"The local training method ( #AUTHOR_TAG )  is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 )  . Compared with the global training method which",0
"various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 )  , error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 )  , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 )  and ranking ( Hopkins and May , 2011 )  ,",0
"the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 )  with modified Kneser-Ney smoothing ( #AUTHOR_TAG )  . In our experiments the translation performances",5
"4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG )  with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 )  .",5
"various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 )  , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG )  , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 )  and ranking ( Hopkins and May , 2011 )  ,",0
"aining mode, incremental training can improve the training efficiency. In the field of machine learning research , incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 )  , but there is little work for tuning parameters of statistical machine translation . The biggest difficulty lies in that the fea- ture vector of a given training example, i.e",0
"Several works have proposed discriminative tech- niques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al., 2008)  used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 )  employed an evaluation metric as a loss function and directly optimized it . (Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011)",1
"Several works have proposed discriminative tech- niques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al., 2008)  used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 )  employed an evaluation metric as a loss function and directly optimized it . (Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011)",1
The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG )  .,5
"(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#AUTHOR_TAG , 2011). Since score of e",0
"Several works have proposed discriminative techniques to train log-linear model for SMT. ( #AUTHOR_TAG ; Blunsom et al. , 2008 )  used maximum likelihood estimation to learn weights for MT. (Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) ",1
"<s> The metric we consider here is derived from an example-based machine translation. To retrieve translation examples for a test sentence , ( #AUTHOR_TAG )  defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :</s>",5
"various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 )  , error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 )  , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 )  and ranking ( Hopkins and May , 2011 )  ,",0
"b . We employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 )  to propose two incremental methods for local training in Algorithm 2 as follows .</s>",5
"Several works have proposed discriminative techniques to train log-linear model for SMT. (Och and Ney, 2002;Blunsom et al., 2008)  used maximum likelihood estimation to learn weights for MT. ( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 )  employed an evaluation metric as a loss function and directly optimized it . (Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011)",1
"We use an in-house developed hierarchical phrase-based translation ( #AUTHOR_TAG )  as our baseline system , and we denote it as In-Hiero . To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using M",5
"We run GIZA + + ( #AUTHOR_TAG )  on the training corpus in both directions ( Koehn et al. , 2003 )  to obtain the word alignment for each sentence pair .",5
"directly optimized it. ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG )  proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .</s>",1
"based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 )  , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 )  , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 )  and ranking ( #AUTHOR_TAG )  , and among which minimum error rate training ( M",0
", our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG )  . However,",1
"Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990) . Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG )  . For the referring expression generation task here, we also need a lexicon with grounded semantics.</s>",2
"How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG )  . In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower per",2
"performed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein , #AUTHOR_TAG  showed that a different feature-topic model improved predictions on a fill-in-the-blank task . Johns and Jones ( 2012)",0
"Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead. Some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 )  , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter cat-",0
"techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010 b). They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG )  to create a bimodal vocabulary describing documents . The topic model",0
"More recently , Silberer et al. ( 2013 )  show that visual attribute classifiers , which have been immensely successful in object recognition ( #AUTHOR_TAG )  , act as excellent substitutes for feature Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013) .</s>",0
"(Chen and Mooney, 2011)  or robot commands (Tellex et al., 2011;Matuszek et al., 2012) . Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; Ordonez et al. , 2011 )  , text illustration ( Joshi et al. , 2006 )  , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )  .</s>",0
"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 )  . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional",0
"That is, we simply take the original mLDA model of #AUTHOR_TAG  (2009) and generalize it in the same way they generalize LDA. At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a",2
"The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG )  . Others provide",0
"#AUTHOR_TAG  extend LDA to allow for the inference of document and topic distributions in a multimodal corpus . In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics. Topics consist of multinomial",0
"The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #AUTHOR_TAG )  , and images are then quantized over the 5,000 codewords . All images",5
"ing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG )  .</s>",1
"act as excellent substitutes for feature norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG )  .</s>",0
", where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 )  ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #AUTHOR_TAG )  .</s>",0
"where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007) . Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #AUTHOR_TAG )  or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 )  .",0
"as interpreting navigation directions (Chen and Mooney, 2011)  or robot commands (Tellex et al., 2011;Matuszek et al., 2012) . Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; #AUTHOR_TAG )  , text illustration ( Joshi et al. , 2006 )  , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )  .</s>",0
"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; #AUTHOR_TAG )  . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" ",0
"In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities. We use the same method as #AUTHOR_TAG  for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair .",5
"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 )  ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG )  and neurological evidence of ties between the language , perceptual and motor systems in the brain (",0
"Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG  . Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012) . It has also been shown to be useful in joint inference of text with visual attributes obtained using",5
", it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of #AUTHOR_TAG b ) 's suggestion that something like a distributional hypothesis of images is plausible .</s>",1
"for the overall ""gist"" of the whole image. It is frequently used in tasks like scene identification , and #AUTHOR_TAG  shows that distance in GIST space correlates well with semantic distance in WordNet . After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.</s>",4
ity prediction. #AUTHOR_TAG a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation . ,0
"as interpreting navigation directions (Chen and Mooney, 2011)  or robot commands (Tellex et al., 2011;Matuszek et al., 2012) . Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAG a ; Ordonez et al. , 2011 )  , text illustration ( Joshi et al. , 2006 )  , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )  .</s>",0
"For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #AUTHOR_TAG )  containing approximately 1.7 B word tokens . We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,8",5
"classifying never-before-seen objects ) , and #AUTHOR_TAG  show that verb clusters can be used to improve activity recognition in videos .</s>",0
"In a similar vein, Steyvers (2010)  showed that a different feature-topic model improved predictions on a fill-in-the-blank task. #AUTHOR_TAG  take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity . Silberer and Lapata (2012)",0
"is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following #AUTHOR_TAG  , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word",5
"In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #AUTHOR_TAG  . We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA.",5
"We also compute GIST vectors ( #AUTHOR_TAG )  for every image using LearGIST ( Douze et al. , 2009 )  . Unlike SURF",5
"(Chen and Mooney, 2011)  or robot commands (Tellex et al., 2011;Matuszek et al., 2012) . Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; Ordonez et al. , 2011 )  , text illustration ( Joshi et al. , 2006 )  , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )  .</s>",0
"., McRae et al. (2005) ). #AUTHOR_TAG  helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 )  in the prediction of association norms . Andrews et al. (2009)  furthered this work by",0
"The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples , Rohrbach et al. ( 2010 )  and #AUTHOR_TAG  show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 )  show that verb clusters can be used to improve activity recognition in videos .",0
"appings. ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #AUTHOR_TAG )  . Multiple synsets exist for each meaning of a word. For",5
"feature norms from weighted mixtures based on textual similarity. #AUTHOR_TAG  introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .</s>",0
". More recently , #AUTHOR_TAG  show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 )  , act as excellent substitutes for feature Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013) .</s>",0
". We do not optimize these hyperparameters or vary them over time. The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #AUTHOR_TAG  .</s>",5
"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 )  . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" ",0
"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 )  ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 )  and neurological evidence of ties between the language , perceptual and motor systems in the brain (",0
"ent Dirichlet Allocation developed by Andrews et al. (2009) . Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG )  . It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks ",0
"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 )  . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional",0
"act as excellent substitutes for feature norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 )  .</s>",0
"Association Norms ( AN ) is a collection of association norms collected by Schulte im #AUTHOR_TAG  . In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind. With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words. After removing",5
"Latent Dirichlet Allocation ( #AUTHOR_TAG )  , or LDA , is an unsupervised Bayesian probabilistic model of text documents . It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ). LDA assumes every document in the corpus is generated using the fol-lowing generative process:</s>",0
"Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 )  , while others choose to employ concepts elicited from psycholinguistic and cognition studies .",0
"The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples , #AUTHOR_TAG  and Socher et al. ( 2013 )  show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 )  show that verb clusters can be used to improve activity recognition in videos .",0
"Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 )  , while others choose to employ concepts elicited from psycholinguistic and cognition studies .",0
"norms. #AUTHOR_TAG  furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In",0
"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 )  . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" ",0
"It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG )  . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks ",0
"as interpreting navigation directions (Chen and Mooney, 2011)  or robot commands (Tellex et al., 2011;Matuszek et al., 2012) . Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; Ordonez et al. , 2011 )  , text illustration ( #AUTHOR_TAG )  , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )  .</s>",0
"where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007) . Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 )  or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG )  . Some",0
"computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 )  and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG )  .</s>",0
"To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG )  for our models . In Vari",5
"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 )  . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" ",0
"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 )  . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" ",0
"ing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )  .</s>",1
"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 )  ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 )  and neurological evidence of ties between the language , perceptual and motor systems in the brain (",0
"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 )  ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 )  and neurological evidence of ties between the language , perceptual and motor systems in the brain (",0
"the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG )  in the prediction of association norms . Andrews et al. (2009)",0
"To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 )  for our models . In Vari",5
"As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is #AUTHOR_TAG b ) . They use a Bag of Visual Words (BoVW) model (Lowe, 2004)  to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.",0
"model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )  . While prior work has used the model only with feature",2
"for each post in the input post sequence. This choice is motivated by an observation we made previously ( #AUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3",2
"Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification ( #AUTHOR_TAG c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 )  .",2
"post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAG a ) comparing scratch translation to post-edit . Many research",2
to shift topics more often. This is in line with our previous findings from ( #AUTHOR_TAG )  that candidates with higher power attempt to shift topics less often than others when responding to moderators .,1
We follow our previous work ( #AUTHOR_TAG b ) and restrict bridging to non-coreferential cases . We also,2
"Feats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAG a ; Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection . Some of these features overlap with the atomic features used in the rule-",2
". in history-based models ( #AUTHOR_TAG )  , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i . This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.</s>",5
"The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )  .</s>",1
<s> 7A11 our results are computed with the evalb program following the now-standard criteria in ( #AUTHOR_TAG )  .</s>,5
"sers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 )  are based on a history-based probability model ( #AUTHOR_TAG )  , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.",0
"The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )  .</s>",1
"the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #AUTHOR_TAG )  .</s>",0
"For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #AUTHOR_TAG )  , as has conditioning on the left-corner child ( Roark and Johnson , 1999 )",0
"we have not currently run tests without them. feature sets, but then efficiency becomes a problem. #AUTHOR_TAG  define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 )  .</s>",0
. We used a publicly available tagger ( #AUTHOR_TAG )  to tag the words and then used these in the input to the system .</s>,5
"The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 )  . The performance of the lexicalized model falls in the middle of",1
"Many statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 )  are based on a history-based probability model ( Black et al. , 1993 )  , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.",0
"For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 )  , as has conditioning on the left-corner child ( #AUTHOR_TAG )  .",0
"The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG )  . The performance of the lexicalized model falls in the middle of",1
"The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 )  . The performance of the lexicalized model falls in the middle of",1
"done in (Ratnaparkhi, 1999) . Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG )  . The difference from previous approaches is in the nature of the input to",1
%. This is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG  and,1
"'s estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 )  and Collins ( 2000 )  , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG  .</s>",0
"'s PCFG-reduction reported in Table 1. Compared to the reranking technique in #AUTHOR_TAG  , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction . While SL-DOP and LS-DOP have been compared before in",1
"ators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001 Bod ( , 2003. But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 )  and #AUTHOR_TAG  , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 )  .</s>",0
"Most DOP models , such as in Bod ( 1993 )  , Goodman ( 1996 )  , #AUTHOR_TAG  , Sima'an ( 2000 )  and Collins & Duffy ( 2002 )  , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most",0
"sers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #AUTHOR_TAG )  .",1
"For our experiments we used the standard division of the WSJ ( #AUTHOR_TAG )  , with sections 2 through 21 for training ( approx . 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.",5
"them with some other statistical parsers (resp. #AUTHOR_TAG  , Charniak 1997 , Collins 1999 and Charniak ",1
"ators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001 Bod ( , 2003. But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG  and Collins ( 2000 )  , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 )  .</s>",0
"And #AUTHOR_TAG  argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 )  who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )  .</s>",0
"Most DOP models , such as in Bod ( 1993 )  , Goodman ( 1996 )  , Bonnema et al. ( 1997 )  , Sima'an ( 2000 )  and #AUTHOR_TAG  , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence . We will refer to these models as",0
"Most DOP models , such as in Bod ( 1993 )  , #AUTHOR_TAG  , Bonnema et al. ( 1997 )  , Sima'an ( 2000 )  and Collins & Duffy ( 2002 )  , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.",0
"Most DOP models , such as in Bod ( 1993 )  , Goodman ( 1996 )  , Bonnema et al. ( 1997 )  , #AUTHOR_TAG  and Collins & Duffy ( 2002 )  , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.",0
"A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account. Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #AUTHOR_TAG  , 2002 ) . Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002) .</s>",0
"a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage , as exemplified by the work of #AUTHOR_TAG  , 1999 ) , Charniak ( 1996  , 1997 ) , Johnson ( 1998 )  , Chiang ( 2000 )  , and many others .</s>",4
The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG  ; Goodman 1998 ) .,0
"And Collins ( 2000 )  argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG  who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )  .</s>",4
"als at the roots and leaves, internal nonterminals elsewhere have probability 1/a. And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG )  .",0
"cases is a reasonable approximation of the most probable parse. #AUTHOR_TAG  , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman",0
systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #AUTHOR_TAG )  .,5
") how to merge. Following the work of #AUTHOR_TAG  , we implement a linear-chain CRF merging system using the following features : stemmed (",5
"For compound splitting , we follow #AUTHOR_TAG  , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .",5
"We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model. #AUTHOR_TAG  showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .</s>",0
"This approach resulted in too many compounds. We follow #AUTHOR_TAG  , for compound merging . We trained a CRF using",5
solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. #AUTHOR_TAG  improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) . Both efforts were ineffective on large data sets.,1
". Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 )  ) or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG )  . Compound",1
important markups in our system). Both efforts were ineffective on large data sets. #AUTHOR_TAG  used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex con- text features.,1
"<s> The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 )  and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG )  .</s>",5
"We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG  . First, possible split points are extracted using SMOR",5
". Virpioja et al. ( 2007 )  , Badr et al. ( 2008 )  , Luong et al. ( 2010 )  , #AUTHOR_TAG  , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread",1
#AUTHOR_TAG  tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010),1
"with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. #AUTHOR_TAG  introduced factored SMT . We use more complex context features. Fraser (2009) ",1
"Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 )  , #AUTHOR_TAG  and others . T",1
"Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG  , Yeniterzi and Oflazer ( 2010 )  and others . Toutanova et. al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information",1
". Other approaches use less deep linguistic resources ( e.g. , POS-tags #AUTHOR_TAG )  or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ",1
"Our approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG )  , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 )  . Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.",2
2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG )  . Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.</s>,0
<s> I A more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG )  .</s>,0
"Combining control strategies depends on a way to differentiate between types of constraints. Proceedings of EACL '99 example , the ALE parser ( #AUTHOR_TAG )  presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case",0
"Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs. See, among others, (Ramakrishnan et al. 1992). As shown in ( #AUTHOR_TAG )  â¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .</s>",0
"Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG )  3 (Meurers and Minnen, 1997)  propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992) . Typed feature structures as normal form ir~'~E terms are merely syntactic objects.",0
"Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #AUTHOR_TAG )  . 3 ( Meurers and Minnen , 1997 )  propose a compilation of lexical rules into",0
"Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs. See , among others , ( #AUTHOR_TAG )  . As",0
ain from an example. The ConTroll grammar development system as described in ( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .</s>,0
"<s> Coroutining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking. See also ( #AUTHOR_TAG ; Naish , 1986 )  .</s>",0
. As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #AUTHOR_TAG )  . Unlike the ALE parser,1
"; Pollard and Sag , 1994 )  as discussed in ( #AUTHOR_TAG a ) and ( Meurers and Minnen , 1997 )  .",2
"` See ( #AUTHOR_TAG )  for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG . append ([~,[~,[~).",0
". This contrasts with one of the traditional approaches ( e.g. , #AUTHOR_TAG  ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .",1
"B- , though we have also used handcoded transducers ( #AUTHOR_TAG )  and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .</s>",5
"At the same time , we believe our method has advantages over the approach developed initially at IBM ( #AUTHOR_TAG  ; Brown et al. 1993 ) for training translation systems automatically .",1
"1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #AUTHOR_TAG  , 203 ) . Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann",0
systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #AUTHOR_TAG )  . Re,0
"ators. In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG )  . While this certainly has appeal as a design methodology, it seems reck",0
"the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG  ; Panaget 1994 ; Wanner 1994 ) .",0
"the network. Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #AUTHOR_TAG a ) .</s>",0
"; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG  ; Sondheimer and Nebel 1986 ) , and Hovy",0
"There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. These include devices such as interleaving the components ( McDonald 1983 ; #AUTHOR_TAG )  , backtracking on failure ( Appelt 1985 ; N",0
"be to abandon the separation; the generator could be a single component that handles all of the work. This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 )  and Danlos ( 1987 )  and , at least implicitly , in #AUTHOR_TAG  and Delin et al. ( 1994 )  ; however ,",0
"allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAG a , 1988c ) . All",0
"Hovy has described another text planner that builds similar plans ( #AUTHOR_TAG b ) . This system, however, starts with a list of information to be expressed and merely arranges it into a coherent pattern; it is thus not a planner in the sense used here (as Hovy makes clear). ",0
4). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG )  .</s>,0
"1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAG a ) , or simply ""what to say"" versus ""how to say it"" ( e.g. ,",0
"Surveys and articles on the topic include Lamarche and Retord ( 1996 )  , de Groote and Retord ( 1996 )  , and #AUTHOR_TAG  . Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained.</s>",0
". An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG  but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .</s>",0
"One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG  , Hepple 1990 , Hendriks 1993 ) . Each sequent has a distinguished",0
"DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases ( #AUTHOR_TAG )  can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types.",4
"other classifier architectures are applicable to the tasks discussed, in particular to DA classification. A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 )  is transformation-based learning ( #AUTHOR_TAG )  . Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which further techniques could be borrowed.",1
"The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #AUTHOR_TAG )  . It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1",1
"and tagging (Church 1988). It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG )  . To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually,",0
"<s> This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #AUTHOR_TAG  . It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.</s>",1
"( unlike , say , the metavariables of standard QLFs [ #AUTHOR_TAG  ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) . Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at Q",0
"In the CLE-QLF approach, as rationally reconstructed by #AUTHOR_TAG  and Crouch and Putman ( 1994 )  , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.",1
"A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #AUTHOR_TAG  . This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume. Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for",1
"approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #AUTHOR_TAG  and Crouch and Pulman ( 1994 )  ;",1
"The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #AUTHOR_TAG  , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) . In the CLE-QLF approach",1
"We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #AUTHOR_TAG )  , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.</s>",0
"The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #AUTHOR_TAG )  , with the resolution process as described here .",3
"The version proposed here combines a basic insight from Lewin ( 1990 )  with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG  , 1991 ) , with some differences that are commented on below . Like",1
"It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 )  and #AUTHOR_TAG  , 1991 ) . Recall",1
"only the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG  , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â",0
"Developing a calculus for reasoning with QLFs is too large a task to be undertaken here. But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG )  work to our own framework . Re",5
"#AUTHOR_TAG  present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .</s>",0
"uraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG  ; Mitkov 1996 , 1998b ) .",0
os and Lopes 1994 ; Strube and Hahn 1996 ; #AUTHOR_TAG  ; T,0
"nan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( #AUTHOR_TAG )  . The L",0
"Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAG a , 2001b ) . For a more detailed survey",0
"-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #AUTHOR_TAG  ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , ",0
"the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #AUTHOR_TAG  ; Hahn and Strube 1997 ; T",0
"). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #AUTHOR_TAG  ; Ge , Hale , and Charniak 1998 ; Card",0
"000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG  ; Mitkov and Stys 1997 ; Mitkov ,",0
"promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #AUTHOR_TAG  ; Kennedy and",0
"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #AUTHOR_TAG  ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .",0
orano 2000 ; #AUTHOR_TAG  ; Mitkov 1999 ; Mitkov and Stys ,0
"The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAG a ) . A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are. In particular, more research should be carried out on the factors influencing the performance of these algorithms",3
; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #AUTHOR_TAG )  ; and proposals related to the evaluation methodology in anaphora resolution (,0
"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #AUTHOR_TAG )  , which was difficult both to represent and to process , and which required considerable human input . However, the pressing",0
"Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG  , Gaizauskas and Humphreys ( 1996 )  , and Kameyama ( 1997 )  . The last decade of the 20th century saw a number of anap",0
"3 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG  ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and",0
"ev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #AUTHOR_TAG  , 1998b ) .</s>",0
"Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG )  , BFP (",0
"0 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG  ; Mitkov , Belguith , and Stys 1998 ) .",0
") gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 )  , Gaizauskas and Humphreys ( 1996 )  , and #AUTHOR_TAG  . The last decade of the 20th century saw a number of anaphora",0
"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG  ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .",0
"Proper names are the main concern of the named-entity recognition subtask ( #AUTHOR_TAG  1998) of information extraction. The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1 There the disambiguation of the first word in a sentence (and in other ambiguous positions) is",0
"e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG  ] , Brill 's [ Brill 1995a ] , and MaxEnt [",1
"approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies , which were briefly described in #AUTHOR_TAG  .</s>",5
"There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #AUTHOR_TAG )  and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) . The Brown corpus represents general English. It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech. The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.",5
"sense per discourse""). Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #AUTHOR_TAG )  . G",0
"- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG )  , neural networks (",0
"corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #AUTHOR_TAG )  , and maximum-entropy modeling ( Reynar",0
", Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #AUTHOR_TAG a ] , and MaxEnt [",1
"ov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ #AUTHOR_TAG  ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that",1
SJ corpus was achieved by a combination of the SATZ system ( #AUTHOR_TAG )  with the Alembic system ( Aberdeen et al. ,1
"-consuming enterprise. For instance , the Alembic workbench ( #AUTHOR_TAG )  contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex . Another well-",0
"). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #AUTHOR_TAG  ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant.",1
"purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #AUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent",1
"In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #AUTHOR_TAG  . We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.",1
", by accounting for the immediate syntactic context and using estimates collected from a training corpus. As #AUTHOR_TAG  rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not . Estimates from the Brown Corpus can be misleading. For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title). It would be misleading to infer from this evidence that the word 'Acts' is always a proper noun.""</s>",1
"purely unsupervised techniques ( e.g. , Baum-Welch [ #AUTHOR_TAG  ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries ,",1
"we had to deal with the case normalization issue. Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG )  to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. . For words that could be normalized to several main forms (polysemy), when secondary forms of",5
"given language, but no supplementary information such as POS or morphological information is required to be present in this list. A variety of such lists for many languages are already available ( e.g. , #AUTHOR_TAG )  . Words in such lists are usually supplemented with morphological and POS information (which is not required by our method",0
approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG  : 0.28 % vs. 0.20 % error rate ) .,1
998). In some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG )  . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class. The disadvantage is in the cost of building a wide-cover,0
0 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG )  . We used these texts because the approach described in this article was,5
"5): a 0.5% error rate. The best performance on the Brown corpus , a 0.2 % error rate , was reported by #AUTHOR_TAG  , who trained a decision tree classifier on a 25-million-word corpus . In",1
"Not much information has been published on abbreviation identification. One of the better-known approaches is described in #AUTHOR_TAG  , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing . This is similar to",0
"sense per discourse""). Since then this idea has been applied to several tasks , including word sense disambiguation ( #AUTHOR_TAG )  and named-entity recognition ( Cucerzan and Yarowsky 1999 ) . G",0
same time as part of this abbreviation and as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD problem can be found in #AUTHOR_TAG  .</s>,0
"our approach. This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG  , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .",1
"proceed. #AUTHOR_TAG  pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names . They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998). ",5
"To test our hypothesis that DCA can be used as a complement to a local-context approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger. Unlike other POS taggers , this POS tagger ( #AUTHOR_TAG )  was also trained to disambiguate sentence boundaries .</s>",5
J corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG )  : a 0.5 % error rate . The,1
"a difficulty for our approach. This is where robust syntactic systems like SATZ ( #AUTHOR_TAG )  or the POS tagger reported in Mikheev ( 2000 )  , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .",1
"lists. But unlike the cache model, it uses a multipass strategy. #AUTHOR_TAG  developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last In our experiments we applied",2
The description of the EAGLE workbench for linguistic engineering ( #AUTHOR_TAG )  mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document . This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions. It is quite similar to,1
"classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG )  . Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the",0
"and in this article we will use these two terms and the term case normalization interchangeably. #AUTHOR_TAG  , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . '' Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.</s>",0
"to other documents. #AUTHOR_TAG  recently described a hybrid method for finding abbreviations and their definitions . This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK). The abbreviation recognizer for these purposes is allowed to overgenerate significantly. There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained. Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter. Candidates then are filtered through a set of known common words and proper names.",0
"optimal performance. For instance , #AUTHOR_TAG  report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words . This is a relatively small training set that can be manually marked in a few hours' time. But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).",1
"chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #AUTHOR_TAG )  .7 As an",0
"ed with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG  , Frederking et al. ( 1994 )  , and Hogan and Frederking ( 1998 )  .</s>",1
"993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #AUTHOR_TAG  attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . ",0
â¢ Learnability ( #AUTHOR_TAG )  â¢ Text generation ( Hovy 1988 ;,0
"Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #AUTHOR_TAG  , Carl ( 1999 )  , and Brown ( 2000 )  , inter alia .</s>",0
"<s> â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( #AUTHOR_TAG  ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and",0
"is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #AUTHOR_TAG  ; Veale and Way 1997 ; Carl 1999 ) .",0
". From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #AUTHOR_TAG )  .</s>",0
. #AUTHOR_TAG  use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. ,0
"ola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; #AUTHOR_TAG  ; Gough , Way , and Hearne 2002 )</s>",0
"â¢ language learning ( #AUTHOR_TAG  ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction (",0
"ed with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 )  , #AUTHOR_TAG  , and Hogan and Frederking ( 1998 )  .</s>",1
"More recently , #AUTHOR_TAG  have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003  , pages 108",0
corpora. #AUTHOR_TAG  replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . ,0
"#AUTHOR_TAG  , 1997 ) assumes that words ending in - ed are verbs . However, given that verbs are not a closed class, in",1
"We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG )  .</s>",3
"longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG  ; Carl 1999 ) .",0
"masculine plural NP. However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG  . G",5
"marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks. #AUTHOR_TAG  , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â > French and English â > Urdu . For the English −→ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English −→ Urdu, Juola (1997 , page 2",0
"1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( #AUTHOR_TAG )  â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough ,",0
"In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG  . In Block'",5
"Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #AUTHOR_TAG  to permit a limited form of insertion in the translation process . As",5
"In their Gaijin system , #AUTHOR_TAG  give a result of 63 % accurate translations obtained for English â > German on a test set of 791 sentences from CorelDRAW manuals .</s>",1
"â¢ language learning ( Green 1979 ; #AUTHOR_TAG  ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction (",0
"â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( #AUTHOR_TAG )  â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation (",0
". For English â > Urdu , #AUTHOR_TAG  , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation. In",0
"approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 )  , #AUTHOR_TAG  , and Brown ( 2000 )  , inter alia .</s>",0
"hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #AUTHOR_TAG  , to generate the `` generalized marker lexicon . ''",5
"osavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( #AUTHOR_TAG )  â¢ Localization ( Sch Â¨ aler 1996 )</s>",0
"Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #AUTHOR_TAG )  is used to segment the phrasal lexicon into a `` marker lexicon . '' The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.",5
"That is, where #AUTHOR_TAG  substitutes variables for various words in his templates, we replace certain lexical items with their marker tag. Given that examples such as ��<DET> a",1
"Nevertheless , #AUTHOR_TAG  , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . '' Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto",0
"approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 )  , Carl ( 1999 )  , and #AUTHOR_TAG  , inter alia .</s>",0
"they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG )  has been used successfully in a number of areas :</s>",0
"All EBMT systems , from the initial proposal by #AUTHOR_TAG  to the recent collection of Carl and Way ( 2003 )  , are premised on the availability of subsentential alignments derived from the input bitext . There is a wealth of",0
"researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992)  identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. #AUTHOR_TAG  combines lexical and dependency mappings to form his generalizations . Other similar approaches include those of Cicekli and Güvenir (1996), McTait and Trujillo (1999) , Carl (1999) , and Brown (2000 , inter alia.</s>",0
"Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #AUTHOR_TAG )  .",0
"in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #AUTHOR_TAG  and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . Pierce and Cardie (2001)  have",0
"the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications. Some examples include text categorization ( #AUTHOR_TAG )  , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Eng",0
"sers. Another technique for making better use of unlabeled data is cotraining ( #AUTHOR_TAG )  , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001)  and  suggests that co-training can be helpful for statistical parsing. ",0
"1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #AUTHOR_TAG  ; Hwa 1998 ) , and",5
"or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( #AUTHOR_TAG )  , and a maximumentropy model (",0
"and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( #AUTHOR_TAG )  , and word sense disambiguation ( F",0
"Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #AUTHOR_TAG )  , we can efficiently compute the probability of the sentence , P ( w | G ) . Similarly, the algorithm can be modified to compute the quantity</s>",5
").)"" Our algorithm is similar to the approach taken by #AUTHOR_TAG  for inducing PCFG parsers .</s>",1
"the system successfully learns a concept only if it has been given annotated training data. For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #AUTHOR_TAG )  .",0
"-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications. Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #AUTHOR_TAG )  , part-of-speech tagging ( Engelson D",0
"icalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #AUTHOR_TAG )  , and Collins 's Model 2 parser ( ",5
". The work of Sarkar (2001)  and  suggests that co-training can be helpful for statistical parsing. #AUTHOR_TAG  have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training . Similar approaches are being explored for parsing Hwa et al. 2003).</s>",0
"). Current state-of-the-art statistical parsers ( #AUTHOR_TAG  ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus ,",0
"structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models ( #AUTHOR_TAG )  , backed-off models ( Collins and Brooks 1995 ) , and a",0
"In the first experiment , we use an induction algorithm ( #AUTHOR_TAG a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs . The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data. In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to",5
"Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly. That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #AUTHOR_TAG )  . The underlying assumption is that an uncertain output is likely to be wrong.</s>",0
"that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing ( Steedman , #AUTHOR_TAG  ; Hwa et al. 2003 ) .</s>",0
"). The head words can be automatically extracted using a heuristic table lookup in the manner described by #AUTHOR_TAG  . For this learning problem,",5
"noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #AUTHOR_TAG )  . Following the",0
entropy of the entire sentence from the tree entropy of the subtrees. The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities). We follow the notation convention of #AUTHOR_TAG  .</s>,5
"algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG )  . For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.</s>",0
"She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG )  . We will refer to this work and the methods and results presented by Schulte",0
"low recall. #AUTHOR_TAG  report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a",1
"extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG  and Collins ( 1997 )  . Then",0
"001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #AUTHOR_TAG  ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman",0
"While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #AUTHOR_TAG  questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level . LFG argues that subcategorization",0
"to induce relative frequencies for the extracted frames. #AUTHOR_TAG  attempts to improve on the approach of Brent ( 1993 )  by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur . He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993) . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.",0
"Applying an absolute threshold of five occurrences, we still generate 162 frame types  from Penn-II and 221 from Penn-III. #AUTHOR_TAG  , by comparison , employ 163 distinct predefined frames .</s>",0
"Unlike our approach , those of #AUTHOR_TAG  and Hockenmaier , Bierner ,</s>",1
"Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG  ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii ",0
"6 frames per verb. #AUTHOR_TAG  predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding</s>",0
"We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #AUTHOR_TAG )  and Penn Chinese Treebank (Xue, Chiou, and Palmer ",5
<s> The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #AUTHOR_TAG )  : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .</s>,0
Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG )  is a member of the family of constraint-based grammars . It posits minimally,0
Lexical functional grammar ( Kaplan and Bresnan 1982 ; #AUTHOR_TAG  ; Dalrymple 2001 ) is a member of the family of constraint-based grammars . It posits minimally,0
"judge in Figure 1. According to #AUTHOR_TAG  , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ . OBJ ",0
"Section 3. #AUTHOR_TAG  evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.",1
"rays anything about the syntactic nature of the constructs in question. Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #AUTHOR_TAG )  . With only a slight modification,",0
. The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG  in combination with a variation of,0
"The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #AUTHOR_TAG  . Applying his technique to approximately four million words of New York Times",0
"We applied lexical-redundancy rules ( #AUTHOR_TAG )  to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects. The resulting precision was very high (from 72.3",5
Both use the evaluation software and triple encoding presented in #AUTHOR_TAG  . The,5
"available PARC 700 Dependency Bank ( #AUTHOR_TAG )  , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators . They report precision of over 88.5% and recall of over 86% (Table 2). The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry. Some, but not all, of these differences are captured by automatic conversion software. A",0
notate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG  and Collins ( 1997 )  ; the whole tree is then converted to a binary tree; heuristics are,5
"ag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ #AUTHOR_TAG  ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .",0
"of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #AUTHOR_TAG  ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .",0
"graphically. In Charniak ( 1996 )  and #AUTHOR_TAG  , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank . We were interested in discovering whether",0
notate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 )  and #AUTHOR_TAG  ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena,5
not predefine the frames to be extracted but rather learn them from the data. #AUTHOR_TAG  describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank . This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank. Each of these sequences was categorized as a modifier or argument. Arguments were then mapped to traditional syntactic functions. For,0
"). #AUTHOR_TAG  present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000) , Marinov and Hemming's system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees. The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 58</s>",0
". #AUTHOR_TAG  also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994)  and Collins (1997) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999)  ranged from 3,014 to 6,099.",0
"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #AUTHOR_TAG  ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [",0
It has been shown ( #AUTHOR_TAG )  that the subcategorization tendencies of verbs vary across linguistic domains .,4
"ization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #AUTHOR_TAG  ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .",0
"by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #AUTHOR_TAG )  dictionaries and adding around 30 frames found by manual inspection .",0
"information. The annotation procedure is dependent on locating the head daughter , for which an amended version of #AUTHOR_TAG  is used . The head is annotated with the LFG equation ↑=↓.",5
"lan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG  ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ]",0
"more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #AUTHOR_TAG )  , containing more than 1,000,000 words and 49,000 sentences .</s>",0
"001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG  ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory",0
". Following Hockenmaier , Bierner , and Baldridge ( 2002 )  , #AUTHOR_TAG  , and Miyao , Ninomiya , and Tsujii ( 2004 )  , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .",5
icon. #AUTHOR_TAG  relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions. B,0
Lexical functional grammar ( #AUTHOR_TAG  ; Bresnan ,0
"is impossible to say how effective their technique is. #AUTHOR_TAG  present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those",0
"extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 )  and #AUTHOR_TAG  . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.",0
"details. #AUTHOR_TAG  argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization . In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.</s>",4
"X. In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #AUTHOR_TAG )  .</s>",3
"X to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above , it is well documented ( #AUTHOR_TAG )  that subcategorization frames ( and their frequencies ) vary across domains . We have extracted frames from two sources",4
mar. #AUTHOR_TAG  explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing . The extraction procedure utilizes a head percolation table as introduced by ,0
"of accession may also be represented graphically. In #AUTHOR_TAG  and Krotov et al. ( 1998 )  , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank . We were",0
"In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998). #AUTHOR_TAG  argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature . Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.",4
"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG  ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ T",0
the induced frames. #AUTHOR_TAG  run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes . The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. ,0
"examples. As a generalization , #AUTHOR_TAG  notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997)  report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In",0
"that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is #AUTHOR_TAG  , which is based on weighted finite-state transducers ( FSTs ) . Our approach is similarly motivated but is based on a different mechanism",1
". We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. #AUTHOR_TAG  , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) . They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995).",0
"<s> Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" ( #AUTHOR_TAG )  : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .</s>",1
"Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #AUTHOR_TAG )  . The numeral (whether it is implicit, as in (",0
"<s> Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #AUTHOR_TAG )  was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that</s>",0
"One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #AUTHOR_TAG  ; Malouf 2000 ) . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Pre",0
"3 The degree of precision of the measurement ( #AUTHOR_TAG  , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .</s>",0
"Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #AUTHOR_TAG )  . We have separated the two phases because, in",1
"of which falls just below the divide while the other falls just above it. This is the strongest version of the sorites paradox ( e.g. , #AUTHOR_TAG )  .",0
"as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #AUTHOR_TAG )  .",0
"This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #AUTHOR_TAG )  , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) . We shall",0
"< x. Which of these should come first? #AUTHOR_TAG  ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say 'the tall candle' when the tallest candle is much t",0
"While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #AUTHOR_TAG )  , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (W",0
"that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #AUTHOR_TAG )  . IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}.</s>",0
"As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous. 9.4.1 A New Perspective on Salience. #AUTHOR_TAG  have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we",0
", high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #AUTHOR_TAG )  . These intricacies include what E",0
"). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG  ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large . Our own proposal",0
"We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #AUTHOR_TAG  , Krahmer and Theune 2002 ) . Before we do this,",0
"should come first? Hermann and Deutsch ( 1976  ; also reported in #AUTHOR_TAG )  show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say 'the tall candle' when the tallest candle is much t",0
"The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #AUTHOR_TAG )  . But IA may be replaced by any other reasonable GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always ""greedily",0
"we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #AUTHOR_TAG  , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .",5
"A similar problem is discussed in the psycholinguistics of interpretation ( #AUTHOR_TAG )  : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.",1
"the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG  ] Chapter 8 ) . Once",0
"Common sense ( as well as the Gricean maxims ; #AUTHOR_TAG )  suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication . We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.",0
"it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation. Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG )  .</s>",0
"What we said above has also disregarded elements of the ""global"" (i.e., not immediately available) context. For some adjectives , including the ones that #AUTHOR_TAG  called evaluative ( as opposed to dimensional ) , this is clearly inadequate . He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms. For example (after Bierwisch 1989),",0
"Minimality. Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #AUTHOR_TAG )  .</s>",0
"-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #AUTHOR_TAG )  . The only practical alternative is to provide the generator with ""crisp"" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database.",4
"Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #AUTHOR_TAG  , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .",0
"While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #AUTHOR_TAG  ; Sonnenschein 1982 ) , the hypothesis that",0
"of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #AUTHOR_TAG )  .",0
"While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #AUTHOR_TAG  ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that",0
"short man, even if Hans is the only other man in the local context (but see #AUTHOR_TAG  1999, discussed in Section 7.2).",0
"NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #AUTHOR_TAG )  : The selected expression should also be felicitous . Consider the question,",0
"One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #AUTHOR_TAG )  . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Pre",0
"4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #AUTHOR_TAG )  . VAGUE uses both of these devices.</s>",0
"ation plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #AUTHOR_TAG  ; Thorisson 1994 , for other plans ) .</s>",0
"explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #AUTHOR_TAG )  : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large . Our own proposal will abstract away from the effects of linguistic context. We",0
"10 cm, and > 12 cm. Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #AUTHOR_TAG )  . If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we",0
"Some generalizations of our method are fairly straightforward. For example , consider a relational description ( cfXXX , #AUTHOR_TAG )  involving a gradable adjective , as in",0
". Many alternative strategies are possible. The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #AUTHOR_TAG  ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .</s>",0
"The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #AUTHOR_TAG )  . An",0
". A more flexible approach is used by #AUTHOR_TAG  , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in",0
"Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG  ; Krahmer and Theune 2002 ) . We have separated the two phases because, in the case of",1
"While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG  ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .",0
".) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG  , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red",0
"be psychologically more plausible, since they are essentially no more than comparisons between objects. #AUTHOR_TAG  asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less",0
"It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG )  . We have",1
"2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG  ; also Section 8.1 of the present article ) .</s>",0
"atians) useless. In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG  ; see our Section 2 ) .</s>",0
The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question. The following four components have been identified as the key elements of a question related to patient care ( #AUTHOR_TAG )  :</s>,0
"Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ) . It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.</s>",5
"osis. Although originally developed as a tool to assist in query formulation , #AUTHOR_TAG  pointed out that PICO frames can be employed to structure IR results for improving precision . PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Mead",0
"We first identified the most informative unigrams and bigrams using the information gain measure ( #AUTHOR_TAG )  , and then selected only the positive outcome predictors using odds ratio ( M",5
"The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #AUTHOR_TAG )  . Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project",1
"LINE abstracts for non-clinical purposes. For example , McKnight and Srinivasan ( 2003 )  describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #AUTHOR_TAG )  . Tbahriti et al. (2006)",0
"if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAG b ) , but these features are also beyond the capabilities of current summarization systems .</s>",1
"et al. 2006). #AUTHOR_TAG  have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance . Note, however, that such labels are",0
"Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG )  . Furthermore",0
"Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #AUTHOR_TAG )  . Nevertheless, the indexing process remains firmly human-centered.</s>",0
"of which concerns our assumptions about the query interface. Previously , a user study ( #AUTHOR_TAG )  has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance . We have argued",1
"The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #AUTHOR_TAG  , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at A",0
"001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG )  extracts relations between the concepts . Both",0
"ied on ad hoc weight selection based on intuition. The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #AUTHOR_TAG )  , which can be described by the following equation:</s>",5
"The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #AUTHOR_TAG  . Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations",1
"Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG )  identifies concepts in free text , and SemRep ( Rindflesch and Fiszman ",0
The work of #AUTHOR_TAG  demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .</s>,0
"notated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG )  . As",5
"ell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG  , 2005 ) . M",0
"empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #AUTHOR_TAG  and Lin (2006) .",0
"<s> After much exploration , #AUTHOR_TAG  discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues . Consider the following segment:</s>",0
") , and then selected only the positive outcome predictors using odds ratio ( #AUTHOR_TAG )  .",5
"lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #AUTHOR_TAG  .</s>",3
"there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #AUTHOR_TAG )  . A number of studies (",0
"In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #AUTHOR_TAG a ) for a brief overview .</s>",0
"A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #AUTHOR_TAG a , 2006b ) . However,",0
5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #AUTHOR_TAG )  .</s>,5
"MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG  ; De",0
"domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #AUTHOR_TAG )  . In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006 a) for a brief overview.",0
"athesaurus. Third , the paradigm of evidence-based medicine ( #AUTHOR_TAG )  provides a task-based model of the clinical information-seeking process . The PICO framework",0
"As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #AUTHOR_TAG )  .</s>",1
"As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #AUTHOR_TAG  ; Hirschman and Gaizauskas 2001 ) .</s>",1
"). For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #AUTHOR_TAG  .</s>",5
"The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG )  , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI ",0
"Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG )  , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .",5
"The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section , which elaborates on preliminary results reported in #AUTHOR_TAG  , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence . For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements",2
"term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task. Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG )  .</s>",3
PICO framework ( #AUTHOR_TAG )  for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system . The confluence of these many factors makes clinical question answering a very exciting area of research.,0
003 ; #AUTHOR_TAG  ; Miyao and Tsujii ,4
"e , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #AUTHOR_TAG  ; Preiss 2003 ; Kaplan et al.",4
"e , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #AUTHOR_TAG  ; Kaplan et al. ",4
". Our re-ranking approach , like the approach to parse re-ranking of #AUTHOR_TAG  , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes . The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings.",1
"in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #AUTHOR_TAG )  , which selects from likely assignments generated by a model which makes stronger independence assumptions . We utilize the top n assignments of our local semantic role labeling model P",5
"Following our previous work ( #AUTHOR_TAG  ; Althaus , Karamanis , and Koller 2004 ) ,",2
"Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG  . We use the automatically built thesaurus of Lin (1998)  to find words similar to each constituent, in order to automatically",4
"In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases. We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #AUTHOR_TAG )  .</s>",0
"systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input , and are difficult to create and maintain ( #AUTHOR_TAG )  . In contrast, the",0
"<s> â¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG )  .</s>",1
"As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction. (Doc-Ret). This method follows a traditional Information Retrieval paradigm ( #AUTHOR_TAG )  , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query . In our case, the query",5
"For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG )  to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear",5
"where specific words in the request (docking station and install) are also mentioned in the response. This situation suggests a response-automation approach that follows the document retrieval paradigm ( #AUTHOR_TAG )  , where a new request is matched with existing response documents ( e-mails ) . However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively.",0
"However, even the automation of responses to the ""easy"" problems is a difficult task. Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven ( #AUTHOR_TAG  ; Watson 1997 ; Delic and Lahaix 1998 ) . These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).</s>",0
). We then use the program Snob ( Wallace and Boulton 1968 ; #AUTHOR_TAG )  to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.,5
"The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG )  and case-based reasoning ( Watson 1997 ) . Such technologies require significant",1
We employed the LIBSVM package ( #AUTHOR_TAG )  .,5
"ob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #AUTHOR_TAG )  . We chose this program because the number of clusters does not have to be",5
We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG )  . Precision measures how much of the information in an automatically generated response is correct (i,5
"The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #AUTHOR_TAG )  . Such technologies require significant human input, and are difficult to create and maintain (Delic and L",1
"including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #AUTHOR_TAG  investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used</s>",1
"<s> 5 Significant bigrams are obtained using the n-gram statistics package NSP ( #AUTHOR_TAG )  , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .</s>",5
"to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG  ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the de",0
"004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #AUTHOR_TAG )  . An important difference between these applications and help-",1
<s> â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #AUTHOR_TAG  ; Roy and Subramaniam 2006 ) .</s>,1
"we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable. Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG )  . However",5
"herence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #AUTHOR_TAG  ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and Mc",0
"A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #AUTHOR_TAG )  . However, in",1
"Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) ( #AUTHOR_TAG ; Soricut and Brill 2006). An important difference between these applications and",1
"Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #AUTHOR_TAG )  . The representativeness of the sample size was not discussed in any of these studies.",1
"In Marom and Zukerman (2007 a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows. â¢ Only an automatic evaluation was performed , which relied on having model responses ( #AUTHOR_TAG  ; Berger et al. 2000 ) .</s>",1
"In #AUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries . These systems addressed the evaluation issue as follows. r Only an automatic evaluation was performed, which relied on having model responses .</s>",0
"In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #AUTHOR_TAG  ; Wallace 2005 ) . We chose this program because the number of clusters does not have to be specified in advance, and it returns a",5
"In FAQs , #AUTHOR_TAG  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document . They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).",0
"â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( #AUTHOR_TAG  ; Leuski et al. 2006 ) . The representativeness of the sample size was not discussed in any of these studies.</s>",1
"on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG  ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails",0
"Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( #AUTHOR_TAG )  for Sent-Pred .",5
"Removing redundant sentences. After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #AUTHOR_TAG  to penalize redundant sentences in cohesive clusters . This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.",5
"The predictive model is a Decision Graph ( #AUTHOR_TAG )  , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is",5
"circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #AUTHOR_TAG )  . These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).</s>",0
"attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG  ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik ",1
"-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG  ; Malik , Subramaniam , and Kaushik ",1
"oun and de Rijke ( 2005)  compared different variants of retrieval techniques. #AUTHOR_TAG  compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval . Two significant differences between help-desk and FAQs are the following.</s>",1
"we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #AUTHOR_TAG )  , but the simple binary bag-of-lemmas representation yielded similar results",5
"also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #AUTHOR_TAG  as follows . The",0
and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #AUTHOR_TAG b ) .,5
", and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #AUTHOR_TAG  ; Delic and Lahaix 1998 ) . These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).</s>",0
They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. #AUTHOR_TAG  compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) . J,0
"Following #AUTHOR_TAG  , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases . However, in our situation, there is not always one single",1
<s> â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG )  .</s>,1
"but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG )  .</s>",0
"Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG )  , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006). An important difference between these applications and help-",1
". Specifically , we used Decision Graphs ( #AUTHOR_TAG )  for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .",5
"13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG )  , with similar results to those obtained with the word-by-word measures .</s>",5
). We then use the program Snob ( #AUTHOR_TAG  ; Wallace 2005 ) to cluster these experiences . Figure 8(a) is a projection of the,5
"In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAG a ) . Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator",5
"Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #AUTHOR_TAG  belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the ""cascade",1
"transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #AUTHOR_TAG )  ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .</s>",4
"V ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG )  ; for",5
"Many researchers use the GIZA + + software package ( #AUTHOR_TAG )  as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .",0
"We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG )  .",5
"add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #AUTHOR_TAG )  . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.",1
<s> 5 The open source Moses ( #AUTHOR_TAG )  toolkit from www.statmt.org/moses/ .</s>,5
"results are based on a corpus of movie subtitles ( #AUTHOR_TAG )  ,",5
cu 2003 ] and rules [ Galley et al. 2004 ; #AUTHOR_TAG  ] ) as well as for MT system combination (,0
"Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #AUTHOR_TAG  ; Liang , Taskar , and Klein ",5
"yielding similar improvements. We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG )  and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves . We trained IBM Model 4 using the default configuration of the</s>",5
"et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #AUTHOR_TAG )  .</s>",4
"In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #AUTHOR_TAG )  . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately",1
"on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #AUTHOR_TAG )  :</s>",5
"solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #AUTHOR_TAG )  , and more recently by the LEAF model ( Fraser and Mar",1
"learning process has appeared before. In the context of word alignment , #AUTHOR_TAG  use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1. This encourages more transitions and hence shorter phrases. For the",0
the En â Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) ,5
this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #AUTHOR_TAG )  . Figure 8 shows the Precision/Recall curves after,5
"importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #AUTHOR_TAG  ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Sny",4
"and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG )  . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following",0
. Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #AUTHOR_TAG )  . This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.,1
"<s> PR is closely related to the work of #AUTHOR_TAG  , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning . They call their method generalized expectation (GE) constraints or alternatively expectation regularization. In the original GE framework, the posteriors of the model on unlabeled data are regularized directly. They train a discriminative model, using conditional likelihood on labeled data and an ""expectation regularization"" penalty term on the unlabeled data:</s>",0
"006). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG  ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of",4
"Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG  ; Chiang et al. 2005 ] ) as well as for MT system combination",0
"For the task of unsupervised dependency parsing , #AUTHOR_TAG  add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing . They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e. The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at",0
We use the agreement checker code developed by #AUTHOR_TAG  and evaluate our baseline ( M,5
"ammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #AUTHOR_TAG )  used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8 Cross-linguistically, a core set containing around 12 tags is often assumed as a �",1
"For better comparison with work of others , we adopt the suggestion made by #AUTHOR_TAG  to evaluate the parsing quality on sentences up to 70 tokens long . We",5
"the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #AUTHOR_TAG )  and the CATiB ( Habash and Roth 2009 ) . Recently, Green and Manning (2010)",0
"We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG )  . Specifically, we use the portion converted from Part 3 of the PATB to",5
"shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG )  : CASE is relevant , not redundant , and can be predicted with sufficient accuracy . It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflaz",4
"For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #AUTHOR_TAG )  , converted to the CATiB Treebank format , as mentioned",5
"18 In this article , we use a newer version of the corpus by #AUTHOR_TAG  than the one we used in",5
"008 ) and the CATiB ( #AUTHOR_TAG )  . Recently, Green and Manning (2010)",0
", we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #AUTHOR_TAG )  ( Section 6 ) . Hohensee and Bender (2012)  have conducted a",5
"To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #AUTHOR_TAG )  .",5
"in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG  ; Habash, Rambow, and Roth 2012)15 (see Table",5
"For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG  . We denote p < 0.05 and p < 0.01 with + and ++ , respectively.",5
"3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #AUTHOR_TAG )  , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the",5
"We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #AUTHOR_TAG )  .",5
"not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #AUTHOR_TAG )  , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit",1
"the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #AUTHOR_TAG  ;  Hab",1
"the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #AUTHOR_TAG )  and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag",5
"Maamouri et al. 2004), the Buckwalter morphological analyzer ( #AUTHOR_TAG )  , and tools using them such as the Morphological Analysis and Disambiguation for Arabic",1
"Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #AUTHOR_TAG  ;",0
"sing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG )  .</s>",4
relations. The result holds for both the MaltParser ( #AUTHOR_TAG )  and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .</s>,5
Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #AUTHOR_TAG )  ;,5
"Much work has been done on the use of morphological features for parsing of morphologically rich languages. #AUTHOR_TAG  report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) . This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+). They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are",0
"<s> 9 We do not relate to specific results in their study because it has been brought to our attention that #AUTHOR_TAG  are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .</s>",1
relations. The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG )  .</s>,5
instead they selectively enrich the core POS tag set with only certain morphological features. A more detailed discussion of the various available Arabic tag sets can be found in #AUTHOR_TAG  .</s>,0
"For the corpus statistics, see Table 1. For more information on CATiB , see #AUTHOR_TAG  and Habash , Faraj , and Roth ( 2009 )  .</s>",0
"So far, we have only evaluated models trained on gold POS tag set and morphological feature values. Some researchers , however , including #AUTHOR_TAG  , train on predicted feature values instead . It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test. But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature",1
"<s> 19 The paper by #AUTHOR_TAG  presents additional , more sophisticated models that we do not use in this article .</s>",1
"As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #AUTHOR_TAG  ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) (",0
"11 #AUTHOR_TAG  reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies . The Nivre �standard� algorithm is also reported there to do better on Arabic,",1
". K�bler, McDonald, and #AUTHOR_TAG  describe a �typical� MaltParser model configuration of attributes and features.",5
"As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG )  , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre ",0
"For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG  , 2008 ; KÃ¼bler , McDonald , and",5
"reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG )  , trained on the PADT . His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.",0
"3). Similarly , #AUTHOR_TAG  report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations . Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the",0
"<s> 7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG  , p. 102 ) .</s>",0
"In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG )  .",5
"uslavsky , and Iomdin 2008 ; #AUTHOR_TAG )  .",1
"#AUTHOR_TAG  have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor . These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs. ",0
"This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG )  . In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.",2
"man et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #AUTHOR_TAG  , Medress 1980 , Reddy 1976 ,",1
"expectation system in the form of examples. Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #AUTHOR_TAG  .</s>",0
There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 )  and the PROLOG synthesis method of #AUTHOR_TAG  .</s>,1
"phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 )  , â¢ a `` conditioning '' facility as described by #AUTHOR_TAG  , â¢ implementation of",3
"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 )  , #AUTHOR_TAG  , Jensen et al. ( 1983 )  , Kwasny and Sondheimer ( 1981 )  , Riesbeck and Schank ( 1976 )  , Thompson ( 1980 )  , Weischedel and Black ( 1980 )  , and Weischedel and Sondheimer ( 1983 )  . A wide variety of techniques have been developed for addressing problems at",1
"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 )  , Granger ( 1983 )  , Jensen et al. ( 1983 )  , Kwasny and Sondheimer ( 1981 )  , Riesbeck and Schank ( 1976 )  , #AUTHOR_TAG  , Weischedel and Black ( 1980 )  , and Weischedel and Sondheimer ( 1983 )  . A wide variety of techniques have been developed for addressing problems at",1
"device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG )  . The expectation system provided the intermediate processing between the errorful output of the speech",0
"in Michalski et al. (1984) . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 )  , assertional statements as in Michalski ( 1980 )  , or semantic nets as in #AUTHOR_TAG  .",1
"existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #AUTHOR_TAG  . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.</s>",1
"Medress 1980 , Reddy 1976 , Walker 1978 , and #AUTHOR_TAG )  . Most of these efforts",1
"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 )  , Granger ( 1983 )  , Jensen et al. ( 1983 )  , #AUTHOR_TAG  , Riesbeck and Schank ( 1976 )  , Thompson ( 1980 )  , Weischedel and Black ( 1980 )  , and Weischedel and Sondheimer ( 1983 )  . A wide variety of techniques have been developed for addressing problems at",1
"described, for example, in Michalski et al. (1984) . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 )  , assertional statements as in #AUTHOR_TAG  , or semantic nets as in Winston ( 1975 )  .",1
". The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #AUTHOR_TAG  where program flowcharts were constructed from traces of their behaviors . However, the ""flowcharts"" in the",1
"We denote the meaning of each sentence Si with the notation M(Si). The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #AUTHOR_TAG )  , a deep parse of Si , or some other representation . A user behavior is represented by a network, or directed graph, of such meanings. At the beginning of a task, the state of the interaction is represented by",0
"Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #AUTHOR_TAG  , Reddy 1976 ,",1
"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #AUTHOR_TAG  , Haton and Pierrel 1976 , Lea 1980 ,",1
"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 )  , Granger ( 1983 )  , #AUTHOR_TAG  , Kwasny and Sondheimer ( 1981 )  , Riesbeck and Schank ( 1976 )  , Thompson ( 1980 )  , Weischedel and Black ( 1980 )  , and Weischedel and Sondheimer ( 1983 )  . A wide variety of techniques have been developed for addressing problems at",1
"[ The current system should be distinguished from an earlier voice system ( VNLC , #AUTHOR_TAG )  , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]</s>",1
"The expectation parser uses an ATN-like representation for its grammar ( #AUTHOR_TAG )  . Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions",5
"semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #AUTHOR_TAG )  .",0
"A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG  , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf",1
"level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #AUTHOR_TAG  .</s>",0
"â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG  , â¢ a `` conditioning '' facility as described by Fink et al. ( 1985 )  , â¢",3
"The problem of handling ill-formed input has been studied by #AUTHOR_TAG  , Granger ( 1983 )  , Jensen et al. ( 1983 )  , Kwasny and Sondheimer ( 1981 )  , Riesbeck and Schank ( 1976 )  , Thompson ( 1980 )  , Weischedel and Black ( 1980 )  , and Weischedel and Sondheimer ( 1983 )  . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue",1
"The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984) . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG  , assertional statements as in Michalski ( 1980 )  , or semantic nets as in Winston ( 1975 )  .",1
"An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #AUTHOR_TAG  , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech",0
"1980 , Medress 1980 , #AUTHOR_TAG  ,",1
"The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #AUTHOR_TAG )  . An",0
determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second. How it is done is beyond the scope of this paper but is explained in detail in #AUTHOR_TAG  .</s>,0
. There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG  and the PROLOG synthesis method of Shapiro ( 1982 )  .</s>,1
"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG  , Lea 1980 , Lowerre and Red",1
further description and discussion of LDOCE.) In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 )  and #AUTHOR_TAG  describe further research in Cambridge utilising different types of information available in LDOCE .</s>,0
"means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. #AUTHOR_TAG  and Akkerman et al. ( 1985 )  provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description . Ingria (1984)",0
". The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG )  to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable d",0
", definitions, examples, and so forth. However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG  for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to",0
of linguistic description. #AUTHOR_TAG  comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .</s>,1
"Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #AUTHOR_TAG :472 ), but these are the only ones which are explicit in the LDOCE coding system.",0
"isation are the Linguistic String Project ( Sager , 1981 )  and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #AUTHOR_TAG )  ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,0",1
0 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #AUTHOR_TAG )  .</s>,0
core' vocabulary in defining the words throughout the dictionary. (Michiels (1982)  contains further description and discussion of LDOCE.) In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG  and Alshawi ( 1987 )  describe further research in Cambridge utilising different types of information available in LDOCE .</s>,0
"This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982,  for further comment). One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #AUTHOR_TAG )  . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis. In the",0
"The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms. To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #AUTHOR_TAG  and references therein ) . PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we",5
"This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982,  for further comment). One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 )  . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis. In the",0
"Two exceptions to this generalisation are the Linguistic String Project ( #AUTHOR_TAG )  and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 )  ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,0",1
"isp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #AUTHOR_TAG )  ; on the other hand a method of access was clearly required , which was flexible enough to support a range of",0
"s and therefore they appear to be counterexamples to our Object Raising rule. In addition , #AUTHOR_TAG  note that our Object Raising rule would assign mean to this category incorrectly .",1
there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG  for details ) . However,0
"Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 )  , Lexical Functional Grammar ( LFG ) ( #AUTHOR_TAG )  -- and on natural language parsing frameworks -- for example",0
""". This deficiency is rectified in the verb classification system employed by #AUTHOR_TAG  in the Brandeis verb catalogue .",1
". The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 )  to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable d",0
"<s> There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #AUTHOR_TAG  , for further details ) . However,",0
"analysis. In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG )  . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.",3
"established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #AUTHOR_TAG ; Bobrow , 1978 )  consult relatively small lexicons , typically generated by hand .",1
"codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #AUTHOR_TAG  , for further discussion ) .</s>",0
for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary. ( #AUTHOR_TAG  contains further description and discussion of LDOCE . ) In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al. (1985)  and Alshawi (1987)  describe further research in,0
"relationship. Expanding on a suggestion of #AUTHOR_TAG  , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we",2
"via different routes. In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG  has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 )  . In addition",5
"to be shown that the information they encode is going to be of some utility for natural language processing. The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #AUTHOR_TAG  , 1985 ) . The codes are doubly articulated;",2
"per entry. Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #AUTHOR_TAG )  .</s>",0
"take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. Michiels ( 1982 )  and #AUTHOR_TAG  provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description . Ingria (1984) ",0
"and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984 a ) , PATR-II ( #AUTHOR_TAG )  -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language . These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then",0
"unciation field is available ; Carter ( 1987 )  has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #AUTHOR_TAG )  . In addition",0
"xicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG )  . A",0
"aining the limits of what can be reliably extracted from the LDOCE coding system. Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #AUTHOR_TAG :460 ff . ) However, only two of these criteria are explicit in the coding system.</s>",3
"In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG )  . Independently",0
We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG  and Stockwell et al. ( 1973 )  . Figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system.,5
". As #AUTHOR_TAG  points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system. To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as L",0
". The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 )  to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable d",0
"to and understand. Many investigators (e.g. Many investigators ( e.g. #AUTHOR_TAG  ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech",4
"In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG )  . The system generated phrase boundaries using information derived from the syntactic structure of a sentence. While we",0
"; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG )  have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech",4
We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG )  . Two concerns motivated our implementation,5
". #AUTHOR_TAG  and Litman and Hirschberg ( 1990 )  also examine the relation between discourse and prosodic phrasing . Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.</s>",0
"dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12 , from #AUTHOR_TAG  , are frequently cited . (Square brackets mark off the NP constituents that contain embed- ded sentences.)</s>",0
"In previous work ( #AUTHOR_TAG )  , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence. While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system. Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing",2
"by special rules outside the grammar proper. #AUTHOR_TAG  proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model . He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.</s>",0
"The psycholinguistic studies of Martin ( 1970 )  , Allen ( 1975 )  , Hillinger et al. ( 1976 )  , Grosjean et al. ( 1979 )  , Dommergues and Grosjean ( 1983 )  , and #AUTHOR_TAG  , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For",0
". 3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG  , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) . This approach to prosodic phrase boundary determination brings us closer to a framework in which",1
"The result is a flattened structure that more accurately reflects the prosodic phrasing. In #AUTHOR_TAG  , this flattening process is not part of the grammar . Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p. 372). Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.",0
"we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on #AUTHOR_TAG  is presented in Selkirk ( 1984 )  , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree . Although a grid may be more descriptively suitable for some aspects of prosody",1
"ations. Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #AUTHOR_TAG  claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984)",0
"The psycholinguistic studies of Martin ( 1970 )  , #AUTHOR_TAG  , Hillinger et al. ( 1976 )  , Grosjean et al. ( 1979 )  , Dommergues and Grosjean ( 1983 )  , and Gee and Grosjean ( 1983 )  , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For",0
"to and understand. Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG  ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And",4
". Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG  use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .</s>",1
"Previous versions of our work , as described in #AUTHOR_TAG  also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In",2
"closely than a sequence of words dominated by two different nodes. This observation has led some researchers , e.g. , #AUTHOR_TAG  , to claim a direct mapping between the syntactic phrase and the prosodic phrase . However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12,",0
"case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG  ( henceforth G&G ) .</s>",1
"The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG  , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing",0
"Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG  , and the account of monos",5
"a construction in real time. Secondly , the cooperative principle of #AUTHOR_TAG  , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader . Finally, it has been shown by Groesser (1981)  that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by",4
"At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others. We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG  , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 )  , who are more interested in addressing psychological and cognitive aspects of discourse coherence . The quoted works seem to be good representatives for each of the directions; they also point to related literature.",1
"least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG) , and their kin. Similarly, the notion",1
<s> W. #AUTHOR_TAG  discussed sentences of the form * This is a chair but you can sit on it .</s>,0
"seems to have three subfunctions: . . Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG  , p. 672 ) .</s>",0
"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG  , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a comput",1
"sable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG  , p. 546 ) gives 10 definitions of a sentence .</s>",0
"The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #AUTHOR_TAG  , Jackendoff ( 1983 )  , Kamp ( 1981 )  , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate",1
"there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG  , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The",1
"it). Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG )  must play a role , too . Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.",0
"According to #AUTHOR_TAG  , p. 67 ) , these two sentences are incoherent . However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.) suddenly (for",1
"most plausible interpretations of predicates. For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG  p. 195 ; Zad",0
"; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 )  and #AUTHOR_TAG  .</s>",0
"; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG  strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph . Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.",4
"The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG )  . Br",1
"and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG  , the `` diagnosis from first principles '' of Reiter ( 1987 )  , `` explainability '' of Poole ( 1988 )  , and the subset principle of Berwick ( 1986 )  . But,",1
The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG )  to see what a formal interpretation of events in time might look like . Since sentences can,0
"chunks are sometimes called ""episodes,"" and sometimes ""paragraphs."" According to #AUTHOR_TAG  , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hier",0
"the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 )  , the `` diagnosis from first principles '' of #AUTHOR_TAG  , `` explainability '' of Poole ( 1988 )  , and the subset principle of Berwick ( 1986 )  .",1
"paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG  ; Patel-Schneider 1985 ) .",1
"as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG  . Hintikka (1985) .</s>",1
"Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAG b ). We will have, however, no need for ""strong"" notions in this paper.",1
"Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG )  does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation . Later, Hobbs (1979",0
The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG  ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since,0
"of the words that appear in the sentence. Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAG a , 1987b ) . Z",0
"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG  , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.",1
"As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG )  . My",0
We have shown elsewhere ( #AUTHOR_TAG  ; Zad,2
"<s> The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG  call it abductive unification/matching , Hobbs ( 1978  , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .</s>",0
"to analyze a fragment of English, and to deal with anaphora. The logical notation of #AUTHOR_TAG  is more sophisticated , and may be considered another possibility . ",1
"to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird ( 1983 )  , #AUTHOR_TAG  , Kamp ( 1981 )  , and implicitly or explicitly by almost all researchers in computational linguistics . As a",1
"presupposition  #AUTHOR_TAG :112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs."" According to Hinds (1979) , paragraphs are made up of segments,",0
""" fails poetically while ""Death is the mother of beauty"" succeeds .... It is precisely this ""grounding"" of logical predicates in other conceptual structures that we would like to capture. We investigate here only the ""grounding"" in logical theories. However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG )  . Woods 1987).</s>",0
"an English grammar. #AUTHOR_TAG  , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . '' Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified",1
"An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG  . These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun",0
"Other reference works could be treated as additional sources of world knowledge.) This type of consultation uses existing natural language texts as a referential level for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #AUTHOR_TAG  , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' . With respect to that",0
"6.1.1 Was the Use of a Gricean Maxim Necessary? Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG )  , or the metarules of Section 5.2? It seems to us that the answer is no.</s>",0
"ics as a formal tool for the analysis of paragraphs. This semantics was constructed ( #AUTHOR_TAG a , 1987b ) as a formal framework for default and commonsense reasoning . It",5
"and their kin. Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985) , the ""diagnosis from first principles"" of Reiter (1987) , ""explainability"" of Poole (1988) , and the subset principle of #AUTHOR_TAG  . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of",1
"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG )  , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.",1
"it). Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG )  or quantifier scoping ( Webber 1983 ) must play a role , too . Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.",0
"The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG  , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds . We have",0
"The idea of using preferences among theories is new, hence it was described in more detail. `` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG )  , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .",1
"Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG  ; Frisch 1987 ; Patel-Schneider 1985 )",1
"set of entities that appear in it). Other factors , such as the role of focus ( #AUTHOR_TAG  , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too . Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.",0
"This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG  , p. 329 ) .</s>",4
"We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples. This approach is taken in computational syntactic grammars (e.g.  #AUTHOR_TAG )  ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .</s>",0
"The idea of using preferences among theories is new, hence it was described in more detail. `` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG  ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .",1
"much."" Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG  .</s>",2
"seems to have three subfunctions: . . Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG  ; cfXXX also Quirk et al. 1972 , p. 672 ) .</s>",0
"Although there are other discussions of the paragraph as a central element of discourse (e.g. #AUTHOR_TAG  , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.",1
"this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird ( 1983 )  , Jackendoff ( 1983 )  , #AUTHOR_TAG  , and implicitly or explicitly by almost all researchers in computational linguistics . As a",1
"; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG  and Haberlandt et al. ( 1980 )  .</s>",0
"We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAG a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment . This",2
about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAG b ) . Zadrozny 1987b).</s>,1
"bs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation. Later , #AUTHOR_TAG  , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .</s>",0
"<s> The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 )  call it abductive unification/matching , #AUTHOR_TAG  , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .</s>",0
". Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG  describes how first order logic can be augmented with such an operator . Ext",0
"ical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance , #AUTHOR_TAG  , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? '' The pairing of these two sentences may be said to create a small",4
"as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. ( 1989 )  , De Mattia and Giachin ( 1989 )  , Niedermair ( 1989 )  , #AUTHOR_TAG  , and Young ( 1989 )  .</s>",0
<s> A formula for the test set perplexity ( #AUTHOR_TAG )  is :13</s>,0
"task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG )  . The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.",5
Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG )  .</s>,5
<s>( #AUTHOR_TAG )  .</s>,0
"Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by Grishman et al. ( 1986 )  and #AUTHOR_TAG  on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area.",1
"The search algorithm is the standard Viterbi search ( #AUTHOR_TAG )  , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .</s>",5
"Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG )  showing up as complements . For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as ""leave."" Thus a flight can ""leave for Chicago from Boston at nine,"" or, equivalently, ""leave at nine for Chicago from Boston."" If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place]s, [to-place]s and [at-time]s is possible.",5
"as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in #AUTHOR_TAG  , De Mattia and Giachin ( 1989 )  , Niedermair ( 1989 )  , Niemann ( 1990 )  , and Young ( 1989 )  .</s>",0
"We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG )  . Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but",5
"To date, four distinct domain-specific versions of TINA have been implemented. The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG )  . The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.",5
"Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG )  represents our most promising approach to this problem . We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level",3
"MIT and Harvard University. The second one, ATIS ( #AUTHOR_TAG  et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work",5
"each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by #AUTHOR_TAG  and Hirschman et al. ( 1975 )  on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area.",1
". Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG )  .</s>",0
"US is a feature, like verb-mode, that is blocked when an [end] node is encountered. To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG )  by its generator . Finally",0
"We_currently have two application domains that can carry on a spoken dialog with a user. One , the VOYAGER domain ( #AUTHOR_TAG )  , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University . The second",5
"The example used to illustrate the power of ATNs ( #AUTHOR_TAG )  , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator . Cases of crossed traces are automatically blocked because the second CUR",1
"as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. ( 1989 )  , De Mattia and Giachin ( 1989 )  , #AUTHOR_TAG  , Niemann ( 1990 )  , and Young ( 1989 )  .</s>",0
IMIT database (Lamel et al. 1986). The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG )  that has been popular within the DARPA community in recent years . The,5
"stream is not deterministic. For the A * algorithm ( #AUTHOR_TAG )  as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .",5
"The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG )  , which uses a segmental-based framework and includes an auditory model in the front-end processing . The",5
). The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG )  and the,1
"as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. ( 1989 )  , De Mattia and Giachin ( 1989 )  , Niedermair ( 1989 )  , Niemann ( 1990 )  , and #AUTHOR_TAG  .</s>",0
"If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG )  To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm",5
"Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG )  , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .",5
theory. The formalization of DLRs provided by #AUTHOR_TAG  defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 . This description can then be given the standard set-theoretical interpretation,0
"This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and Lassez (1988) . The reader is referred to #AUTHOR_TAG  for a more detailed discussion of our use of constraint propagation.",0
"Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG  , 31 ) . A similar method is included",1
"The way these predicates interconnect is represented in Figure 19. 27 #AUTHOR_TAG  argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry . 2",0
that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG )  to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van,0
". This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG )  . Both the",0
ariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG )  . Given a lexical entry as in Figure 15,1
32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG )  can be used to circumvent constraint propagation . Enc,0
"ariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG  . The reader interested in that language and its precise interpretation can find the relevant details in that paper. ",0
"The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG  , 10 ) is a linguistic example . The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.</s>",0
is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG )  . ,0
"explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG )  .</s>",0
. As shown in #AUTHOR_TAG  this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .</s>,4
The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG )  . 29,5
mann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG  ;,1
31). A similar method is included in PATR-II ( #AUTHOR_TAG )  and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .,1
be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG )  . The,1
". Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG )  and the . lexical rules (DLRs; Meurers 1995). 5",0
ation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG  ;,5
"fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG  ; Calcagno and Pollard 1995 ) and the . lexical rules (DLRs;",0
".html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG )  that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.",0
"A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG )  consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time . While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.",1
"A logic that provides the formal architecture required by Pollard and Sag ( 1994 )  was defined by #AUTHOR_TAG  , 1994 ) . The formal language of King allows",0
"The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29 The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG  .",0
can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG  ; Emele 1994 ) .,1
"This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG  , 1994 ) .",0
ne 1992; Riehemann 1993 ; #AUTHOR_TAG  ; Frank 1994 ;,1
<s> 11 #AUTHOR_TAG  proposes to unify these two steps by including an update operator in the description language.</s>,0
32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG  ; Eisele and Dorre 1990 ;,0
<s> The powerful mechanism of lexical rules ( #AUTHOR_TAG )  has been used in many natural language processing systems . In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper.</s>,0
"]). This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG )  , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .",1
"Using an accumulator passing technique ( #AUTHOR_TAG )  , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .",5
"15 #AUTHOR_TAG  show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention . We here assume unification as the application criterion, which formally corresponds to",0
pecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG  ; Oliva 1994 ;,1
ne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG  ;,1
"To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG  ; Gerdemann 1995 ) .",0
that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG )  to operate on those raised elements . Also an analysis treating adjunct extraction via lexical rules (van,0
"16 A linguistic example based on the signature given by #AUTHOR_TAG  would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360",0
". The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG  , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .</s>",2
"Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG  ; Krieger and Ner",1
1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG )  . The lexical entries,1
that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG )  or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.,0
"Based on the research results reported in #AUTHOR_TAG  , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG . We developed a compiler that takes as its",4
"Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( #AUTHOR_TAG )  where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) . A similar method is",1
"In the latter case, we can also take care of transferring the value of z. However , as discussed by #AUTHOR_TAG  , creating several instances of lexical rules can be avoided . Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule. This is accomplished by having each lexical rule predicate call a so-called",4
"specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987  , 215 ) in terms of the setup of #AUTHOR_TAG  , ch . This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.",0
"enged as a mechanism for expressing generalizations over lexical information. In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG  ; Riehemann 1993; Oliva 1994; Frank 1994;",1
"specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG  , 215 ) in terms of the setup of Pollard and Sag ( 1994  , ch . This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.",1
and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG ) </s>,0
"the phrases detected are not necessarily mentions that we need to discover. #AUTHOR_TAG  present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .",0
") are co-referred. For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #AUTHOR_TAG  .</s>",5
"In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG )  .</s>",0
"und and Kuhn (2014) . For Berkeley system , we use the reported results from #AUTHOR_TAG  .",1
"-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 )  , Berkeley system ( #AUTHOR_TAG )  and HOTCoref system ( Bj Â¨ orkelund and ",1
"The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG )  . The",5
"Chunker ( Punyakanok and Roth , 2001 )  6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG )  to identify their heads . When",5
"key exception). Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  Bj Â¨ orkelund and Kuhn , 2014 )  .",0
"Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG  . The BILOUrepresentation suggests learning classifiers that identify the",4
"0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG )  , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We",5
"ed Systems Our developed system is built on the work by #AUTHOR_TAG  , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When",5
we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG  in our experiments .,5
"The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 )  . The",5
"More details can be found in #AUTHOR_TAG  et al. (2013). The difference here is that we also consider the validity of mention heads using �(u),�(m)</s>",0
Several recent works suggest studying coreference jointly with other tasks. Lee et al. ( 2012 )  model entity coreference and event coreference jointly ; #AUTHOR_TAG  consider joint coreference and entity-linking . The,0
to derive mention heads using Collins head rules ( #AUTHOR_TAG )  with gold constituency parsing information and gold named entity information .,5
head detection framework. Our work is inspired by the latent left-linking model in #AUTHOR_TAG  and,5
"We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG )  .",5
"istically-plausible categories. To formalize the notion of what it means for a category to be more ""plausible"", we extend the category generator of our previous work, which we will call P CAT . We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG )  . The grammar may",0
"#AUTHOR_TAG  's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) . They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents. While CCM is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves,",0
"sets of potential supertags. In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG  to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .",5
"We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 )  , which is a transformation of the Penn Treebank ( #AUTHOR_TAG )  ; the CTBCCG ( Tse and Curran , 2010 )  transformation of the Penn Chinese Treebank ( Xue et al. , 2005 )  ; and the CCG-",5
"The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG  . Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label.",1
We use the same splits as #AUTHOR_TAG  .,5
"to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG  , but we do it directly in the grammar . We",1
We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG )  .</s>,2
"We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 )  .4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags. These counts are then combined with estimates of the �openness� of each tag in order to assess its likelihood of appearing with new words.",5
"Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid. Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG )  .</s>",4
We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #AUTHOR_TAG  .</s>,5
"To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 )  and used by #AUTHOR_TAG  that samples entire parse trees . For a sentence w,",5
". Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution. Our strategy is based on the approach presented by #AUTHOR_TAG  .",5
"One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG  , which was specifically designed to capture the linguistic observation made by Radford ( 1988 )  that there are regularities to the contexts in which constituents appear . This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts. For example, the part-of-speech (POS) sequence ADJ NOUN frequently",0
"ent with category pp, and then on the left (\) with a noun phrase (np) that serves as its subject. We follow #AUTHOR_TAG  in allowing a small set of generic , linguistically-plausible unary and binary grammar rules . We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X",5
"from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG  . The",1
"Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG )  for a recent example with further references ) . This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996)",0
"with a period. These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 )  , although the various MERGE operations are , to our knowledge , novel in this form .</s>",0
"The sp-tree is inspired by (Lavoie and Rambow, 1998) . The representations used by Danlos ( 2000 )  , Gardent and Webber ( 1998 )  , or #AUTHOR_TAG  are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes . 4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the",0
"<s> 2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG  .</s>",5
"<s> The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG )  .</s>",1
"<s> The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG  .</s>",5
"described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #AUTHOR_TAG )  .</s>",1
<s> The ICA system ( #AUTHOR_TAG )  aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .</s>,0
"Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG )  extracted systematic polysemy from WordNet . In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method.</s>",2
"is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998) . In our previous work ( #AUTHOR_TAG )  , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the",2
"In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG )  . Rather than using a termdocument matrix, we had followed an approach akin to that of",2
"drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011 a ; #AUTHOR_TAG )  , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009",2
"<s> Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 )  , as have been bilingual stochastic grammars ( #AUTHOR_TAG )  .</s>",0
<s> It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG )  .</s>,5
It can be shown ( #AUTHOR_TAG )  that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of,4
<s>#AUTHOR_TAG  describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .</s>,0
<s> A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG )  .</s>,5
"<s> As ( #AUTHOR_TAG )  show , lexical information improves on NP and VP chunking as well .</s>",3
"The system was trained on the Penn Treebank ( Marcus et al. , 1993 )  WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG  , Collins ( 1997 )  , and Ratnaparkhi ( 1997 )  , and became a common testbed .</s>",1
"<s> A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 )  , Magerman ( 1995 )  , #AUTHOR_TAG  , Ratnaparkhi ( 1997 )  , and Sekine ( 1998 )  ) .</s>",0
"<s> It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG  , for PP attachment ) .</s>",1
<s> Another approach for partial parsing was presented by #AUTHOR_TAG  .</s>,0
"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 )  , Magerman ( 1995 )  , Collins ( 1997 )  , #AUTHOR_TAG  , and Sekine ( 1998 )  ) .</s>",0
"Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG  as might be expected since much less structural data , and no lexical data are being used .</s>",1
"<s> In a similar vain to Skut and Brants ( 1998 )  and #AUTHOR_TAG  , the method extends an existing flat shallow-parsing method to handle composite structures .</s>",3
"<s> A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 )  , #AUTHOR_TAG  , Collins ( 1997 )  , Ratnaparkhi ( 1997 )  , and Sekine ( 1998 )  ) .</s>",0
"The system was trained on the Penn Treebank ( Marcus et al. , 1993 )  WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 )  , Collins ( 1997 )  , and #AUTHOR_TAG  , and became a common testbed .</s>",1
"The system was trained on the Penn Treebank ( Marcus et al. , 1993 )  WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 )  , #AUTHOR_TAG  , and Ratnaparkhi ( 1997 )  , and became a common testbed .</s>",1
"<s> One approach to partial parsing was presented by #AUTHOR_TAG  , who extended a shallow-parsing technique to partial parsing .</s>",0
"<s> In a similar vain to #AUTHOR_TAG  and Buchholz et al. ( 1999 )  , the method extends an existing flat shallow-parsing method to handle composite structures .</s>",3
"the individual adjectives involved. To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG  propose to assign a weight to each link . Say the order a, b occurs m times and the pair {a, b} occurs n times in total. Then the weight of the pair a → b is:</s>",0
"available. More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG )  could be applied to extract semantic classes from the corpus itself .",3
"The simplest strategy for ordering adjectives is what #AUTHOR_TAG  call the direct evidence method . To order the pair {a, b}, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often.</s>",0
". It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997) . In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG )  offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .</s>",3
"and an order must be randomly assigned. #AUTHOR_TAG  propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation . That is, if a ≺ c and c ≺ b, we can conclude that a ≺ b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method. However, the pairs large, new and new, green occur fairly frequently. Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.",0
"of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system. One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAG a ; Langkilde and Knight , 1998 b )",5
". Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 )  and machine translation ( #AUTHOR_TAG )  .",0
"10 Traditionally log(strength) values are called weights, but this paper uses ""weight"" to mean something else. fÎ¸ on demand ( #AUTHOR_TAG )  can pay off here , since only part of fÎ¸ may be needed subsequently . )</s>",0
#AUTHOR_TAG  give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations ⊗ and ⊕ on K. Thus ��,5
"familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG )  .",0
"The availability of toolkits for this weighted case ( #AUTHOR_TAG  ; van Noord and Gerdemann , 2001 )  promises to unify much of statistical NLP .",0
"but they are limited to particular kinds of models and training regimens. For example , the forward-backward algorithm ( Baum , 1972 )  trains only Hidden Markov Models , while ( #AUTHOR_TAG )  trains only stochastic edit distance .</s>",0
"+ , x , â ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG )  .</s>",0
"4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG )  , taking care to write each regexp in the construction as a constant times a probabilistic regexp . A full proof is straightforward",5
"single edge. (Mohri, 2002) . Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG )  .</s>",3
"Per-state joint normalization ( #AUTHOR_TAG b , Â§ 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization. It arises, for example, when training a joint model of the form</s>",1
"The EM algorithm ( #AUTHOR_TAG )  can maximize these functions . Roughly, the E step guess",5
"1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 )  , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG )  , ( 4 )",0
"Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG  ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n",0
"desired. Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001 b ; #AUTHOR_TAG )  .</s>",0
"In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG )  .</s>",5
", in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG  , who allows any regular set ) ,",0
". Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG )  and machine translation ( Knight and Al-Onaizan , 1998 )  . Moreover, once the models are expressed in the finitestate framework,",0
", since then the real work is done by an c-closure step ( #AUTHOR_TAG )  that implements the all-pairs version of algebraic path , whereas all we need",0
"Markovian. Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation. Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG )  .",0
"A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAG a ) . A leisurely journal-length version with more details has been prepared and is available.",2
"A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG )  , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation . These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.</s>",0
"For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG )  , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 )  , ( 4",0
"desired. Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001 b ; Lafferty et al. , 2001 )  .</s>",0
"A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 )  , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation . These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.</s>",0
", but they are limited to particular kinds of models and training regimens. For example , the forward-backward algorithm ( #AUTHOR_TAG )  trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 )  trains only stochastic edit distance .</s>",0
"desired. Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAG b ; Lafferty et al. , 2001 )  .</s>",0
"c , using Improved Iterative Scaling ( Della #AUTHOR_TAG  ;",5
"A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 )  . The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ).",0
"The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG  , and Entrez GENE [ 13 ] ) , PSD database from",5
"Base [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG  , rat --",5
"organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG  , fly",5
"1 ] , worm -- WormBase #AUTHOR_TAG  , Human",5
The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG  . It contains three knowledge sources: the,0
"With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG  . One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text. Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining. Such task is called biological",0
"3 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG  , and En",5
"ustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG  such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence",5
"yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG  , worm --",5
"Pept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG  , and Additionally, several model organism databases or nomenclature databases were used. Correspondences among records from these databases are identified using the rich cross-",5
", mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG  , yeast Saccharomyces Genome Database ( SGD ) [ ",5
Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG  .,5
"Bayes as the learning algorithm. The knowledge sources we use include parts-of-speech, local collocations, and surrounding words. These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG )  .</s>",2
"Tetreault (2001) ). While these approaches have been reasonably successful ( see Mitkov ( 2002 )  ) , #AUTHOR_TAG  speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .",0
"many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ) ; and",1
"approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 )  , #AUTHOR_TAG )  . While these approaches have been reasonably successful (see Mitkov (2002) ), Kehler et al. (2004) ",0
"ifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG )  , which consists of all the Penn Treebank Wall Street Journal articles with the ACE",5
".g., Mitkov (1998) , Tetreault (2001) ). While these approaches have been reasonably successful ( see #AUTHOR_TAG )  , Kehler et al. ( 2004 )  speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .",0
train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG ) ) on these ,5
"( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG )  , a MUC-style NE recognizer to determine the NE type of NPZ . If NPi is determined to be a PERSON or ORGANIZATION,",5
"( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 )  , #AUTHOR_TAG )  . It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not",0
"Learning algorithms. We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG  , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 )  and NE classification ( Collins and Singer , 1999 )  . We apply add-one smoothing to smooth the class posteriors.",5
"We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG )  , and (",5
"After training, the decision tree classifier is used to select an antecedent for each NP in a test text. Following #AUTHOR_TAG  , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj. If no such NP exists, no antecedent is selected for NPj.",4
"Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG )  .",1
", � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002)  and #AUTHOR_TAG  , as described below .</s>",1
Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG )  to acquire a classifier on the training texts for determining whether two NPs are coreferent .,5
"by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG  and Yang et al. (2003) , as described below.</s>",1
"(2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following #AUTHOR_TAG  , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition . In all of our experiments, we use NPs automatically extracted by",5
"Ji et al. ( 2005 )  ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 )  ) or Wikipedia ( Ponzetto and Strube , 2006 )  , and the contextual role played by an NP ( see #AUTHOR_TAG )  .</s>",0
"to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG  , Markert and Nissim ( 2005 )  ) .",0
"Following previous work ( e.g. , #AUTHOR_TAG  and Ponzetto and Strube ( 2006 )  ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent",5
"ner as described in Collins and Singer ( 1999 )  , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG )  and NE classification ( Collins and Singer , 1999 )  .",4
"NPs ( e.g. , Ji et al. ( 2005 )  ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 )  ) or Wikipedia ( #AUTHOR_TAG )  , and the contextual role played by an NP ( see Bean and Riloff ( 2004 )  ) .",0
"ed the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG )  , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 )  ) or Wikipedia ( Ponzetto and Strube , 2006 )  , and the contextual role played by an NP ( see Bean and Riloff ( 2004 )  ) .",0
7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAG a ) ) . Motivated by this</s>,4
"may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG )  .",4
"various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 )  . Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In",0
All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG )  . The MU-MIN scheme is a general framework for the study of gestures in,5
", syntactic and prosodic cues , while #AUTHOR_TAG  examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . ",0
"various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 )  . Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In",0
Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG  for an overview ) . Others,0
"annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 )  1 and corrected kappa ( #AUTHOR_TAG )  2 . Anvil divides the annotations in slices and compares each slice. We",5
"Louwerse et al. ( 2006 )  and Louwerse et al. ( 2007 )  study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG )  and find correlations between the various modalities both within and across speakers . Finally",0
"These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG )  . We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3,",5
"Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. #AUTHOR_TAG  obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 )  examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .",0
"annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG )  1 and corrected kappa ( Brennan and Prediger , 1981 )  2 . Anvil divides the annotations in",5
"various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 )  . Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In",0
"Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG  , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971) . Looking at the cases of disagreement we could see that many of these are due to the fact that the an",0
. Related are also the studies by Rieks op den Akker and Schulz ( 2008 )  and #AUTHOR_TAG  : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .</s>,0
"omena (see McClave (2000)  for an overview). Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example , #AUTHOR_TAG  and Jokinen et al. ( 2008 )  find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 )  show that there is a dependence between focus of attention and assignment of dialogue",0
"omena (see McClave (2000)  for an overview). Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example , Jokinen and Ragni ( 2007 )  and #AUTHOR_TAG  find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 )  show that there is a dependence between focus of attention and assignment of dialogue act",0
"researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008) , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG )  . Looking at the cases of disagreement we could see that many of these are due to the fact that the an",0
"3. These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG )  , but are still sat -isfactory given the high number of categories provided by the scheme.",1
". The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG  , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions . The",1
"The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG )  . Therefore, here we show the results of this classifier.",5
"For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG )  . To",5
"Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006). The Praat tool was used ( #AUTHOR_TAG )  .</s>",5
"then a diagnoser checks the explanation correctness. The diagnoser , based on #AUTHOR_TAG b ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer . At present, the system uses a heuristic matching algorithm to",2
"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG )  , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 )  . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a",0
", and a FUF/SURGE ( #AUTHOR_TAG )  generation system to produce the appropriate text . Templates are used to generate some stock phrases such as ""When you are ready, go on to the next slide.""",5
"but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010) . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG )  . Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.</s>",3
"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 )  , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 )  . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a",0
"Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG )  . The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer. Once the complete answer has been accumulated, the system accepts it and moves on.",5
"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 )  , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 )  . However, most existing systems use pre-authored tutor responses for addressing student errors. The",0
", the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG )  . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006) . Using a deep",3
"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 )  , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 )  . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a",0
"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 )  , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 )  . However, most existing systems use pre-authored tutor responses for addressing student errors. The",0
"answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG )  . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is",5
"semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG  , and an ontology mapping mechanism ( Dzikovska et al. , 2008 a ) to produce a domain-specific semantic representation of the student 's output .",1
"The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG )  . It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically. This allows the system to consistently apply the same tutorial policy across a range of questions. To some extent, this comes at the expense of being able to address individual student misconceptions. However, the system",0
"At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG  .</s>",3
"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  VanLehn et al. , 2007 )  , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG )  . However, most existing systems use pre-authored tutor responses for addressing student errors. The",0
"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 )  , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 )  . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a",0
"The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 )  , and an ontology mapping mechanism ( #AUTHOR_TAG a ) to produce a domain-specific semantic representation of the student 's output .",5
<s> Other factors such as student confidence could be considered as well ( #AUTHOR_TAG )  .</s>,3
"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 )  , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 )  . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a",0
"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 )  , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 )  . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a",0
". Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG )  policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a",2
We use the TRIPS dialogue parser ( #AUTHOR_TAG )  to parse the utterances . The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter,5
"The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG )  to represent the state of the world . At present, the knowledge base represents 14 object",5
"All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 )  , or applying logical inference ( Tatu and Moldovan , 2005 )  , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 )  , incorporate different types of lexical knowledge to support textual inference . Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.",0
"Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG )  . There are several methods to build phrase tables.",0
"ones , DIRT ( #AUTHOR_TAG )  , VerbOcean ( Chklovski and Pantel , 2004 )  , FrameNet ( Baker et al. , 1998 )  , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 )  . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.",0
"All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 )  , or applying logical inference ( Tatu and #AUTHOR_TAG )  , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ;  Bar-Haim et al. , 2008 ) ",0
"Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG )  . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.",5
"Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG )  as an extension of Textual Entailment ( Dagan and Glickman , 2004 )  that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T . The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For",0
"of any TE system. Using the basic solution proposed by ( #AUTHOR_TAG )  as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :</s>",1
"We run TreeTagger ( Schmid , 1994 )  for tokenization , and used the Giza + + ( #AUTHOR_TAG )  to align the tokenized corpora at the word level .",5
"Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 )  , multidocument summarization ( #AUTHOR_TAG )  , automatic evaluation of MT ( Denkowski and Lavie , 2010 )  , and TE ( Dinu and Wang , 2009 )  .",4
"Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in (Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 )  indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .</s>",0
"<s> To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG )  , using each score as a feature .</s>",5
"Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG )  , multidocument summarization ( McKeown et al. , 2002 )  , automatic evaluation of MT ( Denkowski and Lavie , 2010 )  , and TE ( Dinu and Wang , 2009 )  .",4
"literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009) . These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 )  , VerbOcean ( #AUTHOR_TAG )  , FrameNet ( Baker et al. , 1998 )  , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) ",0
"English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG )  ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE. As regards",0
"PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 )  , multidocument summarization ( McKeown et al. , 2002 )  , automatic evaluation of MT ( Denkowski and Lavie , 2010 )  , and TE ( #AUTHOR_TAG )  .",4
"(Dinu and Wang, 2009) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG )  . With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",0
"documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009) . These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 )  , VerbOcean ( Chklovski and Pantel , 2004 )  , FrameNet ( Baker et al. , 1998 )  , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 )  . D",0
"Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 )  as an extension of Textual Entailment ( #AUTHOR_TAG )  that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T . The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For",0
"documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009) . These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 )  , VerbOcean ( Chklovski and Pantel , 2004 )  , FrameNet ( #AUTHOR_TAG )  , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) ",0
"language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction , pruning techniques ( #AUTHOR_TAG )  can be applied to increase the precision of the extracted paraphrases .</s>",0
"language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 )  , multidocument summarization ( McKeown et al. , 2002 )  , automatic evaluation of MT ( #AUTHOR_TAG )  , and TE ( Dinu and Wang , 2009 )  .",4
"Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in (Bentivogli et al., 2010) , even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 )  indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .</s>",0
"For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG )  . Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method",1
"Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG )  . Another",3
). We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG )  to measure the relatedness between words in the dataset .,5
"Second , in line with the findings of ( #AUTHOR_TAG )  , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third",1
"we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG )  . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides",5
"Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG )  . These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001) , VerbOcean (Chklovski and Pantel, 2004) , FrameNet (Baker et al., 1998) , and Wikipedia Kouylekov et al., 2009) .",0
Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG )  or other lexical decision tasks . The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in,0
We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG )  measure ( x ) where the agreement lies around 0.79 .,5
"to construct a semantic analysis based on ""prepared"" and ""unprepared mind"". Similar findings have been proposed by #AUTHOR_TAG  that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . ",0
"the argument structure. #AUTHOR_TAG  tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"". Similar findings have been proposed by Pandharipande (1993)  that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. ",0
"lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ; #AUTHOR_TAG )  where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000)",0
"Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages. Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 )  . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon ",0
"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 )  . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.",0
Hook (1981)  considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #AUTHOR_TAG  argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) ,0
"With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG  ; Bentin , S. and Feldman , 1990 )  specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla . Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological",5
"We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 )  for Bangla morphologically complex words . Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming). The subjects were asked to make a lexical decision whether the given target is a valid word in that language. The same target word is again",5
"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG  ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 )  . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo",0
"We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 )  for Bangla morphologically complex words . Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming). The subjects were asked to make a lexical decision whether the given target is a valid word in that language. The same target word is again",5
"mechanism of the mental lexicon will further our knowledge of how the human brain processes language. Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications. Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG )  and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .</s>",0
"investigations have been conducted yet. On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG )  . Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.</s>",0
"(Taft and Forster, 1975;Taft, 1981; MacKay, 1978) . Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG )  .</s>",0
"A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. #AUTHOR_TAG  considers the second verb V2 as an aspectual complex comparable to the auxiliaries . Butt (1993)",0
"with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  Taft 1975 ; Taft , 2004 )  where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000)",0
"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ;  Grainger , et al. , 1991 ; #AUTHOR_TAG )  . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo",0
"Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAG c ) . The majority of indirect associations can be filtered out by a simple competition",0
"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993 a ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look",0
"uk. Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG )  . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they",0
""" MT on the World Wide Web (Church & I-Iovy, 1993) , certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991) , computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996) .</s>",0
"correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 )  . The likelihoods in the word-to-word",0
"MT on the World Wide Web (Church & I-Iovy, 1993) , certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996 b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ) , computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996) .</s>",0
""" MT on the World Wide Web (Church & I-Iovy, 1993) , certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAG b )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ) , computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996) .",0
"2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 )  or the Dice coefficient ( #AUTHOR_TAG )  . co-occur is called a direct association.",1
"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993 a ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look",0
"in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAG a ) . Then",0
"<s> We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG )  .</s>",5
"burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 )  . To",0
The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports. #AUTHOR_TAG  reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Rec,0
"Co-occurrence With the exception of (Fung, 1998 b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a;Melamed, 1995) . A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to",0
"MT on the World Wide Web ( Church & Hovy , 1993 )  , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996 b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 )  , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996) .",0
"Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG )  . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena.",3
"For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG )  2 . When the L(u",5
"With the exception of ( Fung , 1995 b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995 a ; #AUTHOR_TAG )  . A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and",0
"burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG )  . To",1
"One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima. By using the EM algorithm ( #AUTHOR_TAG )  , they can guarantee convergence towards the globally optimum parameter set . In contrast, the dynamic nature of the competitive linking algorithm",0
"Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG )  . Another interesting extension is to broaden the definition of a ""word"" to include multi-",3
"of link frequencies generated by the competitive linking algorithm. More accurate models can be induced by taking into account various features of the linked tokens. For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG )  .</s>",0
"burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 )  . To",1
"2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG )  or the Dice coefficient ( Smadja , 1992 )  .",1
"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG a ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look",0
"correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG )  . The likelihoods in the word-to-word",0
"from those that do not. This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAG b ) . for their models (Brown et al., 1993",1
"The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG )  , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We",1
"because multiple samples would result in a grammar that would be too large. 11 11 From ( #AUTHOR_TAG )  , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags . Thus, to be convenient, we only conduct experiments with the SAMT system.",4
"structures in our sampler. Differently , #AUTHOR_TAG  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment . We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics.</s>",4
parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG )  .,1
". #AUTHOR_TAG  , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ",1
"for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on ( Galley et al. , 2006 )  and ( #AUTHOR_TAG )  . In the system",5
"Our previous work ( #AUTHOR_TAG )  designed an EMbased method to construct unsupervised trees for tree-based translation models . This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more",1
"parser (Petrov et al., 2006) . Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG )  and use the resulting binary parse trees to build another s2t system .</s>",5
"In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011 b ) .</s>",0
"In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011 b ) .</s>",0
"and Burkett et al. (2010)  focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #AUTHOR_TAG  adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012)  re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012)  utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models",1
"consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG  and decompose the prior probability P0 ( r | N ) into two factors as follows :</s>",5
"substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. #AUTHOR_TAG  further labeled the SCFG rules with POS tags and unsupervised word classes . Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.",1
"Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG )  . Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For",5
"STSG by exploring the space of alignments based on parse trees. #AUTHOR_TAG  re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012)  utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. ",1
The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG )  .</s>,5
"The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on ( #AUTHOR_TAG )  and ( Marcu et al. 2006 ) . In the system, we extract both the minimal",5
"(Chiang, 2007) . #AUTHOR_TAG  employed a Bayesian method to learn discontinuous SCFG rules . This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG",1
"SG, which is more appropriate for tree-based translation models than SCFG. #AUTHOR_TAG  and Burkett et al. ( 2010 )  focused on joint parsing and alignment . They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012)  re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012)  utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their",1
not be worse due to AEs. Our final experiments verify this point and we will conduct a much detailed analysis in future. 9 We only use the minimal GHKM rules ( #AUTHOR_TAG )  here to reduce the complexity of the sampler .</s>,5
"based on (Galley et al., 2006)  and ). In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG )  , and the rules of SPMT Model 1 ( Galley et al. , 2006 )  with phrases up to length L = 5 on the source side .</s>",5
"es. Using the GHKM algorithm ( #AUTHOR_TAG )  , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .",5
"In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG  , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b ) .</s>",0
"In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; #AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b ) .</s>",0
"To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 )  to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG )  respectively .</s>",5
"To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG )  . Then, we binarize the English parse trees using the head binarization",5
"each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011 a ) .</s>",4
"In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG  , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b ) .</s>",0
"of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs. This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG )  .</s>",0
"CFG. Burkett and Klein ( 2008 )  and #AUTHOR_TAG  focused on joint parsing and alignment . They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012)  re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012)  utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we",1
"Liu et al. (2012)  re-trained the linguistic parsers bilingually based on word alignment. #AUTHOR_TAG  utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. ",1
"variables. Inspired by ( Blunsom et al. , 2009 )  and ( #AUTHOR_TAG )  , we define P ( str | frag ) as follows : where csw is the number of words in the source string .</s>",4
"the probability of generating the source string, which contains several source words and variables. Inspired by ( #AUTHOR_TAG )  and ( Cohn and Blunsom , 2009 )  , we define P ( str | frag ) as follows : where csw is the number of words in the source string .</s>",4
"In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG )  5 . This would indicate that the SCFG-based derivation tree as by-product is also not such good for",1
"of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #AUTHOR_TAG  substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . Zollmann and Vogel (2011)",1
"izer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG )  using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses . This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously",0
"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 )  . One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.</s>",0
"There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 )  . Incorporating such techniques would deo crease the system developer workload.",3
Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state. They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG  .</s>,1
"or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG )  . This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) ",5
"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG )  . One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.</s>",0
"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 )  . One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.</s>",0
"WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAG b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999 a ) , and a weather infomiation system ",2
"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 )  . One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.</s>",0
"unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 )  . The language generation module features Common Lisp functions, so there is",3
"Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAG a ) . Thus the system can respond immediately after user pauses when the user has the initiative.",0
"reservation system ( Nakano et al. , 1999 b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999 a ) , and a weather infomiation system ( #AUTHOR_TAG )  . The meeting",2
WIT features an incremental understanding method ( #AUTHOR_TAG b ) that makes it possible to build a robust and real-time system . In,5
. Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG )  .,5
"), but we do not explain these constraints in detail in this paper. The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAG b ) .",5
"To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG )  . One is the CSLU Toolkit (Sutton et al., 1998) , which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain. However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics",0
"The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAG b ) , which is an integrated parsing and discourse processing method . ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of",5
"Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996) . While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more",1
"A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG )  . One such method is to treat different translations of the same term as",1
queries. The one-sided t-test ( #AUTHOR_TAG )  at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .</s>,5
"The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 )  , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG )  . We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources",1
â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG )  on the TREC4 ad-hoc query set .</s>,5
"icon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997 . #AUTHOR_TAG  studied the issue of disambiguation for mono-lingual M.</s>",0
"icon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG  . Sanderson, 1994  studied the issue of disarnbiguation for mono-lingual IR.</s>",0
"Many approaches to cross-lingual IR have been published. One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG )  . For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.</s>",1
"much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG  ; Hull 1997 ) . One such method is to treat different translations of the same term as",1
"term in the original query may be ranked higher than a document that matches translations of different terms in the original query. That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.  However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG )  .</s>",0
". A cooccurrence based stemmer ( #AUTHOR_TAG )  was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon. This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.",5
"Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999  . Our work has focused on cross-lingual retrieval.</s>",1
"Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999  . Our work has focused on cross-lingual retrieval.</s>",1
"<s> Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG )  . Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.</s>",0
"Following #AUTHOR_TAG  , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) . Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a",5
"the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 )  ,",0
"The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG )  . In this case, a full parse tree is represented in a flat form, producing a representation as in the example above. The goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases. The chunk types are based on the syntactic category part of the bracket label in the Treebank. Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name. The phrases are",5
"Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 )  . A",0
"the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAG a ; Charniak , 1997 b ; Ratnaparkhi , 1997 )  ,",0
"<s> Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability. However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997 b ; Charniak , 1997 a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 )  .</s>",0
"We perform our comparison using two state-ofthe-art parsers. For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG )  -- one of the most accurate full parsers around . It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.",5
"<s> Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability. However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAG b ; Charniak , 1997 a ; Collins , 1997 ; Ratnaparkhi , 1997 )  .</s>",0
"The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999) . SNoW ( #AUTHOR_TAG ; Roth , 1998 )  is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example . It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.",5
"over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 )  , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )  .</s>",0
"over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 )  , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )  .</s>",0
"Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 )  . A",0
"Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG )  . A",0
"that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG )  . Second",0
"as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG )  to compare it to other shallow parsers . Table 1 shows",5
"over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 )  , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )  .</s>",0
"<s> Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability. However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997 b ; Charniak , 1997 a ; Collins , 1997 ; #AUTHOR_TAG )  .</s>",0
"The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 )  .",5
"clean"" WSJ data. Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG )  terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.",5
"over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 )  , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )  .</s>",0
"Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG )  and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under",2
"<s> Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG )  that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .</s>",0
"We perform our comparison using two state-ofthe-art parsers. For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 )  -- one of the most accurate full parsers around . It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.",5
"over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 )  , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )  .</s>",0
"The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG )  .",5
"The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999) . SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG )  is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example . It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.",5
account deficit will narrow to only $ 1.8 billion in September .� would be chunked as follows ( Tjong Kim #AUTHOR_TAG )  : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .,0
"that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 )  . Second",0
"over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 )  , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )  .</s>",0
"over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 )  , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )  .</s>",0
"the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; #AUTHOR_TAG )  , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )  .</s>",0
"Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 )  and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under",2
"Training was done on the Penn Treebank ( #AUTHOR_TAG )  Wall Street Journal data , sections 02-21 .",5
"Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG )  . However, their method depended on translator�s intuitive analy- sis of the original grammar. Thus the transla- tion was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and",1
"There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 )  , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG )  , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998) . These works are restricted to each closed community, and the relation between them is not well discussed. Investigating the relation will be apparently valuable for both communities",0
. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG )  . This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We,1
"TNT refers to the HPSG parser ( #AUTHOR_TAG )  , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .",1
"as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ;  Vijay-Shanker and Joshi , 1988 )  and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG )  by a method of grammar conversion . The",0
"Figure 4). FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG )  is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure",0
"ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 )  . These works are re- stricted to each closed community, and the rela- tion between them is not well discussed. Investi- gating the relation will be apparently",0
"paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG )  and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) ",0
<s> LTAG ( #AUTHOR_TAG )  is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar</s>,0
"GO) project (Flickinger, 2000) . In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG )  . Our group has developed a wide-coverage HPS",0
"the Verbmobil project (Kay et al., 1994) . Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG )  , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 )  .</s>",0
", and obscures the equivalence between the original and obtained grammars. Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG )  convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we",1
"(Kay et al., 1994) . Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 )  , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG )  .</s>",0
"An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG )  . A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.",0
"LTAG communities. We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG )  , which is a large-scale FB-LTAG grammar . A",5
"The XTAG Research Group, 2001) . The XTAG group ( #AUTHOR_TAG )  at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars . Development of a large-scale French grammar (A",0
"TAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG )  . Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG",0
"sing time with the LTAG and HPSG parsers. In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG )  , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 )  without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .",1
"Figure 4). FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 )  is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure",0
"the use of HPSGbased processing in practical application contexts . Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG )  . In",0
"ism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 )  . These works are re-stricted to each closed community, and the relation between them is not well discussed. Investigating the relation will be apparently valuable for both communities.",0
"FeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG )  . We applied our system to the XTAG",0
mar conversion enables us to share HPSG parsing techniques in LTAG parsing. Another paper ( #AUTHOR_TAG )  describes the detailed analysis on the factor of the difference of parsing performance .</s>,0
and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG )  6 ( the average length is 6.32 words ) . This result empirically,5
"The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG )  2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;",5
"paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 )  and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) ",0
"ly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000) , ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000) , and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG )  . These works are re-stricted to each closed community, and the relation between them is not well discussed. Investigating the relation will be apparently valuable for both communities.",0
"lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 )  , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 )  , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998) . These works are restricted to each closed",0
<s> The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG )  is the core portion of the RenTAL system .</s>,0
"ism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG )  . The XTAG group (Doran et al., 2000)  at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars. Development of a large-scale",0
"TAG communities. We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG )  3 , which is a large-scale FB-LTAG grammar for English . A",5
"There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 )  , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 )  . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or deriv",0
"domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )  .</s>",0
"denotes the string concatenation operator. mers ( #AUTHOR_TAG ; Porter , 1980 )  demonstrably improve retrieval performance . This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;",0
"There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 )  , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG )  . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or deriv",0
"denotes the string concatenation operator. mers ( Lovins , 1968 ; #AUTHOR_TAG )  demonstrably improve retrieval performance . This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;",0
"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ;  J Â¨ appinen and Niemist Â¨ o , 1988 ; #AUTHOR_TAG )  , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved . In order to cope with such variation,",0
"domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )  .</s>",0
"There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG )  , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 )  . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Emp",0
"<s> ), but at high ones its precision decreases almost dramatically. Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG )  ) one can not really recommend this method .</s>",1
"domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )  .</s>",0
"form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )  .</s>",0
"simple tf-idf metric. The retrieval process relies on the vector space model ( #AUTHOR_TAG )  , with the cosine measure expressing the similarity between a query and a document . The",5
"There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 )  , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 )  . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Emp",0
"roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988) . While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG )  .</s>",0
"ifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG )  ) are incorporated into our system .",3
"There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 )  , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 )  . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or deriv",0
"chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG  ; Ekmekc Â¸ i",0
"domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )  .</s>",0
"domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )  .</s>",0
"incorporated into our system. Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG )  .</s>",3
"form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )  .</s>",0
"documents. Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG )  , turns out to be infeasible , at least for German and related languages .</s>",0
"; Choueka , 1990 ; Popovic and Willett , 1992 ;  Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG )  .",0
"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG  ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 )  , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved . In order to cope with such variation",0
"terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG )  . While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ",0
", such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 )  , ( Aramaki et al. , 2001 )  , ( #AUTHOR_TAG )  , ( Meyers et al. , 2000 )  , ( Matsumoto et al. , 1993 )  , (",0
"-base machine translation EBMT3 ( Sato & Nagao , 1990 )  , ( Sato , 1991 )  , ( Richardson et al. , 2001 )  , ( #AUTHOR_TAG )  .",0
"In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG )  will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two . The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation. Such correspondence is defined in a way that is able to handle some non-standard cases (e.",0
"to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988) . These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG )  . crossed dependencies (Tang & Zaharin, 1995) .</s>",0
"ards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG  to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .</s>",0
"of a BKB, which is needed for the EBMT applications. #AUTHOR_TAG  presented an approach for constructing a BKB based on the S-SSTC .</s>",0
", such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 )  , ( Aramaki et al. , 2001 )  , ( Watanabe et al. , 2000 )  , ( #AUTHOR_TAG )  , ( Matsumoto et al. , 1993 )  , (",0
"It allows the construction of a non-TAL ( Shieber , 1994 )  , ( #AUTHOR_TAG )  . As a",0
"From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG )  . The MTT point of view,",0
"992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG )  , ( Sato , 1991 )  , ( Richardson et al. , 2001 )  , ( Al-Adhaileh & Tang , 1999 )  .",0
"omena exist between different languages, that cause challenges for synchronized formalisms. In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG  cases ) . Shieber (1994)  cases).",0
"to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 )  , ( Aramaki et al. , 2001 )  , ( Watanabe et al. , 2000 )  , ( Meyers et al. , 2000 )  , ( #AUTHOR_TAG )  , ( kaji et al. , ",0
"<s> There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation. Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )  , such as the relation between syntax and semantic .</s>",0
"languages). For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 )  , ( Aramaki , 2001 )  , ( AlAdhaileh & Tang , 1999 )  , ( Sato & Nagao , 1990 )  , ( Sato , 1991 )  , ( #AUTHOR_TAG )  , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus",0
", such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG )  , ( Aramaki et al. , 2001 )  , ( Watanabe et al. , 2000 )  , ( Meyers et al. , 2000 )  , ( Matsumoto et al. , 1993 )  ,",0
<s> There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation. Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )  .</s>,0
"pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"". For more details on the proprieties of SSTC , see #AUTHOR_TAG  .</s>",0
corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG )  and Boitet & Zaharin ( 1988 )  . and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.</s>,5
", and example-base machine translation EBMT3 ( Sato & Nagao , 1990 )  , ( Sato , 1991 )  , ( #AUTHOR_TAG )  , ( Al-Adhaileh & Tang , 1999 )  .",0
"It allows the construction of a non-TAL ( #AUTHOR_TAG )  , ( Harbusch & Poller , 2000 )  .",0
") , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 )  , ( #AUTHOR_TAG )  , ( Richardson et al. , 2001 )  , ( Al-Adhaileh & Tang , 1999 )  .",0
"in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 )  and #AUTHOR_TAG  . and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.</s>",5
"However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG )  . Recent work (Banko and Brill, 2001;Curran and Moens, 2002)  has suggested that some tasks will benefit from using significantly more data.",0
"systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG )  and speech recognition ( Hacioglu and Pellom , 2003 )  . Web services",0
". Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 )  and co-training ( #AUTHOR_TAG )  . Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.</s>",0
"There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG )  and the Alembic Workbench ( Day et al. , 1997 )  ) as well as NLP tools and resources that can be manipulated from the GUI .",0
"totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 )  and speech recognition ( #AUTHOR_TAG )  . Web services will allow components developed by different researchers in different locations to be composed to build larger systems.</s>",0
"recognisers to be rapidly composed from many elemental components. For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG )  will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .</s>",3
"The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 )  and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG )  . We have",4
"The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG )  and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 )  . We",4
"Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG  that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 )  . Other attempts to address efficiency include the fast Transformation Based Learning (",0
"This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG )  . However, the source code for these tools is not freely available, so they cannot be extended.</s>",0
"However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003) . Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG )  has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For",0
"will be manually annotated. For example , 10 million words of the American National Corpus ( Ide et al. , 2002 )  will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG )  , currently used for training POS taggers . This will require more efficient learning algorithms and implementations.</s>",0
", making it ideal for experimentation. It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG )  .</s>",2
"There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 )  and the Alembic Workbench ( #AUTHOR_TAG )  ) as well as NLP tools and resources that can be manipulated from the GUI . For",0
"For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG )  . GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000)",0
<s> Software engineering research on Generative Programming ( #AUTHOR_TAG )  attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems . Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.,0
"data will be manually annotated. For example , 10 million words of the American National Corpus ( #AUTHOR_TAG )  will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 )  , currently used for training POS taggers . This will require more efficient learning algorithms and implementations.",0
"A number of stand-alone tools have also been developed. For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG )  perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their",0
"templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces. To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG )  , and for the dynamic version we will use configuration classes .</s>",5
"However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003) . Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 )  has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For",0
"A number of stand-alone tools have also been developed. For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 )  perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their",0
"Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG )  and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 )  have been applied to many different problems , so a",4
"(Malouf, 2002) . Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG )  which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 )  . The",0
"<s> The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG )  . An",0
. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG )  . We expect even,5
"(Malouf, 2002) . Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 )  which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG )  . The TNT POS tagger (Brants, 2000)",0
"Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG )  . Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.</s>",0
"The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 )  and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 )  . We have",4
"Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 )  that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG )  . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001)  which dramatically speeds up training TBL systems, and the translation of TBL rules into finite",0
"<s> The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 )  . An",0
"Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001)  which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997) . The TNT POS tagger ( #AUTHOR_TAG )  has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .</s>",0
"ontology editors (Cunningham et al., 2002) . GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG )  which the GUI is built on top of . This allows components to be highly configurable and simplifies the addition of new components to the system.</s>",0
"<s> ""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language. Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 )  describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 )  show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .</s>",0
"Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG )  . Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain, basic",0
"using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , ( Barzilay and Lee , 2003 )  and ( #AUTHOR_TAG )  learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (Glickman and Dagan, 2003)  use clustering and",0
stemming from different news source. And ( #AUTHOR_TAG )  use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .</s>,0
"constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG )  . This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way. For",3
"using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , ( #AUTHOR_TAG )  and ( Shinyanma et al. , 2002 )  learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (Glickman and Dagan, 2003)  use clustering and",0
"To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG )  . Johnson et al., 2002)",5
"of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance , ( #AUTHOR_TAG )  acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .",0
"partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG )  . techniques.",3
"known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG )  ; and more recently , the much more sophisticated TSNLP ( Test",1
"The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 )  . However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.",1
"its degree of overgeneration (does it generate only the sentences of the described language?) While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG )  are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .",0
"The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 )  . However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.",1
"25 000 verbal expressions. In particular , ( #AUTHOR_TAG )  lists the converses of some 3 500 predicative nouns .</s>",3
"As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG )  thus ensuring an additional level of abstraction . The",5
". For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG )  can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions . In",3
"and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG )  . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a",0
"partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 )  . techniques.",3
", ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 )  describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG )  show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .</s>",0
Semantic construction proceeds from the derived tree ( #AUTHOR_TAG )  rather than -- as is more common in TAG -- from the derivation tree . This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.,0
"<s> ""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language. Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG )  describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 )  show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .</s>",0
"The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG )  . However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.",1
"For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG )  and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs . For complementing this database and for converse constructions",0
"In this work, we use the Arabic root extraction technique in (El Kourdi, 2004) . It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 )  , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .",4
"In this work, we use the Arabic root extraction technique in (El Kourdi, 2004) . It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG )  , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .",4
"Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG )  , minimum description length principal ( Lang , 1995 )  , and",0
"extraction algorithm used or to different data sets. This work is a continuation of that initiated in ( #AUTHOR_TAG )  , which reports an overall NB classification correctness of 75.6 %",2
"To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG )  . In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set",1
A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG )  . More,0
"In Arabic, however, the use of stems will not yield satisfactory categorization. This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG )  , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root . The infix form (or stem) needs further to be processed in order to obtain the root. This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998)",0
"istic. ( #AUTHOR_TAG )  has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand",0
"A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999) . More recently , ( #AUTHOR_TAG )  has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) ",0
"text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG )  .",0
"In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG )  . TF-IDF",1
". ( #AUTHOR_TAG )  proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance . Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .</s>",4
"Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 )  , minimum description length principal ( #AUTHOR_TAG )  , and the X2",0
"Many machine learning algorithms have been applied for many years to text categorization. include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG )  , ( Creecy and Masand , 1992 )  and ( Wiene and Pedersen , 1995 )  , respectively .</s>",0
"TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG )  . Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.</s>",0
"A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999) . More recently , ( Sebastiani , 2002 )  has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG )  , ( Crammer and Singer , 2003 )  , and ( Lewis et al. , 2004 )  .</s>",0
"<s> The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents. For example , ( #AUTHOR_TAG )  discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .</s>",0
"in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 )  , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 )  or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG  , for example ) .",1
"The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003)  and called TermoStat. The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG )  . Note that",5
"A number of applications have relied on distributional analysis (Harris, 1971)  in order to build classes of semantically related terms. This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG  , for example ) , does not specify the relationship itself . Hence,",0
"More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG  uses derivational morphology ; Grabar and Zweigenbaum ( 2002 )  use , as a starting point , a number of identical characters .</s>",0
user ). The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG  and called TER1vloSTAT . The ten most specic nouns have been produced by comparing our corpus,5
"<s> More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 )  uses derivational morphology ; #AUTHOR_TAG  use , as a starting point , a number of identical characters .</s>",0
"in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990) , on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG )  or, more frequently, on a combination of the two (Smadja, 1993;  Kilgarri and ",1
"ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG )  , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â ) of the elements one wants to acquire and their context. The contextual patterns produced can then be applied to",0
"However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG )  , i.e. methods that rely on the form of terms or on the information gathered from contexts . (In some cases, an additional resource, such as a",1
"<s> In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization. The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG )  .</s>",4
"<s> In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG )  .</s>",4
"ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG )  and called qualia relations ( Bouillon et al. , 2001 )  . Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs. However, the N-V",0
"work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval. Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG  with WoRDNET relations ) .</s>",1
"ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 )  and called qualia relations ( #AUTHOR_TAG )  . Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.",0
"The main benefits of this acquisition technique lie in the inferred patterns. Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG )  for a review ) , these patterns allow :</s>",0
"The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool. ASARES is presented in detail in ( #AUTHOR_TAG )  . We simply give a short account of its basic principles herein.</s>",5
"A number of applications have relied on distributional analysis ( #AUTHOR_TAG )  in order to build classes of semantically related terms . This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996,  for",0
"derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 )  .</s>",0
"has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 )  , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .",0
", we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG )  : we will call the instances of textual references to objects/abstractions mentions , which can be either named (",5
"words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG )  . For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.",1
"These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG  Arabic data . The experiments are performed on a clearly specified partition of the data, so",5
"Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG )  .</s>",5
"derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG )  .</s>",0
<s> The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG )  .</s>,1
"Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 )  . The formation of broken plurals is common, more complex and often irregular.",0
"has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG )  , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .",0
"The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG )  . One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.",0
"<s> the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG )  ( e.g. OrgGovernmental , FacilityPath , etc. ) .</s>",5
"has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 )  , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .",0
<s>#AUTHOR_TAG  demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation . A trigram language model was used to score and select among hypothesized segmentations determined by a set,5
described in . Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG )  . We formulate the mention detection task as a sequence classification problem. While this approach is,5
Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG )  and the coreference resolution system is similar to,1
"The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG )  . We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively. For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1 ,",0
"Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG )  . For Arabic, since words are morphologically derived from a list of roots (stems",5
"has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 )  , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .",0
"However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data. We seeked to exploit this ability to generalize to improve the dictionary based model. As in ( #AUTHOR_TAG )  , we used unsupervised training data which is automatically segmented to discover previously unseen stems . In our case,",5
"has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 )  , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .",0
"The system is trained on the Arabic ACE 2003 and part of the 2004 data. We introduce here a clearly defined and replicable split of the #AUTHOR_TAG  data , so that future investigations can accurately and correctly compare against the results presented here .</s>",5
"final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG )  based backoff language model .",5
"has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 )  , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .",0
"has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 )  , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .",0
". ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG )  for how ECM-F is computed ) ,",5
"Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG )  . The formation of broken plurals is common, more complex and often irregular.",0
"Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 )  and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG )  . Both systems are built",1
"features in the mention detection system. As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG )  where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc )",5
"where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG )  .</s>",5
"via email to participate in the experiment. Thus, they were not supervised during the experiment. #AUTHOR_TAG  observed that some annotators were not familiar with the exact definition of semantic relatedness . Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points. The manual was presented to the subjects before the experiment and",4
"Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction. It is defined on different kinds of textual units, e.g. documents, parts of a document (e.g. words and their surrounding context), words or concepts ( #AUTHOR_TAG ) . ",0
"In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG )  . Table",0
"<s> According to #AUTHOR_TAG  , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .</s>",0
"To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #AUTHOR_TAG  pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs . This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8). The plot clearly shows an empty horizontal band with no judgments. The connection between averaged judgments and standard deviation",1
"setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG  . Finkelstein et al. (2002)",0
plicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by #AUTHOR_TAG  with 10 subjects . Table 1</s>,0
"We extracted word pairs from three different domain-specific corpora (see Table 2). This is motivated by the aim to enable research in information retrieval incorporating SR measures. In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG )  systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .</s>",4
"Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( Lesk , 1986 )  , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 )  , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 )  or distributional ( #AUTHOR_TAG )  . The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.</s>",0
"Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG )  , the German equivalent to WordNet , as a sense inventory for each word .",5
"Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG )  .4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.</s>",0
"In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. #AUTHOR_TAG  reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.",1
"We used the revised experimental setup ( #AUTHOR_TAG )  , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs . We annotated semantic relatedness instead of similarity and included also non noun-noun pairs",5
"In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 )  and #AUTHOR_TAG  . We used the revised experimental setup",1
"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 )  , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG )  , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 )  or distributional ( Weeds and Weir , 2005 )  . The knowledge sources used for computing relatedness can be as different as",0
"3 Dissimilar words can be semantically related, e.g. via functional relationships (night -dark) or when they are antonyms (high -low). Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG )  .</s>",0
"In the seminal work by #AUTHOR_TAG  , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards . Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card. Miller and Charles (1991)  replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by ",0
"The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG )  or malapropism detection ( Budanitsky and Hirst , 2006 )  . ",0
"#AUTHOR_TAG  argue for application-specific evaluation of similarity measures , because measures are always used for some task . But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another. Application-based evaluation can",0
as is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG )  .</s>,5
"did not report inter-subject correlation for their larger dataset. #AUTHOR_TAG  reported a correlation of r = .69 . Test subjects were trained students of computational linguistics, and word pairs were selected analytically.</s>",1
"GermaNet contains only a few conceptual glosses. As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG )  . We removed words which had more than three senses.</s>",5
"of. Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations. #AUTHOR_TAG  pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .</s>",0
"isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006) . #AUTHOR_TAG  annotated a larger set of word pairs ( 353 ) , too . They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics",0
"The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG )  . The resulting list of POS-tagged lemmas",5
"Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( Lesk , 1986 )  , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 )  , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 )  or distributional ( Weeds and Weir , 2005 )  . The knowledge sources used for computing relatedness can be as different as",0
"Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( Lesk , 1986 )  , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 )  , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 )  or distributional ( Weeds and Weir , 2005 )  . The knowledge sources used for computing relatedness can be as different as",0
"Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( #AUTHOR_TAG )  , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 )  , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 )  or distributional ( Weeds and Weir , 2005 )  . The knowledge sources used for computing relatedness can be as different as",0
"If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG )  . 6 Pairs containing such words are not suitable for evaluation. To limit",0
"ness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car). Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG )  , because human annotators tend to select only highly related pairs connected by relations they are aware of . Automatic corpus-based selection of word pairs is more objective, leading to",0
"only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #AUTHOR_TAG  did not report inter-subject correlation for their larger dataset . Gurevych (2006)  reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically.</s>",1
". This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995)  reported a correlation of r=.9026. 10 #AUTHOR_TAG  reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . ",1
"and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #AUTHOR_TAG  replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German . She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in ",0
") may only be established by a certain group. Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG  .</s>",1
"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 )  , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 )  , information-based ( Resnik , 1995 ; #AUTHOR_TAG )  or distributional ( Weeds and Weir , 2005 )  . The knowledge sources used for computing relatedness can be as different as",0
"OS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAG b ) and shingling techniques described by Chakrabarti ( 2002 )  .</s>",3
"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 )  , KWiCFinder ( Fletcher , 2004 a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )  .</s>",0
"In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998  : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG )  .",0
"In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 )  . While such an approach promises much in terms of",0
"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 )  , KWiCFinder ( Fletcher , 2004 a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG )  .</s>",0
"of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 )  , KWiCFinder ( #AUTHOR_TAG a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )  .",0
"ck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG  , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 )  . Studies have used several different methods to mine web data.",0
"A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG )  . Tools based on static corpora do not suffer from this problem, e.g. BNCweb 7 , developed at the University of",0
"atic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 )  . Studies have used several different",0
"006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG )  . As the size of a corpus increases, a near linear",0
"In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG  : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 )  .",0
"ise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG )  , distributed virtual worlds ( Hughes et al. , 2005 )  and digital library management ( Walkerdine and Rayson , 2004 )  .",0
"infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG )  . Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems. Examples include SETI@",0
"ise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 )  , distributed virtual worlds ( Hughes et al. , 2005 )  and digital library management ( #AUTHOR_TAG )  .",0
"ck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 )  . Studies have used several different methods to mine web data.",0
"ck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG )  . Studies have used several different methods to mine web data.",0
Turney (2001)  extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. #AUTHOR_TAG  built a corpus by iteratively searching Google for a small set of seed terms .,0
"corpus collection to new document types. Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG )  .</s>",0
"web is used as a corpus (Kilgarriff and Grefenstette, 2003) . Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG )  . This topic generated intense interest at workshops held at the University of Heidelberg (October 2004),",0
"server. The Gsearch system ( #AUTHOR_TAG )  also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the",0
"Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse. Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG )  and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .</s>",0
"ck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 )  . Studies have used several different methods to mine web data.",0
"istics (Kilgarriff and Grefenstette, 2003) . Studies have used several different methods to mine web data. #AUTHOR_TAG  extracts word co-occurrence probabilities from unlabelled text collected from a web crawler . Baroni and Bernardini (2004)  built a corpus by iteratively searching Google for a small set of seed terms.",0
"is not sufficiently robust. Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAG a ) . Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.",0
"006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 )  . As the size of a corpus increases, a near linear",0
<s>#AUTHOR_TAG </s>,0
We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work. We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG )  . N,5
"space. In fact, they were used expecially in academic writing with some success. Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG )  .",0
"Following the example of #AUTHOR_TAG  , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by ",5
"least the foundation of the web (Berners-Lee, 1999) . For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG )  .",0
"model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004) . Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG )  .</s>",0
"'. On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG  . Wikipedia (2005) . Now personal wiki tools are arising for brainstorming and mind mapping. See Section 4 for further aspects.",0
"a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG )  . If nobody claims the document for himself, it will fall in the public domain",0
"The main source of Novelle are wikis and blogs. While wikis have spread from a detailed design ( #AUTHOR_TAG )  , unfortunately blogs have not been designed under a model .",0
"We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #AUTHOR_TAG )  .",5
"Generally speaking , we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG )  may bring to an unsatisfactory representation of the context . The only way to retrieve information is through a search engine or a calendar, i.e. the date of the 'post' -a lexia in the jargon of bloggers.</s>",0
"The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #AUTHOR_TAG )  .",0
"by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG )  . Nowadays the use",0
"1.1 Hypertext as a New Writing Space #AUTHOR_TAG  was the first scholar who stressed the impact of the digital revolution to the medium of writing. Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning. When Guten",0
"AJAX is not a technology in itself but a term that refers to the use of a group of technologies together, in particular Javascript and XML. In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #AUTHOR_TAG )  .</s>",0
"Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel. Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs. Every arc always has a definite direction , i.e. arcs are arrows ( #AUTHOR_TAG )  .</s>",0
<s> The Ruby on #AUTHOR_TAG  framework permits us to quickly develop web applications without rewriting common functions and classes .</s>,5
"its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself. Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG )  . Figure 1 shows the model.",0
"feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG )  emphasize annotation , comment , and strong editing . They give more power to readers, eventually filling the gap -the so-called active readers become authors as well. This situation could make",0
"a set of lexias, but every document is only the set of historical versions of the document itself. Generally, people avoid commenting, preferring to edit each document. The paradigm is ""write many , read many"" ( #AUTHOR_TAG )  .</s>",0
paradigm to create the graphical user interface. AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG )  .</s>,0
"A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German ( Brants et al. , 2002 )  and Portuguese ( #AUTHOR_TAG )  , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 )  and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more",1
"models for predicting the next parser action (Black et al., 1992) . • Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002) . â¢ Graph transformations for recovering nonprojective structures ( #AUTHOR_TAG )  .</s>",5
"A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German ( #AUTHOR_TAG )  and Portuguese ( Afonso et al. , 2002 )  , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 )  and Slovene ( DËzeros",1
<s> The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 )  and extended to labeled dependency parsing by #AUTHOR_TAG  .</s>,5
". Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 )  , Chinese ( Chen et al. , 2003 )  , Danish ( Kromann , 2003 )  , and Swedish ( #AUTHOR_TAG )  . Japanese (Kawata and Bartels, 2000)",0
"3-6. Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 )  , Chinese ( Chen et al. , 2003 )  , Danish ( Kromann , 2003 )  , and Swedish ( Nilsson et al. , 2005 )  . Japanese ",0
"-6. Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 )  , Chinese ( #AUTHOR_TAG )  , Danish ( Kromann , 2003 )  , and Swedish ( Nilsson et al. , 2005 )  . Japanese ",0
"• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006) . â¢ History-based feature models for predicting the next parser action ( #AUTHOR_TAG )  . • Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002) .",5
"<s> All experiments have been performed using MaltParser ( #AUTHOR_TAG )  , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1",5
"We use support vector machines to predict the next parser action from a feature vector representing the history. More specifically , we use LIBSVM ( #AUTHOR_TAG )  with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .",5
<s> 6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #AUTHOR_TAG )  .</s>,1
"<s> Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG  .</s>",0
"For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG )  . To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t.</s>",0
"3-6. Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG )  , Chinese ( Chen et al. , 2003 )  , Danish ( Kromann , 2003 )  , and Swedish ( Nilsson et al. , 2005 )  . Japanese ",0
"Japanese ( #AUTHOR_TAG )  , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .</s>",1
"• History-based feature models for predicting the next parser action (Black et al., 1992) . Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG )  . • Graph transformations for recovering nonprojective structures .",5
"length 2). By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG )  exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) . It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.</s>",1
"of semantics in various tasks. #AUTHOR_TAG  have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003)  in the same domain, we present a generative approach that attempts to",0
"value can still be extracted from them. For example , #AUTHOR_TAG  experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow",0
"MM states). #AUTHOR_TAG  describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space . Each sentence/vector is then mul- tiplied by this matrix, and new HMM models are re-computed from the projected data.</s>",5
". Such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG )  or summarization system ( McKeown et al. , 2003 )  . We chose to focus on randomized controlled trials because they",3
"Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #AUTHOR_TAG )  . However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.",0
"Following Ruch et al. ( 2003 )  and #AUTHOR_TAG  , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts . The four states in our HMMs correspond to the information that",5
"Meyer, 1990;Swales, 1990; Orȃsan, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG )  , information retrieval ( Tbahriti et al. , 2005 )  , information extraction ( Mizuta et al. , 2005 )  , and question answering .",0
"Using the section labels , the HMM was trained using the HTK toolkit ( #AUTHOR_TAG )  , which efficiently performs the forward-backward algorithm and BaumWelch estimation . For testing, we",5
"Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction""  , ""methods""  , ""results""  , and ""conclusions""  ( SalangerMeyer , 1990 ; #AUTHOR_TAG  ; OrËasan , 2001 ) . The ability to explicitly identify these sections in un-structured text could play an",0
"of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #AUTHOR_TAG )  . This technique provides two",1
"Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #AUTHOR_TAG ; Ng and Jordan , 2001 )  . However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.",0
", and the availability of software that leverages this knowledge -- MetaMap ( #AUTHOR_TAG )  for concept identification and SemRep ( Rindflesch and Fiszman , 2003 )  for relation extraction -- provide a foundation for studying the",0
"Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG )  . Our task is closer to the work of Teufel and Moens (2000) , who looked at the problem of intellectual attribution in scientific texts.</s>",1
"Meyer, 1990) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG )  . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (",0
"<s> Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985;Marcu and Echihabi, 2002) . Our task is closer to the work of #AUTHOR_TAG  , who looked at the problem of intellectual attribution in scientific texts .</s>",1
"performance. Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 )  or summarization system ( #AUTHOR_TAG )  . We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are",3
<s> Table (b) again reproduces the results from #AUTHOR_TAG  (2003) for a comparable task on a different subset of 206 unstructured abstracts.</s>,1
"Orȃsan, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 )  , information retrieval ( Tbahriti et al. , 2005 )  , information extraction ( #AUTHOR_TAG )  , and question answering .",0
"Meyer, 1990) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 )  . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (",0
"Meyer, 1990;Swales, 1990; Orȃsan, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 )  , information retrieval ( #AUTHOR_TAG )  , information extraction ( Mizuta et al. , 2005 )  , and question answering .",0
"The table also presents the closest comparable experimental results reported by #AUTHOR_TAG  .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE",1
"Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 )  . Our task is closer to the work of Teufel and Moens (2000) , who looked at the problem of intellectual attribution in scientific texts.</s>",1
abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #AUTHOR_TAG ) </s>,0
"in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG )  .",0
"Meyer, 1990) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 )  . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (",0
"and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 )  for concept identification and SemRep ( #AUTHOR_TAG )  for relation extraction -- provide a foundation for studying the role of semantics in various tasks .",0
"Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG  is a noteworthy point of reference and comparison . However, our study differs in several important respects. Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques. In contrast, because the",1
"using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG )  .</s>",0
"We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG )  , meant for primary school students . The text gradually introduces concepts related to precipitation, and explains them. Its nature makes it appropriate for the semantic analysis task in an incremental approach. The system will mimic the way in which a human reader accumulates knowledge and uses what was written before",5
"of the domain (Rosario and Hearst, 2001)  or the system (Gomez, 1998) . Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG )  ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 )  ) . L",0
"system builds a graph centered on the main element (often the head) of P . This idea was inspired by #AUTHOR_TAG  , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) .",4
"In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments. Most approaches rely on VerbNet ( #AUTHOR_TAG )  and FrameNet ( Baker et al. , 1998 )  to provide associations between verbs and semantic roles , that are then mapped onto the",0
"The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #AUTHOR_TAG a ) .</s>",4
"Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998) . In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG )  or the system ( Gomez , 1998 )  . Such systems extract information from some types of syntactic units (",0
"of the domain (Rosario and Hearst, 2001)  or the system (Gomez, 1998) . Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 )  ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 )  ) . L",0
"In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #AUTHOR_TAG )  . The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary.</s>",0
"Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998) . In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 )  or the system ( #AUTHOR_TAG )  . Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ;",0
is essential to see how elements of meaning are interconnected. This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 . He was a grammarian who analysed Sanskrit ( #AUTHOR_TAG )  . The idea resurfaced forcefully at several points in the,0
"instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 )  and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )  .</s>",0
"verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants -for example, agent or instrument -to such grammatical elements as subject, direct object, indirect object. This idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 )  .</s>",0
"instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 )  and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )  .</s>",0
"The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #AUTHOR_TAG )  . The parser, written in Prolog, implements a classic constituency English",5
"of the domain (Rosario and Hearst, 2001)  or the system (Gomez, 1998) . Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 )  ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG )  ) . L",0
". The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 )  . Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005) .</s>",0
". The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG )  . Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005) .</s>",0
"Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text. This design idea was adopted from TANKA ( #AUTHOR_TAG b ) . The only manually encoded knowledge is a",5
"of the domain (Rosario and Hearst, 2001)  or the system (Gomez, 1998) . Such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 )  ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 )  ) . L",0
Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG )  . In,0
"The list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAG a ) . Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena. The resulting list is the",5
"Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #AUTHOR_TAG )  , we tried to adapt the same approach to the German-English language pair . It turned out that there is a larger variety of long reordering patterns in this case.",4
". Future research should apply the work of #AUTHOR_TAG  and Blunsom and Osborne ( 2008 )  , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .</s>",3
". Future research should apply the work of Blunsom et al. ( 2008 )  and #AUTHOR_TAG  , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .</s>",3
"In our prior work ( #AUTHOR_TAG )  , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 )  could be tailored to our peer-review domain",2
utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ( Details of how the average-expert model performs can be found in our prior work ( #AUTHOR_TAG )  . )</s>,2
"We follow our previous work ( #AUTHOR_TAG )  in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 )  . Each word",2
"The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG )  . (Schmitt et al., 2009) . From",2
"until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 )  . There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993) .</s>",4
"Following #AUTHOR_TAG  , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document . Since, as",5
In this section we describe in detail the baseline NER system we use. It is inspired by the system described in #AUTHOR_TAG  . Because NER annotations are commonly not nested (for,4
"agmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG )  , which successfully utilized such language models to represent word window contexts of target words . However, we note that other richer types of language models,",4
"work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 )  ( Colton et al. , 2012 )  ( Jiang and Zhou , 2008 )  or song lyrics ( #AUTHOR_TAG )  ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.",1
"Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 )  and #AUTHOR_TAG  , which deal with automatic generation of classic fill in the blank questions . Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.",4
". Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG )  ( Colton et al. , 2012 )  ( Jiang and Zhou , 2008 )  or song lyrics ( Wu et al. , 2013 )  ( Ramakrishnan A et al.",1
"work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 )  ( Colton et al. , 2012 )  ( Jiang and Zhou , 2008 )  or song lyrics ( Wu et al. , 2013 )  ( Ramakrishnan #AUTHOR_TAG )  , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.",1
"of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 )  and create multiple features for length using a decision tree ( J48 ) . We use",2
"raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG )  and create multiple features for length using a decision tree ( J48 )",2
"The shape-based metric. The only disambiguation metric that we used in our previous work ( #AUTHOR_TAG b ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process",2
