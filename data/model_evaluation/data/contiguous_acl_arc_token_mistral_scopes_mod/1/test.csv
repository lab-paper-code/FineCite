token_context,word_context,seg_context,sent_cotext,label
"['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) ,#AUTHOR_TAG ,Carl ( 1999 ) , andBrown ( 2000 ) , inter alia']","['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) ,#AUTHOR_TAG ,Carl ( 1999 ) , andBrown ( 2000 ) , inter alia']","['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) ,#AUTHOR_TAG ,Carl ( 1999 ) , andBrown ( 2000 ) , inter alia']","['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) ,#AUTHOR_TAG ,Carl ( 1999 ) , andBrown ( 2000 ) , inter alia']",0
"['\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â']","['\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation']","['\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â']","['\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996']",0
"['we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX#AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .']","['we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX#AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .']","['we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX#AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .']","['translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX#AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .']",0
"['', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one( #AUTHOR_TAG )']","['', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one( #AUTHOR_TAG )']","['', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one( #AUTHOR_TAG )']","['', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one( #AUTHOR_TAG )']",0
"['', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', '']","['', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', '']","['', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', '']","['', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', '']",0
"['\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ;#AUTHOR_TAG ; Gough , Way , and Hearne 2002']","['monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ;#AUTHOR_TAG ; Gough , Way , and Hearne 2002']","['\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ;#AUTHOR_TAG ; Gough , Way , and Hearne 2002']","['\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ;#AUTHOR_TAG ; Gough , Way , and Hearne 2002']",0
"['\x80¢ language learning( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 )']","['\x80¢ language learning( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 )']","['\x80¢ language learning( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 )']","['\x80¢ language learning( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002']",0
"['are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described byFrederking and Nirenburg ( 1994 ) ,#AUTHOR_TAG , andHogan and Frederking ( 1998 )']","['are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described byFrederking and Nirenburg ( 1994 ) ,#AUTHOR_TAG , andHogan and Frederking ( 1998 )']","['are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described byFrederking and Nirenburg ( 1994 ) ,#AUTHOR_TAG , andHogan and Frederking ( 1998 )']","['are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described byFrederking and Nirenburg ( 1994 ) ,#AUTHOR_TAG , andHogan and Frederking ( 1998 )']",1
"['recently ,#AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , andCarl ( 2003  , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments']","['recently ,#AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , andCarl ( 2003  , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments,']","['recently ,#AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , andCarl ( 2003  , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments,']","['recently ,#AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , andCarl ( 2003  , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, and from there they suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those described in [Carl and Way 2003].']",0
"['a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', '']","['a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', '']","['such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', '']","['', '#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', '']",0
"[', 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', '']","[', 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', '']","[', 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', '']","[', 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.', '']",1
"['', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm( #AUTHOR_TAG )']","['', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm( #AUTHOR_TAG )']","['', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm( #AUTHOR_TAG )']","['', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm( #AUTHOR_TAG )']",3
"['', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ;#AUTHOR_TAG ; Carl 1999 ) .']","['', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ;#AUTHOR_TAG ; Carl 1999 ) .']","['', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ;#AUTHOR_TAG ; Carl 1999 ) .']","['', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ;#AUTHOR_TAG ; Carl 1999 ) .']",0
"['We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on#AUTHOR_TAG .', 'Grefen']","['here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on#AUTHOR_TAG .', 'Grefenstette']","['We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on#AUTHOR_TAG .', 'Grefen']","['problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on#AUTHOR_TAG .', '']",5
"['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', '']","['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', '']","['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', '']","['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', '']",0
"['¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization( #AUTHOR_TAG ) â\x80¢ insights into universal grammar ( Juola 1998 )']","['language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization( #AUTHOR_TAG ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way']","['¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization( #AUTHOR_TAG ) â\x80¢ insights into universal grammar ( Juola 1998 )']","['\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization( #AUTHOR_TAG ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002']",0
"['a final processing stage , we generalize over the marker lexicon following a process found in#AUTHOR_TAG .', 'In']","['a final processing stage , we generalize over the marker lexicon following a process found in#AUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", '']","['a final processing stage , we generalize over the marker lexicon following a process found in#AUTHOR_TAG .', 'In']","['a final processing stage , we generalize over the marker lexicon following a process found in#AUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", '']",5
"['', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on#AUTHOR_TAG to permit a limited form of insertion in the translation process .', '']","['', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on#AUTHOR_TAG to permit a limited form of insertion in the translation process .', '']","['', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on#AUTHOR_TAG to permit a limited form of insertion in the translation process .', '']","['', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on#AUTHOR_TAG to permit a limited form of insertion in the translation process .', '']",5
"['', 'In their Gaijin system ,#AUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals']","['', 'In their Gaijin system ,#AUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals']","['', 'In their Gaijin system ,#AUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals']","['', 'In their Gaijin system ,#AUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals']",1
"['\x80¢ language learning ( Green 1979 ;#AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 )']","['\x80¢ language learning ( Green 1979 ;#AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 )']","['\x80¢ language learning ( Green 1979 ;#AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 )']","['\x80¢ language learning ( Green 1979 ;#AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002']",0
"['\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction( #AUTHOR_TAG ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 )']","['\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction( #AUTHOR_TAG ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 )']","['\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction( #AUTHOR_TAG ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 )']","['\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction( #AUTHOR_TAG ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002']",0
"['36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu ,#AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', '']","['when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu ,#AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', '']","['36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu ,#AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', '']","['', 'For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu ,#AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', '']",0
"['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) ,#AUTHOR_TAG , andBrown ( 2000 ) , inter alia']","['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) ,#AUTHOR_TAG , andBrown ( 2000 ) , inter alia']","['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) ,#AUTHOR_TAG , andBrown ( 2000 ) , inter alia']","['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) ,#AUTHOR_TAG , andBrown ( 2000 ) , inter alia']",0
"['xicon is segmented using the marker hypothesis to produce a marker lexicon.', 'This is then generalized , following a methodology based on#AUTHOR_TAG , to generate the `` generalized marker lexicon .']","['', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', 'This is then generalized , following a methodology based on#AUTHOR_TAG , to generate the `` generalized marker lexicon .']","['', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', 'This is then generalized , following a methodology based on#AUTHOR_TAG , to generate the `` generalized marker lexicon .']","['', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', ""This is then generalized , following a methodology based on#AUTHOR_TAG , to generate the `` generalized marker lexicon . ''"", '']",5
"['osavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation( #AUTHOR_TAG ) â\x80¢ Localization ( Sch Â¨ aler 1996']","['\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation( #AUTHOR_TAG ) â\x80¢ Localization ( Sch Â¨ aler 1996']","['osavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation( #AUTHOR_TAG ) â\x80¢ Localization ( Sch Â¨ aler 1996']","['\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation( #AUTHOR_TAG ) â\x80¢ Localization ( Sch Â¨ aler 1996']",0
"[""set of translations is stored separately , and for each set the `` marker hypothesis ''( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes']","[""set of translations is stored separately , and for each set the `` marker hypothesis ''( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.', '']","[""set of translations is stored separately , and for each set the `` marker hypothesis ''( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes']","[""set of translations is stored separately , and for each set the `` marker hypothesis ''( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.', '']",5
"['is, where#AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the']","['is, where#AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the']","['is, where#AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by']","['is, where#AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme.', '']",1
"['of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless ,#AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is']","['of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless ,#AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is']","[', marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless ,#AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment,']","['', 'Juola\'s (1994 Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless ,#AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', '']",0
"['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) ,Carl ( 1999 ) , and#AUTHOR_TAG , inter alia']","['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) ,Carl ( 1999 ) , and#AUTHOR_TAG , inter alia']","['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) ,Carl ( 1999 ) , and#AUTHOR_TAG , inter alia']","['', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) ,Carl ( 1999 ) , and#AUTHOR_TAG , inter alia']",0
"['', 'More specifically , the notion of the phrasal lexicon ( used first by#AUTHOR_TAG ) has been used successfully in a number of areas']","['', 'More specifically , the notion of the phrasal lexicon ( used first by#AUTHOR_TAG ) has been used successfully in a number of areas']","['', 'More specifically , the notion of the phrasal lexicon ( used first by#AUTHOR_TAG ) has been used successfully in a number of areas']","['', 'More specifically , the notion of the phrasal lexicon ( used first by#AUTHOR_TAG ) has been used successfully in a number of areas']",0
"['EBMT systems , from the initial proposal by#AUTHOR_TAG to the recent collection ofCarl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', '']","['EBMT systems , from the initial proposal by#AUTHOR_TAG to the recent collection ofCarl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', '']","['EBMT systems , from the initial proposal by#AUTHOR_TAG to the recent collection ofCarl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', '']","['EBMT systems , from the initial proposal by#AUTHOR_TAG to the recent collection ofCarl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', '']",0
"['to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include']","['to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include']","['to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include']","['', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Güvenir (1996), McTait and Trujillo (1999) ,Carl (1999) , andBrown (2000 , inter alia']",0
"['V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. ,#AUTHOR_TAG ) .', '']","['V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. ,#AUTHOR_TAG ) .', '']","['V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. ,#AUTHOR_TAG ) .', '']","['V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. ,#AUTHOR_TAG ) .', '']",0
"['', 'The work of#AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', '']","['', 'The work of#AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', '']","['', 'The work of#AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', '']","['', 'The work of#AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', '']",0
"['notating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill ']","['annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill']","['notating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Br']","['selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 )']",0
"['', 'Another technique for making better use of unlabeled data is cotraining( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .', '']","['', 'Another technique for making better use of unlabeled data is cotraining( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .', '']","['', 'Another technique for making better use of unlabeled data is cotraining( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .', '']","['', 'Another technique for making better use of unlabeled data is cotraining( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .', '']",0
"[""domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although']","[""domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although']","["", we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are']","['', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", '']",5
"['.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '']","['it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '']","['', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '']","['', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '']",0
"['ing ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 )']","['base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 )']","['base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 )']","['', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 )']",0
"['the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the']","['the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the']","['the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the']","['the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the']",5
"['', 'Our algorithm is similar to the approach taken by#AUTHOR_TAG for inducing PCFG parsers']","['', 'Our algorithm is similar to the approach taken by#AUTHOR_TAG for inducing PCFG parsers']","['', 'Our algorithm is similar to the approach taken by#AUTHOR_TAG for inducing PCFG parsers']","['', 'Our algorithm is similar to the approach taken by#AUTHOR_TAG for inducing PCFG parsers']",1
"['learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data( #AUTHOR_TAG ) .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santor']","['learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data( #AUTHOR_TAG ) .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini,']","['learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data( #AUTHOR_TAG ) .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini,']","['learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data( #AUTHOR_TAG ) .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).', '']",0
"[', sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 200']","['in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 )']","['in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( F']","['selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 )']",0
"[""domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ;#AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are']","[""domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ;#AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers,']","["", we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ;#AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are']","['', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ;#AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.', '']",5
"['', '#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', '']","['', '#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', '']","['', '#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', '']","['', '#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', '']",0
"[').', 'Current state-of-the-art statistical parsers( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', '']","['it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992).', 'Current state-of-the-art statistical parsers( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', '']","['it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992).', 'Current state-of-the-art statistical parsers( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', '']","['learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992).', 'Current state-of-the-art statistical parsers( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', '']",0
"['or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '']","['or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '']","['to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '']","['common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '']",0
"['the first experiment , we use an induction algorithm( #AUTHOR_TAG a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', 'The']","['the first experiment , we use an induction algorithm( #AUTHOR_TAG a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', 'The']","['the first experiment , we use an induction algorithm( #AUTHOR_TAG a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', 'The algorithm']","['the first experiment , we use an induction algorithm( #AUTHOR_TAG a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', '']",5
"['of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example( #AUTHOR_TAG ) .', 'The underlying assumption is that an uncertain output is likely to be wrong']","['of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example( #AUTHOR_TAG ) .', 'The underlying assumption is that an uncertain output is likely to be wrong']","['of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example( #AUTHOR_TAG ) .', 'The underlying assumption is that an uncertain output is likely to be wrong']","['of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example( #AUTHOR_TAG ) .', 'The underlying assumption is that an uncertain output is likely to be wrong']",0
"['', 'Similar approaches are being explored for parsing ( Steedman ,#AUTHOR_TAG ; Hwa et al. 2003 )']","['', 'Similar approaches are being explored for parsing ( Steedman ,#AUTHOR_TAG ; Hwa et al. 2003 )']","['', 'Similar approaches are being explored for parsing ( Steedman ,#AUTHOR_TAG ; Hwa et al. 2003 )']","['', 'Similar approaches are being explored for parsing ( Steedman ,#AUTHOR_TAG ; Hwa et al. 2003 )']",0
"['in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by#AUTHOR_TAG .', '']","['in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by#AUTHOR_TAG .', '']","['Collins-Brooks PP-attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by#AUTHOR_TAG .', '']","['Collins-Brooks PP-attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by#AUTHOR_TAG .', '']",5
"['', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model( #AUTHOR_TAG ) .', '']","['', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model( #AUTHOR_TAG ) .', '']","['', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model( #AUTHOR_TAG ) .', '']","['', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model( #AUTHOR_TAG ) .', '']",0
"['', 'We follow the notation convention of#AUTHOR_TAG']","['', 'We follow the notation convention of#AUTHOR_TAG']","['', 'We follow the notation convention of#AUTHOR_TAG']","['', 'We follow the notation convention of#AUTHOR_TAG']",5
"['hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ;#AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical']","['hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ;#AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical']","['traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ;#AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical']","['traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ;#AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical']",0
"['', 'The largest lexical evaluation we know of is that of Schulte imWalde (2002 b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3']","['', 'The largest lexical evaluation we know of is that of Schulte imWalde (2002 b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3']","['', 'The largest lexical evaluation we know of is that of Schulte imWalde (2002 b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3']","['', 'The largest lexical evaluation we know of is that of Schulte imWalde (2002 b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3']",0
"['', '#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', '']","['', '#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', '']","['', '#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', '']","['', '#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', 'Precision was quite high (95%), but recall was low (84%).', '']",1
"['extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of#AUTHOR_TAG andCollins ( 1997 )']","['extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of#AUTHOR_TAG andCollins ( 1997 )']","['', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of#AUTHOR_TAG andCollins ( 1997 ) .', '']","['', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of#AUTHOR_TAG andCollins ( 1997 ) .', '']",0
"['; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [#AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and']","['; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [#AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and']","['; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [#AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [#AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information']",0
"['many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) ,#AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'L']","['many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) ,#AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', '']","['many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) ,#AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'L']","['many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) ,#AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'LFG argues that subcategorization requirements are best stated at the f-structure level, in functional rather than phrasal terms.', 'This is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives of phrase structural position.', '']",0
"['', '#AUTHOR_TAG attempts to improve on the approach ofBrent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted']","['frames.', '#AUTHOR_TAG attempts to improve on the approach ofBrent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted']","['', '#AUTHOR_TAG attempts to improve on the approach ofBrent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted']","['', '#AUTHOR_TAG attempts to improve on the approach ofBrent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', '']",0
"['lying an absolute threshold of five occurrences, we still generate 162 frame types from Penn-II and 221 from Penn-III.', '#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames']","['lying an absolute threshold of five occurrences, we still generate 162 frame types from Penn-II and 221 from Penn-III.', '#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames']","['lying an absolute threshold of five occurrences, we still generate 162 frame types from Penn-II and 221 from Penn-III.', '#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames']","['lying an absolute threshold of five occurrences, we still generate 162 frame types from Penn-II and 221 from Penn-III.', '#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames']",0
"['', 'Unlike our approach , those of#AUTHOR_TAG and Hockenmaier , Bierner , andBaldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .', '']","['', 'Unlike our approach , those of#AUTHOR_TAG and Hockenmaier , Bierner , andBaldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .', '']","['', 'Unlike our approach , those of#AUTHOR_TAG and Hockenmaier , Bierner , andBaldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .', '']","['', 'Unlike our approach , those of#AUTHOR_TAG and Hockenmaier , Bierner , andBaldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .', '']",1
"['ide from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', '']","['ide from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', '']","['ide from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', '']","['ide from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', '']",0
"['', '#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames']","['', '#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames']","['', '#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames']","['', '#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames']",0
"['have also applied our more general unification grammar acquisition methodology to the TIGER Treebank( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', 'The']","['have also applied our more general unification grammar acquisition methodology to the TIGER Treebank( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', 'The']","['have also applied our more general unification grammar acquisition methodology to the TIGER Treebank( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', '']","['have also applied our more general unification grammar acquisition methodology to the TIGER Treebank( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', '']",5
['subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs'],['subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs'],['subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs'],['subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs'],0
"['ical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ;#AUTHOR_TAG ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C']","['ical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ;#AUTHOR_TAG ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '']","['ical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ;#AUTHOR_TAG ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C']","['ical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ;#AUTHOR_TAG ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '']",0
"['ical functional grammar ( Kaplan and Bresnan 1982 ;#AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits']","['ical functional grammar ( Kaplan and Bresnan 1982 ;#AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '']","['ical functional grammar ( Kaplan and Bresnan 1982 ;#AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C']","['ical functional grammar ( Kaplan and Bresnan 1982 ;#AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '']",0
"['', 'According to#AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list']","['', 'According to#AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list']","['', 'According to#AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list']","['', 'According to#AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', '']",0
"['.', '#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', '']","['3.', '#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', '']","['', '#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', '']","['', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.', '#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that of Schulte imWalde (2002 b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).', '']",1
"['neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause( #AUTHOR_TAG ) .', '']","['neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause( #AUTHOR_TAG ) .', '']","['neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause( #AUTHOR_TAG ) .', '']","['syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause( #AUTHOR_TAG ) .', '']",0
"['.', ""The extraction procedure utilizes a head percolation table as introduced by#AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a']","['', ""The extraction procedure utilizes a head percolation table as introduced by#AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a']","['.', ""The extraction procedure utilizes a head percolation table as introduced by#AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a set of lex']","['', ""The extraction procedure utilizes a head percolation table as introduced by#AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", '']",0
"['', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following#AUTHOR_TAG .', '']","['', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following#AUTHOR_TAG .', '']","['', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following#AUTHOR_TAG .', '']","['', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following#AUTHOR_TAG .', '']",0
['applied lexical-redundancy rules( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts:'],['applied lexical-redundancy rules( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts:'],['applied lexical-redundancy rules( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts:'],"['applied lexical-redundancy rules( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.', '']",5
"['we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in#AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-struct']","['we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in#AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-struct']","['we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in#AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-struct']","['order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in#AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-struct']",5
"['', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section']","['', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature,']","['', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature,']","['', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry.', 'Some, but not all, of these differences are captured by automatic conversion software.', '']",0
['specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on#AUTHOR_TAG andCollins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct'],['specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on#AUTHOR_TAG andCollins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct'],['specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on#AUTHOR_TAG andCollins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in'],"['', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on#AUTHOR_TAG andCollins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', '']",5
"[', and combinatory categorial grammar [ CCG ] [#AUTHOR_TAG ] ) , the']","[', and combinatory categorial grammar [ CCG ] [#AUTHOR_TAG ] ) , the lexicon is the central']","[', and combinatory categorial grammar [ CCG ] [#AUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information']",['#AUTHOR_TAG'],0
"['', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ;#AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', '']","['', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ;#AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', '']","['', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ;#AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', '']","['', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ;#AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', '']",0
['Recent work by#AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .'],"['', '. Recent work by#AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .Briscoe and Carroll (1997)']",['Recent work by#AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .Briscoe and Carroll (1997)'],"['', '. Recent work by#AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .Briscoe and Carroll (1997) use a handwritten head-lexicalized, context', '']",0
"['rate of accession may also be represented graphically.', 'In Charniak ( 1996 ) and#AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of']","['rate of accession may also be represented graphically.', 'In Charniak ( 1996 ) and#AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the']","['rate of accession may also be represented graphically.', 'In Charniak ( 1996 ) and#AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of']","['rate of accession may also be represented graphically.', 'In Charniak ( 1996 ) and#AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', '']",0
"['ually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based onMagerman ( 1994 ) and#AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and']","['', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based onMagerman ( 1994 ) and#AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and']","['ually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based onMagerman ( 1994 ) and#AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and']","['andNakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based onMagerman ( 1994 ) and#AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', '']",5
"['', '#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arg']","['', '#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were']","['', '#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arg']","['', '#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', '']",0
"['or more).', '#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', '']","['or more).', '#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', '']","['or more).', '#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', '']","['', '#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', '']",0
"['', '#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches ofMagerman (1994) andCollins (1997) .', '']","['', '#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches ofMagerman (1994) andCollins (1997) .', '']","['', '#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches ofMagerman (1994) andCollins (1997) .', 'Then the elementary trees are read off in']","['', '#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches ofMagerman (1994) andCollins (1997) .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', '']",0
"['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [#AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CC']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [#AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [#AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [#AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information']",0
"['.', 'It has been shown( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .', '']","['humor.', 'It has been shown( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .', '']","['.', 'It has been shown( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .', '']","['', 'It has been shown( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .', '']",4
"['ide from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ;#AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', '']","['ide from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ;#AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', '']","['ide from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ;#AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', '']","['ide from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ;#AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', '']",0
"['163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames']","['predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames']","['', 'Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames']","['', 'Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .', '']",0
"['', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of#AUTHOR_TAG is used .', '']","['', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of#AUTHOR_TAG is used .', '']","['', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of#AUTHOR_TAG is used .', 'The head is annotated with the LFG equation ↑=↓.', '']","['', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of#AUTHOR_TAG is used .', 'The head is annotated with the LFG equation ↑=↓.', '']",5
"['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ;#AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and comb']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ;#AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ;#AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and comb']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ;#AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information']",0
"['more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences']","['more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences']","['more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences']","['', 'However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences']",0
"['xical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [#AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CC']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [#AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG']","['xical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [#AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [#AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information']",0
"['of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , andBaldridge ( 2002 ) ,#AUTHOR_TAG , and Miyao , Ninomiya , andTsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lex']","['of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , andBaldridge ( 2002 ) ,#AUTHOR_TAG , and Miyao , Ninomiya , andTsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon']","['of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , andBaldridge ( 2002 ) ,#AUTHOR_TAG , and Miyao , Ninomiya , andTsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon']","['', 'This can be expressed as a measure of the coverage of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , andBaldridge ( 2002 ) ,#AUTHOR_TAG , and Miyao , Ninomiya , andTsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon from Section 23.', '']",5
"['', '#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '']","['', '#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '']","['', '#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '']","['', '#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '']",0
"['ical functional grammar( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits']","['ical functional grammar( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '']","['ical functional grammar( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C']","['ical functional grammar( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '']",0
"['', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame']","['', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', '']","['', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', '']","['', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', '']",0
"['', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches ofMagerman ( 1994 ) and#AUTHOR_TAG .', '']","['Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches ofMagerman ( 1994 ) and#AUTHOR_TAG .', '']","['', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches ofMagerman ( 1994 ) and#AUTHOR_TAG .', '']","['', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches ofMagerman ( 1994 ) and#AUTHOR_TAG .', '']",0
"['', '#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', '']","['details.', '#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', '']","['', '#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', '']","['', '#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', '']",4
"['', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank( #AUTHOR_TAG )']","['', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank( #AUTHOR_TAG )']","['', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank( #AUTHOR_TAG )']","['', 'Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX.', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank( #AUTHOR_TAG )']",3
"['drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX']","['drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX']","['drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX']","['drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .', '']",4
"['', '#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', 'The extraction procedure utilizes a head percolation table as introduced by']","['the extraction of a lexicon essentially amounts to the creation of a grammar.', '#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', 'The extraction procedure utilizes a head percolation table as introduced by']","['the extraction of a lexicon essentially amounts to the creation of a grammar.', '#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', 'The extraction procedure utilizes a head percolation table as introduced by']","['', '#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', ""The extraction procedure utilizes a head percolation table as introduced byMagerman (1995) in combination with a variation ofCollins's (1997) approach to the differentiation between complement and adjunct."", '']",0
"['rate of accession may also be represented graphically.', 'In#AUTHOR_TAG andKrotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were']","['rate of accession may also be represented graphically.', 'In#AUTHOR_TAG andKrotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were']","['rate of accession may also be represented graphically.', 'In#AUTHOR_TAG andKrotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were']","['rate of accession may also be represented graphically.', 'In#AUTHOR_TAG andKrotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', '']",0
"['', '#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue']","['genre (Carroll and Rooth 1998).', '#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue']","['roll and Rooth 1998).', '#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue']","['', '#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue']",4
"['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ;#AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ;#AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ;#AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades']","['modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ;#AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information']",0
"['', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', '']","['', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', '']","['', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', '']","['', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', '']",0
"['', 'As a generalization ,#AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', '']","['', 'As a generalization ,#AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', '']","['', 'As a generalization ,#AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', '']","['', 'As a generalization ,#AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', '']",0
"['believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is#AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', '']","['believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is#AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', '']","['believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is#AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', '']","['believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is#AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', '']",1
"['aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', '#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995).']","['aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', '#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995).']","['aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', '#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6']","['sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', '#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', '']",0
"['ed in this way , gradable adjectives are an extreme example of the ""efficiency of language""( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations']","['ed in this way , gradable adjectives are an extreme example of the ""efficiency of language""( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations']","['ed in this way , gradable adjectives are an extreme example of the ""efficiency of language""( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations']","['ed in this way , gradable adjectives are an extreme example of the ""efficiency of language""( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations']",1
"[', but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX ,#AUTHOR_TAG ) .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about']","[', but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX ,#AUTHOR_TAG ) .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about']","[', but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX ,#AUTHOR_TAG ) .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3),']","[', what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX ,#AUTHOR_TAG ) .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large.', '']",0
"['ases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. ,#AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such']","['ases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. ,#AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such']","['ases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. ,#AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such']","['ases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. ,#AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such']",0
"['area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. ,#AUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '']","['area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. ,#AUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '']","['area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. ,#AUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '']","['area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. ,#AUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '']",0
"['3 The degree of precision of the measurement( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size']","['3 The degree of precision of the measurement( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size']","['3 The degree of precision of the measurement( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size']","['3 The degree of precision of the measurement( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size']",0
"['recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ;#AUTHOR_TAG ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '']","['recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ;#AUTHOR_TAG ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '']","['recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ;#AUTHOR_TAG ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '']","['recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ;#AUTHOR_TAG ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '']",1
"['and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox ( e.g. ,#AUTHOR_TAG )']","['and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox ( e.g. ,#AUTHOR_TAG )']","['and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox ( e.g. ,#AUTHOR_TAG )']","['', 'This is the strongest version of the sorites paradox ( e.g. ,#AUTHOR_TAG )']",0
"['will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 ,#AUTHOR_TAG ) .', '']","['will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 ,#AUTHOR_TAG ) .', '']","['will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 ,#AUTHOR_TAG ) .', '']","['will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 ,#AUTHOR_TAG ) .', '']",0
"[', as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', '']","['input, as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', '']","['as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', '']","['', 'We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', '']",0
"[') width < x.', 'Which of these should come first?', '#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', '']","['< x, (c) width > x, and (d) width < x.', 'Which of these should come first?', '#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', '']","[') width < x.', 'Which of these should come first?', '#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', '']","['', 'Which of these should come first?', '#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', '']",0
"['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ;#AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '(W']","['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ;#AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '(Wildly']","['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ;#AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '(W']","['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ;#AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '']",0
"[""BestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level ,#AUTHOR_TAG ) .', 'IA Pl']","[""BestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level ,#AUTHOR_TAG ) .', 'IA Plur']","[""BestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level ,#AUTHOR_TAG ) .', 'IA Plur can refer to']","[""BestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level ,#AUTHOR_TAG ) .', '']",0
"['A New Perspective on Salience.', ""#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", '']","['A New Perspective on Salience.', ""#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", '']","['1 A New Perspective on Salience.', ""#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", '']","['', '9.4.1 A New Perspective on Salience.', ""#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.', '']",0
"['', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month( #AUTHOR_TAG ) .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when']","['', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month( #AUTHOR_TAG ) .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when']","['', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month( #AUTHOR_TAG ) .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are']","['', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month( #AUTHOR_TAG ) .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible']",0
"['(cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '']","['(cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '']","['(cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '']","[', what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '']",0
"['will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects( #AUTHOR_TAG , Krahmer and Theune 2002 ) .', '']","['will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects( #AUTHOR_TAG , Krahmer and Theune 2002 ) .', '']","['will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects( #AUTHOR_TAG , Krahmer and Theune 2002 ) .', '']","['will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects( #AUTHOR_TAG , Krahmer and Theune 2002 ) .', '']",0
"['; also reported in#AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .', '']","['; also reported in#AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .', '']","['; also reported in#AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .', '']","['', 'Hermann and Deutsch ( 1976  ; also reported in#AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .', '']",0
"['account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ;#AUTHOR_TAG ) .', '']","['account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ;#AUTHOR_TAG ) .', '']","['account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ;#AUTHOR_TAG ) .', '']","['account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ;#AUTHOR_TAG ) .', '']",0
"['we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.', 'Following#AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as']","['we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.', 'Following#AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as']","['we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.', 'Following#AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as']","['', 'Following#AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.', '']",5
"['', 'A similar problem is discussed in the psycholinguistics of interpretation( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague']","['', 'A similar problem is discussed in the psycholinguistics of interpretation( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague']","['', 'A similar problem is discussed in the psycholinguistics of interpretation( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions']","['', 'A similar problem is discussed in the psycholinguistics of interpretation( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .', '']",1
['may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [#AUTHOR_TAG ] Chapter'],"['may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [#AUTHOR_TAG ] Chapter 8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since']","['may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [#AUTHOR_TAG ] Chapter 8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since it is the']",['#AUTHOR_TAG'],0
"['sense ( as well as the Gricean maxims ;#AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', '']","['sense ( as well as the Gricean maxims ;#AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', '']","['sense ( as well as the Gricean maxims ;#AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', '']","['sense ( as well as the Gricean maxims ;#AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', '']",0
"['4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence( #AUTHOR_TAG )']","['4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence( #AUTHOR_TAG )']","['-size), and the algorithm of Section 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence( #AUTHOR_TAG )']","['', 'If there exists a formula for mapping three dimensions into one (e.g., length × width × height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence( #AUTHOR_TAG )']",0
"['', 'For some adjectives , including the ones that#AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989']","['', 'For some adjectives , including the ones that#AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989']","['', 'For some adjectives , including the ones that#AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989']","['', 'For some adjectives , including the ones that#AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989']",0
"['imality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles( #AUTHOR_TAG )']","['imality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles( #AUTHOR_TAG )']","['imality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles( #AUTHOR_TAG )']","['imality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles( #AUTHOR_TAG )']",0
"['G resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available( #AUTHOR_TAG ) .', '']","['the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available( #AUTHOR_TAG ) .', '']","['G resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available( #AUTHOR_TAG ) .', '']","['the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available( #AUTHOR_TAG ) .', '']",4
"['', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm( #AUTHOR_TAG , Section 8.6.2 )']","['', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm( #AUTHOR_TAG , Section 8.6.2 )']","['', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm( #AUTHOR_TAG , Section 8.6.2 )']",['( #AUTHOR_TAG'],0
"['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ;#AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates']","['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ;#AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates']","['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ;#AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '(']","['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ;#AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '']",0
"['9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. ,#AUTHOR_TAG ) .', '']","['have to decide whether to say the patients that are old or the patients that are not young.', '9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. ,#AUTHOR_TAG ) .', '']","['9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. ,#AUTHOR_TAG ) .', '']","['', 'For example, the generator will have to decide whether to say the patients that are old or the patients that are not young.', '9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. ,#AUTHOR_TAG ) .', '']",0
"['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ;#AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it']","['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ;#AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it']","['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ;#AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which']","['IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ;#AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '']",0
"['specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see#AUTHOR_TAG 1999, discussed in Section 7.2).', '']","['specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see#AUTHOR_TAG 1999, discussed in Section 7.2).', '']","['For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see#AUTHOR_TAG 1999, discussed in Section 7.2).', '']","['', '(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see#AUTHOR_TAG 1999, discussed in Section 7.2).', '']",0
"['G has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ;#AUTHOR_TAG ) : The selected expression should also be felicitous .', '']","['G has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ;#AUTHOR_TAG ) : The selected expression should also be felicitous .', '']","['G has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ;#AUTHOR_TAG ) : The selected expression should also be felicitous .', '']","['G has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ;#AUTHOR_TAG ) : The selected expression should also be felicitous .', '']",0
"['area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ;#AUTHOR_TAG ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others']","['area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ;#AUTHOR_TAG ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '']","['area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ;#AUTHOR_TAG ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others']","['area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ;#AUTHOR_TAG ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '']",0
"['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX ,#AUTHOR_TAG ) .', 'VAGUE uses both of these devices']","['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX ,#AUTHOR_TAG ) .', 'VAGUE uses both of these devices']","['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX ,#AUTHOR_TAG ) .', 'VAGUE uses both of these devices']","['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX ,#AUTHOR_TAG ) .', 'VAGUE uses both of these devices']",0
"['', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX#AUTHOR_TAG ; Thorisson 1994 , for other plans )']","['', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX#AUTHOR_TAG ; Thorisson 1994 , for other plans )']","['', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX#AUTHOR_TAG ; Thorisson 1994 , for other plans )']","['', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX#AUTHOR_TAG ; Thorisson 1994 , for other plans )']",0
"['numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ;#AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '']","['numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ;#AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '']","['numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ;#AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '']","['', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ;#AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '']",0
"['', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX ,#AUTHOR_TAG ) .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", '']","['', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX ,#AUTHOR_TAG ) .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", '']","['', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX ,#AUTHOR_TAG ) .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", '']","['', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX ,#AUTHOR_TAG ) .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", '']",0
"['generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX ,#AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .', '']","['generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX ,#AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .', '']","['generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX ,#AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .', '']","['generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX ,#AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .', '']",0
"['', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans )']","['', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans )']","['', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans )']","['', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans )']",0
"['inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. ,#AUTHOR_TAG ) .', 'An example of such an inference rule is the one that transforms a list of the form']","['inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. ,#AUTHOR_TAG ) .', 'An example of such an inference rule is the one that transforms a list of the form']","['inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. ,#AUTHOR_TAG ) .', 'An example of such an inference rule is the one that transforms a list of the form mouse, >10 cm into one of']","['inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. ,#AUTHOR_TAG ) .', '']",0
"['', 'A more flexible approach is used by#AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', '']","['fixed boundary values instead.', 'A more flexible approach is used by#AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', '']","['', 'A more flexible approach is used by#AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', '']","['NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead.', 'A more flexible approach is used by#AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', '']",0
"['recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '']","['recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '']","['recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '']","['recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '']",1
"['IA is generally thought to be consistent with findings on human language production( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates']","['IA is generally thought to be consistent with findings on human language production( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates']","['IA is generally thought to be consistent with findings on human language production( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '']","['IA is generally thought to be consistent with findings on human language production( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '']",0
"['', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension( #AUTHOR_TAG , pages 10 -- 12 ) .', '(']","['', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension( #AUTHOR_TAG , pages 10 -- 12 ) .', '']","['', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension( #AUTHOR_TAG , pages 10 -- 12 ) .', '(']","['', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension( #AUTHOR_TAG , pages 10 -- 12 ) .', '']",0
"['.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene']","['they are essentially no more than comparisons between objects.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene']","['.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene would contain three distractors: (']","['', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', '']",0
"['has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process( #AUTHOR_TAG ) .', '']","['has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process( #AUTHOR_TAG ) .', '']","['has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process( #AUTHOR_TAG ) .', '']","['has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process( #AUTHOR_TAG ) .', '']",1
"['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions( #AUTHOR_TAG ; also Section 8.1 of the present article )']","['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions( #AUTHOR_TAG ; also Section 8.1 of the present article )']","['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions( #AUTHOR_TAG ; also Section 8.1 of the present article )']","['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions( #AUTHOR_TAG ; also Section 8.1 of the present article )']",0
"['', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account( #AUTHOR_TAG ; see our Section 2 )""]","['', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account( #AUTHOR_TAG ; see our Section 2 )""]","['', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account( #AUTHOR_TAG ; see our Section 2 )""]","['', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account( #AUTHOR_TAG ; see our Section 2 )""]",0
"['second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care( #AUTHOR_TAG )']","['second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care( #AUTHOR_TAG )']","['second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care( #AUTHOR_TAG )']","['second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care( #AUTHOR_TAG )']",0
"['Average Precision (MAP) is the average of precision values after each relevant document is retrieved( #AUTHOR_TAG ) .', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall']","['Average Precision (MAP) is the average of precision values after each relevant document is retrieved( #AUTHOR_TAG ) .', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall']","['Average Precision (MAP) is the average of precision values after each relevant document is retrieved( #AUTHOR_TAG ) .', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall']","['Average Precision (MAP) is the average of precision values after each relevant document is retrieved( #AUTHOR_TAG ) .', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall']",5
"['', 'Although originally developed as a tool to assist in query formulation ,#AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', '']","['', 'Although originally developed as a tool to assist in query formulation ,#AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', 'PICO-based querying in information retrieval']","['', 'Although originally developed as a tool to assist in query formulation ,#AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', '']","['', 'Although originally developed as a tool to assist in query formulation ,#AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', '']",0
"['.', 'We first identified the most informative unigrams and bigrams using the information gain measure( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', '']","['features.', 'We first identified the most informative unigrams and bigrams using the information gain measure( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', '']","['.', 'We first identified the most informative unigrams and bigrams using the information gain measure( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', '']","['', 'We first identified the most informative unigrams and bigrams using the information gain measure( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', '']",5
"['addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ;#AUTHOR_TAG ) ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the']","['addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ;#AUTHOR_TAG ) ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the']","['addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ;#AUTHOR_TAG ) ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.', '']","['addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ;#AUTHOR_TAG ) ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.', '']",1
"['literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also#AUTHOR_TAG ) .', '']","['literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also#AUTHOR_TAG ) .', '']","['literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also#AUTHOR_TAG ) .', '']","['literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also#AUTHOR_TAG ) .', '']",0
"['', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization( #AUTHOR_TAG b ) , but these features are also beyond the capabilities of current summarization systems']","['', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization( #AUTHOR_TAG b ) , but these features are also beyond the capabilities of current summarization systems']","['', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization( #AUTHOR_TAG b ) , but these features are also beyond the capabilities of current summarization systems']","['', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization( #AUTHOR_TAG b ) , but these features are also beyond the capabilities of current summarization systems']",1
"['', '#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', '']","['', '#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', '']","['', '#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', '']","['', '#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', '']",0
"[', the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ;#AUTHOR_TAG ) .', '']","[', the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ;#AUTHOR_TAG ) .', '']","[', the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ;#AUTHOR_TAG ) .', '']","['', 'However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ;#AUTHOR_TAG ) .', '']",0
"['.', 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content( #AUTHOR_TAG ) .', '']","['', 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content( #AUTHOR_TAG ) .', 'Nevertheless, the indexing process remains firmly human-centered']","['.', 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content( #AUTHOR_TAG ) .', '']","['', 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content( #AUTHOR_TAG ) .', 'Nevertheless, the indexing process remains firmly human-centered']",0
"['', 'Previously , a user study( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', '']","['', 'Previously , a user study( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', '']","['', 'Previously , a user study( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', '']","['', 'Previously , a user study( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', '']",1
"['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. ,#AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. ,#AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. ,#AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. ,#AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']",0
"[', software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep( #AUTHOR_TAG ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST']","[', software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep( #AUTHOR_TAG ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '']","[', software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep( #AUTHOR_TAG ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '']","['', 'Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep( #AUTHOR_TAG ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '']",0
"['ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification( #AUTHOR_TAG ) , which can be described by the following equation']","['ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification( #AUTHOR_TAG ) , which can be described by the following equation']","['ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification( #AUTHOR_TAG ) , which can be described by the following equation']","['attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification( #AUTHOR_TAG ) , which can be described by the following equation']",5
"['feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by#AUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope']","['feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by#AUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope).', '']","['feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by#AUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope']","['feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by#AUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope).', '']",1
"['', 'Second , software for utilizing this ontology already exists : MetaMap( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST']","['', 'Second , software for utilizing this ontology already exists : MetaMap( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '']","['', 'Second , software for utilizing this ontology already exists : MetaMap( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '']","['', 'Second , software for utilizing this ontology already exists : MetaMap( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '']",0
"['', 'The work of#AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision']","['', 'The work of#AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision']","['', 'The work of#AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision']","['', 'The work of#AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision']",0
"['extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine( #AUTHOR_TAG ) .', '']","['extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine( #AUTHOR_TAG ) .', '']","['extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine( #AUTHOR_TAG ) .', '']","['extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine( #AUTHOR_TAG ) .', '']",5
"['994 ;#AUTHOR_TAG , 2005']","['Wykoff 1994 ;#AUTHOR_TAG , 2005']","['Wykoff 1994 ;#AUTHOR_TAG , 2005']",['#AUTHOR_TAG'],0
"['answer should be.', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see#AUTHOR_TAG andLin (2006)']","['answer should be.', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see#AUTHOR_TAG andLin (2006)']","['to first define what a good answer should be.', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see#AUTHOR_TAG andLin (2006) .', '']","['', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see#AUTHOR_TAG andLin (2006) .', '']",0
"['much exploration ,#AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment']","['much exploration ,#AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment']","['much exploration ,#AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment']","['much exploration ,#AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment']",0
"['', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio( #AUTHOR_TAG ) .', '']","['', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio( #AUTHOR_TAG ) .', '']","['', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio( #AUTHOR_TAG ) .', '']","['', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio( #AUTHOR_TAG ) .', '']",5
"['items , might provide physicians a better overview of the information landscape ; see , for example ,#AUTHOR_TAG']","['items , might provide physicians a better overview of the information landscape ; see , for example ,#AUTHOR_TAG']","['integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example ,#AUTHOR_TAG']","['', 'Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example ,#AUTHOR_TAG']",3
"['', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions( #AUTHOR_TAG ) ."", 'A']","['', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions( #AUTHOR_TAG ) ."", 'A']","['', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions( #AUTHOR_TAG ) ."", 'A number of']","['', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions( #AUTHOR_TAG ) ."", '']",0
"['', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see#AUTHOR_TAG a ) for a brief overview']","['', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see#AUTHOR_TAG a ) for a brief overview']","['', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see#AUTHOR_TAG a ) for a brief overview']","['', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see#AUTHOR_TAG a ) for a brief overview']",0
"['of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed( #AUTHOR_TAG a , 2006b ) .', '']","['of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed( #AUTHOR_TAG a , 2006b ) .', '']","['', 'A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed( #AUTHOR_TAG a , 2006b ) .', '']","['', 'A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed( #AUTHOR_TAG a , 2006b ) .', '']",0
"['potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy( #AUTHOR_TAG )']","['potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy( #AUTHOR_TAG )']","['potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy( #AUTHOR_TAG )']","['potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy( #AUTHOR_TAG )']",5
"['', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity( #AUTHOR_TAG ;""]","['', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity( #AUTHOR_TAG ;""]","['', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", '']","['', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", '']",0
"['', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start( #AUTHOR_TAG ) .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006 a) for a brief overview']","['', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start( #AUTHOR_TAG ) .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006 a) for a brief overview']","['', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start( #AUTHOR_TAG ) .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006 a) for a brief overview']","['', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start( #AUTHOR_TAG ) .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006 a) for a brief overview']",0
"['the paradigm of evidence-based medicine( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .', 'The P']","['the paradigm of evidence-based medicine( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .', 'The']","['the paradigm of evidence-based medicine( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .', '']","['', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third , the paradigm of evidence-based medicine( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .', '']",0
"['', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ;#AUTHOR_TAG )']","['', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ;#AUTHOR_TAG )']","['', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ;#AUTHOR_TAG )']","['', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ;#AUTHOR_TAG )']",1
"['', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 )']","['', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 )']","['', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 )']","['', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 )']",1
"['', 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in#AUTHOR_TAG']","['', 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in#AUTHOR_TAG']","['', 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in#AUTHOR_TAG']","['', 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in#AUTHOR_TAG']",5
"['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 ,#AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 ,#AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 ,#AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 ,#AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']",0
"['knowledge extractors rely extensively on MetaMap( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', '']","['knowledge extractors rely extensively on MetaMap( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', '']","['knowledge extractors rely extensively on MetaMap( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', '']","['knowledge extractors rely extensively on MetaMap( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', '']",5
"['automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in#AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an']","['automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in#AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an']","['automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in#AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example']","['automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in#AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', '']",2
"['', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed( #AUTHOR_TAG )']","['clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed( #AUTHOR_TAG )']","['the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed( #AUTHOR_TAG )']","['function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed( #AUTHOR_TAG )']",3
"['', 'The PICO framework( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section']","['', 'The PICO framework( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', '']","['', 'The PICO framework( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', '']","['', 'The PICO framework( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', '']",0
"['2003 ; Preiss 2003 ;#AUTHOR_TAG ; Miyao and Tsujii 2004 ) .', '']","['blems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ;#AUTHOR_TAG ; Miyao and Tsujii 2004 ) .', '']","['2003 ; Preiss 2003 ;#AUTHOR_TAG ; Miyao and Tsujii 2004 ) .', '']","['blems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ;#AUTHOR_TAG ; Miyao and Tsujii 2004 ) .', '']",4
"['blems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ;#AUTHOR_TAG ; Preiss 2003 ; Kaplan et al.']","['blems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ;#AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ;']","['blems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ;#AUTHOR_TAG ; Preiss 2003 ; Kaplan et al.']","['blems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ;#AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', '']",4
['2003 ;#AUTHOR_TAG ; Kaplan et al.'],"['blems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ;#AUTHOR_TAG ; Kaplan et al. 2004 ;']",['2003 ;#AUTHOR_TAG ; Kaplan et al.'],"['blems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ;#AUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', '']",4
"['', 'Our re-ranking approach , like the approach to parse re-ranking of#AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', '']","['', 'Our re-ranking approach , like the approach to parse re-ranking of#AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', '']","['', 'Our re-ranking approach , like the approach to parse re-ranking of#AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', '']","['', 'Our re-ranking approach , like the approach to parse re-ranking of#AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', '']",1
"['in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n']","['in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n']","['in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n assignments of']","['', 'Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.', '']",5
"['our previous work( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the']","['our previous work( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the']","['our previous work( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering']","['our previous work( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .', '']",2
"['ally , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by#AUTHOR_TAG .', 'We use the']","['ally , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by#AUTHOR_TAG .', 'We use the']","['ally , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by#AUTHOR_TAG .', 'We use the']","['ally , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by#AUTHOR_TAG .', '']",4
"['.', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs( #AUTHOR_TAG )']","['cases.', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs( #AUTHOR_TAG )']","['', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs( #AUTHOR_TAG )']","['', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs( #AUTHOR_TAG )']",0
"['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain( #AUTHOR_TAG ) .', '']","['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain( #AUTHOR_TAG ) .', '']","['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain( #AUTHOR_TAG ) .', '']","['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain( #AUTHOR_TAG ) .', '']",0
"['\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ;#AUTHOR_TAG )']","['\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ;#AUTHOR_TAG )']","['\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ;#AUTHOR_TAG )']","['\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ;#AUTHOR_TAG )']",1
"['stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', '']","['stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', '']","['stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', '']","['stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', '']",5
"['', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ;#AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2']","['', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ;#AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2']","['', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ;#AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2']","['', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ;#AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .', '']",5
"['example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .', '']","['example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .', '']","['example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .', '']","['example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .', 'However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively']",0
"['.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']","['even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']","['even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']","[', even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']",0
"['', 'We then use the program Snob ( Wallace and Boulton 1968 ;#AUTHOR_TAG ) to cluster these experiences .', 'Figure 8(a)']","['', 'We then use the program Snob ( Wallace and Boulton 1968 ;#AUTHOR_TAG ) to cluster these experiences .', 'Figure 8(a)']","['', 'We then use the program Snob ( Wallace and Boulton 1968 ;#AUTHOR_TAG ) to cluster these experiences .', 'Figure 8(a)']","['', 'We then use the program Snob ( Wallace and Boulton 1968 ;#AUTHOR_TAG ) to cluster these experiences .', '']",5
"['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such']","['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such']","['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such technologies require significant human input,']","['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', '']",1
"['We employed the LIBSVM package( #AUTHOR_TAG ) .', '']","['We employed the LIBSVM package( #AUTHOR_TAG ) .', '']","['We employed the LIBSVM package( #AUTHOR_TAG ) .', '']","['', '7 We employed the LIBSVM package( #AUTHOR_TAG ) .', 'prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.', '']",5
"['our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ;#AUTHOR_TAG ) .', 'We']","['our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ;#AUTHOR_TAG ) .', 'We']","['our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ;#AUTHOR_TAG ) .', 'We chose this program']","['', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ;#AUTHOR_TAG ) .', '']",5
"['use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ;#AUTHOR_TAG ) .', 'Prec']","['use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ;#AUTHOR_TAG ) .', 'Precision measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response.', '']","['use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ;#AUTHOR_TAG ) .', 'Prec']","['use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ;#AUTHOR_TAG ) .', 'Precision measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response.', '']",5
"['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning( #AUTHOR_TAG ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 199']","['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning( #AUTHOR_TAG ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', '']","['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning( #AUTHOR_TAG ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', '']","['automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning( #AUTHOR_TAG ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', '']",1
"['.', '#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-m']","['templates.', '#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-m']","['', '#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-m']","['', '#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-m']",1
"['5 Significant bigrams are obtained using the n-gram statistics package NSP( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation )']","['5 Significant bigrams are obtained using the n-gram statistics package NSP( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation )']","['5 Significant bigrams are obtained using the n-gram statistics package NSP( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation )']","['5 Significant bigrams are obtained using the n-gram statistics package NSP( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation )']",5
"['on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ;#AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ;#AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ;#AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ;#AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '']",0
"['applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;#AUTHOR_TAG ) .', '']","['applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;#AUTHOR_TAG ) .', '']","['applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;#AUTHOR_TAG ) .', '']","['applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;#AUTHOR_TAG ) .', '']",1
['\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed )( #AUTHOR_TAG ; Roy and Subramaniam 2006 )'],['\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed )( #AUTHOR_TAG ; Roy and Subramaniam 2006 )'],['\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed )( #AUTHOR_TAG ; Roy and Subramaniam 2006 )'],['\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed )( #AUTHOR_TAG ; Roy and Subramaniam 2006 )'],1
"['.', 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures( #AUTHOR_TAG ) .', '']","[""quality, so that the judges' assessments would be comparable."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures( #AUTHOR_TAG ) .', '']","['.', 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures( #AUTHOR_TAG ) .', '']","['', ""In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures( #AUTHOR_TAG ) .', '']",5
"['', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 )']","['', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 )']","['', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 )']","['', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 )']",0
"['common way to combine different models consists of selecting the model that is most confident regarding its decision( #AUTHOR_TAG ) .', '']","['common way to combine different models consists of selecting the model that is most confident regarding its decision( #AUTHOR_TAG ) .', '']","['common way to combine different models consists of selecting the model that is most confident regarding its decision( #AUTHOR_TAG ) .', '']","['common way to combine different models consists of selecting the model that is most confident regarding its decision( #AUTHOR_TAG ) .', '']",1
"['applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions)( #AUTHOR_TAG ; Soricut and Brill 2006).', '']","['applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions)( #AUTHOR_TAG ; Soricut and Brill 2006).', '']","['applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions)( #AUTHOR_TAG ; Soricut and Brill 2006).', '']","['applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions)( #AUTHOR_TAG ; Soricut and Brill 2006).', '']",1
"['\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ;#AUTHOR_TAG ) .', 'The representativeness of the sample size was not discussed in any of these studies']","['\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ;#AUTHOR_TAG ) .', 'The representativeness of the sample size was not discussed in any of these studies']","['\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ;#AUTHOR_TAG ) .', 'The representativeness of the sample size was not discussed in any of these studies']","['\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ;#AUTHOR_TAG ) .', 'The representativeness of the sample size was not discussed in any of these studies']",1
"['a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses( #AUTHOR_TAG ; Berger et al. 2000 )']","['a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses( #AUTHOR_TAG ; Berger et al. 2000 )']","['a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses( #AUTHOR_TAG ; Berger et al. 2000 )']","['a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses( #AUTHOR_TAG ; Berger et al. 2000 )']",1
"['#AUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses']","['#AUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses']","['#AUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses']","['#AUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses']",0
"['', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion( #AUTHOR_TAG ; Wallace 2005 ) .', '']","['', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion( #AUTHOR_TAG ; Wallace 2005 ) .', '']","['', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion( #AUTHOR_TAG ; Wallace 2005 ) .', '']","['', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion( #AUTHOR_TAG ; Wallace 2005 ) .', '']",5
"['FAQs ,#AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijk']","['FAQs ,#AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun']","['FAQs ,#AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijk']","['FAQs ,#AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', '']",0
"['\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours( #AUTHOR_TAG ; Leuski et al. 2006 )']","['\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours( #AUTHOR_TAG ; Leuski et al. 2006 )']","['\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours( #AUTHOR_TAG ; Leuski et al. 2006 )']","['\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours( #AUTHOR_TAG ; Leuski et al. 2006 ) .', 'The representativeness of the sample size was not discussed in any of these studies']",1
"['( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ;#AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ;#AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ;#AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ;#AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '']",0
"['', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs( #AUTHOR_TAG ) for Sent-Pred .']","['', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs( #AUTHOR_TAG ) for Sent-Pred']","['', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs( #AUTHOR_TAG ) for Sent-']",['( #AUTHOR_TAG )'],5
"['.', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by#AUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e']","['sentences.', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by#AUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its']","['', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by#AUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its']","['', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by#AUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented).', '']",5
"['', 'The predictive model is a Decision Graph( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle']","['', 'The predictive model is a Decision Graph( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle']","['', 'The predictive model is a Decision Graph( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle .', '']","['', 'The predictive model is a Decision Graph( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle .', '']",5
"['.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ;#AUTHOR_TAG ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']","['customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ;#AUTHOR_TAG ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']","['', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ;#AUTHOR_TAG ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']","[', even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ;#AUTHOR_TAG ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']",0
"['are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ;#AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ;#AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ;#AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ;#AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '']",1
"['are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ;#AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ;#AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ;#AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '']","['are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ;#AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '']",1
"['', '#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', 'Two']","['', '#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', 'Two']","['', '#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', '']","['', '#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', '']",1
"['For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .', '']","['For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .', '7']","['For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .', '']","['', '6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .', '']",5
"['proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by#AUTHOR_TAG as follows .', '']","['proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by#AUTHOR_TAG as follows .', '']","['proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by#AUTHOR_TAG as follows .', 'The merging category corresponds to techniques where the individual methods affect each other in different ways (this']","['', 'They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by#AUTHOR_TAG as follows .', ""The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories)."", '']",0
"['we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system( #AUTHOR_TAG b ) .', '']","['we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system( #AUTHOR_TAG b ) .', '']","['we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system( #AUTHOR_TAG b ) .', '']","['', 'In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system( #AUTHOR_TAG b ) .', '']",5
"['.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ;#AUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']","['customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ;#AUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']","['', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ;#AUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']","['', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ;#AUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998']",0
"['', '#AUTHOR_TAG']","['', '#AUTHOR_TAG']","['', '#AUTHOR_TAG']","['', '#AUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .', '']",0
"['#AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', '']","['#AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', '']","['#AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', '']","['#AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', '']",1
['\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ;#AUTHOR_TAG )'],['\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ;#AUTHOR_TAG )'],['\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ;#AUTHOR_TAG )'],['\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ;#AUTHOR_TAG )'],1
"['we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ;#AUTHOR_TAG )']","['we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ;#AUTHOR_TAG )']","['we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ;#AUTHOR_TAG )']","['', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ;#AUTHOR_TAG )']",0
"['applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ;#AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', '']","['applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ;#AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', '']","['applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ;#AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', '']","['applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ;#AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', '']",1
"['', 'Specifically , we used Decision Graphs( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .']","['', 'Specifically , we used Decision Graphs( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred']","['', 'Specifically , we used Decision Graphs( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-']",['( #AUTHOR_TAG )'],5
"['We also employed sequence-based measures using the ROUGE tool set( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures']","['We also employed sequence-based measures using the ROUGE tool set( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures']","['We also employed sequence-based measures using the ROUGE tool set( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures']","['13 We also employed sequence-based measures using the ROUGE tool set( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures']",5
"['', 'We then use the program Snob( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8']","['', 'We then use the program Snob( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a)']","['', 'We then use the program Snob( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8']","['', 'We then use the program Snob( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', '']",5
"['order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system( #AUTHOR_TAG a ) .', '']","['order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system( #AUTHOR_TAG a ) .', '']","['order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system( #AUTHOR_TAG a ) .', '']","['order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system( #AUTHOR_TAG a ) .', '']",5
"['', ""The question answering system developed by#AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", '']","['', ""The question answering system developed by#AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", '']","['', ""The question answering system developed by#AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", '']","['', ""The question answering system developed by#AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", '']",1
"['005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 )']","['has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 )']","['has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 )']","['', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 )']",4
"['PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', '']","['PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', '']","['PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', '']","['the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here η is an optimization precision, α is a step size chosen with the strong Wolfe's rule (Nocedal and Wright 1999)."", 'Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', '']",5
['See#AUTHOR_TAG for further discussion'],['See#AUTHOR_TAG for further discussion'],['phrase probabilities. See#AUTHOR_TAG for further discussion'],['#AUTHOR_TAG'],0
"['heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency']","['heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency']","['local heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .', '']","['', 'IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .', '']",0
"['the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule( #AUTHOR_TAG ) ."", 'Here, ']","['the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule( #AUTHOR_TAG ) ."", 'Here,']","['the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule( #AUTHOR_TAG ) ."", 'Here, ']","['the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule( #AUTHOR_TAG ) ."", '']",5
"['', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model( #AUTHOR_TAG ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '']","['alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model( #AUTHOR_TAG ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '']","['', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model( #AUTHOR_TAG ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '']","['alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model( #AUTHOR_TAG ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '']",1
['5 The open source Moses( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/'],['5 The open source Moses( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/'],['5 The open source Moses( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/'],['5 The open source Moses( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/'],5
"['Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', '']","['Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', '']","['shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', '']","['', 'Following Ganchev, Gillenwater, and Taskar (2009) , we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', '']",5
"['04 ;#AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', '']","[';#AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', '']","['04 ;#AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', '']",['#AUTHOR_TAG'],0
"['possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', '']","['possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', '']","['possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', '']","['possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'Using this decoding we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.', 'This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link.', '']",5
"['', 'We used a standard implementation of IBM Model 4( #AUTHOR_TAG ) and']","['', 'We used a standard implementation of IBM Model 4( #AUTHOR_TAG ) and']","['', 'We used a standard implementation of IBM Model 4( #AUTHOR_TAG ) and']","['', 'We used a standard implementation of IBM Model 4( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .', '']",5
"['; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages( #AUTHOR_TAG )']","['; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages( #AUTHOR_TAG )']","['; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages( #AUTHOR_TAG )']","['', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages( #AUTHOR_TAG )']",4
"['', 'The standard approach is to train two models independently and then intersect their predictions( #AUTHOR_TAG ) .', '']","['', 'The standard approach is to train two models independently and then intersect their predictions( #AUTHOR_TAG ) .', '']","['', 'The standard approach is to train two models independently and then intersect their predictions( #AUTHOR_TAG ) .', '']","['', 'The standard approach is to train two models independently and then intersect their predictions( #AUTHOR_TAG ) .', 'However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source).', '']",1
"['of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y )( #AUTHOR_TAG )']","['of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y )( #AUTHOR_TAG )']","['of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y )( #AUTHOR_TAG )']","['of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y )( #AUTHOR_TAG )']",5
"['', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ;#AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', '']","['alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ;#AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', '']","['', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ;#AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', '']","['alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ;#AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '']",1
"['idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment ,#AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every']","['idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment ,#AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every']","['idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment ,#AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability']","['idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment ,#AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', '']",0
"[').', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings( #AUTHOR_TAG ) .', '']","['Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings( #AUTHOR_TAG ) .', '']","[').', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings( #AUTHOR_TAG ) .', '']","['', 'Following Ganchev, Gillenwater, and Taskar (2009) , we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings( #AUTHOR_TAG ) .', '']",5
"['this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union( #AUTHOR_TAG ) .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better,']","['this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union( #AUTHOR_TAG ) .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better,']","['this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union( #AUTHOR_TAG ) .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better,']","['', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union( #AUTHOR_TAG ) .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', '']",5
"['their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ;#AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay ']","['their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ;#AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay']","['their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ;#AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 )']","['', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ;#AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 )']",4
"['indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations( #AUTHOR_TAG ) .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', '']","['indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations( #AUTHOR_TAG ) .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', '']","['indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations( #AUTHOR_TAG ) .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', '']","['', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations( #AUTHOR_TAG ) .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', '']",0
"['discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final( #AUTHOR_TAG ) .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', '']","['discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final( #AUTHOR_TAG ) .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', '']","['discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final( #AUTHOR_TAG ) .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', '']","['discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final( #AUTHOR_TAG ) .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', '']",1
"['is closely related to the work of#AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', '']","['is closely related to the work of#AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', '']","['is closely related to the work of#AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', '']","['is closely related to the work of#AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', 'In the original GE framework, the posteriors of the model on unlabeled data are regularized directly.', 'They train a discriminative model, using conditional likelihood on labeled data and an ""expectation regularization"" penalty term on the unlabeled data']",0
"['', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages( #AUTHOR_TAG ; Hwa et al. ']","['', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay']","['', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzil']","['', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 )']",4
"['', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [#AUTHOR_TAG ; Chiang et al.']","['', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [#AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', '']","['', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [#AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', '']","['', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [#AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', '']",0
"['', ""For the task of unsupervised dependency parsing ,#AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", '']","['', ""For the task of unsupervised dependency parsing ,#AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", '']","['', ""For the task of unsupervised dependency parsing ,#AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", '']","['', ""For the task of unsupervised dependency parsing ,#AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", '']",0
"['', 'We use the agreement checker code developed by#AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-']","['', 'We use the agreement checker code developed by#AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using']","['', 'We use the agreement checker code developed by#AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using']","['', 'We use the agreement checker code developed by#AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .', '']",5
"['', 'In comparison, the tag set of the Buckwalter Morphological Analyzer( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006;']","['', 'In comparison, the tag set of the Buckwalter Morphological Analyzer( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006;']","['', 'In comparison, the tag set of the Buckwalter Morphological Analyzer( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006; Petrov, Das, and']","['', 'In comparison, the tag set of the Buckwalter Morphological Analyzer( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006; Petrov, Das, and McDonald 2012).', '']",1
"['better comparison with work of others , we adopt the suggestion made by#AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', '']","['better comparison with work of others , we adopt the suggestion made by#AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', '']","['better comparison with work of others , we adopt the suggestion made by#AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', '']","['better comparison with work of others , we adopt the suggestion made by#AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', '']",5
"['2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ;#AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ;#AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .', '']","['ic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ;#AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ;#AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .', '']",0
"['use the Columbia Arabic Treebank ( CATiB )( #AUTHOR_TAG ) .', '']","['use the Columbia Arabic Treebank ( CATiB )( #AUTHOR_TAG ) .', '']","['use the Columbia Arabic Treebank ( CATiB )( #AUTHOR_TAG ) .', '']","['use the Columbia Arabic Treebank ( CATiB )( #AUTHOR_TAG ) .', 'Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.', ""CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations."", 'It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).', 'The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation).', '']",5
"['.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Niv']","['three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky,']","['among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre,']","['languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;']",4
"['all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', '']","['all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', '']","['all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', '']","['all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', '']",5
"['18 In this article , we use a newer version of the corpus by#AUTHOR_TAG than the one we used in Marton , Habash , andRambow ( 2011 )']","['18 In this article , we use a newer version of the corpus by#AUTHOR_TAG than the one we used in Marton , Habash , andRambow ( 2011 )']","['18 In this article , we use a newer version of the corpus by#AUTHOR_TAG than the one we used in Marton , Habash , andRambow ( 2011 )']","['18 In this article , we use a newer version of the corpus by#AUTHOR_TAG than the one we used in Marton , Habash , andRambow ( 2011 )']",5
"['In this article , we use an in-house system which provides functional gender , number , and rationality features( #AUTHOR_TAG ) .']","['In this article , we use an in-house system which provides functional gender , number , and rationality features( #AUTHOR_TAG ) . See Section']","['In this article , we use an in-house system which provides functional gender , number , and rationality features( #AUTHOR_TAG ) .']",['( #AUTHOR_TAG )'],5
"['0 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB( #AUTHOR_TAG ) .', '']","['; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB( #AUTHOR_TAG ) .', '']","[', results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB( #AUTHOR_TAG ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB( #AUTHOR_TAG ) .', '']",0
"['', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser( #AUTHOR_TAG ) ( Section 6 ) .', '']","['', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser( #AUTHOR_TAG ) ( Section 6 ) .', '']","['', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser( #AUTHOR_TAG ) ( Section 6 ) .', '']","['', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser( #AUTHOR_TAG ) ( Section 6 ) .', '']",5
"['BER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality( #AUTHOR_TAG ) .']","['feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality( #AUTHOR_TAG )']","['xical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality( #AUTHOR_TAG ) .']","['ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality( #AUTHOR_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .', '']",5
"['.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'It']","['the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'It']","['the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', '']","['', 'Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', '']",5
"['', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and#AUTHOR_TAG ."", 'We']","['also given.', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and#AUTHOR_TAG ."", 'We']","['', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and#AUTHOR_TAG ."", 'We denote p < 0.05 and p < 0.01 with +']","['results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination.', 'Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given.', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and#AUTHOR_TAG ."", 'We denote p < 0.05 and p < 0.01 with + and ++ , respectively']",5
"['all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and#AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use']","['all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and#AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use']","['all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and#AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set""']","['all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and#AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', '']",5
"['resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource( #AUTHOR_TAG ) .']","['resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource( #AUTHOR_TAG )']","['the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource( #AUTHOR_TAG ) .']","['', 'This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource( #AUTHOR_TAG ) .19', '']",5
"['available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB)( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir']","['available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB)( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM']","['available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB)( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer']","['available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB)( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', '']",1
"['ter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012).', 'The Elixir']","['the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012).', 'The Elixir-FM']","['), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer']","['available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012).', '']",1
"[""following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 )( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the""]","[""following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 )( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted""]","[""following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 )( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values""]",['( #AUTHOR_TAG )'],5
"['available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir']","['available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM']","['available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer']","['available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', '']",1
"['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT )( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT )( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT )( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT )( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']",0
"['', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ;#AUTHOR_TAG )']","['', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ;#AUTHOR_TAG )']","['', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ;#AUTHOR_TAG )']","['', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ;#AUTHOR_TAG )']",4
"['', 'The result holds for both the MaltParser( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 )']","['both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 )']","['', 'The result holds for both the MaltParser( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 )']","['', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 )']",5
"[""the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW )( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both""]","[""the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW )( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both""]","[""the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW )( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both""]",['( #AUTHOR_TAG )'],5
"['work has been done on the use of morphological features for parsing of morphologically rich languages.', '#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', '']","['work has been done on the use of morphological features for parsing of morphologically rich languages.', '#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', '']","['work has been done on the use of morphological features for parsing of morphologically rich languages.', '#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for']","['work has been done on the use of morphological features for parsing of morphologically rich languages.', '#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', '']",0
"['9 We do not relate to specific results in their study because it has been brought to our attention that#AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication )']","['9 We do not relate to specific results in their study because it has been brought to our attention that#AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication )']","['9 We do not relate to specific results in their study because it has been brought to our attention that#AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication )']","['9 We do not relate to specific results in their study because it has been brought to our attention that#AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication )']",1
"['both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser( #AUTHOR_TAG )']","['both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser( #AUTHOR_TAG )']","['SA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser( #AUTHOR_TAG )']","['', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser( #AUTHOR_TAG )']",5
"['', 'A more detailed discussion of the various available Arabic tag sets can be found in#AUTHOR_TAG']","['', 'A more detailed discussion of the various available Arabic tag sets can be found in#AUTHOR_TAG']","['', 'A more detailed discussion of the various available Arabic tag sets can be found in#AUTHOR_TAG']","['', 'A more detailed discussion of the various available Arabic tag sets can be found in#AUTHOR_TAG']",0
"['', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see#AUTHOR_TAG and Habash , Faraj , andRoth ( 2009 )']","['', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see#AUTHOR_TAG and Habash , Faraj , andRoth ( 2009 )']","['', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see#AUTHOR_TAG and Habash , Faraj , andRoth ( 2009 )']","['', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see#AUTHOR_TAG and Habash , Faraj , andRoth ( 2009 )']",0
"['far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including#AUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', '']","['far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including#AUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', '']","['far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including#AUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', '']","['far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including#AUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', '']",1
"['19 The paper by#AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article']","['19 The paper by#AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article']","['19 The paper by#AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article']","['19 The paper by#AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article']",1
"['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ;#AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ;#AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ;#AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ;#AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']",0
"['11#AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', 'The Niv']","['11#AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', 'The Nivre �standard� algorithm is also reported there to do better on Arabic,']","['11#AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', 'The Nivre �standard� algorithm is also reported there to do better on Arabic']","['11#AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', '']",1
"['K�bler, McDonald, and#AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.']","['at buf[0], and so on. K�bler, McDonald, and#AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13', '']","['at buf[0], and so on. K�bler, McDonald, and#AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.']",['#AUTHOR_TAG'],5
"['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ;#AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ;#AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ;#AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']","['for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ;#AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '']",0
"['all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald ,']","['all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald ,']","['all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald ,']",['( #AUTHOR_TAG'],5
"['', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser']","['', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser']","['', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser']","['', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser']",0
"['', 'Similarly ,#AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We']","['', 'Similarly ,#AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We']","['', 'Similarly ,#AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We']","['', 'Similarly ,#AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', '']",0
"['7 We ignore the rare ""false idafa"" construction( #AUTHOR_TAG , p. 102 )']","['7 We ignore the rare ""false idafa"" construction( #AUTHOR_TAG , p. 102 )']","['7 We ignore the rare ""false idafa"" construction( #AUTHOR_TAG , p. 102 )']","['7 We ignore the rare ""false idafa"" construction( #AUTHOR_TAG , p. 102 )']",0
"['this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser( #AUTHOR_TAG ) .', '']","['this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser( #AUTHOR_TAG ) .', '']","['this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser( #AUTHOR_TAG ) .', '']","['this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', '']",5
"[', Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ;#AUTHOR_TAG ) .', 'Our work is the first to']","[', Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ;#AUTHOR_TAG ) .', 'Our work is the first to']","[', Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ;#AUTHOR_TAG ) .', 'Our work is the first to']","['', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ;#AUTHOR_TAG ) .', '']",1
"['', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs']","['6).', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '']","['Section 6).', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '']","['', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '']",0
"['article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ;#AUTHOR_TAG ) .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in']","['article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ;#AUTHOR_TAG ) .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in']","['article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ;#AUTHOR_TAG ) .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', '']","['article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ;#AUTHOR_TAG ) .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', '']",2
"['1980 , Haton and Pierrel 1976 , Lea 1980 ,#AUTHOR_TAG , Medress 1980 , Reddy 1976 ,']","['1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 ,#AUTHOR_TAG , Medress 1980 , Reddy 1976 ,']","['1980 , Haton and Pierrel 1976 , Lea 1980 ,#AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and']","['number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 ,#AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '']",1
"['the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in#AUTHOR_TAG""]","['the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in#AUTHOR_TAG""]","['the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in#AUTHOR_TAG""]","['the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in#AUTHOR_TAG""]",0
"['.', 'There is some literature on procedure acquisition such as the LISP synthesis work described inBiermann et al. ( 1984 ) and the PROLOG synthesis method of#AUTHOR_TAG']","['structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described inBiermann et al. ( 1984 ) and the PROLOG synthesis method of#AUTHOR_TAG']","['', 'There is some literature on procedure acquisition such as the LISP synthesis work described inBiermann et al. ( 1984 ) and the PROLOG synthesis method of#AUTHOR_TAG']","['', 'There is some literature on procedure acquisition such as the LISP synthesis work described inBiermann et al. ( 1984 ) and the PROLOG synthesis method of#AUTHOR_TAG']",1
"[""\x80¢ a `` conditioning '' facility as described by#AUTHOR_TAG , â""]","[""use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described byBiermann and Krishnaswamy ( 1976 ) , â\x80¢ a `` conditioning '' facility as described by#AUTHOR_TAG , â\x80¢ implementation of new""]","[""\x80¢ a `` conditioning '' facility as described by#AUTHOR_TAG , â""]","[""\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described byBiermann and Krishnaswamy ( 1976 ) , â\x80¢ a `` conditioning '' facility as described by#AUTHOR_TAG , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns""]",3
"['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,#AUTHOR_TAG ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,#AUTHOR_TAG ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,#AUTHOR_TAG ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,#AUTHOR_TAG ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']",1
"['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,#AUTHOR_TAG ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,#AUTHOR_TAG ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,#AUTHOR_TAG ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,#AUTHOR_TAG ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']",1
"['', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 ,#AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE']","['', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 ,#AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']","['', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 ,#AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']","['', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 ,#AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']",0
"['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as inMinsky and Papert ( 1969 ) , assertional statements as inMichalski ( 1980 ) , or semantic nets as in#AUTHOR_TAG .', '']","['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as inMinsky and Papert ( 1969 ) , assertional statements as inMichalski ( 1980 ) , or semantic nets as in#AUTHOR_TAG .', '']","['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as inMinsky and Papert ( 1969 ) , assertional statements as inMichalski ( 1980 ) , or semantic nets as in#AUTHOR_TAG .', '']","['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as inMinsky and Papert ( 1969 ) , assertional statements as inMichalski ( 1980 ) , or semantic nets as in#AUTHOR_TAG .', '']",1
"['', 'Another dialogue acquisition system has been developed by#AUTHOR_TAG .', '']","['', 'Another dialogue acquisition system has been developed by#AUTHOR_TAG .', '']","['', 'Another dialogue acquisition system has been developed by#AUTHOR_TAG .', '']","['', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by#AUTHOR_TAG .', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable']",1
"['976 , Walker 1978 , and#AUTHOR_TAG ) .', 'Most of these efforts concentrated on']","[', Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and#AUTHOR_TAG ) .', 'Most of these efforts concentrated on']","['dy 1976 , Walker 1978 , and#AUTHOR_TAG ) .', 'Most of these efforts concentrated on']","['number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and#AUTHOR_TAG ) .', '']",1
"['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,#AUTHOR_TAG ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,#AUTHOR_TAG ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,#AUTHOR_TAG ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,#AUTHOR_TAG ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']",1
"['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as inMinsky and Papert ( 1969 ) , assertional statements as in#AUTHOR_TAG , or semantic nets as inWinston ( 1975 ) .', '']","['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as inMinsky and Papert ( 1969 ) , assertional statements as in#AUTHOR_TAG , or semantic nets as inWinston ( 1975 ) .', '']","['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as inMinsky and Papert ( 1969 ) , assertional statements as in#AUTHOR_TAG , or semantic nets as inWinston ( 1975 ) .', '']","['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as inMinsky and Papert ( 1969 ) , assertional statements as in#AUTHOR_TAG , or semantic nets as inWinston ( 1975 ) .', '']",1
"['', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by#AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', '']","['', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by#AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', '']","['', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by#AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', '']","['', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by#AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', '']",1
"['denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .', 'A user behavior']","['denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .', 'A user behavior']","['denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .', 'A user behavior is represented']","['denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .', '']",0
"['and Reddy 1980 ,#AUTHOR_TAG , Reddy 1976 , Walker ']","['1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 ,#AUTHOR_TAG , Reddy 1976 , Walker 1978 , and']","['and Reddy 1980 ,#AUTHOR_TAG , Reddy 1976 , Walker 1978 , and']","['number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 ,#AUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '']",1
"['number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 ,#AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy ']","['number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 ,#AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress']","['number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 ,#AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 ,']","['number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 ,#AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '']",1
"['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,#AUTHOR_TAG ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,#AUTHOR_TAG ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,#AUTHOR_TAG ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied byCarbonell and Hayes ( 1983 ) ,Granger ( 1983 ) ,#AUTHOR_TAG ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']",1
"['current system should be distinguished from an earlier voice system ( VNLC ,#AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word .']","['current system should be distinguished from an earlier voice system ( VNLC ,#AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word .']","['The current system should be distinguished from an earlier voice system ( VNLC ,#AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word .']","['', '[ The current system should be distinguished from an earlier voice system ( VNLC ,#AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word .']",1
"['expectation parser uses an ATN-like representation for its grammar( #AUTHOR_TAG ) .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conj']","['expectation parser uses an ATN-like representation for its grammar( #AUTHOR_TAG ) .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979).', '']","['expectation parser uses an ATN-like representation for its grammar( #AUTHOR_TAG ) .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conj']","['expectation parser uses an ATN-like representation for its grammar( #AUTHOR_TAG ) .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979).', '']",5
"[').', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE ,#AUTHOR_TAG ) .', '']","['andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE ,#AUTHOR_TAG ) .', '']","[').', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE ,#AUTHOR_TAG ) .', '']","['', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE ,#AUTHOR_TAG ) .', '']",0
"['number of speech understanding systems have been developed during the past fifteen years( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 ,']","['number of speech understanding systems have been developed during the past fifteen years( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 ,']","['number of speech understanding systems have been developed during the past fifteen years( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 ,']","['number of speech understanding systems have been developed during the past fifteen years( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '']",1
"['level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in#AUTHOR_TAG']","['level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in#AUTHOR_TAG']","['dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in#AUTHOR_TAG']","['', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in#AUTHOR_TAG']",0
"[""\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by#AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described byFink et al. ( 1985 ) , â""]","[""\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by#AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described byFink et al. ( 1985 ) , â\x80¢ implementation of new""]","[""\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by#AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described byFink et al. ( 1985 ) , â""]","[""\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by#AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described byFink et al. ( 1985 ) , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns""]",3
"['problem of handling ill-formed input has been studied by#AUTHOR_TAG ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied by#AUTHOR_TAG ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied by#AUTHOR_TAG ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']","['problem of handling ill-formed input has been studied by#AUTHOR_TAG ,Granger ( 1983 ) ,Jensen et al. ( 1983 ) ,Kwasny and Sondheimer ( 1981 ) ,Riesbeck and Schank ( 1976 ) ,Thompson ( 1980 ) ,Weischedel and Black ( 1980 ) , andWeischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '']",1
"['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in#AUTHOR_TAG , assertional statements as inMichalski ( 1980 ) , or semantic nets as inWinston ( 1975 ) .', '']","['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in#AUTHOR_TAG , assertional statements as inMichalski ( 1980 ) , or semantic nets as inWinston ( 1975 ) .', '']","['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in#AUTHOR_TAG , assertional statements as inMichalski ( 1980 ) , or semantic nets as inWinston ( 1975 ) .', '']","['VNLCE processor may be considered to be a learning system of the tradition described, for example, inMichalski et al. (1984) .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in#AUTHOR_TAG , assertional statements as inMichalski ( 1980 ) , or semantic nets as inWinston ( 1975 ) .', '']",1
"['.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC )( #AUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (']","['system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC )( #AUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']","['.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC )( #AUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (']","['', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC )( #AUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']",0
"['980 ,#AUTHOR_TAG , Walker 1978 , and']","['Medress 1980 ,#AUTHOR_TAG , Walker 1978 , and']","['ress 1980 ,#AUTHOR_TAG , Walker 1978 , and']",['#AUTHOR_TAG'],1
"['expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions( #AUTHOR_TAG ) .', 'An attempt has been made to build as deep a parse as']","['expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions( #AUTHOR_TAG ) .', 'An attempt has been made to build as deep a parse as']","['expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions( #AUTHOR_TAG ) .', 'An attempt has been made to build as deep a parse as']","['expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions( #AUTHOR_TAG ) .', 'An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', '']",0
"['.', 'How it is done is beyond the scope of this paper but is explained in detail in#AUTHOR_TAG']","['by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in#AUTHOR_TAG']","['that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in#AUTHOR_TAG']","['', 'The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in#AUTHOR_TAG']",0
"['.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in#AUTHOR_TAG and the PROLOG synthesis method ofShapiro ( 1982 )']","['structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in#AUTHOR_TAG and the PROLOG synthesis method ofShapiro ( 1982 )']","['', 'There is some literature on procedure acquisition such as the LISP synthesis work described in#AUTHOR_TAG and the PROLOG synthesis method ofShapiro ( 1982 )']","['', 'There is some literature on procedure acquisition such as the LISP synthesis work described in#AUTHOR_TAG and the PROLOG synthesis method ofShapiro ( 1982 )']",1
"['1980 ,#AUTHOR_TAG , Lea 1980 , Lower']","['number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 ,#AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress']","['1980 ,#AUTHOR_TAG , Lea 1980 , Lower']","['number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 ,#AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '']",1
"['', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ;Alshawi et al. ( 1985 ) and#AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE']","['', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ;Alshawi et al. ( 1985 ) and#AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE']","['', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ;Alshawi et al. ( 1985 ) and#AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE']","['chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '(Michiels (1982) contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ;Alshawi et al. ( 1985 ) and#AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE']",0
"['', '#AUTHOR_TAG andAkkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation']","['', '#AUTHOR_TAG andAkkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation']","['', '#AUTHOR_TAG andAkkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation']","['', '#AUTHOR_TAG andAkkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated']",0
"['research described below is taking place in the context of three collaborative projects( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['research described below is taking place in the context of three collaborative projects( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['', 'The research described below is taking place in the context of three collaborative projects( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['', 'The research described below is taking place in the context of three collaborative projects( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']",0
"['each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see#AUTHOR_TAG for further discussion ) .', '']","['each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see#AUTHOR_TAG for further discussion ) .', '']","['each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see#AUTHOR_TAG for further discussion ) .', '']","['', 'However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see#AUTHOR_TAG for further discussion ) .', 'For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', '']",0
"['', '#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated']","['', '#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated']","['', '#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated']","['', '#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated']",1
"[', there are other syntactic and semantic tests for this distinction, (see eg.#AUTHOR_TAG :472 ), but these are the only ones which are explicit in the LDOCE coding system']","[', there are other syntactic and semantic tests for this distinction, (see eg.#AUTHOR_TAG :472 ), but these are the only ones which are explicit in the LDOCE coding system']","[', there are other syntactic and semantic tests for this distinction, (see eg.#AUTHOR_TAG :472 ), but these are the only ones which are explicit in the LDOCE coding system']","[', there are other syntactic and semantic tests for this distinction, (see eg.#AUTHOR_TAG :472 ), but these are the only ones which are explicit in the LDOCE coding system']",0
"['', 'Two exceptions to this generalisation are the Linguistic String Project( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', '']","['consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', '']","['', 'Two exceptions to this generalisation are the Linguistic String Project( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', '']","['', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', '']",1
"['', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and#AUTHOR_TAG )']","['', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and#AUTHOR_TAG )']","['', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and#AUTHOR_TAG )']","['', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and#AUTHOR_TAG )']",0
"['.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ;#AUTHOR_TAG andAlshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE']","[""the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '(Michiels (1982) contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ;#AUTHOR_TAG andAlshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE']","['.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ;#AUTHOR_TAG andAlshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE']","['chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '(Michiels (1982) contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ;#AUTHOR_TAG andAlshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE']",0
"['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (seeMichiels, 1982,  for further comment).', 'One approach to this problem is that taken by the ASCOT project( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .', '']","['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (seeMichiels, 1982,  for further comment).', 'One approach to this problem is that taken by the ASCOT project( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .', '']","['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (seeMichiels, 1982,  for further comment).', 'One approach to this problem is that taken by the ASCOT project( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .', '']","['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (seeMichiels, 1982,  for further comment).', 'One approach to this problem is that taken by the ASCOT project( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', '']",0
"['output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system( #AUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', '']","['output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system( #AUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', '']","['output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system( #AUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', '']","['output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system( #AUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', '']",5
"['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (seeMichiels, 1982,  for further comment).', 'One approach to this problem is that taken by the ASCOT project( #AUTHOR_TAG ; Akkerman , 1986 ) .', '']","['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (seeMichiels, 1982,  for further comment).', 'One approach to this problem is that taken by the ASCOT project( #AUTHOR_TAG ; Akkerman , 1986 ) .', '']","['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (seeMichiels, 1982,  for further comment).', 'One approach to this problem is that taken by the ASCOT project( #AUTHOR_TAG ; Akkerman , 1986 ) .', '']","['type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (seeMichiels, 1982,  for further comment).', 'One approach to this problem is that taken by the ASCOT project( #AUTHOR_TAG ; Akkerman , 1986 ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', '']",0
"['', 'Two exceptions to this generalisation are the Linguistic String Project( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former emplo']","['hand.', 'Two exceptions to this generalisation are the Linguistic String Project( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of']","['', 'Two exceptions to this generalisation are the Linguistic String Project( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former emplo']",['( #AUTHOR_TAG )'],1
"['is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. ,#AUTHOR_TAG ) ;']","['is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. ,#AUTHOR_TAG ) ;']","['is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. ,#AUTHOR_TAG ) ;']","['', 'Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. ,#AUTHOR_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape']",0
"['', 'In addition ,#AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb']","['', 'In addition ,#AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb']","['to our Object Raising rule.', 'In addition ,#AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb']","['four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require.', 'None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule.', 'In addition ,#AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb']",1
"['', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see#AUTHOR_TAG for details ) .', '']","['', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see#AUTHOR_TAG for details ) .', '']","['', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see#AUTHOR_TAG for details ) .', '']","['', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see#AUTHOR_TAG for details ) .', '']",0
"['developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG )( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG )( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG )( Kay , 1984 a ) , PATR-II( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual']","['developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG )( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG )( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG )( Kay , 1984 a ) , PATR-II( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', '']","['developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG )( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG )( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG )( Kay , 1984 a ) , PATR-II( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', '']","['developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG )( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG )( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG )( Kay , 1984 a ) , PATR-II( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', '']",0
"['', 'This deficiency is rectified in the verb classification system employed by#AUTHOR_TAG in the Brandeis verb catalogue']","['if it collocates with a modal such as ""would"".', 'This deficiency is rectified in the verb classification system employed by#AUTHOR_TAG in the Brandeis verb catalogue']","['', 'This deficiency is rectified in the verb classification system employed by#AUTHOR_TAG in the Brandeis verb catalogue']","['', 'This deficiency is rectified in the verb classification system employed by#AUTHOR_TAG in the Brandeis verb catalogue']",1
"['research described below is taking place in the context of three collaborative projects( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['research described below is taking place in the context of three collaborative projects( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['', 'The research described below is taking place in the context of three collaborative projects( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['', 'The research described below is taking place in the context of three collaborative projects( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']",0
"['are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see#AUTHOR_TAG , for further details ) .', '']","['are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see#AUTHOR_TAG , for further details ) .', '']","['are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see#AUTHOR_TAG , for further details ) .', '']","['are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see#AUTHOR_TAG , for further details ) .', 'However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries']",0
"['', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg.#AUTHOR_TAG ) .', '']","['', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg.#AUTHOR_TAG ) .', '']","['', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg.#AUTHOR_TAG ) .', '']","['', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg.#AUTHOR_TAG ) .', '']",3
"['xicons and even those which employ very comprehensive grammars (eg.', '#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .', '']","['', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', '#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .', '']","['', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', '#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .', '']","['', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', '#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .', '']",1
"['', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see#AUTHOR_TAG , for further discussion )']","['', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see#AUTHOR_TAG , for further discussion )']","['', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see#AUTHOR_TAG , for further discussion )']","['', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see#AUTHOR_TAG , for further discussion )']",0
"[""000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE .']","[""the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE .']","[""000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE .']","['chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE . )', 'In this paper we focus on the exploitation of the LDOCE grammar coding system;Alshawi et al. (1985) andAlshawi (1987) describe further research in Cambridge utilising different types of information available in LDOCE']",0
"['', 'Expanding on a suggestion of#AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction']","['', 'Expanding on a suggestion of#AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction;']","['', 'Expanding on a suggestion of#AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction;']","['', 'Expanding on a suggestion of#AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', '']",2
"['the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ;#AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure( Huttenlocher and Zue , 1983 ) .', '']","['the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ;#AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure( Huttenlocher and Zue , 1983 ) .', '']","['the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ;#AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure( Huttenlocher and Zue , 1983 ) .', '']","['the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ;#AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure( Huttenlocher and Zue , 1983 ) .', '']",5
"['the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of#AUTHOR_TAG , 1985 ) .', 'The']","['the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of#AUTHOR_TAG , 1985 ) .', 'The']","['the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of#AUTHOR_TAG , 1985 ) .', 'The codes are doubly artic']","['the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of#AUTHOR_TAG , 1985 ) .', '']",2
"['', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English( #AUTHOR_TAG )']","['', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English( #AUTHOR_TAG )']","['', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English( #AUTHOR_TAG )']","['', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English( #AUTHOR_TAG )']",0
"['', 'Michiels ( 1982 ) and#AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the L']","['', 'Michiels ( 1982 ) and#AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the']","['', 'Michiels ( 1982 ) and#AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated']","['', 'Michiels ( 1982 ) and#AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated']",0
"['developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG )( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG )( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG )( Kay , 1984 a ) , PATR-II( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the']","['developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG )( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG )( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG )( Kay , 1984 a ) , PATR-II( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies']","['developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG )( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG )( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG )( Kay , 1984 a ) , PATR-II( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the']","['developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG )( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG )( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG )( Kay , 1984 a ) , PATR-II( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', '']",0
"['dictionary search through the pronunciation field is available ;Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure( #AUTHOR_TAG ) .', '']","['dictionary search through the pronunciation field is available ;Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure( #AUTHOR_TAG ) .', '']","['dictionary search through the pronunciation field is available ;Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure( #AUTHOR_TAG ) .', '']","['the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ;Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure( #AUTHOR_TAG ) .', '']",0
"['.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as#AUTHOR_TAG ) .', '']","['dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as#AUTHOR_TAG ) .', '']","['', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as#AUTHOR_TAG ) .', '']","['', 'Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as#AUTHOR_TAG ) .', '']",0
"['', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed( #AUTHOR_TAG :460 ff .']","['', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed( #AUTHOR_TAG :460 ff .']","['', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed( #AUTHOR_TAG :460 ff .']","['', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed( #AUTHOR_TAG :460 ff . )', 'However, only two of these criteria are explicit in the coding system']",3
"['a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information( #AUTHOR_TAG ) .', '']","['a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information( #AUTHOR_TAG ) .', '']","['a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information( #AUTHOR_TAG ) .', '']","['', 'In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information( #AUTHOR_TAG ) .', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']",0
