token_context,word_context,seg_context,sent_cotext,label
"['with integrated language models (Huang and Chiang, 2007).', 'the first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of']","['with integrated language models (Huang and Chiang, 2007).', 'the first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding']","['with integrated language models (Huang and Chiang, 2007).', 'the first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of']","['forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).', 'the first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding']",2
"['##an and harabagiu 2004 ), information extraction ( #AUTHOR_TAG ), and machine translation (']","['answering ( narayanan and harabagiu 2004 ), information extraction ( #AUTHOR_TAG ), and machine translation ( boas 2002 ).', 'with the efforts of many']","['##an and harabagiu 2004 ), information extraction ( #AUTHOR_TAG ), and machine translation (']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( narayanan and harabagiu 2004 ), information extraction ( #AUTHOR_TAG ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
"['prove that our method is effective, we also make a comparison between the performances of our system and #AUTHOR_TAG, Xue ( 2008 ).', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and #AUTHOR_TAG, Xue ( 2008 ).', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and #AUTHOR_TAG, Xue ( 2008 ).', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and #AUTHOR_TAG, Xue ( 2008 ).', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']",1
"['.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( #AUTHOR_TAG ), information extraction ( sur']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( #AUTHOR_TAG ), information extraction ( surdeanu et al. 2003 ), and machine translation ( boas 2002 ).', 'with the efforts of many']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( #AUTHOR_TAG ), information extraction ( sur']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( #AUTHOR_TAG ), information extraction ( surdeanu et al. 2003 ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
['words are extracted from the semantic knowledge - base of contemporary chinese ( #AUTHOR_TAG )'],['words are extracted from the semantic knowledge - base of contemporary chinese ( #AUTHOR_TAG )'],"['##cat ( semantic category ) of predicate, sem - cat of first word, semcat of head word, semcat of last word, semcat of predicate + semcat of first word, semcat of predicate + semcat of last word, predicate + semcat of head word, semcat of predicate + head word.', 'the semantic categories of verbs and other words are extracted from the semantic knowledge - base of contemporary chinese ( #AUTHOR_TAG )']","['##cat ( semantic category ) of predicate, sem - cat of first word, semcat of head word, semcat of last word, semcat of predicate + semcat of first word, semcat of predicate + semcat of last word, predicate + semcat of head word, semcat of predicate + head word.', 'the semantic categories of verbs and other words are extracted from the semantic knowledge - base of contemporary chinese ( #AUTHOR_TAG )']",5
"['made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature']","['made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature']","['the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reass']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( #AUTHOR_TAG ) was built, Xue and Palmer ( 2005 ) and Xue ( 2008 ) have']","['collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( #AUTHOR_TAG ) was built, Xue and Palmer ( 2005 ) and Xue ( 2008 ) have']","['the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( #AUTHOR_TAG ) was built, Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reass']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( #AUTHOR_TAG ) was built, Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( #AUTHOR_TAG, xue']","['of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( #AUTHOR_TAG, xue']","['the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( #AUTHOR_TAG, xue 2008 ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( #AUTHOR_TAG, xue 2008 ) reassured these findings']",4
['has built a semantic role classifier'],['has built a semantic role classifier'],"['has built a semantic role classifier exploiting the interdependence of semantic roles.', 'it has turned the single point classification problem']","['has built a semantic role classifier exploiting the interdependence of semantic roles.', 'it has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'se - mantic context features indicates the features ex - tracted from the arguments around the current one.', 'we can use window size to represent the scope of the context.', 'window size [ - m, n ] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu - ments will be utilized for the classification of cur - rent semantic role.', 'there are two kinds of argu - ment sequences in Jiang et al. (2005), and we only test the linear sequence.', 'take the sentence in fig - ure 1 as an example.', 'the linear sequence of the arguments in this sentence is : _ _ _ _ ( until then ), _ _ _ _ ( the insurance company ), _ ( has ), _ _ _ _ _ ( for the sanxia project ), _ _ _ _ ( in - surance services ).', 'for the argument _ ( has ), if the semantic context window size is [ - 1, 2 ], the seman - tic context features e. g. headword, phrase type and etc. of _ _ _ _ ( the insurance company ), _ _ _ _ _ ( for the sanxia project ) and _ _ _ _ ( insurance services ) will be utilized to serve the classification task of _ ( has )']",5
"['we make discriminations of arguments and adjuncts, the analysis is still coarse - grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement.', 'however, the impact of this idea is limited due to that the amount of the research target, arg2, is few in propbank.', 'what if we']","['we make discriminations of arguments and adjuncts, the analysis is still coarse - grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement.', 'however, the impact of this idea is limited due to that the amount of the research target, arg2, is few in propbank.', 'what if we']","['we make discriminations of arguments and adjuncts, the analysis is still coarse - grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement.', 'however, the impact of this idea is limited due to that the amount of the research target, arg2, is few in propbank.', 'what if']","['we make discriminations of arguments and adjuncts, the analysis is still coarse - grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement.', 'however, the impact of this idea is limited due to that the amount of the research target, arg2, is few in propbank.', 'what if we could extend the idea of hierarchical architecture to the single semantic role level?', 'would that help the improvement of src']",1
"[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as #AUTHOR_TAG, Xue and Palmer ( 2005 ) and Xue ( 2008 ).', 'Sun and Jurafsky (2004)']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as #AUTHOR_TAG, Xue and Palmer ( 2005 ) and Xue ( 2008 ).', 'Sun and Jurafsky (2004)']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as #AUTHOR_TAG, Xue and Palmer ( 2005 ) and Xue ( 2008 ).', 'Sun and Jurafsky (2004)']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as #AUTHOR_TAG, Xue and Palmer ( 2005 ) and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ), #AUTHOR_TAG.', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ), #AUTHOR_TAG.', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ), #AUTHOR_TAG.', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']","['prove that our method is effective, we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ), #AUTHOR_TAG.', 'Xue (2008) is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed Xue (2008) with a relative error reduction rate of 9. 8 %']",1
"['', 'be found in figure 2, which is similar with that in #AUTHOR_TAG']","['', 'predicate may be still useful because that the information, provided by the high - level description of selfdescriptive features, e. g.', 'phrase type, are limited.', 'be found in figure 2, which is similar with that in #AUTHOR_TAG']","['', 'be found in figure 2, which is similar with that in #AUTHOR_TAG']","['semantic role classifiers always did the classification problem in one - step.', 'however, in this paper, we did src in two steps.', 'the architectures of hierarchical semantic role classifiers can 2 extra features e. g.', 'predicate may be still useful because that the information, provided by the high - level description of selfdescriptive features, e. g.', 'phrase type, are limited.', 'be found in figure 2, which is similar with that in #AUTHOR_TAG']",1
"['pos, predicate + head word, predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from #AUTHOR_TAG']","['pos, predicate + head word, predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from #AUTHOR_TAG']","['pos, predicate + head word, predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from #AUTHOR_TAG']","[', subcat frame, phrase type, first word, last word, subcat frame +, predicate, path, head word and its pos, predicate + head word, predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from #AUTHOR_TAG']",5
"['##anu et al. 2003 ), and machine translation ( #AUTHOR_TAG ).', 'with the efforts of many']","['answering ( narayanan and harabagiu 2004 ), information extraction ( surdeanu et al. 2003 ), and machine translation ( #AUTHOR_TAG ).', 'with the efforts of many']","['##anu et al. 2003 ), and machine translation ( #AUTHOR_TAG ).', 'with the efforts of many researchers ( carreras and']","['role labeling ( srl ) was first defined in Gildea and Jurafsky (2002).', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( narayanan and harabagiu 2004 ), information extraction ( surdeanu et al. 2003 ), and machine translation ( #AUTHOR_TAG ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
"['##8 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with #AUTHOR_TAG, however a bit different from Xue and Palmer ( 2005 )']","['of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with #AUTHOR_TAG, however a bit different from Xue and Palmer ( 2005 )']","['##8 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with #AUTHOR_TAG, however a bit different from Xue and Palmer ( 2005 )']","['use chinese propbank 1. 0 ( ldc number : ldc2005t23 ) in our experiments.', 'propbank 1. 0 includes the annotations for files chtb _ 001. fid', 'to chtb _ 931. fid,', 'or the first 250k words of the chinese treebank 5. 1.', 'for the experiments, the data of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with #AUTHOR_TAG, however a bit different from Xue and Palmer ( 2005 )']",5
"[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), Xue and Palmer ( 2005 ) and #AUTHOR_TAG.', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), Xue and Palmer ( 2005 ) and #AUTHOR_TAG.', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), Xue and Palmer ( 2005 ) and #AUTHOR_TAG.', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), Xue and Palmer ( 2005 ) and #AUTHOR_TAG.', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005, #AUTHOR_TAG ) reass']","['of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005, #AUTHOR_TAG ) reassured these findings']","['the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005, #AUTHOR_TAG ) reass']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005, #AUTHOR_TAG ) reassured these findings']",0
"['collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', '#AUTHOR_TAG']","['collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', '#AUTHOR_TAG']","['the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky (2004), and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), #AUTHOR_TAG and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), #AUTHOR_TAG and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl']","[', the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), #AUTHOR_TAG and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as Sun and Jurafsky ( 2004 ), #AUTHOR_TAG and Xue ( 2008 ).', 'Sun and Jurafsky (2004) did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and Xue (2008) have produced more complete and systematic research on chinese srl.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['##8 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with Xue ( 2008 ), however a bit different from #AUTHOR_TAG']","['of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with Xue ( 2008 ), however a bit different from #AUTHOR_TAG']","['##8 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with Xue ( 2008 ), however a bit different from #AUTHOR_TAG']","['use chinese propbank 1. 0 ( ldc number : ldc2005t23 ) in our experiments.', 'propbank 1. 0 includes the annotations for files chtb _ 001. fid', 'to chtb _ 931. fid,', 'or the first 250k words of the chinese treebank 5. 1.', 'for the experiments, the data of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with Xue ( 2008 ), however a bit different from #AUTHOR_TAG']",1
['include : voice from #AUTHOR_TAG'],['include : voice from #AUTHOR_TAG'],['candidate feature templates include : voice from #AUTHOR_TAG'],['candidate feature templates include : voice from #AUTHOR_TAG'],5
"['of sentences from the chinese treebank ( #AUTHOR_TAG ).', 'it is constituted of two parts.', 'one is the labeled data, which indicates the positions of the predicates and']","['of sentences from the chinese treebank ( #AUTHOR_TAG ).', 'it is constituted of two parts.', 'one is the labeled data, which indicates the positions of the predicates and']","['chinese propbank has labeled the predicateargument structures of sentences from the chinese treebank ( #AUTHOR_TAG ).', 'it is constituted of two parts.', 'one is the labeled data, which indicates the positions of the predicates and']","['chinese propbank has labeled the predicateargument structures of sentences from the chinese treebank ( #AUTHOR_TAG ).', 'it is constituted of two parts.', 'one is the labeled data, which indicates the positions of the predicates and its arguments in the chinese treebank.', 'the other is a dictionary which lists the frames of all the labeled predicates.', 'figure 1 is an example from the propbank 1.', 'we put the word - by - word translation and the translation of the whole sentence below the example.', 'it is quite a complex sentence, as there are many semantic roles in it.', 'in this sentence, all the semantic roles of the verb [UNK] [UNK] ( provide ) are presented in the syntactic tree.', 'we can separate the semantic roles into two groups']",5
"['role labeling ( srl ) was first defined in #AUTHOR_TAG.', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source,']","['role labeling ( srl ) was first defined in #AUTHOR_TAG.', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question']","['role labeling ( srl ) was first defined in #AUTHOR_TAG.', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question']","['role labeling ( srl ) was first defined in #AUTHOR_TAG.', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( narayanan and harabagiu 2004 ), information extraction ( surdeanu et al. 2003 ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
"['( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', '']","['( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', '']","['instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', '']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', '']","['( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', '']","['instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', '']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ).', '']","['( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ).', '']","['instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ).', '']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( #AUTHOR_TAG ) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( #AUTHOR_TAG ) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of wikipedia']","['of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of wikipedia information to improve the disambiguation process.', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG )']",0
"['##the weps - 1 corpus includes data from the web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']","['##the weps - 1 corpus includes data from the web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']","['##the weps - 1 corpus includes data from the web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']","['##the weps - 1 corpus includes data from the web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']",5
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 )']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) (Agirre and Edmonds, 2006) and cross - document coreference ( cdc ) (Bagga and Baldwin, 1998).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 )']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['of the extracted features ( #AUTHOR_TAG ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['of the extracted features ( #AUTHOR_TAG ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people (Artiles et al., 2005).', 'as the amount of']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people (Artiles et al., 2005).', 'as the amount of']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people (Artiles et al., 2005).', 'as the amount of']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people (Artiles et al., 2005).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']",0
"['instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007) -.', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperform']","['instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007) -.', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly']","['instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007) -.', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperform']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007) -.', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['.', 'it provides a fine grained ne recognition covering 100 different ne types ( #AUTHOR_TAG ).', 'given the sparseness of most of these fine - grained ne types, we have merged them in coarser groups : event, facility, location, person, organisation, product, periodx, timex and numex']","['7 is a rule based english analyser that includes many functionalities ( pos tagger, stemmer, chunker, named entity ( ne ) tagger, dependency analyser, parser, etc ).', 'it provides a fine grained ne recognition covering 100 different ne types ( #AUTHOR_TAG ).', 'given the sparseness of most of these fine - grained ne types, we have merged them in coarser groups : event, facility, location, person, organisation, product, periodx, timex and numex']","['7 is a rule based english analyser that includes many functionalities ( pos tagger, stemmer, chunker, named entity ( ne ) tagger, dependency analyser, parser, etc ).', 'it provides a fine grained ne recognition covering 100 different ne types ( #AUTHOR_TAG ).', 'given the sparseness of most of these fine - grained ne types, we have merged them in coarser groups : event, facility, location, person, organisation, product, periodx, timex and numex']","['7 is a rule based english analyser that includes many functionalities ( pos tagger, stemmer, chunker, named entity ( ne ) tagger, dependency analyser, parser, etc ).', 'it provides a fine grained ne recognition covering 100 different ne types ( #AUTHOR_TAG ).', 'given the sparseness of most of these fine - grained ne types, we have merged them in coarser groups : event, facility, location, person, organisation, product, periodx, timex and numex']",5
"['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names (Spink et al., 2004).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people ( #AUTHOR_TAG ).', 'as the amount of']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names (Spink et al., 2004).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people ( #AUTHOR_TAG ).', 'as the amount of']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names (Spink et al., 2004).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people ( #AUTHOR_TAG ).', 'as the amount of']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names (Spink et al., 2004).', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people ( #AUTHOR_TAG ).', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']",0
"['of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of wikipedia']","['of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of wikipedia']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of wikipedia information to improve the disambiguation process.', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign ( #AUTHOR_TAG ), consists of grouping search results for a given name according to the different people that share it']","['is interested in.', 'the user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'in some cases, the existence of a predominant person ( such as a celebrity or a historical figure ) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign ( #AUTHOR_TAG ), consists of grouping search results for a given name according to the different people that share it']","['situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'the user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'in some cases, the existence of a predominant person ( such as a celebrity or a historical figure ) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign ( #AUTHOR_TAG ), consists of grouping search results for a given name according to the different people that share it']","['situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'the user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'in some cases, the existence of a predominant person ( such as a celebrity or a historical figure ) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign ( #AUTHOR_TAG ), consists of grouping search results for a given name according to the different people that share it']",0
"['have used the testbeds from weps - 1 ( #AUTHOR_TAG, 2007 ) 2 and weps -']","['have used the testbeds from weps - 1 ( #AUTHOR_TAG, 2007 ) 2 and weps - 2 (Artiles et al., 2009) evaluation campaigns 3']","['have used the testbeds from weps - 1 ( #AUTHOR_TAG, 2007 ) 2 and weps -']","['have used the testbeds from weps - 1 ( #AUTHOR_TAG, 2007 ) 2 and weps - 2 (Artiles et al., 2009) evaluation campaigns 3']",5
"['( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ).', '']","['( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ).', '']","['instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ).', '']","['most used feature for the web people search task, however, are nes.', 'Ravin (1999) introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ).', 'for instance, Blume (2005) uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) ( Agirre and Edmonds , 2006 ) and cross - document coreference ( cdc ) ( #AUTHOR_TAG ).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) ( Agirre and Edmonds , 2006 ) and cross - document coreference ( cdc ) ( #AUTHOR_TAG ).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task - web people search - on its own (Artiles et al., 2005;Artiles et al., 2007)']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) ( Agirre and Edmonds , 2006 ) and cross - document coreference ( cdc ) ( #AUTHOR_TAG ).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task - web people search - on its own (Artiles et al., 2005;Artiles et al., 2007)']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd ) ( Agirre and Edmonds , 2006 ) and cross - document coreference ( cdc ) ( #AUTHOR_TAG ).', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task - web people search - on its own (Artiles et al., 2005;Artiles et al., 2007)']",0
"['.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features ( #AUTHOR_TAG ).', 'due to the complexity of systems, the results of the weps evaluation do not provide a direct answer regarding the advantages of using nes over other computationally lighter features such as bow or word n - grams.', 'but the weps campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'in the next section we describe this dataset and how it has been adapted for our purposes']","['the 16 teams that submitted results for the first weps campaign, 10 of them 1 used nes in their document representation.', 'this makes nes the second most common type of feature ; only the bow feature was more popular.', 'other features used by the systems include noun phrases (Chen and Martin, 2007), word n - grams (Popescu and Magnini, 2007), emails and urls ( del valle - Agudo et al., 2007), etc.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features ( #AUTHOR_TAG ).', 'due to the complexity of systems, the results of the weps evaluation do not provide a direct answer regarding the advantages of using nes over other computationally lighter features such as bow or word n - grams.', 'but the weps campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'in the next section we describe this dataset and how it has been adapted for our purposes']","['.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features ( #AUTHOR_TAG ).', 'due to the complexity of systems, the results of the weps evaluation do not provide a direct answer regarding the advantages of using nes over other computationally lighter features such as bow or word n - grams.', 'but the weps campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'in the next section we describe this dataset and how it has been adapted for our purposes']","['the 16 teams that submitted results for the first weps campaign, 10 of them 1 used nes in their document representation.', 'this makes nes the second most common type of feature ; only the bow feature was more popular.', 'other features used by the systems include noun phrases (Chen and Martin, 2007), word n - grams (Popescu and Magnini, 2007), emails and urls ( del valle - Agudo et al., 2007), etc.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features ( #AUTHOR_TAG ).', 'due to the complexity of systems, the results of the weps evaluation do not provide a direct answer regarding the advantages of using nes over other computationally lighter features such as bow or word n - grams.', 'but the weps campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'in the next section we describe this dataset and how it has been adapted for our purposes']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) -.', 'other representations use the link structure (Malin, 2005) or generate graph']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', '']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'nevertheless, the full document text is present in most systems, sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) -.', 'other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the']","['our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the']","['our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']","['addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring sps for verb classification.', 'considerable research has been done on sp acquisition most of which has involved collecting argument headwords from data and generalizing to word - net classes.', 'Brockmann and Lapata ( 2003 ) have showed that wordnet - based approaches do not always outperform simple frequency - based models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']",3
"['our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the']","['our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the']","['our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']","['addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring sps for verb classification.', 'considerable research has been done on sp acquisition most of which has involved collecting argument headwords from data and generalizing to word - net classes.', 'Brockmann and Lapata ( 2003 ) have showed that wordnet - based approaches do not always outperform simple frequency - based models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ).', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']",3
"['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']",3
"['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case']",3
"['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ), although training would be much slower compared to using generative models, as in our case']","['affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ), although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ), although training would be much slower compared to using generative models, as in our case']",3
"['external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']",3
"['external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']",3
"['external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']","['external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( Basili et al. , 2005 a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG )']",3
"['more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( #AUTHOR_TAG a']","['more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( #AUTHOR_TAG a']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( #AUTHOR_TAG a']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features ( #AUTHOR_TAG a ; Basili et al. , 2005 b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees, ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 )']",3
"['##p tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory - efficient trie implementation based on a succinct']","['range of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory - efficient trie implementation based on a succinct']","['range of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory - efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']","['plan to apply our method to wider range of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory - efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage']",3
"['reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this']","['reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this']","['reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']","['reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']",2
"['##n system ( #AUTHOR_TAG ).', 'however, ualign uses deep syntactic analysis and hand - crafted heuristics in its model']","['our model uses a fixed set of lexical items that correspond to the class of function words.', 'with respect to the focus on function words, our reordering model is closely related to the ualign system ( #AUTHOR_TAG ).', 'however, ualign uses deep syntactic analysis and hand - crafted heuristics in its model']","['our model uses a fixed set of lexical items that correspond to the class of function words.', 'with respect to the focus on function words, our reordering model is closely related to the ualign system ( #AUTHOR_TAG ).', 'however, ualign uses deep syntactic analysis and hand - crafted heuristics in its model']","['reordering model is closely related to the model proposed by Zhang and Gildea (2005 ; 2007a ), with respect to conditioning the reordering predictions on lexical items.', 'these related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.', 'with respect to the focus on function words, our reordering model is closely related to the ualign system ( #AUTHOR_TAG ).', 'however, ualign uses deep syntactic analysis and hand - crafted heuristics in its model']",1
"['. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by #AUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori ( o ( l i, st ), o ( r i, st ) | y i, st ), which conditions the reordering on the lexical identity of the function word alignment ( but']","['model o ( li, s a t ), o ( ri, s a t ), i. e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by #AUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori ( o ( l i, st ), o ( r i, st ) | y i, st ), which conditions the reordering on the lexical identity of the function word alignment ( but']","['. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by #AUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori ( o ( l i, st ), o ( r i, st ) | y i, st ), which conditions the reordering on the lexical identity of the function word alignment ( but']","['model o ( li, s a t ), o ( ri, s a t ), i. e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by #AUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori ( o ( l i, st ), o ( r i, st ) | y i, st ), which conditions the reordering on the lexical identity of the function word alignment ( but independent of the lexical identity of its neighboring phrases ).', 'in particular, o maps the reordering into one of the following four orientation values ( borrowed from Nagata et al. (2006) ) with respect to the function word : monotone adjacent ( ma ), monotone gap ( mg ), reverse adjacent ( ra ) and reverse gap ( rg ).', 'the monotone / reverse distinction indicates whether the projected order follows the original order, while the adjacent / gap distinction indicates whether the pro - this heuristic is commonly used in learning phrase pairs from parallel text.', 'the maximality ensures the uniqueness of l and r']",5
"['reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this']","['reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this']","['reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']","['reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ).', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']",2
"['. whether li, s a t and ri, s a t extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of #AUTHOR_TAG.', 'taking']","['fwi + 1, s a t ), i. e. whether li, s a t and ri, s a t extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of #AUTHOR_TAG.', 'taking d ( f w i1, st ) as a case in point, this model takes the']","['. whether li, s a t and ri, s a t extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of #AUTHOR_TAG.', 'taking']","['model d ( fwi a 1, s a t ), d ( fwi + 1, s a t ), i. e. whether li, s a t and ri, s a t extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of #AUTHOR_TAG.', 'taking d ( f w i1, st ) as a case in point, this model takes the']",5
"['our previous work ( #AUTHOR_TAG ), we started an initial investigation on conversation entailment.', 'we have']","['our previous work ( #AUTHOR_TAG ), we started an initial investigation on conversation entailment.', 'we have']","['our previous work ( #AUTHOR_TAG ), we started an initial investigation on conversation entailment.', 'we have collected a dataset of 875 instances.', 'each instance consists of a conversation segment and a hypothesis ( as described in section 1 ).', 'the hypotheses are statements about conversation participants and are further categorized into four types : about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'we developed an approach that is motivated by previous work on textual entailment.', 'we use clauses in the logic - based approaches as the underlying representation of our system.', 'based on this representation, we apply a two stage entailment process similar to mac Cartney et al. (2006) developed for textual entailment : an alignment stage followed by an entailment stage']","['our previous work ( #AUTHOR_TAG ), we started an initial investigation on conversation entailment.', 'we have collected a dataset of 875 instances.', 'each instance consists of a conversation segment and a hypothesis ( as described in section 1 ).', 'the hypotheses are statements about conversation participants and are further categorized into four types : about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'we developed an approach that is motivated by previous work on textual entailment.', 'we use clauses in the logic - based approaches as the underlying representation of our system.', 'based on this representation, we apply a two stage entailment process similar to mac Cartney et al. (2006) developed for textual entailment : an alignment stage followed by an entailment stage']",2
"['the implicit modeling of argument consistency, we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in table 1']","['the implicit modeling of argument consistency, we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in table 1']","['the implicit modeling of argument consistency, we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in table 1']","['the implicit modeling of argument consistency, we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in table 1']",2
"['that in our original work ( #AUTHOR_TAG ), only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']","['that in our original work ( #AUTHOR_TAG ), only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']","['that in our original work ( #AUTHOR_TAG ), only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']","['that in our original work ( #AUTHOR_TAG ), only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']",1
"['.', 'again this model is trained from our development data described in Zhang and Chai (2009).', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG )']","['string representation of paths is used to capture both the subject consistency and the object consistency.', 'since they are non - numerical features, and the variability of their values can be extremely large, so we applied an instance - based classification model ( e. g., k - nearest neighbor ) to determine alignments between verb terms.', 'we measure the distance between two path features by their minimal string edit distance, and then simply use the euclidean distance to measure the closeness between any two verbs.', 'again this model is trained from our development data described in Zhang and Chai (2009).', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG )']","['. g., k - nearest neighbor ) to determine alignments between verb terms.', 'we measure the distance between two path features by their minimal string edit distance, and then simply use the euclidean distance to measure the closeness between any two verbs.', 'again this model is trained from our development data described in Zhang and Chai (2009).', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG )']","['string representation of paths is used to capture both the subject consistency and the object consistency.', 'since they are non - numerical features, and the variability of their values can be extremely large, so we applied an instance - based classification model ( e. g., k - nearest neighbor ) to determine alignments between verb terms.', 'we measure the distance between two path features by their minimal string edit distance, and then simply use the euclidean distance to measure the closeness between any two verbs.', 'again this model is trained from our development data described in Zhang and Chai (2009).', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG )']",5
"['address this limitation, our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment.', 'the problem was']","['address this limitation, our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment.', 'the problem was']","['address this limitation, our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment.', 'the problem was formulated as follows : given a conversation discourse d and a hypothesis h concerning its participant, the goal was to identify whether d entails']","['address this limitation, our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment.', 'the problem was formulated as follows : given a conversation discourse d and a hypothesis h concerning its participant, the goal was to identify whether d entails h.', 'for instance, as in example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', 'while our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data.', 'it is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome']",2
"['our previous work ( #AUTHOR_TAG ), conversation entailment is formulated as the following : given a conversation segment d which is represented by a set of clauses d = d1 a ... a  dm, and a hypothesis']","['our previous work ( #AUTHOR_TAG ), conversation entailment is formulated as the following : given a conversation segment d which is represented by a set of clauses d = d1 a ... a  dm, and a hypothesis']","['our previous work ( #AUTHOR_TAG ), conversation entailment is formulated as the following : given a conversation segment d which is represented by a set of clauses d = d1 a ... a  dm, and a hypothesis h represented by another set of clauses h = h1 a ... a  hn, the prediction on whether d entails h is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1...']","['our previous work ( #AUTHOR_TAG ), conversation entailment is formulated as the following : given a conversation segment d which is represented by a set of clauses d = d1 a ... a  dm, and a hypothesis h represented by another set of clauses h = h1 a ... a  hn, the prediction on whether d entails h is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1... dm as follows.', 'this is based on a simple as - sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses']",2
"['the kernel we used.', 'we use the structures previously used by #AUTHOR_TAG, and propose one new structure.', 'although we experiment']","['the kernel we used.', 'we use the structures previously used by #AUTHOR_TAG, and propose one new structure.', 'although we experimented with']","['the kernel we used.', 'we use the structures previously used by #AUTHOR_TAG, and propose one new structure.', 'although we experimented with']","['learning machines are one of the most popular machines used for classification problems.', 'the objective of a typical classification problem is to learn a function that separates the data into different classes.', 'the data is usually in the form of features extracted from abstract objects like strings, trees, etc.', 'a drawback of learning by using complex functions is that complex functions do not generalize well and thus tend to over - fit.', 'the research community therefore prefers linear classifiers over other complex classifiers.', 'but more often than not, the data is not linearly separable.', 'it can be made linearly separable by increasing the dimensionality of data but then learning suffers from the curse of dimensionality and classification becomes computationally intractable.', 'this is where kernels come to the rescue.', 'the well - known kernel trick aids us in finding similarity between feature vectors in a high dimensional space without having to write down the expanded feature space.', 'the essence of kernel methods is that they compare two feature vectors in high dimensional space by using a dot product that is a function of the dot product of feature vectors in the lower dimensional space.', 'moreover, convolution kernels ( first introduced by Haussler (1999) ) can be used to compare abstract objects instead of feature vectors.', 'this is because these kernels involve a recursive calculation over the "" parts "" of a discrete structure.', 'this calculation is usually made computationally efficient using dynamic programming techniques.', 'therefore, convolution kernels alleviate the need of feature extraction ( which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data ).', 'therefore, we use convolution kernels with a linear learning machine ( support vector machines ) for our classification task.', 'now we present the "" discrete "" structures followed by the kernel we used.', 'we use the structures previously used by #AUTHOR_TAG, and propose one new structure.', 'although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'all the structures and their combinations are derived from a variation of the underlying structures, phrase structure trees ( pst ) and dependency trees ( dt ).', 'for all trees we first extract their path enclosed tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).', 'we use the stanford parser (Klein and Manning, 2003) to get the basic psts and dts.', 'following are the structures that we refer to in our experiments and results section : pet : this refers to the smallest common phrase structure tree that contains the two target entities.', 'dependency words ( dw ) tree : this is the smallest common dependency tree that contains the two target entities.', 'in figure 1, since the target entities are at the leftmost and rightmost']",5
"['the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in']","['of social event detection and classification.', 'we show that data sampling techniques play a crucial role for the task of relation detection.', 'through oversampling we achieve an increase in f1 - measure of 22. 2 % absolute over a baseline system.', 'our experiments show that as a result of how language expresses the relevant information, dependency - based structures are best suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in']","['the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in achieving the best accuracy for']","['this paper, we have introduced the novel tasks of social event detection and classification.', 'we show that data sampling techniques play a crucial role for the task of relation detection.', 'through oversampling we achieve an increase in f1 - measure of 22. 2 % absolute over a baseline system.', 'our experiments show that as a result of how language expresses the relevant information, dependency - based structures are best suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks']",1
"['is balanced, sqgrw ( sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes ) achieves the best recall.', 'here, the pet and gr kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where gr performed much worse']","['is balanced, sqgrw ( sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes ) achieves the best recall.', 'here, the pet and gr kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where gr performed much worse']","['in the undersampled system, when the data is balanced, sqgrw ( sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes ) achieves the best recall.', 'here, the pet and gr kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where gr performed much worse']","['event detection is the task of detecting if any social event exists between a pair of entities in a sentence.', 'we formulate the problem as a binary classification task by labeling an example that does not have a social event as class - 1 and by labeling an example that either has an inr or cog social event as class 1.', 'first we present results for our baseline system.', 'our baseline system uses various structures and their combinations but without any data balancing.', '1 presents results for our baseline system.', 'grammatical relation tree structure ( gr ), a structure derived from dependency tree by replacing the words by their grammatical relations achieves the best precision.', 'this is probably because the clas - sifier learns that if both the arguments of a predicate contain target entities then it is a social event.', 'among kernels for single structures, the path enclosed tree for psts ( pet ) achieves the best recall.', 'furthermore, a combination of structures derived from psts and dts performs best.', 'the sequence kernels, perform much worse than sqgrw ( f1 - measure as low as 0. 45 ).', 'since it is the same case for all subsequent experiments, we omit them from the discussion.', 'we now turn to experiments involving sampling.', 'table 2 presents results for under - sampling, i. e. randomly removing examples belonging to the negative class until its size matches the positive class.', 'table 2 shows a large gain in f1 - measure of 9. 72 % absolute over the baseline system ( table 1 ).', 'we found that worst performing kernel with under - sampling is sk1 with an f1 - measure of 39. 2 % which is better than the best performance without undersampling.', 'these results make it clear that doing under - sampling greatly improves the performance of the classifier, despite the fact that we are using less training data ( fewer negative examples ).', 'this is as expected because we are evaluating on f1 - measure and the classifier is optimizing for accuracy.', 'absolute.', 'as in the baseline system, a combination of structures performs best.', 'as in the undersampled system, when the data is balanced, sqgrw ( sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes ) achieves the best recall.', 'here, the pet and gr kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where gr performed much worse than pet for ace data.', 'this exemplifies the difference in the nature of our event annotations from that of ace relations.', 'since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger']",1
"['results also confirm the insights gained by #AUTHOR_TAG,']","['results also confirm the insights gained by #AUTHOR_TAG,']","['results also confirm the insights gained by #AUTHOR_TAG,']","['results also confirm the insights gained by #AUTHOR_TAG, who observed that in crossdomain polarity analysis adding more training data is not always beneficial.', 'apparently even the smallest training dataset ( cameras ) contain enough feature instances to learn a model which performs well on the testing data']",1
"['( Blitzer et al. , 2007 ; #AUTHOR_TAG ), perform in comparison to our approach.', 'since three of the features we']","['( Blitzer et al. , 2007 ; #AUTHOR_TAG ), perform in comparison to our approach.', 'since three of the features we']","['that the opinion target vocabularies are substantially different.', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user -']","['this paper, we have shown how a crf - based approach for opinion target extraction performs in a single - and cross - domain setting.', 'we have presented a comparative evaluation of our approach on datasets from four different domains.', 'in the single - domain setting, our crf - based approach outperforms a supervised baseline on all four datasets.', 'our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'our crf - based approach also yields promising results in the crossdomain setting.', 'the features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user - generated from web 2. 0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'it is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user - generated discourse']",3
"['( #AUTHOR_TAG ; Jiang and Zhai , 2007 ), perform in comparison to our approach.', 'since three of the features we']","['( #AUTHOR_TAG ; Jiang and Zhai , 2007 ), perform in comparison to our approach.', 'since three of the features we']","['that the opinion target vocabularies are substantially different.', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user -']","['this paper, we have shown how a crf - based approach for opinion target extraction performs in a single - and cross - domain setting.', 'we have presented a comparative evaluation of our approach on datasets from four different domains.', 'in the single - domain setting, our crf - based approach outperforms a supervised baseline on all four datasets.', 'our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'our crf - based approach also yields promising results in the crossdomain setting.', 'the features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ), perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'although our data is user - generated from web 2. 0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'it is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user - generated discourse']",3
"['implementation of the transition - based dependency parsing frame - work ( #AUTHOR_TAG ) using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in Zhang and Clark (2008) with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack']","['implementation of the transition - based dependency parsing frame - work ( #AUTHOR_TAG ) using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in Zhang and Clark (2008) with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word iden - tity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label iden - tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left']","['implementation of the transition - based dependency parsing frame - work ( #AUTHOR_TAG ) using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in Zhang and Clark (2008) with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word iden - tity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label iden - tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left']","['implementation of the transition - based dependency parsing frame - work ( #AUTHOR_TAG ) using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in Zhang and Clark (2008) with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word iden - tity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label iden - tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunc - tions are included']",5
"['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ).', 'the work of Chang et al. (2007)']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ).', 'the work of Chang et al. (2007)']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ).', 'the work of Chang et al. (2007)']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in section 5.', 'in these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'for our setting this would mean using weak application specific signals to improve dependency parsing.', 'though we explore such ideas in our experiments, in particular for semi - supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training']",0
"['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc Donald et al., 2005).', 'we use the non - projective k - best mst algorithm to generate k - best lists ( #AUTHOR_TAG ),']","['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc Donald et al., 2005).', 'we use the non - projective k - best mst algorithm to generate k - best lists ( #AUTHOR_TAG ),']","['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc Donald et al., 2005).', 'we use the non - projective k - best mst algorithm to generate k - best lists ( #AUTHOR_TAG ),']","['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc Donald et al., 2005).', 'we use the non - projective k - best mst algorithm to generate k - best lists ( #AUTHOR_TAG ), where k = 8 for the experiments in this paper.', 'the graphbased parser features used in the experiments in this paper are defined over a word, w i at position i ; the head of this word w  ( i ) where  ( i ) provides the index of the head word ; and partof - speech tags of these words t i.', 'we use the following set of features similar to mc Donald et al. ( 2005)']",5
"['data.', 'consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the penntreebank.', 'table 2 we consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'the question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'root - f1 scores from table 2 suggest that one simple question is "" what is the main verb of this sentence? ""', 'for sentences that are questions.', 'in most cases this task is straight - forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'we feel this is a realistic partial']","['data.', 'consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the penntreebank.', 'table 2 we consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'the question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'root - f1 scores from table 2 suggest that one simple question is "" what is the main verb of this sentence? ""', 'for sentences that are questions.', 'in most cases this task is straight - forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'we feel this is a realistic partial']","['data.', 'consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the penntreebank.', 'table 2 we consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'the question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'root - f1 scores from table 2 suggest that one simple question is "" what is the main verb of this sentence? ""', 'for sentences that are questions.', 'in most cases this task is straight - forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'we feel this is a realistic partial']","['application of the augmented - loss framework is to improve parser domain portability in the presence of partially labeled data.', 'consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the penntreebank.', 'table 2 we consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'the question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'root - f1 scores from table 2 suggest that one simple question is "" what is the main verb of this sentence? ""', 'for sentences that are questions.', 'in most cases this task is straight - forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'we feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data']",1
['rules are similar to those from #AUTHOR_TAG'],['##our rules are similar to those from #AUTHOR_TAG'],['##our rules are similar to those from #AUTHOR_TAG'],['##our rules are similar to those from #AUTHOR_TAG'],1
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( #AUTHOR_TAG ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( #AUTHOR_TAG ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( #AUTHOR_TAG ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( #AUTHOR_TAG ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['implementation of graph - based parsing algorithms with an arc - factored parameterization ( #AUTHOR_TAG ).', 'we use the non - projective k - best mst algorithm to generate k - best lists (Hall, 2007),']","['implementation of graph - based parsing algorithms with an arc - factored parameterization ( #AUTHOR_TAG ).', 'we use the non - projective k - best mst algorithm to generate k - best lists (Hall, 2007),']","['implementation of graph - based parsing algorithms with an arc - factored parameterization ( #AUTHOR_TAG ).', 'we use the non - projective k - best mst algorithm to generate k - best lists (Hall, 2007),']","['implementation of graph - based parsing algorithms with an arc - factored parameterization ( #AUTHOR_TAG ).', 'we use the non - projective k - best mst algorithm to generate k - best lists (Hall, 2007), where k = 8 for the experiments in this paper.', 'the graph - based parser features used in the experiments in this paper are defined over a word, wi at po - sition i ; the head of this word w _ ( i ) where _ ( i ) provides the index of the head word ; and part - of - speech tags of these words ti.', 'we use the following set of features similar to mc Donald et al. (2005)']",5
"['it is similar in vein to self - training ( #AUTHOR_TAG ), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the']","['it is similar in vein to self - training ( #AUTHOR_TAG ), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the']","['it is similar in vein to self - training ( #AUTHOR_TAG ), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( #AUTHOR_TAG ), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in their work, they are interested in learning a parameterization over translation phrases ( including the underlying wordalignment ) which optimizes the bleu score.', 'their goal is considerably different ; they want to incorporate additional features into their model and define an objective function which allows them to do so ; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic ( parsing ) objectives']",1
"['correct reordering of an aligned target sentences.', 'we use a reordering score based on the reordering penalty from the meteor scoring metric.', 'though we could have used a further downstream measure like bleu, meteor has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where for every two treebank updates we make one augmented - loss update, 1 is a 1 - to - 1 mix, and 2 is where we make twice as many augmented - loss updates as treebank updates']","['correct reordering of an aligned target sentences.', 'we use a reordering score based on the reordering penalty from the meteor scoring metric.', 'though we could have used a further downstream measure like bleu, meteor has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where for every two treebank updates we make one augmented - loss update, 1 is a 1 - to - 1 mix, and 2 is where we make twice as many augmented - loss updates as treebank updates']","['correct reordering of an aligned target sentences.', 'we use a reordering score based on the reordering penalty from the meteor scoring metric.', 'though we could have used a further downstream measure like bleu, meteor has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where for every two treebank updates we make one augmented - loss update, 1 is a 1 - to - 1 mix, and 2 is where we make twice as many augmented - loss updates as treebank updates']","['our experiments we work with a set of english - japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.', 'we use a reordering score based on the reordering penalty from the meteor scoring metric.', 'though we could have used a further downstream measure like bleu, meteor has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where for every two treebank updates we make one augmented - loss update, 1 is a 1 - to - 1 mix, and 2 is where we make twice as many augmented - loss updates as treebank updates']",4
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( #AUTHOR_TAG ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( #AUTHOR_TAG ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( #AUTHOR_TAG ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( Wang et al. , 2007 ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( #AUTHOR_TAG ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['transition - based : an implementation of the transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - ofspeech tags of the first four words on the buffer and of the top two words on the stack']","['transition - based : an implementation of the transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - ofspeech tags of the first four words on the buffer and of the top two words on the stack ; the word']","['transition - based : an implementation of the transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - ofspeech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left and right']","['transition - based : an implementation of the transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - ofspeech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunctions are included']",5
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['terms of treebank data, the primary training corpus is the penn wall street journal treebank ( ptb ) ( #AUTHOR_TAG ).', 'we also make use of the brown corpus, and the question treebank ( qtb )']","['terms of treebank data, the primary training corpus is the penn wall street journal treebank ( ptb ) ( #AUTHOR_TAG ).', 'we also make use of the brown corpus, and the question treebank ( qtb )']","['terms of treebank data, the primary training corpus is the penn wall street journal treebank ( ptb ) ( #AUTHOR_TAG ).', 'we also make use of the brown corpus, and the question treebank ( qtb )']","['terms of treebank data, the primary training corpus is the penn wall street journal treebank ( ptb ) ( #AUTHOR_TAG ).', 'we also make use of the brown corpus, and the question treebank ( qtb )']",5
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['work that is most similar to ours is that of #AUTHOR_TAG,']","['work that is most similar to ours is that of #AUTHOR_TAG,']","['work that is most similar to ours is that of #AUTHOR_TAG,']","['work that is most similar to ours is that of #AUTHOR_TAG, who introduced the constraint driven learning algorithm ( codl ).', 'their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data ( what we call extrinsic datasets ).', 'for each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'these induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'the augmented - loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented - loss functions directly ( rather than adding a set of examples to the training set ).', 'unlike the codl approach, we do not perform complete optimization on each iteration over the unlabeled dataset ; rather, we incorporate the updates in our online learning algorithm.', 'as mentioned earlier, codl is one example of learning algorithms that use weak supervision, others include mann and mc - Callum (2010) and Ganchev et al. (2010).', 'again, these works are typically interested in using the extrinsic metric - or, in general, extrinsic information - to optimize the intrinsic metric in the absence of any labeled intrinsic data.', 'our goal is to optimize both simultaneously']",1
"['and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in their work, they are interested in learning a parameterization over translation phrases ( including the underlying wordalignment ) which optimizes the bleu score.', 'their goal is considerably different ; they want to incorporate additional features into their model and define an objective function which allows them to do so ; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic ( parsing ) objectives']",1
"['recent study by #AUTHOR_TAG also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by #AUTHOR_TAG also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by #AUTHOR_TAG also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in']","['recent study by #AUTHOR_TAG also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc Closky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in their work, they are interested in learning a parameterization over translation phrases ( including the underlying wordalignment ) which optimizes the bleu score.', 'their goal is considerably different ; they want to incorporate additional features into their model and define an objective function which allows them to do so ; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic ( parsing ) objectives']",1
['like information extraction ( #AUTHOR_TAG ) and'],['like information extraction ( #AUTHOR_TAG ) and'],"['like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 )']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( mc Donald and Nivre, 2007) and these dependencies are typically the most meaningful for down - stream tasks, e. g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 )']",0
"['., #AUTHOR_TAG, by']","['to the standard perceptron proof, e. g., #AUTHOR_TAG, by']","['., #AUTHOR_TAG, by']","['', 'identical to the standard perceptron proof, e. g., #AUTHOR_TAG, by inserting in loss - separability for normal separability']",0
"['attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks,']","['attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for']","['attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks,']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010)']",4
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ).', 'in such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'the standard setting involves training the base parser and applying it to a development set ( this is often done in a cross - validated jack - knife training framework ).', 'the reranker can then be trained to optimize for the downstream or extrinsic objective.', 'while this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser']","['obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ).', 'in such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'the standard setting involves training the base parser and applying it to a development set ( this is often done in a cross - validated jack - knife training framework ).', 'the reranker can then be trained to optimize for the downstream or extrinsic objective.', 'while this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser']","['obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ).', 'in such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'the standard setting involves training the base parser and applying it to a development set ( this is often done in a cross - validated jack - knife training framework ).', 'the reranker can then be trained to optimize for the downstream or extrinsic objective.', 'while this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser']","['obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ).', 'in such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'the standard setting involves training the base parser and applying it to a development set ( this is often done in a cross - validated jack - knife training framework ).', 'the reranker can then be trained to optimize for the downstream or extrinsic objective.', 'while this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser']",0
['entailment ( #AUTHOR_TAG )'],['entailment ( #AUTHOR_TAG )'],"['like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG )']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( mc Donald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG )']",0
"['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007)']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007)']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007)']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc Callum , 2010 ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in section 5.', 'in these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'for our setting this would mean using weak application specific signals to improve dependency parsing.', 'though we explore such ideas in our experiments, in particular for semi - supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training']",1
"['implementation of the transition - based dependency parsing framework ( #AUTHOR_TAG ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8.', 'beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack']","['implementation of the transition - based dependency parsing framework ( #AUTHOR_TAG ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8.', 'beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word']","['implementation of the transition - based dependency parsing framework ( #AUTHOR_TAG ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8.', 'beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left']","['implementation of the transition - based dependency parsing framework ( #AUTHOR_TAG ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8.', 'beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunctions are included']",5
"['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( #AUTHOR_TAG ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007)']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( #AUTHOR_TAG ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007)']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( #AUTHOR_TAG ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007)']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( #AUTHOR_TAG ), posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ).', 'the work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in section 5.', 'in these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'for our setting this would mean using weak application specific signals to improve dependency parsing.', 'though we explore such ideas in our experiments, in particular for semi - supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training']",0
['attachment score ( las ) ( #AUTHOR_TAG )'],['attachment score ( las ) ( #AUTHOR_TAG )'],['attachment score ( las ) ( #AUTHOR_TAG )'],"['the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented - loss for each one.', 'augmented - loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'we show empirical results for two extrinsic loss - functions ( optimizing for the downstream task ) : machine translation and domain adaptation ; and for one intrinsic loss - function : an arclength parsing score.', 'for some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( uas ) and labeled attachment score ( las ) ( #AUTHOR_TAG )']",5
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), mt reordering (Xu et al., 2009), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( #AUTHOR_TAG ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( #AUTHOR_TAG ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( #AUTHOR_TAG ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering ( #AUTHOR_TAG ), sentiment analysis ( Nakagawa et al. , 2010 ), mt reordering ( Xu et al. , 2009 ), and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data (Gildea, 2001; mc Closky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
['and data used in our experiments are based on the work of #AUTHOR_TAG'],['and data used in our experiments are based on the work of #AUTHOR_TAG'],['and data used in our experiments are based on the work of #AUTHOR_TAG'],['and data used in our experiments are based on the work of #AUTHOR_TAG'],5
"['by knn - svm ( #AUTHOR_TAG ), we propose a local training method, which trains sentence - wise weights instead of a single weight, to address the above two problems.', 'compared with global training methods, such as mert, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'this online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'to put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set']","['by knn - svm ( #AUTHOR_TAG ), we propose a local training method, which trains sentence - wise weights instead of a single weight, to address the above two problems.', 'compared with global training methods, such as mert, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'this online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'to put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set']","['by knn - svm ( #AUTHOR_TAG ), we propose a local training method, which trains sentence - wise weights instead of a single weight, to address the above two problems.', 'compared with global training methods, such as mert, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'this online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'to put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set']","['this paper, inspired by knn - svm ( #AUTHOR_TAG ), we propose a local training method, which trains sentence - wise weights instead of a single weight, to address the above two problems.', 'compared with global training methods, such as mert, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'this online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'to put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set']",4
"['by (#AUTHOR_TAG, 2003 ; Smith and Eisner, 2006), we approximate the error in ( 5 ) by the expected loss, and then derive the following function : x 2']","['by (#AUTHOR_TAG, 2003 ; Smith and Eisner, 2006), we approximate the error in ( 5 ) by the expected loss, and then derive the following function : x 2iiwwbii2 +']","['by (#AUTHOR_TAG, 2003 ; Smith and Eisner, 2006), we approximate the error in ( 5 ) by the expected loss, and then derive the following function : x 2']","['to the existence of l2 norm in objective function ( 5 ), the optimization algorithm mert can not be applied for this question since the exact line search routine does not hold here.', ""motivated by (#AUTHOR_TAG, 2003 ; Smith and Eisner, 2006), we approximate the error in ( 5 ) by the expected loss, and then derive the following function : x 2iiwwbii2 + a j = 1 systems nist02 nist05 nist06 nist08 moses 30. 39 26. 31 25. 34 19. 07 moses hier 33. 68 26. 94 26. 28 18. 65 in - hiero 31. 24 27. 07 26. 32 19. 03 table 1 : the performance comparison of the baseline inhiero vs moses and moses hier. with exp [ w  h ( fj, e ) ] p ( e | fj ; w ) = ( 7 ) ee'ec ; exp [ w  h ( fj, e') ], where  > 0 is a real number valued smoother."", 'one can see that, in the extreme case, for   * oc, ( 6 ) converges to ( 5 )']",4
"['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']","['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']","['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']","['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']",0
"['to some translation examples, which is similar as example - based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ).', 'instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminative']","['method resorts to some translation examples, which is similar as example - based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ).', 'instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights']","['to some translation examples, which is similar as example - based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ).', 'instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminative']","['method resorts to some translation examples, which is similar as example - based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ).', 'instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights']",1
"['2 depicts that testing each sentence with local training method takes 2. 9 seconds, which is comparable to the testing time 2. 0 seconds with global training method 4.', 'this shows that the local method is efficient.', 'further, compared to the retrieval, the local training is not the bottleneck.', 'actually, if we use lsh technique ( #AUTHOR_TAG ) in retrieval process, the local method can be easily']","['2 depicts that testing each sentence with local training method takes 2. 9 seconds, which is comparable to the testing time 2. 0 seconds with global training method 4.', 'this shows that the local method is efficient.', 'further, compared to the retrieval, the local training is not the bottleneck.', 'actually, if we use lsh technique ( #AUTHOR_TAG ) in retrieval process, the local method can be easily']","['2 depicts that testing each sentence with local training method takes 2. 9 seconds, which is comparable to the testing time 2. 0 seconds with global training method 4.', 'this shows that the local method is efficient.', 'further, compared to the retrieval, the local training is not the bottleneck.', 'actually, if we use lsh technique ( #AUTHOR_TAG ) in retrieval process, the local method can be easily']","['2 depicts that testing each sentence with local training method takes 2. 9 seconds, which is comparable to the testing time 2. 0 seconds with global training method 4.', 'this shows that the local method is efficient.', 'further, compared to the retrieval, the local training is not the bottleneck.', 'actually, if we use lsh technique ( #AUTHOR_TAG ) in retrieval process, the local method can be easily scaled to a larger training data']",3
"['##ques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ).', 'compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'it is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006)']","['local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ).', 'compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'it is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006)']","['local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ).', 'compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'it is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006)']","['local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ).', 'compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'it is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006)']",0
"['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']","['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']",0
"['##aword corpus using the srilm toolkits ( Stolcke , 2002 ) with modified kneser - ney smoothing ( #AUTHOR_TAG ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( Stolcke , 2002 ) with modified kneser - ney smoothing ( #AUTHOR_TAG ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( Stolcke , 2002 ) with modified kneser - ney smoothing ( #AUTHOR_TAG ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( Stolcke , 2002 ) with modified kneser - ney smoothing ( #AUTHOR_TAG ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']",5
"['##aword corpus using the srilm toolkits ( #AUTHOR_TAG ) with modified kneser - ney smoothing ( Chen and Goodman , 1998 ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( #AUTHOR_TAG ) with modified kneser - ney smoothing ( Chen and Goodman , 1998 ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( #AUTHOR_TAG ) with modified kneser - ney smoothing ( Chen and Goodman , 1998 ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits ( #AUTHOR_TAG ) with modified kneser - ney smoothing ( Chen and Goodman , 1998 ).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']",5
"['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']","['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']",0
"['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ), but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of a given training example, i. e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'in this section, we will investigate the incremental trainingmethodsinsmtscenario']","['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ), but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of a given training example, i. e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'in this section, we will investigate the incremental trainingmethodsinsmtscenario']","['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ), but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of a given training example, i. e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'in this section, we will investigate the incremental trainingmethodsinsmtscenario']","['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ), but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of a given training example, i. e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'in this section, we will investigate the incremental trainingmethodsinsmtscenario']",0
"['to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling ( #AUTHOR_TAG )']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling ( #AUTHOR_TAG )']","['', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling ( #AUTHOR_TAG )']","['run giza + + (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling ( #AUTHOR_TAG )']",5
"['via tuning as ranking (#AUTHOR_TAG, 2011 ).', 'since score of e11 is greater than that of e12, ( 1, 0 ) corresponds to a possitive example denoted as   , and ( 1, 0 ) corresponds to a negative example denoted as  * .', 'since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'however, one can obtain e11 and e21 with weights : ( 1, 1 ) and ( 1, 1 ),']","['b ). the nonlinearly separable classification problem transformed from ( a ) via tuning as ranking (#AUTHOR_TAG, 2011 ).', 'since score of e11 is greater than that of e12, ( 1, 0 ) corresponds to a possitive example denoted as   , and ( 1, 0 ) corresponds to a negative example denoted as  * .', 'since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'however, one can obtain e11 and e21 with weights : ( 1, 1 ) and ( 1, 1 ), respectively']","['via tuning as ranking (#AUTHOR_TAG, 2011 ).', 'since score of e11 is greater than that of e12, ( 1, 0 ) corresponds to a possitive example denoted as   , and ( 1, 0 ) corresponds to a negative example denoted as  * .', 'since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'however, one can obtain e11 and e21 with weights : ( 1, 1 ) and ( 1, 1 ), respectively']","['b ). the nonlinearly separable classification problem transformed from ( a ) via tuning as ranking (#AUTHOR_TAG, 2011 ).', 'since score of e11 is greater than that of e12, ( 1, 0 ) corresponds to a possitive example denoted as   , and ( 1, 0 ) corresponds to a negative example denoted as  * .', 'since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'however, one can obtain e11 and e21 with weights : ( 1, 1 ) and ( 1, 1 ), respectively']",0
"['works have proposed discriminative techniques to train log - linear model for smt.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum']","['works have proposed discriminative techniques to train log - linear model for smt.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum']","['works have proposed discriminative techniques to train log - linear model for smt.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['metric we consider here is derived from an example - based machine translation.', 'to retrieve translation examples for a test sentence, ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']","['metric we consider here is derived from an example - based machine translation.', 'to retrieve translation examples for a test sentence, ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']","['metric we consider here is derived from an example - based machine translation.', 'to retrieve translation examples for a test sentence, ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']","['metric we consider here is derived from an example - based machine translation.', 'to retrieve translation examples for a test sentence, ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']",5
"['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']","['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']",0
"['k j = 1, in the rest of this paper.', 'our goal is to find an optimal weight, denoted by w i, which is a local weight and used for decoding the sentence t i.', 'unlike the global method which performs tuning on the whole development set dev + d i as in algorithm 1, w i can be incrementally learned by optimizing on d i based on w b.', 'we employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in']","['k j = 1, in the rest of this paper.', 'our goal is to find an optimal weight, denoted by w i, which is a local weight and used for decoding the sentence t i.', 'unlike the global method which performs tuning on the whole development set dev + d i as in algorithm 1, w i can be incrementally learned by optimizing on d i based on w b.', 'we employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in']","['k j = 1, in the rest of this paper.', 'our goal is to find an optimal weight, denoted by w i, which is a local weight and used for decoding the sentence t i.', 'unlike the global method which performs tuning on the whole development set dev + d i as in algorithm 1, w i can be incrementally learned by optimizing on d i based on w b.', 'we employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in algorithm 2 as follows']","['the notations in algorithm 2, w b is the baseline weight, d i = { f i j, c i j, r i j } k j = 1 denotes training examples for t i.', 'for the sake of brevity, we will drop the index i, d i = { f j, c j, r j } k j = 1, in the rest of this paper.', 'our goal is to find an optimal weight, denoted by w i, which is a local weight and used for decoding the sentence t i.', 'unlike the global method which performs tuning on the whole development set dev + d i as in algorithm 1, w i can be incrementally learned by optimizing on d i based on w b.', 'we employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in algorithm 2 as follows']",5
"['estimation to learn weights for mt.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['estimation to learn weights for mt.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['use an in - house developed hierarchical phrase - based translation ( #AUTHOR_TAG ) as our baseline system, and we denote it as in - hiero.', 'to obtain satisfactory baseline performance, we tune in - hiero system for 5 times using mert, and then se - (Koehn et al., 2007).', '']","['use an in - house developed hierarchical phrase - based translation ( #AUTHOR_TAG ) as our baseline system, and we denote it as in - hiero.', 'to obtain satisfactory baseline performance, we tune in - hiero system for 5 times using mert, and then se - (Koehn et al., 2007).', '']","['use an in - house developed hierarchical phrase - based translation ( #AUTHOR_TAG ) as our baseline system, and we denote it as in - hiero.', 'to obtain satisfactory baseline performance, we tune in - hiero system for 5 times using mert, and then se - (Koehn et al., 2007).', '']","['use an in - house developed hierarchical phrase - based translation ( #AUTHOR_TAG ) as our baseline system, and we denote it as in - hiero.', 'to obtain satisfactory baseline performance, we tune in - hiero system for 5 times using mert, and then se - (Koehn et al., 2007).', 'both of these systems are with default setting.', 'all three systems are trained by mert with 100 best candidates']",5
"['run giza + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english']","['run giza + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']","['run giza + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a.']","['run giza + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits (Stolcke, 2002) with modified kneser - ney smoothing (Chen and Goodman, 1998).', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric (Papineni et al., 2002) and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling (Koehn, 2004)']",5
"['estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for mt.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', '']","['is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']","['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ), error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ), margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ), and among which minimum error rate training ( mert ) ( Och , 2003 ) is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline']",0
"[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ).', ""however, what is more interesting here is that while graph - based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40 % performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes""]","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ).', ""however, what is more interesting here is that while graph - based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40 % performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes""]","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ).', ""however, what is more interesting here is that while graph - based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40 % performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes""]","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ).', ""however, what is more interesting here is that while graph - based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40 % performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes""]",1
"['lower level visual features (Harnad, 1990).', 'previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i. e., identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ).', 'for the referring expression generation task here, we also need a']","['lower level visual features (Harnad, 1990).', 'previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i. e., identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ).', 'for the referring expression generation task here, we also need a']","['lower level visual features (Harnad, 1990).', 'previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i. e., identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ).', 'for the referring expression generation task here, we also need a lexicon with grounded semantics']","['semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990).', 'previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i. e., identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ).', 'for the referring expression generation task here, we also need a lexicon with grounded semantics']",2
"['in our previous work ( #AUTHOR_TAG ).', 'in that work, the main focus is on reference resolution : given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'since robots need to collaborate with']","['in our previous work ( #AUTHOR_TAG ).', 'in that work, the main focus is on reference resolution : given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'since robots need to collaborate with']","['this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ).', 'in that work, the main focus is on reference resolution : given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation ( reg ) becomes an equally important problem in situated dialogue.', 'robots have much lower perceptual capabilities of the environment']","['this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ).', 'in that work, the main focus is on reference resolution : given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation ( reg ) becomes an equally important problem in situated dialogue.', 'robots have much lower perceptual capabilities of the environment than humans.', 'how can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to']",2
"['models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis,']","['allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis,']","['models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis,']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['in - stead.', 'some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['in - stead.', 'some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter cat - egory, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) )']",0
"['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
"['.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. ( 2013 ) show that visual attribute classifiers, which have been immensely successful in object recognition ( #AUTHOR_TAG ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. ( 2013 ) show that visual attribute classifiers, which have been immensely successful in object recognition ( #AUTHOR_TAG ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. ( 2013 ) show that visual attribute classifiers, which have been immensely successful in object recognition ( #AUTHOR_TAG ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. ( 2013 ) show that visual attribute classifiers, which have been immensely successful in object recognition ( #AUTHOR_TAG ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
"['illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']","['illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']","['Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 )']",0
"['distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['is, we simply take the original mlda model of #AUTHOR_TAG ( 2009 ) and']","['is, we simply take the original mlda model of #AUTHOR_TAG ( 2009 ) and']","['is, we simply take the original mlda model of #AUTHOR_TAG ( 2009 ) and']","['is, we simply take the original mlda model of #AUTHOR_TAG ( 2009 ) and generalize it in the same way they generalize lda.', 'at first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi']",2
"['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010 a ; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012)']",0
"['extend lda to allow for the inference of document and topic distributions in a multimodal corpus.', 'in']","['extend lda to allow for the inference of document and topic distributions in a multimodal corpus.', 'in']","['extend lda to allow for the inference of document and topic distributions in a multimodal corpus.', 'in']","['extend lda to allow for the inference of document and topic distributions in a multimodal corpus.', 'in their model, a document consists of a set of ( word, feature ) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'topics consist of multinomial distributions over words,  k, but are extended to also include multinomial distributions over features,  k.', 'the generative process is amended to include these feature distributions']",0
"[', we compute a simple bag of visual words ( bovw ) model for our images using surf keypoints (Bay et al., 2008).', 'surf is a method for selecting points - of - interest within an image.', 'it is faster and more forgiving than the commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering ( #AUTHOR_TAG ), and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']","[', we compute a simple bag of visual words ( bovw ) model for our images using surf keypoints (Bay et al., 2008).', 'surf is a method for selecting points - of - interest within an image.', 'it is faster and more forgiving than the commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering ( #AUTHOR_TAG ), and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']","[', we compute a simple bag of visual words ( bovw ) model for our images using surf keypoints (Bay et al., 2008).', 'surf is a method for selecting points - of - interest within an image.', 'it is faster and more forgiving than the commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering ( #AUTHOR_TAG ), and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']","[', we compute a simple bag of visual words ( bovw ) model for our images using surf keypoints (Bay et al., 2008).', 'surf is a method for selecting points - of - interest within an image.', 'it is faster and more forgiving than the commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering ( #AUTHOR_TAG ), and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']",5
"['these features ( Andrews et al. , 2009 ; #AUTHOR_TAG )']","['these features ( Andrews et al. , 2009 ; #AUTHOR_TAG )']","['these features ( Andrews et al. , 2009 ; #AUTHOR_TAG )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG )']",1
"['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG )']","['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG )']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG )']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG )']",0
"['. g., #AUTHOR_TAG )']","['others choose to employ concepts elicited from psycholin - guistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. ( 2004 ) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., #AUTHOR_TAG )']","['. g., #AUTHOR_TAG )']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the us - age of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholin - guistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. ( 2004 ) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., #AUTHOR_TAG )']",0
"['##aches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( #AUTHOR_TAG ) or robot']","['language grounding problem has come in many different flavors with just as many different ap - proaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( #AUTHOR_TAG ) or robot']","['language grounding problem has come in many different flavors with just as many different ap - proaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( #AUTHOR_TAG ) or robot']","['language grounding problem has come in many different flavors with just as many different ap - proaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ).', 'some efforts have tackled tasks such as automatic image caption generation ( feng and la - pata, 2010a ; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identifica - tion of twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012)']",0
"['#AUTHOR_TAG ), text illustration ( Joshi et al. , 2006 ), or']","['#AUTHOR_TAG ), text illustration ( Joshi et al. , 2006 ), or']","['#AUTHOR_TAG ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; #AUTHOR_TAG ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']",0
"['distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; #AUTHOR_TAG ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; #AUTHOR_TAG ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; #AUTHOR_TAG ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; #AUTHOR_TAG ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non - textual modalities.', 'we use the same method as #AUTHOR_TAG for']","['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non - textual modalities.', 'we use the same method as #AUTHOR_TAG for']","['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non - textual modalities.', 'we use the same method as #AUTHOR_TAG for']","['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non - textual modalities.', ""we use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus, a feature is selected stochastically from the word's feature distribution, creating a word - feature pair."", 'words without grounded features are all given the same placeholder feature, also resulting in a word - feature pair. 5', 'that is, for the feature norm modal - ity, we generate ( word, feature norm ) pairs ; for the surf modality, we generate ( word, codeword ) pairs, etc.', 'the resulting stochastically generated corpus is used in its corresponding experiments']",5
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']",0
"['experiments are based on the multimodal extension of latent dirichlet allocation developed by #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'these multimodal lda models (']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']",5
"['see that the image modalities are much more useful than they are in compositionality prediction.', 'the surf modality does extremely well in particular, but the gist features also provide statistically significant improvements over the text - only model.', 'since the surf and gist image features tend to capture object - likeness and scene - likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', 'this seems to provide additional evidence of #AUTHOR_TAG']","['see that the image modalities are much more useful than they are in compositionality prediction.', 'the surf modality does extremely well in particular, but the gist features also provide statistically significant improvements over the text - only model.', 'since the surf and gist image features tend to capture object - likeness and scene - likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', 'this seems to provide additional evidence of #AUTHOR_TAG']","['see that the image modalities are much more useful than they are in compositionality prediction.', 'the surf modality does extremely well in particular, but the gist features also provide statistically significant improvements over the text - only model.', 'since the surf and gist image features tend to capture object - likeness and scene - likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', ""this seems to provide additional evidence of #AUTHOR_TAG b )'s suggestion that something like a distributional hypothesis of images is plausible""]","['see that the image modalities are much more useful than they are in compositionality prediction.', 'the surf modality does extremely well in particular, but the gist features also provide statistically significant improvements over the text - only model.', 'since the surf and gist image features tend to capture object - likeness and scene - likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', ""this seems to provide additional evidence of #AUTHOR_TAG b )'s suggestion that something like a distributional hypothesis of images is plausible""]",1
"['also compute gist vectors (Oliva and Torralba, 2001) for every image using leargist (Douze et al., 2009).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and #AUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors (Oliva and Torralba, 2001) for every image using leargist (Douze et al., 2009).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and #AUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors (Oliva and Torralba, 2001) for every image using leargist (Douze et al., 2009).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and #AUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors (Oliva and Torralba, 2001) for every image using leargist (Douze et al., 2009).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and #AUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']",4
"['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
['tasks such as automatic image caption generation ( #AUTHOR_TAG a'],['tasks such as automatic image caption generation ( #AUTHOR_TAG a'],['tasks such as automatic image caption generation ( #AUTHOR_TAG a'],"['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAG a ; Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']",0
"['web corpus created by the wacky group ( #AUTHOR_TAG ) containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles']","['web corpus created by the wacky group ( #AUTHOR_TAG ) containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles ; removing all stopwords ; removing word types with a total frequency of less than 500 ; and removing documents with a length shorter than 100.', 'the']","['our text modality, we use dewac, a large german web corpus created by the wacky group ( #AUTHOR_TAG ) containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles ; removing all stopwords ; removing word types with a total frequency of less than 500 ; and removing documents with a length shorter than 100.', 'the resulting corpus has 1, 038, 883 documents consisting of 75, 678 word types and 466m word tokens']","['our text modality, we use dewac, a large german web corpus created by the wacky group ( #AUTHOR_TAG ) containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles ; removing all stopwords ; removing word types with a total frequency of less than 500 ; and removing documents with a length shorter than 100.', 'the resulting corpus has 1, 038, 883 documents consisting of 75, 678 word types and 466m word tokens']",5
"['greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']",0
"['models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association']","['allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association']","['models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following #AUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for']","['it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following #AUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for']","['it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following #AUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for all possible pairs of words in our vocabulary, we compute the negative symmetric kl divergence between the two words.', 'we then compute the percentile ranks of similarity for each word pair, e. g., "" cat "" is more similar to "" dog "" than 97. 3 % of the rest of the vocabulary.', 'we report the weighted mean percentile ranks for all cue - association pairs, i. e., if a cue - association is given more than once, it is counted more than once']","[', we also evaluate using the association norms data set described in section 3. since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following #AUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for all possible pairs of words in our vocabulary, we compute the negative symmetric kl divergence between the two words.', 'we then compute the percentile ranks of similarity for each word pair, e. g., "" cat "" is more similar to "" dog "" than 97. 3 % of the rest of the vocabulary.', 'we report the weighted mean percentile ranks for all cue - association pairs, i. e., if a cue - association is given more than once, it is counted more than once']",5
"['compatibility with the multimodal latent dirichlet allocation model of #AUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda and provided significant gains in their ability to predict association norms over traditional text - only lda.', 'surf']","['compatibility with the multimodal latent dirichlet allocation model of #AUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda and provided significant gains in their ability to predict association norms over traditional text - only lda.', 'surf']","['compatibility with the multimodal latent dirichlet allocation model of #AUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda and provided significant gains in their ability to predict association norms over traditional text - only lda.', 'surf features also provided significant gains over text - only lda in predicting the compositionality of noun compounds']","['this paper, we evaluated the role of low - level image features, surf and gist, for their compatibility with the multimodal latent dirichlet allocation model of #AUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda and provided significant gains in their ability to predict association norms over traditional text - only lda.', 'surf features also provided significant gains over text - only lda in predicting the compositionality of noun compounds']",5
"['also compute gist vectors ( #AUTHOR_TAG ) for every image using leargist ( Douze et al. , 2009 ).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors ( #AUTHOR_TAG ) for every image using leargist ( Douze et al. , 2009 ).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors ( #AUTHOR_TAG ) for every image using leargist ( Douze et al. , 2009 ).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors ( #AUTHOR_TAG ) for every image using leargist ( Douze et al. , 2009 ).', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']",5
"['illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']","['illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']","['Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; Ordonez et al. , 2011 ), text illustration ( Joshi et al. , 2006 ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG )']",0
"['. g., mc Rae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive -']","['others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent']","['. g., mc Rae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperform']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', '#AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis ( Deerwester et al. , 1990 ) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['.', 'to name a few examples, Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']",0
"['noun - to - imagenet synset mappings.', 'imagenet is a large - scale and widely used image database, built on top of wordnet, which maps words into groups of images, called synsets ( #AUTHOR_TAG ).', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides']","['noun - to - imagenet synset mappings.', 'imagenet is a large - scale and widely used image database, built on top of wordnet, which maps words into groups of images, called synsets ( #AUTHOR_TAG ).', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides']","['##abian german ) is our new data set of german noun - to - imagenet synset mappings.', 'imagenet is a large - scale and widely used image database, built on top of wordnet, which maps words into groups of images, called synsets ( #AUTHOR_TAG ).', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides']","['##ernetle ( "" little imagenet "" in swabian german ) is our new data set of german noun - to - imagenet synset mappings.', 'imagenet is a large - scale and widely used image database, built on top of wordnet, which maps words into groups of images, called synsets ( #AUTHOR_TAG ).', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides mappings from german noun types to images of the nouns via imagenet']",5
"['this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal']","['allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal']","['this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']","['helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition ( Farhadi et al. , 2009 ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition ( Farhadi et al. , 2009 ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition ( Farhadi et al. , 2009 ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition ( Farhadi et al. , 2009 ), act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
['the other parameters are selected as the best from #AUTHOR_TAG'],['the other parameters are selected as the best from #AUTHOR_TAG'],['the other parameters are selected as the best from #AUTHOR_TAG'],"['all settings, we fix all dirichlet priors at 0. 1, use a learning rate 0. 7, and use minibatch sizes of 1024 documents.', 'we do not optimize these hyperparameters or vary them over time.', 'the high dirichlet priors are chosen to prevent sparsity in topic distributions, while the other parameters are selected as the best from #AUTHOR_TAG']",5
"['distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']",0
"['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ).', 'it has also been shown to be useful in joint inference of text with visual']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ).', 'it has also been shown to be useful in joint inference of text with visual']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'these multimodal lda models (']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']",0
"['distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 )']","['norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 )']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 )']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is Feng and Lapata (2010 b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 )']",0
"['norms ( an ) is a collection of association norms collected by schulte im #AUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the']","['norms ( an ) is a collection of association norms collected by schulte im #AUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the']","['norms ( an ) is a collection of association norms collected by schulte im #AUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the data set contains a total of 95, 214 cue - response pairs for 1, 012 nouns and 5, 716 response types']","['norms ( an ) is a collection of association norms collected by schulte im #AUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the data set contains a total of 95, 214 cue - response pairs for 1, 012 nouns and 5, 716 response types']",5
"['dirichlet allocation ( #AUTHOR_TAG ), or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that all documents are probabilistically generated from a shared set of k common topics, where each topic is a multinomial distribution over the vocabulary ( notated as  ), and documents are modeled as mixtures of these shared topics ( notated as  ).', 'lda assumes every document in the corpus is generated using the fol - lowing generative process']","['dirichlet allocation ( #AUTHOR_TAG ), or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that all documents are probabilistically generated from a shared set of k common topics, where each topic is a multinomial distribution over the vocabulary ( notated as  ), and documents are modeled as mixtures of these shared topics ( notated as  ).', 'lda assumes every document in the corpus is generated using the fol - lowing generative process']","['dirichlet allocation ( #AUTHOR_TAG ), or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that all documents are probabilistically generated from a shared set of k common topics, where each topic is a multinomial distribution over the vocabulary ( notated as  ), and documents are modeled as mixtures of these shared topics ( notated as  ).', 'lda assumes every document in the corpus is generated using the fol - lowing generative process']","['dirichlet allocation ( #AUTHOR_TAG ), or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that all documents are probabilistically generated from a shared set of k common topics, where each topic is a multinomial distribution over the vocabulary ( notated as  ), and documents are modeled as mixtures of these shared topics ( notated as  ).', 'lda assumes every document in the corpus is generated using the fol - lowing generative process']",0
"['raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ),']","['raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['greatly from efforts to unify the two modalities.', 'to name a few examples, #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples, #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos']",0
"['raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ),']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG further']","['allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis,']","['latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG further']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. (2007) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['obtained using visual classifiers ( #AUTHOR_TAG ).', 'these']","['obtained using visual classifiers ( #AUTHOR_TAG ).', 'these']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ).', 'these multimodal lda models (']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by Andrews et al. (2009).', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ).', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']",0
"['Ordonez et al. , 2011 ), text illustration ( #AUTHOR_TAG ), or']","['Ordonez et al. , 2011 ), text illustration ( #AUTHOR_TAG ), or']","['Ordonez et al. , 2011 ), text illustration ( #AUTHOR_TAG ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010 a ; Ordonez et al. , 2011 ), text illustration ( #AUTHOR_TAG ), or automatic location identification of twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 )']",0
"['( Tellex et al. , 2011 ; #AUTHOR_TAG ).', 'some efforts have']","['( Tellex et al. , 2011 ; #AUTHOR_TAG ).', 'some efforts have']","['( Tellex et al. , 2011 ; #AUTHOR_TAG ).', 'some efforts']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ).', 'some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010 a ; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012)']",0
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG )']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG )']",0
"['solve these scaling issues, we implement online variational bayesian inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a']","['solve these scaling issues, we implement online variational bayesian inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a']","['solve these scaling issues, we implement online variational bayesian inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']","['solve these scaling issues, we implement online variational bayesian inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']",5
"['distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAG a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010 b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012 a ; Bruni et al. , 2012 b ; Silberer et al. , 2013 ).', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words "" (Bruni et al., 2012 b )']",0
"['these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']","['these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']","['these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 )']",1
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']",0
"['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr, Von Ahn ( 2006 ) ), computing power, improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ; Tettamanti et al. , 2005 ;  aziz - Zadeh et al. , 2006 )']",0
"['latent semantic analysis ( #AUTHOR_TAG ) in the prediction of association norms.', 'Andrews et al. (2009) further']","['allocation outperformed latent semantic analysis ( #AUTHOR_TAG ) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis,']","['latent semantic analysis ( #AUTHOR_TAG ) in the prediction of association norms.', 'Andrews et al. (2009) further']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g., Nelson et al. (2004) ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc Rae et al. (2005) ).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis ( #AUTHOR_TAG ) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein, Steyvers (2010) showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['solve these scaling issues, we implement online variational bayesian inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a']","['solve these scaling issues, we implement online variational bayesian inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a']","['solve these scaling issues, we implement online variational bayesian inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']","['solve these scaling issues, we implement online variational bayesian inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']",5
"['norms.', 'the first work to do this with topic models is #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['norms.', 'the first work to do this with topic models is #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012 a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012 b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)']",0
"['of latent dirichlet allocation.', 'this model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ).', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model and provide improved representations of word meaning.', 'we also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'finally, we describe two ways to extend the model by']","['of latent dirichlet allocation.', 'this model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ).', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model and provide improved representations of word meaning.', 'we also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'finally, we describe two ways to extend the model by']","['was originally developed by Andrews et al. (2009) and is based on a generalization of latent dirichlet allocation.', 'this model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ).', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model and provide improved representations of word meaning.', 'we also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'finally, we describe two ways to extend the model by']","['this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'the model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of latent dirichlet allocation.', 'this model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ).', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model and provide improved representations of word meaning.', 'we also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'finally, we describe two ways to extend the model by incorporating three or more modalities.', 'we find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'we release both our code and data to the community for future research.']",2
"['we made previously ( #AUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their']","['we made previously ( #AUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their']","['a stance sequence, with one stance la - bel for each post in the input post sequence.', 'this choice is motivated by an observation we made previously ( #AUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their']","['p2, on the other hand, we recast sc as a se - quence labeling task.', 'in other words, we train a sc model that assumes as input a post sequence and outputs a stance sequence, with one stance la - bel for each post in the input post sequence.', 'this choice is motivated by an observation we made previously ( #AUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.']",2
"['dependencybased features capture the syntactic dependencies, frame - semantic features encode the semantic representation of the concepts in a sentence.', 'following our previous work on stance classification ( #AUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from semafor ( Das et al. , 2010 ).', 'frame - word interaction features encode whether two words appear in different elements of the same frame.', 'hence, each frame - word interaction feature consists of ( 1 ) the name of the frame f from which it is created,']","['dependencybased features capture the syntactic dependencies, frame - semantic features encode the semantic representation of the concepts in a sentence.', 'following our previous work on stance classification ( #AUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from semafor ( Das et al. , 2010 ).', 'frame - word interaction features encode whether two words appear in different elements of the same frame.', 'hence, each frame - word interaction feature consists of ( 1 ) the name of the frame f from which it is created,']","['dependencybased features capture the syntactic dependencies, frame - semantic features encode the semantic representation of the concepts in a sentence.', 'following our previous work on stance classification ( #AUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from semafor ( Das et al. , 2010 ).', 'frame - word interaction features encode whether two words appear in different elements of the same frame.', 'hence, each frame - word interaction feature consists of ( 1 ) the name of the frame f from which it is created,']","['- semantic features.', 'while dependencybased features capture the syntactic dependencies, frame - semantic features encode the semantic representation of the concepts in a sentence.', 'following our previous work on stance classification ( #AUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from semafor ( Das et al. , 2010 ).', 'frame - word interaction features encode whether two words appear in different elements of the same frame.', 'hence, each frame - word interaction feature consists of ( 1 ) the name of the frame f from which it is created,']",2
"['follows our previous work #AUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype (Langlais et al., 2000), caitra (Koehn, 2009 b ), thot ( ortiz - martinez and Casacuberta, 2014), transcenter (Denkowski et al., 2014 b ), and casmacat (Alabau et al., 2013).', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative']","['who compared scratch, post - edit, and simple interactive modes.', 'however, he used undergraduate, non - professional subjects, and did not consider re - tuning.', 'our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype (Langlais et al., 2000), caitra (Koehn, 2009 b ), thot ( ortiz - martinez and Casacuberta, 2014), transcenter (Denkowski et al., 2014 b ), and casmacat (Alabau et al., 2013).', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative']","['process study most similar to ours is that of Koehn (2009 a ), who compared scratch, post - edit, and simple interactive modes.', 'however, he used undergraduate, non - professional subjects, and did not consider re - tuning.', 'our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype (Langlais et al., 2000), caitra (Koehn, 2009 b ), thot ( ortiz - martinez and Casacuberta, 2014), transcenter (Denkowski et al., 2014 b ), and casmacat (Alabau et al., 2013).', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the hci literature']","['process study most similar to ours is that of Koehn (2009 a ), who compared scratch, post - edit, and simple interactive modes.', 'however, he used undergraduate, non - professional subjects, and did not consider re - tuning.', 'our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype (Langlais et al., 2000), caitra (Koehn, 2009 b ), thot ( ortiz - martinez and Casacuberta, 2014), transcenter (Denkowski et al., 2014 b ), and casmacat (Alabau et al., 2013).', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the hci literature']",2
"['her power.', ""in other words, the variation in the topic shifting tendencies is significantly correlated with the candidates'recent poll standings."", 'candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'this is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators.', 'it is also in line with the findings by Prabhakaran et al. (2013']","[""and candidate's power."", 'we obtain a highly significant ( p = 0. 002 ) negative correlation between topic shift tendency of a candidate ( pi ) and his / her power.', ""in other words, the variation in the topic shifting tendencies is significantly correlated with the candidates'recent poll standings."", 'candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'this is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators.', 'it is also in line with the findings by Prabhakaran et al. (2013']","['her power.', ""in other words, the variation in the topic shifting tendencies is significantly correlated with the candidates'recent poll standings."", 'candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'this is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators.', 'it is also in line with the findings by']","[""1 shows the pearson's product correlation between each topical feature and candidate's power."", 'we obtain a highly significant ( p = 0. 002 ) negative correlation between topic shift tendency of a candidate ( pi ) and his / her power.', ""in other words, the variation in the topic shifting tendencies is significantly correlated with the candidates'recent poll standings."", 'candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'this is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators.', 'it is also in line with the findings by Prabhakaran et al. (2013 a ) that candidates with higher power tend not to interrupt others.', 'on the other hand, we did not obtain any significant correlation for the features proposed by Nguyen et al. (2013)']",1
"['or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993; lobner, 1998 ).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun ( different - head coreference ).', 'we follow our previous work ( #AUTHOR_TAG b ) and']","['or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993; lobner, 1998 ).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun ( different - head coreference ).', 'we follow our previous work ( #AUTHOR_TAG b ) and']","['or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993; lobner, 1998 ).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun ( different - head coreference ).', 'we follow our previous work ( #AUTHOR_TAG b ) and restrict bridging']","['or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993; lobner, 1998 ).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun ( different - head coreference ).', 'we follow our previous work ( #AUTHOR_TAG b ) and restrict bridging to non - coreferential cases.', 'we also exclude comparative anaphora (Modjeska et al., 2003)']",2
"['into mlsystem rulefeats as the features.', 'mlsystem rulefeats + atomfeats we augment mlsystem rulefeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAG a ; Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection.', 'some of these features overlap with the atomic features used in the rule - based system']","['into mlsystem rulefeats as the features.', 'mlsystem rulefeats + atomfeats we augment mlsystem rulefeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAG a ; Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection.', 'some of these features overlap with the atomic features used in the rule - based system']","[', we report the mlsystem rulefeats we provide mlsystem rulefeats with the same knowledge resources as the rule - based system.', 'all rules from the rule - based system are incorporated into mlsystem rulefeats as the features.', 'mlsystem rulefeats + atomfeats we augment mlsystem rulefeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAG a ; Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection.', 'some of these features overlap with the atomic features used in the rule - based system']","['in isnotes, 71 % of np antecedents occur in the same or up to two sentences prior to the anaphor.', 'initial experiments show that increasing the window size more than two sentences decreases the performance. 8', 'to deal with data imbalance, the svm light parameter is set according to the ratio between positive and negative instances in the training set. 9', 'to compare the learning - based approach to the rulebased system described in section 3 directly, we report the mlsystem rulefeats we provide mlsystem rulefeats with the same knowledge resources as the rule - based system.', 'all rules from the rule - based system are incorporated into mlsystem rulefeats as the features.', 'mlsystem rulefeats + atomfeats we augment mlsystem rulefeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAG a ; Hou et al. , 2013 b ) on bridging anaphora recognition and antecedent selection.', 'some of these features overlap with the atomic features used in the rule - based system']",2
"['both the phrase structure tree and the input sentence.', 'at each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'this sequence of decisions is the derivation of the tree, which we will denote d1,..., dm.', ""because there is a one - to - one mapping from phrase structure trees to our derivations, the probability of a derivation p ( di,..., dm ) is equal to the joint probability of the derivation's tree and the input sentence."", 'the probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history - based models ( #AUTHOR_TAG ), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d, _ 1, which is called the derivation history at step i.', 'this allows us to use the chain rule for conditional']","['both the phrase structure tree and the input sentence.', 'at each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'this sequence of decisions is the derivation of the tree, which we will denote d1,..., dm.', ""because there is a one - to - one mapping from phrase structure trees to our derivations, the probability of a derivation p ( di,..., dm ) is equal to the joint probability of the derivation's tree and the input sentence."", 'the probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history - based models ( #AUTHOR_TAG ), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d, _ 1, which is called the derivation history at step i.', 'this allows us to use the chain rule for conditional']","['both the phrase structure tree and the input sentence.', 'at each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'this sequence of decisions is the derivation of the tree, which we will denote d1,..., dm.', ""because there is a one - to - one mapping from phrase structure trees to our derivations, the probability of a derivation p ( di,..., dm ) is equal to the joint probability of the derivation's tree and the input sentence."", 'the probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history - based models ( #AUTHOR_TAG ), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d, _ 1, which is called the derivation history at step i.', 'this allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its']","['probability model we use is generative and history - based.', 'generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'at each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'this sequence of decisions is the derivation of the tree, which we will denote d1,..., dm.', ""because there is a one - to - one mapping from phrase structure trees to our derivations, the probability of a derivation p ( di,..., dm ) is equal to the joint probability of the derivation's tree and the input sentence."", 'the probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history - based models ( #AUTHOR_TAG ), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d, _ 1, which is called the derivation history at step i.', 'this allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']",5
"['_ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']","['_ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']","['_ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG )']",1
['##11 our results are computed with the evalb program following the now - standard criteria in ( #AUTHOR_TAG )'],['##a11 our results are computed with the evalb program following the now - standard criteria in ( #AUTHOR_TAG )'],['##11 our results are computed with the evalb program following the now - standard criteria in ( #AUTHOR_TAG )'],['##a11 our results are computed with the evalb program following the now - standard criteria in ( #AUTHOR_TAG )'],5
"['statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history - based probability model ( #AUTHOR_TAG ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases']","['statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history - based probability model ( #AUTHOR_TAG ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases']","['statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history - based probability model ( #AUTHOR_TAG ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases']","['statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history - based probability model ( #AUTHOR_TAG ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']",0
"['_ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']","['_ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']","['_ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 )']",1
"[""feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( #AUTHOR_TAG )""]","[""feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( #AUTHOR_TAG )""]","['a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( #AUTHOR_TAG )""]","['probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'the difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'one alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 we extended the left - corner parsing model in a few minor ways using grammar transforms.', 'we replace chomsky adjunction structures ( i. e.', 'structures of the form [ x [ x... ] [ y... ] ] ) with a special "" modifier "" link in the tree ( becoming [ x... [ mod y  requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'we also compiled some frequent chains of non - branching nodes ( such as [ s [ vp... 1 ] ) into a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( #AUTHOR_TAG )""]",0
"['is the parent, conditioning on which has been found to be beneficial ( #AUTHOR_TAG ),']","['is the parent, conditioning on which has been found to be beneficial ( #AUTHOR_TAG ),']","['any ).', 'for right - branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial ( #AUTHOR_TAG ),']","['principle we apply when designing d ( top, ) and f is that we want the inductive bias to reflect structural locality.', 'for this reason, d ( top ) includes nodes which are structurally local to top,.', ""these nodes are the left - corner ancestor of top, ( which is below top, on the stack ), top's left - corner child ( its leftmost child, if any ), and top's most recent child ( which was top, _ 1, if any )."", 'for right - branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial ( #AUTHOR_TAG ), as has conditioning on the left - corner child ( Roark and Johnson , 1999 ).', 'because these inputs include the history features of both the left - corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i 1, and thus ( by induction ) any information from the entire previous derivation history could in principle be stored in the history features.', 'thus this model is making no a priori hard independence assumptions, just a priori soft biases']",0
"[').', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', '#AUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the']","['a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', '#AUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the']","['a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( Collins , 2000 )""]","['probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'the difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'one alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 we extended the left - corner parsing model in a few minor ways using grammar transforms.', 'we replace chomsky adjunction structures ( i. e.', 'structures of the form [ x [ x... ] [ y... ] ] ) with a special "" modifier "" link in the tree ( becoming [ x... [ mod y  requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'we also compiled some frequent chains of non - branching nodes ( such as [ s [ vp... 1 ] ) into a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features ( Collins , 2000 )""]",0
"['compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']","['compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']","['compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']","['these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system']",5
"['model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexical']","['of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']",1
"['statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history - based probability model ( Black et al. , 1993 ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases']","['statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history - based probability model ( Black et al. , 1993 ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases']","['statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history - based probability model ( Black et al. , 1993 ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases']","['statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history - based probability model ( Black et al. , 1993 ), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns ) (Lane and Henderson, 2001;Henderson, 2000).', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state - of - the - art for parsing the penn treebank']",0
"['is the parent, conditioning on which has been found to be beneficial ( Johnson , 1998 ), as has conditioning on the left - corner child ( #AUTHOR_TAG ).', 'because these']","['is the parent, conditioning on which has been found to be beneficial ( Johnson , 1998 ), as has conditioning on the left - corner child ( #AUTHOR_TAG ).', 'because these']","['any ).', 'for right - branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial ( Johnson , 1998 ), as has conditioning on the left - corner child ( #AUTHOR_TAG ).', 'because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i - 1,']","['principle we apply when designing d ( top, ) and f is that we want the inductive bias to reflect structural locality.', 'for this reason, d ( top ) includes nodes which are structurally local to top,.', ""these nodes are the left - corner ancestor of top, ( which is below top, on the stack ), top's left - corner child ( its leftmost child, if any ), and top's most recent child ( which was top, _ 1, if any )."", 'for right - branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial ( Johnson , 1998 ), as has conditioning on the left - corner child ( #AUTHOR_TAG ).', 'because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i - 1, and thus ( by induction ) any information from the entire previous derivation history could in principle be stored in the history features.', 'thus this model is making no a priori hard independence assumptions, just a priori soft biases']",0
"['model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexical']","['of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']",1
"['model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexical']","['of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ).', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'the best current model (Collins, 2000) has only 6 % less precision error and only 11 % less recall error than the lexicalized model.', 'the ssn parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'it is also achieved without any explicit notion of lexical head']",1
"['inducing a finite set of features for representing the derivation history.', 'the method is a form of multi - layered artificial neural network called simple synchrony networks (Lane and Henderson, 2001;Henderson, 2000).', 'the outputs of this network are probability estimates computed with a log - linear model ( also known as a maximum entropy model ), as is done in (Ratnaparkhi, 1999).', 'log - linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ).', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcraft']","['inducing a finite set of features for representing the derivation history.', 'the method is a form of multi - layered artificial neural network called simple synchrony networks (Lane and Henderson, 2001;Henderson, 2000).', 'the outputs of this network are probability estimates computed with a log - linear model ( also known as a maximum entropy model ), as is done in (Ratnaparkhi, 1999).', 'log - linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ).', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcrafted features, but instead we use a finite vector of real - valued features which are induced as part of the neural network training process.', 'these induced features represent the information about the derivation history which the training process has']","['this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'the method is a form of multi - layered artificial neural network called simple synchrony networks (Lane and Henderson, 2001;Henderson, 2000).', 'the outputs of this network are probability estimates computed with a log - linear model ( also known as a maximum entropy model ), as is done in (Ratnaparkhi, 1999).', 'log - linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ).', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcrafted features, but instead we use a finite vector of real - valued features which are induced as part of the neural network training process.', 'these induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'in neural networks these feature vectors are called the hidden layer activations, but']","['this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'the method is a form of multi - layered artificial neural network called simple synchrony networks (Lane and Henderson, 2001;Henderson, 2000).', 'the outputs of this network are probability estimates computed with a log - linear model ( also known as a maximum entropy model ), as is done in (Ratnaparkhi, 1999).', 'log - linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ).', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcrafted features, but instead we use a finite vector of real - valued features which are induced as part of the neural network training process.', 'these induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'in neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features']",1
"['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', 'this is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and bods pcfg - reduction reported in table 1.', 'compared to the reranking technique in Collins (2000),']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', 'this is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and bods pcfg - reduction reported in table 1.', 'compared to the reranking technique in Collins (2000),']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', 'this is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and bods pcfg - reduction reported in table 1.', 'compared to the reranking technique in Collins (2000),']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', 'this is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and bods pcfg - reduction reported in table 1.', 'compared to the reranking technique in Collins (2000), who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']",1
"['2003 ). #AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', ""compute the best parse with dop1's subtrees, reporting a 5. 1 % relative reduction in error rate over the model in Bod (2001) on the wsj. Collins ( 1999 ) furthermore showed"", ""how bonnema et al.'s ( 1999 ) and Goodman (2002) estimators can be""]","['2003 ). #AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', ""compute the best parse with dop1's subtrees, reporting a 5. 1 % relative reduction in error rate over the model in Bod (2001) on the wsj. Collins ( 1999 ) furthermore showed"", ""how bonnema et al.'s ( 1999 ) and Goodman (2002) estimators can be""]","['2003 ). #AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', ""compute the best parse with dop1's subtrees, reporting a 5. 1 % relative reduction in error rate over the model in Bod (2001) on the wsj. Collins ( 1999 ) furthermore showed"", ""how bonnema et al.'s ( 1999 ) and Goodman (2002) estimators can be incorporated in his pc""]","['##bi n - best search ( bod 2001 ), or by restricting the set of subtrees ( sima', ""' an 1999 ; chappelier et al. 2002 ). Bod (1993) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the"", ""most probable parse. Sima'an (1995) goodman (, 1998 developed a polynomial time pcfg - reduction of"", ""dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar. while goodman's method does still not allow for an"", 'efficient computation of the most probable parse in dop1, it does efficiently compute the "" maximum constituents parse "", i. e. the parse tree which is most likely to have the largest number of correct constituents. Goodman (1996 b showed that dop1\'s subtree estimation method is statistically biased and inconsistent. Johnson (1998 a ) solved this problem by', 'training the subtree probabilities by a maximum likelihood procedure based on expectation - maximization. this resulted in a statistically consistent model dubbed ml - dop. however, ml', '- dop suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from. cross - validation is needed to avoid this problem. but even with cross - validation,', ""ml - dop is outperformed by the much simpler dop1 model on both the atis and ovis treebanks ( bod 2000b ). Bod (2000 observed that another problem with dop1's subtree - estimation method is that it provides more probability to nodes with more"", 'subtrees, and therefore more probability to larger subtrees. as an alternative, Bonnema et al. (1999)', 'propose a subtree estimator which reduces the probability of a tree by a factor of two for each non - root non', '- terminal it contains. Bonnema et al. (1999) used an alternative technique which samples a', 'fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. although bod', ""' s method obtains very competitive results on the wall"", 'street journal ( wsj ) task, the parsing time was reported to be over 200 seconds per sentence ( bod 2003 ). #AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', ""compute the best parse with dop1's subtrees, reporting a 5. 1 % relative reduction in error rate over the model in Bod (2001) on the wsj. Collins ( 1999 ) furthermore showed"", ""how bonnema et al.'s ( 1999 ) and Goodman (2002) estimators can be incorporated in his pcfgreduction, but did not report any experiments with these"", 'reductions']",0
"['- reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to #AUTHOR_TAG""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to #AUTHOR_TAG""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to #AUTHOR_TAG""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to #AUTHOR_TAG""]",0
"['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', ""this is roughly an 11 % relative reduction in error rate over Charniak (2000) and bod's pcfg - reduction reported in table 1."", 'compared to the reranking technique in #AUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', ""this is roughly an 11 % relative reduction in error rate over Charniak (2000) and bod's pcfg - reduction reported in table 1."", 'compared to the reranking technique in #AUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', ""this is roughly an 11 % relative reduction in error rate over Charniak (2000) and bod's pcfg - reduction reported in table 1."", 'compared to the reranking technique in #AUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']","['2. results of sl - dop and ls - dop on the wsj ( sentences 100 words ) note that there is an increase in accuracy for both sl - dop and ls - dop if the value of n increases from 1 to 12.', 'but while the accuracy of sl - dop decreases after n = 14 and converges to simplicity - dop, the accuracy of ls - dop continues to increase and converges to likelihood - dop.', 'the highest accuracy is obtained by sl - dop at 12 n 14 : an lp of 90. 8 % and an lr of 90. 7 %.', ""this is roughly an 11 % relative reduction in error rate over Charniak (2000) and bod's pcfg - reduction reported in table 1."", 'compared to the reranking technique in #AUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']",1
"['- reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to Charniak ( 2000 ) and #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]",0
"[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), #AUTHOR_TAG, Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), #AUTHOR_TAG, Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), #AUTHOR_TAG, Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), #AUTHOR_TAG, Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of Bod (2001) given in section 2. 2']",0
"['- reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and #AUTHOR_TAG ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained,']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and #AUTHOR_TAG ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained,']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and #AUTHOR_TAG ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained,']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and #AUTHOR_TAG ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the pcfg - reduction.', 'in the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001)']",1
"['our experiments we used the standard division of the wsj ( #AUTHOR_TAG ), with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 241']","['our experiments we used the standard division of the wsj ( #AUTHOR_TAG ), with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 2416 sentences 100 words ) ; section 22 was used as development set.', 'as usual, all trees were stripped']","['our experiments we used the standard division of the wsj ( #AUTHOR_TAG ), with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 2416 sentences 100 words ) ; section 22 was used as development set.', 'as usual, all trees were stripped off']","['our experiments we used the standard division of the wsj ( #AUTHOR_TAG ), with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 2416 sentences 100 words ) ; section 22 was used as development set.', 'as usual, all trees were stripped off their semantic tags, co - reference information and quotation marks.', 'without loss of generality, all trees were converted to binary branching ( and were reconverted to n - ary trees after parsing ).', 'we employed the same unknown ( category ) word model as in Bod (2001), based on statistics on word - endings, hyphenation and capitalization in combination with good - turing ( bod 1998 : 85 - 87 ).', 'we used "" evalb "" 4 to compute the standard parseval scores for our results ( manning & schiitze 1999 ).', 'we focused on the labeled precision ( lp ) and labeled recall ( lr ) scores, as these are commonly used to rank parsing systems']",5
"['- reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', '#AUTHOR_TAG, charniak 1997, collins 1999 and char']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', '#AUTHOR_TAG, charniak 1997, collins 1999 and charniak 2000 ).', '( 1996 ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained,']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', '#AUTHOR_TAG, charniak 1997, collins 1999 and char']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', '#AUTHOR_TAG, charniak 1997, collins 1999 and charniak 2000 ).', '( 1996 ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in Bod (2001 bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the pcfg - reduction.', 'in the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001)']",1
"['- reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to #AUTHOR_TAG and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to #AUTHOR_TAG and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to #AUTHOR_TAG and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and Bod's (2001) estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', 'Bod (2001 bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to #AUTHOR_TAG and Collins ( 2000 ), bonnema et al.'s estimator performs worse and is comparable to Collins ( 1996 )""]",0
"['##k 2000 ; goodman 1998 ).', ""and #AUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has""]","['', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""and #AUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has""]","['', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""and #AUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of (""]","['other innovation of dop was to take ( in principle ) all corpus fragments, of any size, rather than a small subset.', 'this innovation has not become generally adopted yet : many approaches still work either with local trees, i. e. single level rules with limited means of information percolation, or with restricted fragments, as in stochastic tree - adjoining grammar ( schabes 1992 ; chiang 2000 ) that do not include nonlexicalized fragments.', 'however, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'while the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head - words, later models showed the importance of including context from higher nodes in the tree ( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g.', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""and #AUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]",0
"[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence."", 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence."", 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence."", 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence."", 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of Bod (2001) given in section 2. 2']",0
"[""dop models, such as in Bod ( 1993 ), #AUTHOR_TAG, Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), #AUTHOR_TAG, Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), #AUTHOR_TAG, Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","[""dop models, such as in Bod ( 1993 ), #AUTHOR_TAG, Bonnema et al. ( 1997 ), Sima'an ( 2000 ) and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e."", 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of Bod (2001) given in section 2. 2']",0
"['dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), #AUTHOR_TAG and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","['dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), #AUTHOR_TAG and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","['dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), #AUTHOR_TAG and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will']","['dop models, such as in Bod ( 1993 ), Goodman ( 1996 ), Bonnema et al. ( 1997 ), #AUTHOR_TAG and Collins & Duffy ( 2002 ), use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of Bod (2001) given in section 2. 2']",0
"['range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by #AUTHOR_TAG, 2002 ).', 'here we will only sketch this pc']","['range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by #AUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on Goodman (2002)']","['dop1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by #AUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on Goodman (2002)']","['dop1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by #AUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on Goodman (2002)']",0
"['by the work of #AUTHOR_TAG, 1999 ), Charniak ( 1996 , 1997 ), Johnson ( 1998 ), Chiang ( 2000 ), and many']","['by the work of #AUTHOR_TAG, 1999 ), Charniak ( 1996 , 1997 ), Johnson ( 1998 ), Chiang ( 2000 ), and many']","['by the work of #AUTHOR_TAG, 1999 ), Charniak ( 1996 , 1997 ), Johnson ( 1998 ), Chiang ( 2000 ), and many others']","['##egner 1992 ; pereira and schabes 1992 ).', 'the dop model, on the other hand, was the first model ( to the best of our knowledge ) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'this approach has now gained wide usage, as exemplified by the work of #AUTHOR_TAG, 1999 ), Charniak ( 1996 , 1997 ), Johnson ( 1998 ), Chiang ( 2000 ), and many others']",4
"['#AUTHOR_TAG ; goodman 1998 ).', 'And Collins (2000) argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has']","['#AUTHOR_TAG ; goodman 1998 ).', 'And Collins (2000) argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has']","['#AUTHOR_TAG ; goodman 1998 ).', 'And Collins (2000) argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (']","['other innovation of dop was to take ( in principle ) all corpus fragments, of any size, rather than a small subset.', 'this innovation has not become generally adopted yet : many approaches still work either with local trees, i. e. single level rules with limited means of information percolation, or with restricted fragments, as in stochastic tree - adjoining grammar ( schabes 1992 ; chiang 2000 ) that do not include non - lexicalized fragments.', 'however, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'while the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head - words, later models showed the importance of including context from higher nodes in the tree ( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g. collins 1999 ; #AUTHOR_TAG ; goodman 1998 ).', 'And Collins (2000) argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of ( all ) tree fragments as proposed in Bod (1992)']",0
['been carried out in #AUTHOR_TAG who use exactly the same set of ('],['been carried out in #AUTHOR_TAG who use exactly the same set of ('],"['', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""And Collins ( 2000 ) argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of (""]","['other innovation of dop was to take ( in principle ) all corpus fragments, of any size, rather than a small subset.', 'this innovation has not become generally adopted yet : many approaches still work either with local trees, i. e. single level rules with limited means of information percolation, or with restricted fragments, as in stochastic tree - adjoining grammar ( schabes 1992 ; chiang 2000 ) that do not include nonlexicalized fragments.', 'however, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'while the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head - words, later models showed the importance of including context from higher nodes in the tree ( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g.', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""And Collins ( 2000 ) argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 )""]",4
"['a1 ( #AUTHOR_TAG ).', ""goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability."", 'this means that summing up over derivations of a tree in dop yields the same probability as summing over all the isomorphic derivations in the pcfg.', ""note that goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence : there may still be exponentially many derivations generating the same tree."", 'but goodman shows that with his pc']","['##man then shows by simple induction that subderivations headed by a with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1 / a.', 'and subderivations headed by a1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1 / a1 ( #AUTHOR_TAG ).', ""goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability."", 'this means that summing up over derivations of a tree in dop yields the same probability as summing over all the isomorphic derivations in the pcfg.', ""note that goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence : there may still be exponentially many derivations generating the same tree."", 'but goodman shows that with his pcfg - reduction he can efficiently compute the aforementioned maximum constituents parse.', ""moreover, goodman's pcfg reduction may also be used to estimate the most probable parse by viterbi n - best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the wsj subtrees to do this, goodman's method can do the same job with a more compact grammar""]","['by a1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1 / a1 ( #AUTHOR_TAG ).', ""goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability."", 'this means that summing up over derivations of a tree in dop yields the same probability as summing over all the isomorphic derivations in the pcfg.', ""note that goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence : there may still be exponentially many derivations generating the same tree."", 'but goodman shows that with his pc']","['##man then shows by simple induction that subderivations headed by a with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1 / a.', 'and subderivations headed by a1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1 / a1 ( #AUTHOR_TAG ).', ""goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability."", 'this means that summing up over derivations of a tree in dop yields the same probability as summing over all the isomorphic derivations in the pcfg.', ""note that goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence : there may still be exponentially many derivations generating the same tree."", 'but goodman shows that with his pcfg - reduction he can efficiently compute the aforementioned maximum constituents parse.', ""moreover, goodman's pcfg reduction may also be used to estimate the most probable parse by viterbi n - best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the wsj subtrees to do this, goodman's method can do the same job with a more compact grammar""]",0
"['et al. 2002 ).', ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', ""while goodman's method does""]","['definition ).', 'Bod (1993) showed how standard parsing techniques can be applied to dop1 by converting subtrees into rules.', ""however, the problem of computing the most probable parse turns out to be np - hard ( sima'an 1996 ), mainly because the same parse tree can be generated by exponentially many derivations."", ""many implementations of dop1 therefore estimate the most probable parse by monte carlo techniques ( bod 1998 ; chappelier & rajman 2000 ), or by viterbi n - best search ( bod 2001 ), or by restricting the set of subtrees ( sima'an 1999 ; chappelier et al. 2002 )."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', ""while goodman's method does""]","[""##a'an 1999 ; chappelier et al. 2002 )."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', 'while goodman\'s method does still not allow for an efficient computation of the most probable parse in dop1, it does efficiently compute the "" maximum constituents parse "", i. e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998 b showed that dop1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000 a ) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on expectation - maximization.', 'this resulted in a statistically consistent model dubbed ml - dop.', 'however, ml - dop suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'cross - validation is needed to avoid this problem.', 'but even with cross - validation, ml - dop is outperformed by the much simpler dop1 model on both the atis and ovis treebanks ( bod 2000b ).', ""Bonnema et al. (1999) observed that another problem with dop1's subtree - estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'as an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non - root non - terminal it contains.', 'Bod (2001) used an']","['instantiation of dop which has received considerable interest is the model known as dop1 2 ( bod 1992 ).', 'dop1 combines subtrees from a treebank by means of node - substitution and computes the probability of a tree from the normalized frequencies of the subtrees ( see section 2 for a full definition ).', 'Bod (1993) showed how standard parsing techniques can be applied to dop1 by converting subtrees into rules.', ""however, the problem of computing the most probable parse turns out to be np - hard ( sima'an 1996 ), mainly because the same parse tree can be generated by exponentially many derivations."", ""many implementations of dop1 therefore estimate the most probable parse by monte carlo techniques ( bod 1998 ; chappelier & rajman 2000 ), or by viterbi n - best search ( bod 2001 ), or by restricting the set of subtrees ( sima'an 1999 ; chappelier et al. 2002 )."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', 'while goodman\'s method does still not allow for an efficient computation of the most probable parse in dop1, it does efficiently compute the "" maximum constituents parse "", i. e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998 b showed that dop1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000 a ) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on expectation - maximization.', 'this resulted in a statistically consistent model dubbed ml - dop.', 'however, ml - dop suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'cross - validation is needed to avoid this problem.', 'but even with cross - validation, ml - dop is outperformed by the much simpler dop1 model on both the atis and ovis treebanks ( bod 2000b ).', ""Bonnema et al. (1999) observed that another problem with dop1's subtree - estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'as an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non - root non - terminal it contains.', 'Bod (2001) used an']",0
"['systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit ( #AUTHOR_TAG ).', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with moses, trained on all german training data.', 'the dev set is wmt - 2009 - a and the test set is wmt -']","['workshop on machine translation shared task on translation. 7', 'there are 82, 740 parallel sentences from news - commentary09. de - en and 1, 418, 115 parallel sentences from europarl - v4. de - en.', 'the monolingual data contains 9. 8 m sentences. 8', 'o build the baseline, the data was tokenized using the moses tokenizer and lowercased.', 'we use giza + + to generate alignments, by running 5 iterations of model 1, 5 iterations of the hmm model, and 4 iterations of model 4. we symmetrize using the "" grow - diag - final - and "" heuristic.', 'our moses systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit ( #AUTHOR_TAG ).', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with moses, trained on all german training data.', 'the dev set is wmt - 2009 - a and the test set is wmt - 2009 - b, and we report end - to - end case sensitive bleu scores against the unmodified reference']","['.', 'we use giza + + to generate alignments, by running 5 iterations of model 1, 5 iterations of the hmm model, and 4 iterations of model 4. we symmetrize using the "" grow - diag - final - and "" heuristic.', 'our moses systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit ( #AUTHOR_TAG ).', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with moses, trained on all german training data.', 'the dev set is wmt - 2009 - a and the test set is wmt -']","['evaluate our end - to - end system, we perform the well - studied task of news translation, using the moses smt package.', 'we use the english / german data released for the 2009 acl workshop on machine translation shared task on translation. 7', 'there are 82, 740 parallel sentences from news - commentary09. de - en and 1, 418, 115 parallel sentences from europarl - v4. de - en.', 'the monolingual data contains 9. 8 m sentences. 8', 'o build the baseline, the data was tokenized using the moses tokenizer and lowercased.', 'we use giza + + to generate alignments, by running 5 iterations of model 1, 5 iterations of the hmm model, and 4 iterations of model 4. we symmetrize using the "" grow - diag - final - and "" heuristic.', 'our moses systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit ( #AUTHOR_TAG ).', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with moses, trained on all german training data.', 'the dev set is wmt - 2009 - a and the test set is wmt - 2009 - b, and we report end - to - end case sensitive bleu scores against the unmodified reference sgml file.', 'the blind test set used is wmt - 2009 - blind ( all lines )']",5
"['have to be taken : i ) where to merge and ii ) how to merge.', 'following the work of #AUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed ( separated ) surface form, part - of - speech14 and frequencies from the training corpus for bigrams']","['have to be taken : i ) where to merge and ii ) how to merge.', 'following the work of #AUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed ( separated ) surface form, part - of - speech14 and frequencies from the training corpus for bigrams / merging of word and word + 1, word as true prefix, word + 1 as true suffix, plus frequency comparisons of these.', 'the crf is trained on the split monolingual data.', 'it']","['translation, compound parts have to be resynthesized into compounds before inflection.', 'two decisions have to be taken : i ) where to merge and ii ) how to merge.', 'following the work of #AUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed ( separated ) surface form, part - of - speech14 and frequencies from the training corpus for bigrams']","['translation, compound parts have to be resynthesized into compounds before inflection.', 'two decisions have to be taken : i ) where to merge and ii ) how to merge.', 'following the work of #AUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed ( separated ) surface form, part - of - speech14 and frequencies from the training corpus for bigrams / merging of word and word + 1, word as true prefix, word + 1 as true suffix, plus frequency comparisons of these.', 'the crf is trained on the split monolingual data.', 'it only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al., 2006)']",5
"['compound splitting, we follow #AUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008) ) or are ( almost ) knowledge - free (']","['compound splitting, we follow #AUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008) ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight (2003) ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']","['compound splitting, we follow #AUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008) ) or are ( almost ) knowledge - free (']","['compound splitting, we follow #AUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008) ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight (2003) ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']",5
"['use an additional target factor to obtain the coarse pos for each stem, applying a 7 - gram pos model.', '#AUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']","['use an additional target factor to obtain the coarse pos for each stem, applying a 7 - gram pos model.', '#AUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']","['use an additional target factor to obtain the coarse pos for each stem, applying a 7 - gram pos model.', '#AUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']","['use an additional target factor to obtain the coarse pos for each stem, applying a 7 - gram pos model.', '#AUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']",0
"['., Koehn and Knight (2003) ). compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow #AUTHOR_TAG, for compound merging.', 'we trained a crf']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the ge - ometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne (2008) ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight (2003) ). compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow #AUTHOR_TAG, for compound merging.', 'we trained a crf']","['., Koehn and Knight (2003) ). compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow #AUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merg - ing ) on one of our two test sets']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the ge - ometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne (2008) ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight (2003) ). compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow #AUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merg - ing ) on one of our two test sets']",5
"['to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions']","['to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions']","['to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions']","['a stem such as brother, toutanova et.', 'al\'s system might generate the "" stem and inflection "" corresponding to and his brother.', 'viewing and and his as inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex context features']",1
"['., #AUTHOR_TAG ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']","[', #AUTHOR_TAG ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']","['., #AUTHOR_TAG ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po S-tags Stymne ( 2008 ) ) or are ( almost ) knowledge - free ( e. g., #AUTHOR_TAG ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']",1
"['mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification']","['mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification']","['the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification']","['a stem such as brother, toutanova et. als system might generate the stem and inflection corresponding to and his brother.', 'viewing and and his as inflection is problematic since a map - ping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., ad - jectives ) separating his and brother.', 'this required mapping is a significant problem for generaliza - tion.', 'we view this issue as a different sort of prob - lem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from stems to in - flected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex con - text features']",1
"['based on smor, a morphological analyzer / generator of german ( Schmid et al. , 2004 ) and the bitpar parser, which is a state - of - the - art parser of german ( #AUTHOR_TAG )']","['based on smor, a morphological analyzer / generator of german ( Schmid et al. , 2004 ) and the bitpar parser, which is a state - of - the - art parser of german ( #AUTHOR_TAG )']","['based on smor, a morphological analyzer / generator of german ( Schmid et al. , 2004 ) and the bitpar parser, which is a state - of - the - art parser of german ( #AUTHOR_TAG )']","['key linguistic knowledge sources that we use are morphological analysis and generation of german based on smor, a morphological analyzer / generator of german ( Schmid et al. , 2004 ) and the bitpar parser, which is a state - of - the - art parser of german ( #AUTHOR_TAG )']",5
"['prepare the training data by splitting compounds in two steps, following the technique of #AUTHOR_TAG.', 'first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.', 'training data is then stemmed as described in section 2. 3.', 'the formerly modifying words of the compound ( in our example the words to the left of the right']","['prepare the training data by splitting compounds in two steps, following the technique of #AUTHOR_TAG.', 'first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.', 'training data is then stemmed as described in section 2. 3.', 'the formerly modifying words of the compound ( in our example the words to the left of the rightmost word ) do not have a stem markup assigned, except for two cases : i ) they are nouns themselves or ii ) they are particles separated from a verb.', 'in these cases, former modifiers are represented identically to']","['prepare the training data by splitting compounds in two steps, following the technique of #AUTHOR_TAG.', 'first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.', 'training data is then stemmed as described in section 2. 3.', 'the formerly modifying words of the compound ( in our example the words to the left of the rightmost word ) do not have a stem markup assigned, except for two cases : i ) they are nouns themselves or ii ) they are particles separated from a verb.', 'in these cases, former modifiers are represented identically to']","['prepare the training data by splitting compounds in two steps, following the technique of #AUTHOR_TAG.', 'first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.', 'training data is then stemmed as described in section 2. 3.', 'the formerly modifying words of the compound ( in our example the words to the left of the rightmost word ) do not have a stem markup assigned, except for two cases : i ) they are nouns themselves or ii ) they are particles separated from a verb.', 'in these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization']",5
"['shows that the feature - based approach is superior for english - to - german smt.', 'this is a striking result considering state - of - theart performance of german parsing is poor compared with the best performance on english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', 'Virpioja et al. ( 2007 ), Badr et al. ( 2008 ), Luong et al. ( 2010 ), #AUTHOR_TAG, and']","['shows that the feature - based approach is superior for english - to - german smt.', 'this is a striking result considering state - of - theart performance of german parsing is poor compared with the best performance on english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', 'Virpioja et al. ( 2007 ), Badr et al. ( 2008 ), Luong et al. ( 2010 ), #AUTHOR_TAG, and']","['shows that the feature - based approach is superior for english - to - german smt.', 'this is a striking result considering state - of - theart performance of german parsing is poor compared with the best performance on english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', 'Virpioja et al. ( 2007 ), Badr et al. ( 2008 ), Luong et al. ( 2010 ), #AUTHOR_TAG, and']","['have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step.', 'the direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation.', 'however, it is reasonable to expect that the use of features ( and morphological generation ) could also be problematic as this requires the use of morphologically - aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation.', 'despite this, our research clearly shows that the feature - based approach is superior for english - to - german smt.', 'this is a striking result considering state - of - theart performance of german parsing is poor compared with the best performance on english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', 'Virpioja et al. ( 2007 ), Badr et al. ( 2008 ), Luong et al. ( 2010 ), #AUTHOR_TAG, and others are primarily concerned with using morpheme segmentation in smt, which is a useful approach for dealing with issues of word - formation.', 'however, this does not deal directly with linguistic features marked by inflection.', 'in german these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.', 'so it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing']",1
"['.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions']","['inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions']","['inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions']","['a stem such as brother, toutanova et.', 'al\'s system might generate the "" stem and inflection "" corresponding to and his brother.', 'viewing and and his as inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored smt.', 'we use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex context features']",1
"['.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', '#AUTHOR_TAG introduced factored smt.', 'we use more complex context']","['inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', '#AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions']","['.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', '#AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions']","['a stem such as brother, toutanova et. als system might generate the stem and inflection corresponding to and his brother.', 'viewing and and his as inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', '#AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex context features']",1
"['( i. e., feature functions on the aligned english ), such as those of Avramidis and Koehn ( 2008 ), #AUTHOR_TAG and others.', 'touta']","['( i. e., feature functions on the aligned english ), such as those of Avramidis and Koehn ( 2008 ), #AUTHOR_TAG and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source""]","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of Avramidis and Koehn ( 2008 ), #AUTHOR_TAG and others.', 'touta']","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of Avramidis and Koehn ( 2008 ), #AUTHOR_TAG and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'using additional source side information beyond the markup did not produce a gain in performance']",1
"['( i. e., feature functions on the aligned english ), such as those of #AUTHOR_TAG, Yeniterzi and Oflazer ( 2010 ) and others.', 'touta']","['( i. e., feature functions on the aligned english ), such as those of #AUTHOR_TAG, Yeniterzi and Oflazer ( 2010 ) and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source""]","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of #AUTHOR_TAG, Yeniterzi and Oflazer ( 2010 ) and others.', 'touta']","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of #AUTHOR_TAG, Yeniterzi and Oflazer ( 2010 ) and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'using additional source side information beyond the markup did not produce a gain in performance']",1
"['. g., pos - tags #AUTHOR_TAG ) or are ( almost ) knowledge - free (']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., pos - tags #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight ( 2003 ) ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf']","['. g., pos - tags #AUTHOR_TAG ) or are ( almost ) knowledge - free (']","['compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., pos - tags #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g., Koehn and Knight ( 2003 ) ).', 'compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow Stymne and Cancedda (2011), for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']",1
"['approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ), which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ).', 'therefore, the task of relation extraction is most closely related to the tasks']","['approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ), which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ).', 'therefore, the task of relation extraction is most closely related to the tasks']","['approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ), which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ).', 'therefore, the task of relation extraction is most closely related to the tasks']","['approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ), which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ).', 'therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', 'researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation - specific semantics (Zelenko et al., 2003;Culotta and Sorensen, 2004).', 'to the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005).', 'Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees.', 'they, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees.', 'we believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by Harabagiu et al. (2005)']",2
"['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ).', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]","['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ).', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]","['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ).', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]","['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ).', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]",0
['a more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG )'],['a more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG )'],['a more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG )'],['a more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG )'],0
"[', the ale parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom - up or topdown.', 'in the case of selective magic parsing we use so - called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'a literal ( goal ) is considered a parse lype literal ( goal ) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1 all types in the type hierarchy can be used as parse types.', 'this way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'however, in the remainder we will concentrate on a']","[', the ale parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom - up or topdown.', 'in the case of selective magic parsing we use so - called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'a literal ( goal ) is considered a parse lype literal ( goal ) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1 all types in the type hierarchy can be used as parse types.', 'this way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'however, in the remainder we will concentrate on a']","[', the ale parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom - up or topdown.', 'in the case of selective magic parsing we use so - called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'a literal ( goal ) is considered a parse lype literal ( goal ) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1 all types in the type hierarchy can be used as parse types.', 'this way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'however, in the remainder we will concentrate on a specific class of parse types : we assume the specification of type sign and']","['control strategies depends on a way to differentiate between types of constraints.', ""proceedings of eacl'99 example, the ale parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom - up or topdown."", 'in the case of selective magic parsing we use so - called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'a literal ( goal ) is considered a parse lype literal ( goal ) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1 all types in the type hierarchy can be used as parse types.', 'this way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'however, in the remainder we will concentrate on a specific class of parse types : we assume the specification of type sign and its subtypes as parse types.', '11 this choice is based on the observation that the constraints on type sign and its sub - types play an important guiding role in the parsing process and are best interpreted bottom - up given the lexical orientation of i - ipsg.', ""the parsing process corresponding to such a parse type specification is represented schematically in figure 8. starting from the lexical entries, i. e., word word word figure 8 : schematic representation of the selective magic parsing process the : r ~'l definite clauses that specify the word objects in the grammar, phrases are built bottomup by matching the parse type literals of the definite clauses in the grammar against the edges in the table."", 'the non - parse type literals are processed according to the top - down control strategy 1the notion of a parse type literal is closely related to that of a memo literal as in ( johnson and d Srre, 1995)']",0
"['others, ( ramakrishnan et al. 1992 ).', 'as shown in ( #AUTHOR_TAG )']","['others, ( ramakrishnan et al. 1992 ).', 'as shown in ( #AUTHOR_TAG ) a the presented research was carried out at the university of tubingen, germany, as part of the sonderforschungsbereich 340']","['others, ( ramakrishnan et al. 1992 ).', 'as shown in ( #AUTHOR_TAG )']","['is a compilation technique originally developed for goal - directed bottom - up processing of logic programs.', 'see, among others, ( ramakrishnan et al. 1992 ).', 'as shown in ( #AUTHOR_TAG ) a the presented research was carried out at the university of tubingen, germany, as part of the sonderforschungsbereich 340']",0
"['; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into t ~ r / : definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""typed feature structures as normal form ir ~'~ e terms are merely syntactic objects""]","['; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into t ~ r / : definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""typed feature structures as normal form ir ~'~ e terms are merely syntactic objects""]","['; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into t ~ r / : definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""typed feature structures as normal form ir ~'~ e terms are merely syntactic objects""]","['feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( hpsg ; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into t ~ r / : definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""typed feature structures as normal form ir ~'~ e terms are merely syntactic objects""]",0
"['phrase structure grammar ( #AUTHOR_TAG ).', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules']","['can be used as the basis for implementations of head - driven phrase structure grammar ( #AUTHOR_TAG ).', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules']","['feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( #AUTHOR_TAG ).', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules']","['feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( #AUTHOR_TAG ).', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into tit definite clauses which are used to restrict lexical entries.', '( g Stz and Meurers, 1997 b ) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints. 4', 'because of space limitations we have to refrain from an example.', 'the controll grammar development system as described in ( g Stz and Meurers, 1997 b ) implements the above men - tioned techniques for compiling an hpsg theory into typed feature grammars']",0
"['others, ( #AUTHOR_TAG ).', 'as shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature']","['others, ( #AUTHOR_TAG ).', 'as shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature']","['others, ( #AUTHOR_TAG ).', 'as shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature']","['is a compilation technique originally developed for goal - directed bottom - up processing of logic programs.', 'see, among others, ( #AUTHOR_TAG ).', 'as shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature grammars a type of constraint - logic grammar based on typed feature logic ( tgv : ; g Stz, 1995).', 'typed feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( hpsg ; Pollard and Sag, 1994) as discussed in ( g Stz and Meurers, 1997 a ) and (Meurers and Minnen, 1997).', 'typed feature grammar constraints that are inexpensive to resolve are dealt with using the top - down interpreter of the controll grammar development system ( g Stz and Meurers, 1997 b ) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation']",0
['grammar development system as described in ( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars'],['grammar development system as described in ( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars'],"['from an example.', 'the controll grammar development system as described in ( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars']","['##e (King, 1994) for a discussion of the appropriateness of t ~ -  : for hpsg and a comparison with other feature logic approaches designed for hpsg.', 'append ( [ ~, [ ~, [ ~ ).', 'Meurers, 1997 b ) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 because of space limitations we have to refrain from an example.', 'the controll grammar development system as described in ( #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars']",0
"['( #AUTHOR_TAG ; Naish , 1986 )']","['( #AUTHOR_TAG ; Naish , 1986 )']","['( #AUTHOR_TAG ; Naish , 1986 )']","['##outining appears under many different guises, like for example, suspension, residuation, ( goal ) freezing, and blocking.', 'see also ( #AUTHOR_TAG ; Naish , 1986 )']",0
"['proposed parser is related to the so - called lemma table deduction system ( johnson and d Srre, 1995) which allows the user to specify whether top - down sub - computations are to be tabled.', ""in contrast to johnson and dsrre's deduction system, though, the selective magic parsing approach combines top - down and bottom - up control strategies."", 'as such it resembles the parser of the grammar development system attribute language engine ( ale ) of ( #AUTHOR_TAG ).', '']","['proposed parser is related to the so - called lemma table deduction system ( johnson and d Srre, 1995) which allows the user to specify whether top - down sub - computations are to be tabled.', ""in contrast to johnson and dsrre's deduction system, though, the selective magic parsing approach combines top - down and bottom - up control strategies."", 'as such it resembles the parser of the grammar development system attribute language engine ( ale ) of ( #AUTHOR_TAG ).', '']","['proposed parser is related to the so - called lemma table deduction system ( johnson and d Srre, 1995) which allows the user to specify whether top - down sub - computations are to be tabled.', ""in contrast to johnson and dsrre's deduction system, though, the selective magic parsing approach combines top - down and bottom - up control strategies."", 'as such it resembles the parser of the grammar development system attribute language engine ( ale ) of ( #AUTHOR_TAG ).', '']","['proposed parser is related to the so - called lemma table deduction system ( johnson and d Srre, 1995) which allows the user to specify whether top - down sub - computations are to be tabled.', ""in contrast to johnson and dsrre's deduction system, though, the selective magic parsing approach combines top - down and bottom - up control strategies."", 'as such it resembles the parser of the grammar development system attribute language engine ( ale ) of ( #AUTHOR_TAG ).', 'unlike the ale parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub - computations are tabled / filtered.', 'feature grammars on the basis of an example and introduce a dynamic bottom - up interpreter that can be used for gom - directed interpretation of magic - compiled typed feature grammars']",1
"['; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAG a )']","['; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAG a )']","['; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAG a )']","['is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature grammars a type of constraint - logic grammar based on typed feature logic ( tgv : ; g Stz, 1995).', 'typed feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( hpsg ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAG a ) and ( Meurers and Minnen , 1997 ).', 'typed feature grammar constraints that are inexpensive to resolve are dealt with using the top - down interpreter of the controll grammar development system ( g Stz and Meurers, 1997 b ) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation']",2
['see ( #AUTHOR_TAG )'],['see ( #AUTHOR_TAG )'],['see ( #AUTHOR_TAG )'],"['see ( #AUTHOR_TAG ) for a discussion of the appropriateness of tig for hpsg and a comparison with other feature logic approaches designed for hpsg.', 'append ( [ ~, [ ~, [ ~ ).', 'Meurers, 1997 b ) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 because of space limitations we have to refrain from an example.', 'the controll grammar development system as described in ( g Stz and Meurers, 1997 b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars']",0
"['., #AUTHOR_TAG ; watanabe 1995 ) to posi']","[', #AUTHOR_TAG ; watanabe 1995 ) to posing the translation problem,']","['., #AUTHOR_TAG ; watanabe 1995 ) to posing the translation problem,']","['is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages.', 'instead, the aim is to produce bilingual ( i. e., synchronized, see below ) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.', 'for example, headwords in both languages are chosen to force a synchronized alignment ( for better or worse ) in order to simplify cases involving so - called head - switching.', 'this contrasts with one of the traditional approaches ( e. g., #AUTHOR_TAG ; watanabe 1995 ) to posing the translation problem, i. e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language']",1
"['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']",5
"['the same time, we believe our method has advantages over the approach developed initially at ibm ( #AUTHOR_TAG ; brown et al. 1993 ) for training translation systems automatically.', 'one advantage is that our method attempts to model the natural decomposition of sentences']","['the same time, we believe our method has advantages over the approach developed initially at ibm ( #AUTHOR_TAG ; brown et al. 1993 ) for training translation systems automatically.', 'one advantage is that our method attempts to model the natural decomposition of sentences']","['the same time, we believe our method has advantages over the approach developed initially at ibm ( #AUTHOR_TAG ; brown et al. 1993 ) for training translation systems automatically.', 'one advantage is that our method attempts to model the natural decomposition of sentences']","['the same time, we believe our method has advantages over the approach developed initially at ibm ( #AUTHOR_TAG ; brown et al. 1993 ) for training translation systems automatically.', 'one advantage is that our method attempts to model the natural decomposition of sentences into phrases.', 'another is that the compilation of this decomposition into lexically anchored finite - state head transducers produces implementations that are much more efficient than those for the ibm model.', 'in particular, our search algorithm finds optimal transductions of test sentences in less than "" real time "" on a 300mhz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application']",1
"[""and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner'' ( #AUTHOR_TAG, 203 )."", '']","[""and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner'' ( #AUTHOR_TAG, 203 )."", '']","[""and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner'' ( #AUTHOR_TAG, 203 )."", '']","['##n constructs its plans using a hierarchical planning algorithm ( nilsson 1980 ).', 'the planner first checks all of its top - level plans to see which have effects that match the goal.', ""each matching plan's preconditions are checked ; if they are currently ( believed to be ) true, the planner then attempts to find all instantiations of the plan's body."", ""1 a the body of a plan can be an action or sequence of actions, a goal or sequence 9 moore and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner'' ( #AUTHOR_TAG, 203 )."", ""their planner uses plan structures similar to igen's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from rhetorical structure theory ( mann and thompson 1987 )."", 'in igen, the plans can involve any goals or actions that could be achieved via communication']",0
"['; #AUTHOR_TAG ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['; #AUTHOR_TAG ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['##t 1994 ; #AUTHOR_TAG ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'the text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'the names given to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; hovy 1988a ), or simply "" what to say "" versus "" how to say it "" ( e. g., danlos 1987 ; reithinger 1990 ).', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; #AUTHOR_TAG ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']",0
"['argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ).', 'while this']","['argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ).', 'while this']","[', reiter has even argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ).', 'while this']","['opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'whatever problems result will be handled as best they can, on a case - by - case basis.', 'this approach is the one taken ( implicitly or explicitly ) in the majority of generators.', 'in fact, reiter has even argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ).', 'while this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', ""certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity""]",0
['added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; pan'],"['added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']","['), and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; pan']","['in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'the text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'the names given to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; hovy 1988a ), or simply "" what to say "" versus "" how to say it "" ( e. g., danlos 1987 ; reithinger 1990 ).', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']",0
['; #AUTHOR_TAG'],['; #AUTHOR_TAG'],['##ad and robin 1992 ; penman 1989 ; #AUTHOR_TAG'],"['point here is not just that igen can produce different lexical realizations for a particular concept.', 'if that were the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network ( or similar device ) to test various features of the information being expressed.', 'the planner could supply whatever information is needed to drive the network.', 'something like this approach is in fact used in some systems ( e. g., elhadad and robin 1992 ; penman 1989 ; #AUTHOR_TAG a )']",0
"['##lt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; sondheimer and nebel 1986 ), and']","[""appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( hovy 1988a, 1988c )."", '']","['##lt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; sondheimer and nebel 1986 ), and']","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""these include devices such as interleaving the components ( mcdonald 1983 ; appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( hovy 1988a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '']",0
"['#AUTHOR_TAG ), back']","['#AUTHOR_TAG ), backtracking on failure ( appelt 1985 ; nogier']","['#AUTHOR_TAG ), back']","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""these include devices such as interleaving the components ( mcdonald 1983 ; #AUTHOR_TAG ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( mann 1983 ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( hovy 1988a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.']",0
"['in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and, at least implicitly, in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however, under this approach, all of the flexibility and simplicity of modular design is lost']","['in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and, at least implicitly, in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however, under this approach, all of the flexibility and simplicity of modular design is lost']","['in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and, at least implicitly, in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however, under this approach, all of the flexibility and simplicity of modular design is lost']","['possible response would be to abandon the separation ; the generator could be a single component that handles all of the work.', 'this approach has occasionally been taken, as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and, at least implicitly, in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however, under this approach, all of the flexibility and simplicity of modular design is lost']",0
"[""##vy's notion of restrictive ( i. e., bottom - up ) planning ( #AUTHOR_TAG a,""]","[""appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( mann 1983 ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( #AUTHOR_TAG a, 1988c )."", '']","[""##vy's notion of restrictive ( i. e., bottom - up ) planning ( #AUTHOR_TAG a,""]","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""these include devices such as interleaving the components ( mcdonald 1983 ; appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( mann 1983 ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( #AUTHOR_TAG a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '']",0
"['( #AUTHOR_TAG b ).', 'this system, however, starts with a list of']","['( #AUTHOR_TAG b ).', 'this system, however, starts with a list of']","['##vy has described another text planner that builds similar plans ( #AUTHOR_TAG b ).', 'this system, however, starts with a list of']","['##vy has described another text planner that builds similar plans ( #AUTHOR_TAG b ).', 'this system, however, starts with a list of information to be expressed and merely arranges it into a coherent pattern ; it is thus not a planner in the sense used here ( as hovy makes clear ).', '10 since text planning was not the primary focus of this work, igen is designed to simply assume that any false preconditions are unattainable.', ""igen's planner divides the requirements of a plan into two parts : the preconditions, which are not planned for, and those in the plan body, which are."", 'this has no']",0
"['##t 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG )']","['added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG )']","['##t 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG )']","['in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'the text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'the names given to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; hovy 1988a ), or simply "" what to say "" versus "" how to say it "" ( e. g., danlos 1987 ; reithinger 1990 ).', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG )']",0
"['., mcdonald 1983 ; #AUTHOR_TAG a ), or simply "" what to say "" versus "" how to say it "" (']","['to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; #AUTHOR_TAG a ), or simply "" what to say "" versus "" how to say it "" ( e. g., danlos 1987 ; reithinger 1990 ).', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the']","['., mcdonald 1983 ; #AUTHOR_TAG a ), or simply "" what to say "" versus "" how to say it "" (']","['in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'the text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'the names given to the components vary ; they have been called "" strategic "" and "" tactical "" components ( e. g., mckeown 1985 ; thompson 1977 ; danlos 1987 ) 1, "" planning "" and "" realization "" ( e. g., mcdonald 1983 ; #AUTHOR_TAG a ), or simply "" what to say "" versus "" how to say it "" ( e. g., danlos 1987 ; reithinger 1990 ).', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; wanner 1994 ). reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']",0
"['and articles on the topic include Lamarche and Retord ( 1996 ), de Groote and Retord ( 1996 ), and #AUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self - contained']","['and articles on the topic include Lamarche and Retord ( 1996 ), de Groote and Retord ( 1996 ), and #AUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self - contained']","['and articles on the topic include Lamarche and Retord ( 1996 ), de Groote and Retord ( 1996 ), and #AUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self - contained']","['and articles on the topic include Lamarche and Retord ( 1996 ), de Groote and Retord ( 1996 ), and #AUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self - contained']",0
"['the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['17 ) by a result of Zielonka (1981), the lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of lambek calculus.', 'combinatory categorial grammar does not concern itself with the capture of all ( or only ) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design.', 'an approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']",0
"['approach to this problem consists in defining, within the cut - free atomic - id space, normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG,']","['approach to this problem consists in defining, within the cut - free atomic - id space, normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG, hepple 1990,']","['approach to this problem consists in defining, within the cut - free atomic - id space, normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG,']","['approach to this problem consists in defining, within the cut - free atomic - id space, normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG, hepple 1990, hendriks 1993 ).', 'each sequent has a distinguished category formula ( underlined ) on which rule applications are keyed : in the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i. e., provided. l is not needed, f ~ a is a theorem of the lambek calculus iff f ~ a is a theorem of the regulated calculus.', 'however, apart from the issue regarding. l, there is a general cause for dissatisfaction with this approach : it assumes the initial presence of the entire sequent to be proved, i. e., it is in principle nonincremental ; on the other hand, allowing incrementality on the basis of cut would reinstate with a vengeance the problem of spurious ambiguity, for then what are to be the cut formulas?', 'consequently, the sequent approach is ill - equipped to address the basic asymmetry of language - - the asymmetry of its processing in time - - - and has never been forwarded in a model of the kind of processing phenomena cited in the introduction']",0
"['that different das use distinctive word strings.', 'it is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases and da types.', 'for example, 92. 4 % of the uh - huh\'s occur in backchannels, and 88. 4 % of the trigrams "" < start > do you "" occur in yes - no - questions.', 'to leverage this information source, without hand - coding knowledge about which words are indicative of which das, we will use statistical language models that model the full word sequences associated with each da type.', '5. 1. 1', 'classification from']","['that different das use distinctive word strings.', 'it is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases and da types.', 'for example, 92. 4 % of the uh - huh\'s occur in backchannels, and 88. 4 % of the trigrams "" < start > do you "" occur in yes - no - questions.', 'to leverage this information source, without hand - coding knowledge about which words are indicative of which das, we will use statistical language models that model the full word sequences associated with each da type.', '5. 1. 1', 'classification from']","['classification using words is based on the observation that different das use distinctive word strings.', 'it is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases and da types.', 'for example, 92. 4 % of the uh - huh\'s occur in backchannels, and 88. 4 % of the trigrams "" < start > do you "" occur in yes - no - questions.', 'to leverage this information source, without hand - coding knowledge about which words are indicative of which das, we will use statistical language models that model the full word sequences associated with each da type.', '5. 1. 1', 'classification from true words.', '']","['classification using words is based on the observation that different das use distinctive word strings.', 'it is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases and da types.', 'for example, 92. 4 % of the uh - huh\'s occur in backchannels, and 88. 4 % of the trigrams "" < start > do you "" occur in yes - no - questions.', 'to leverage this information source, without hand - coding knowledge about which words are indicative of which das, we will use statistical language models that model the full word sequences associated with each da type.', '5. 1. 1', 'classification from true words.', 'assuming that the true ( hand - transcribed ) words of utterances are given as evidence, we can compute word - based likelihoods p ( wiu ) in a straightforward way, by building a statistical language model for each of the 42 das.', 'all das of a particular type found in the training corpus were pooled, and a da - specific trigram model was estimated using standard techniques ( katz backoff [ katz 1987 ] with witten - bell discounting [ witten and bell 1991 ] )']",4
"['to da classification.', 'a nonprobabilistic approach for da labeling proposed by samuel, carberry, and vijay - Shanker ( 1998 ) is transformation - based learning ( #AUTHOR_TAG ).', 'finally it should be noted that there are other tasks with a mathematical structure similar to that of da tagging, such as shallow parsing for natural language processing ( munk 1999 ) and dna classification tasks ( ohler,']","['to da classification.', 'a nonprobabilistic approach for da labeling proposed by samuel, carberry, and vijay - Shanker ( 1998 ) is transformation - based learning ( #AUTHOR_TAG ).', 'finally it should be noted that there are other tasks with a mathematical structure similar to that of da tagging, such as shallow parsing for natural language processing ( munk 1999 ) and dna classification tasks ( ohler, harbeck, and niemann 1999 ), from which further']","['to da classification.', 'a nonprobabilistic approach for da labeling proposed by samuel, carberry, and vijay - Shanker ( 1998 ) is transformation - based learning ( #AUTHOR_TAG ).', 'finally it should be noted that there are other tasks with a mathematical structure similar to that of da tagging, such as shallow parsing for natural language processing ( munk 1999 ) and dna classification tasks ( ohler,']","['the work mentioned so far uses statistical models of various kinds.', 'as we have shown here, such models offer some fundamental advantages, such as modularity and composability ( e. g., of discourse grammars with da models ) and the ability to deal with noisy input ( e. g., from a speech recognizer ) in a principled way.', 'however, many other classifier architectures are applicable to the tasks discussed, in particular to da classification.', 'a nonprobabilistic approach for da labeling proposed by samuel, carberry, and vijay - Shanker ( 1998 ) is transformation - based learning ( #AUTHOR_TAG ).', 'finally it should be noted that there are other tasks with a mathematical structure similar to that of da tagging, such as shallow parsing for natural language processing ( munk 1999 ) and dna classification tasks ( ohler, harbeck, and niemann 1999 ), from which further techniques could be borrowed']",1
"['##l, jelinek, and mercer 1983 ) and tagging ( #AUTHOR_TAG ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct (']","[', hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( #AUTHOR_TAG ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( dermatas and']","['##l, jelinek, and mercer 1983 ) and tagging ( #AUTHOR_TAG ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct (']","['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( #AUTHOR_TAG ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( dermatas and kokkinakis 1995 ).', 'to minimize the total number of utterance labeling errors, we need to maximize the probability of getting each da label correct individually, i. e., we need to maximize p ( uile ) for each i = 1..... n.', 'we can compute the per - utterance posterior da probabilities by summing']",1
"[').', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( #AUTHOR_TAG ).', 'to']","['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( church 1988 ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( #AUTHOR_TAG ).', 'to']","['##l, jelinek, and mercer 1983 ) and tagging ( church 1988 ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( #AUTHOR_TAG ).', 'to']","['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( church 1988 ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( #AUTHOR_TAG ).', 'to minimize the total number of utterance labeling errors, we need to maximize the probability of getting each da label correct individually, i. e., we need to maximize p ( uile ) for each i = 1..... n.', 'we can compute the per - utterance posterior da probabilities by summing']",0
"[""is doing essentially the same job as pereira's pronoun abstraction schema in #AUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']","[""is doing essentially the same job as pereira's pronoun abstraction schema in #AUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as pereira's pronoun abstraction schema in #AUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as pereira's pronoun abstraction schema in #AUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']",1
"['correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution ( unlike, say, the metavariables of standard qlfs [ #AUTHOR_TAG']","['correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution ( unlike, say, the metavariables of standard qlfs [ #AUTHOR_TAG']","['with constructs representing the interpretation of context - dependent elements ( pronouns, ellipsis, focus, etc. ).', 'these constructs correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution ( unlike, say, the metavariables of standard qlfs [ #AUTHOR_TAG ], or the labels of udrs [ reyle 1996']","['is required is that qlfs are, as here, expressed in a typed higher - order logic, augmented with constructs representing the interpretation of context - dependent elements ( pronouns, ellipsis, focus, etc. ).', 'these constructs correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution ( unlike, say, the metavariables of standard qlfs [ #AUTHOR_TAG ], or the labels of udrs [ reyle 1996 ], which are motivated entirely by the mechanisms that operate on them after grammatical processing ).', 'syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at qlf ( again unlike standard qlfs ) but are assumed to be available as components of the linguistic context.', '']",0
"['reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","[""implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","['reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in Alshawi (1990, 1992 ), and implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']",1
"['third problem arises with the approach to the semantics of qlfs that this notion of the relationship between qlf and rqlf encourages one to adopt : it is that taken by #AUTHOR_TAG.', 'this describes the semantics of qlfs via a supervaluation over the semantics of the rqlfs that they subsume.', 'although the problem does not arise for the simple fragment']","['third problem arises with the approach to the semantics of qlfs that this notion of the relationship between qlf and rqlf encourages one to adopt : it is that taken by #AUTHOR_TAG.', 'this describes the semantics of qlfs via a supervaluation over the semantics of the rqlfs that they subsume.', 'although the problem does not arise for the simple fragment']","['third problem arises with the approach to the semantics of qlfs that this notion of the relationship between qlf and rqlf encourages one to adopt : it is that taken by #AUTHOR_TAG.', 'this describes the semantics of qlfs via a supervaluation over the semantics of the rqlfs that they subsume.', 'although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many qlfs subsumed rqlfs that are not actually permitted by the resolution rules : for example, those that can only arise via a violation of scoping or binding constraints.', 'the role of resolution rules ( for perfectly good presentational reasons ) is completely ignored by their treatment.', 'however, it is really the case that in giving the semantics of a qlf, one is interested only in the set of rqlfs that are obtainable from it under closure of the resolution rules.', '']","['third problem arises with the approach to the semantics of qlfs that this notion of the relationship between qlf and rqlf encourages one to adopt : it is that taken by #AUTHOR_TAG.', 'this describes the semantics of qlfs via a supervaluation over the semantics of the rqlfs that they subsume.', 'although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many qlfs subsumed rqlfs that are not actually permitted by the resolution rules : for example, those that can only arise via a violation of scoping or binding constraints.', 'the role of resolution rules ( for perfectly good presentational reasons ) is completely ignored by their treatment.', 'however, it is really the case that in giving the semantics of a qlf, one is interested only in the set of rqlfs that are obtainable from it under closure of the resolution rules.', 'ideally, therefore, we would like a formal reconstruction of resolution rules as well.', 'this is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right.', 'anyone who has built a wide - coverage system knows that the range of context - dependent phenomena encountered in real life is a lot wider than the preoccupations of many linguists might suggest.', 'in the cle, for example, contextual resolution forms a larger part of the system than do syntactic and semantic processing.', 'unfortunately, in the cle there is no formal theory of resolution rules, and thus no prospect of capturing their role in assigning a semantics to qlfs']",1
"['then go on to compare the current approach with that of some other theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified discourse representation theory ( reyle 1993 )']","['then go on to compare the current approach with that of some other theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified discourse representation theory ( reyle 1993 )']","['then go on to compare the current approach with that of some other theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified discourse representation theory ( reyle 1993 )']","['then go on to compare the current approach with that of some other theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified discourse representation theory ( reyle 1993 ) ; and the ` ` glue language approach of Dalrymple et al. ( 1996 )']",1
"['starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in #AUTHOR_TAG, 1992 ),']","['starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in #AUTHOR_TAG, 1992 ),']","['starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in #AUTHOR_TAG, 1992 ),']","[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in #AUTHOR_TAG, 1992 ), and implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as ra - tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994), the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']",1
"['assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( #AUTHOR_TAG ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']","['assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( #AUTHOR_TAG ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']","['assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( #AUTHOR_TAG ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']","['assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( #AUTHOR_TAG ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']",0
"['types of ambiguity, even syntactic ambiguity.', ""the more conservative approach is to try to integrate existing statistical disambiguation schemes for qlfs, either individually or in a ` ` packed'' structure ( #AUTHOR_TAG ), with the resolution process as described here."", '']","['types of ambiguity, even syntactic ambiguity.', ""the more conservative approach is to try to integrate existing statistical disambiguation schemes for qlfs, either individually or in a ` ` packed'' structure ( #AUTHOR_TAG ), with the resolution process as described here."", 'alternatively, i believe it is worth']","['.', 'one is to adopt pinkal\'s "" radical underspecification "" approach ( pinkal 1995 ) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""the more conservative approach is to try to integrate existing statistical disambiguation schemes for qlfs, either individually or in a ` ` packed'' structure ( #AUTHOR_TAG ), with the resolution process as described here."", '']","['are several stategies that might be pursued.', 'one is to adopt pinkal\'s "" radical underspecification "" approach ( pinkal 1995 ) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""the more conservative approach is to try to integrate existing statistical disambiguation schemes for qlfs, either individually or in a ` ` packed'' structure ( #AUTHOR_TAG ), with the resolution process as described here."", 'alternatively, i believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here']",3
"['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'the version proposed here combines a basic insight from Lewin ( 1990 ) with higher - order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG, 1991 ), with some differences that are commented on below.', ""like pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by lewin."", 'we analyze quantified nps at the qlf level as illustrated in the qlf for : we assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( alshawi 1990 ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '']","['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'the version proposed here combines a basic insight from Lewin ( 1990 ) with higher - order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG, 1991 ), with some differences that are commented on below.', ""like pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by lewin."", 'we analyze quantified nps at the qlf level as illustrated in the qlf for : we assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( alshawi 1990 ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '']","['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'the version proposed here combines a basic insight from Lewin ( 1990 ) with higher - order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG, 1991 ), with some differences that are commented on below.', ""like pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by lewin."", 'we analyze quantified nps at the qlf level as illustrated in the qlf for : we assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( alshawi 1990 ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '']","['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'the version proposed here combines a basic insight from Lewin ( 1990 ) with higher - order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG, 1991 ), with some differences that are commented on below.', ""like pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by lewin."", 'we analyze quantified nps at the qlf level as illustrated in the qlf for : we assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( alshawi 1990 ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '']",1
"['#AUTHOR_TAG, 1991 ).', 'recall that in their treatment, quantified noun phrases are treated in two stages : firstly, what they call a "" free variable "" of type e is introduced in the np position, with an associated "" quantifier assumption, "" which is added as a kind of premise.', 'at a later stage the quantifier']","['#AUTHOR_TAG, 1991 ).', 'recall that in their treatment, quantified noun phrases are treated in two stages : firstly, what they call a "" free variable "" of type e is introduced in the np position, with an associated "" quantifier assumption, "" which is added as a kind of premise.', 'at a later stage the quantifier']","['#AUTHOR_TAG, 1991 ).', 'recall that in their treatment, quantified noun phrases are treated in two stages : firstly, what they call a "" free variable "" of type e is introduced in the np position, with an associated "" quantifier assumption, "" which is added as a kind of premise.', 'at a later stage the quantifier assumption is "" discharged, "" capturing all occurrences of the free variable.', 'thus their analysis of something like every manager disappeared would proceed as follows']","['is interesting to compare this analysis with that described in dalrymple, shieber, and Pereira ( 1991 ) and #AUTHOR_TAG, 1991 ).', 'recall that in their treatment, quantified noun phrases are treated in two stages : firstly, what they call a "" free variable "" of type e is introduced in the np position, with an associated "" quantifier assumption, "" which is added as a kind of premise.', 'at a later stage the quantifier assumption is "" discharged, "" capturing all occurrences of the free variable.', 'thus their analysis of something like every manager disappeared would proceed as follows']",1
"['are produced ( #AUTHOR_TAG, 47 ), but']","['the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG, 47 ), but']","['the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG, 47 ), but']","['the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG, 47 ), but without the need for a free variable constraint - - the hou algorithm will not produce any solutions in which a previously bound variable becomes free ; a the equivalences are reversible, and thus the above sentences cart be generated from scoped logical forms ; a partial scopings are permitted ( see reyle [ 19961 ) a scoping can be freely interleaved with other types of reference resolution ; a unscoped or partially scoped forms are available for inference or for generation at every stage']",0
"['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs ( #AUTHOR_TAG ) work to our own framework.', 'reyle points out that many of the inferences involving underspecified']","['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs ( #AUTHOR_TAG ) work to our own framework.', 'reyle points out that many of the inferences involving underspecified']","['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs ( #AUTHOR_TAG ) work to our own framework.', 'reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context']","['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs ( #AUTHOR_TAG ) work to our own framework.', 'reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.', 'his example is']",5
['present an illustrative first - order fragment along these'],['present an illustrative first - order fragment along these'],['present an illustrative first - order fragment along these lines and are able to supply a coherent formal semantics for the clf - qlfs'],"['present an illustrative first - order fragment along these lines and are able to supply a coherent formal semantics for the clf - qlfs themselves, using a technique essentially equivalent to supervaluations : a qlf is true iff all its possible rqlfs are, false iff they are all false, and undefined otherwise']",0
"['#AUTHOR_TAG ; mitkov 1996, 1998b )']","['#AUTHOR_TAG ; mitkov 1996, 1998b )']","['#AUTHOR_TAG ; mitkov 1996, 1998b )']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; nasukawa 1994 ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ; #AUTHOR_TAG ; mitkov 1996, 1998b )']",0
['#AUTHOR_TAG ; tetrea'],['#AUTHOR_TAG ; tetreault 1999 )'],['#AUTHOR_TAG ; tetreault 1999 )'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; #AUTHOR_TAG ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"[""##bs 1978 ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( #AUTHOR_TAG )."", 'the lrc is an']","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( hobbs 1978 ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( #AUTHOR_TAG )."", 'the lrc is an']","[""##bs 1978 ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( #AUTHOR_TAG )."", 'the lrc is an']","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( hobbs 1978 ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( #AUTHOR_TAG )."", 'the lrc is an alternative to the original bfp algorithm in that it processes utterances incrementally.', 'it works by first searching for an antecedent in the current sentence ; if none can be found, it continues the search on the cf - list of the previous and the other preceding utterances in a left - to - right fashion']",0
"['and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAG a, 2001b ).', '']","['and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAG a, 2001b ).', '']","['and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAG a, 2001b ).', '']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAG a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['#AUTHOR_TAG ; williams,']","['#AUTHOR_TAG ; williams,']","['#AUTHOR_TAG ; williams,']","['of the earlier work in anaphora resolution heavily exploited domain and lin - guistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; nasukawa 1994 ; #AUTHOR_TAG ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']",0
['#AUTHOR_TAG ;'],['#AUTHOR_TAG ; hahn and strube 1997 ; tetreault 1999 )'],['#AUTHOR_TAG ;'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; #AUTHOR_TAG ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b )']",0
"['#AUTHOR_TAG ; ge, hale, and char']","['#AUTHOR_TAG ; ge, hale, and charniak 1998']","['#AUTHOR_TAG ; ge, hale, and char']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; #AUTHOR_TAG ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['#AUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone']","['#AUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone']","['#AUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; #AUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
['#AUTHOR_TAG ; kennedy and bog'],"['#AUTHOR_TAG ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']",['#AUTHOR_TAG ; kennedy and bog'],"['of the earlier work in anaphora resolution heavily exploited domain and lin - guistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; #AUTHOR_TAG ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']",0
['#AUTHOR_TAG ; carbonell and brown'],['#AUTHOR_TAG ; carbonell and brown'],"['#AUTHOR_TAG ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; #AUTHOR_TAG ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments']",0
"['#AUTHOR_TAG ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995']","['#AUTHOR_TAG ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn']","['#AUTHOR_TAG ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and char']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; #AUTHOR_TAG ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field ( #AUTHOR_TAG']","['last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field ( #AUTHOR_TAG']","['last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field ( #AUTHOR_TAG']","['last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field ( #AUTHOR_TAG a ).', 'a fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge - poor methods are.', 'in particular, more research should be carried out on the factors influencing the performance of these algorithms.', 'one of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.', 'more work toward the proposal of consistent and comprehensive evaluation is necessary ; so too is work in multilingual contexts.', 'some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future']",3
['#AUTHOR_TAG )'],['#AUTHOR_TAG )'],['#AUTHOR_TAG )'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['; #AUTHOR_TAG ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['; #AUTHOR_TAG ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['carter 1987 ; rich and luperfoy 1988 ; #AUTHOR_TAG ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; #AUTHOR_TAG ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments']",0
"['impetus to the development of coreference resolution algorithms and systems, such as those described in #AUTHOR_TAG, Gaizauskas and Humphreys ( 1996 ), and Kameyama ( 1997 ).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multil']","['impetus to the development of coreference resolution algorithms and systems, such as those described in #AUTHOR_TAG, Gaizauskas and Humphreys ( 1996 ), and Kameyama ( 1997 ).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn']","['a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #AUTHOR_TAG, Gaizauskas and Humphreys ( 1996 ), and Kameyama ( 1997 ).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, hum']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #AUTHOR_TAG, Gaizauskas and Humphreys ( 1996 ), and Kameyama ( 1997 ).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
['#AUTHOR_TAG ; mitkov and barbu 2000'],['#AUTHOR_TAG ; mitkov and barbu 2000'],['#AUTHOR_TAG ; mitkov and barbu 2000'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; #AUTHOR_TAG ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['1997 ; #AUTHOR_TAG, 1998b )']","[', and preston 1996 ; baldwin 1997 ; #AUTHOR_TAG, 1998b )']","[', and preston 1996 ; baldwin 1997 ; #AUTHOR_TAG, 1998b )']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; nasukawa 1994 ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ; baldwin 1997 ; #AUTHOR_TAG, 1998b )']",0
"[""##bs's naive algorithm ( #AUTHOR_TAG ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( strube""]","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( #AUTHOR_TAG ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( strube""]","[""##bs's naive algorithm ( #AUTHOR_TAG ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( strube 1998 )."", 'the lrc is an']","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( #AUTHOR_TAG ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( strube 1998 )."", 'the lrc is an alternative to the original bfp algorithm in that it processes utterances incrementally.', 'it works by first searching for an antecedent in the current sentence ; if none can be found, it continues the search on the cf - list of the previous and the other preceding utterances in a left - to - right fashion']",0
"['#AUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995']","['#AUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn']","['#AUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge,']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; #AUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. ( 1995 ), Gaizauskas and Humphreys ( 1996 ), and #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multil']","['impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. ( 1995 ), Gaizauskas and Humphreys ( 1996 ), and #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn']","['a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. ( 1995 ), Gaizauskas and Humphreys ( 1996 ), and #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, hum']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. ( 1995 ), Gaizauskas and Humphreys ( 1996 ), and #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a (Mitkov , 2001 b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; carter 1987 ; rich and luperfoy'],['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; carter 1987 ; rich and luperfoy'],"['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments']",0
['( #AUTHOR_TAG 1998 ) of'],['( #AUTHOR_TAG 1998 ) of'],"['names are the main concern of the named - entity recognition subtask ( #AUTHOR_TAG 1998 ) of information extraction.', 'the main objective of this subtask is the identification of proper names and also their']","['names are the main concern of the named - entity recognition subtask ( #AUTHOR_TAG 1998 ) of information extraction.', 'the main objective of this subtask is the identification of proper names and also their classification into semantic categories ( person, organization, location, etc. ). 1', 'there the disambiguation of the first word in a sentence ( and in other ambiguous positions ) is one of the central problems : about 20 % of named entities occur in ambiguous positions.', 'for instance, the word black in the sentenceinitial position can stand for a persons surname but can also refer to the color.', 'even in multiword capitalized phrases, the first word can belong to the rest of the phrase or can be just an external modifier.', 'in the sentence daily, mason and partners lost their court case, it is clear that daily, mason and partners is the name of a company.', 'in the sentence unfortunately, mason and partners lost their court case, the name of the company does not include the word unfortunately, but the word daily is just as common a word as unfortunately']",0
['[ #AUTHOR_TAG'],['[ #AUTHOR_TAG'],"[""necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ #AUTHOR_TAG ], brill's [ brill 1995a""]","['our markup convention ( section 2 ), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'consequently a pos tagger can naturally treat them similarly to any other ambiguous words.', 'there is, however, one difference in the implementation of such a tagger.', 'normally, a pos tagger operates on text spans that form a sentence.', 'this requires resolving sentence boundaries before tagging.', ""we see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ #AUTHOR_TAG ], brill's [ brill 1995a ], and maxent [ ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which pos information does not depend on the previous and following history.', 'this issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'for instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'at the same time since this token is unambiguous, it is not affected by the history.', 'a trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other']",1
"['the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in #AUTHOR_TAG']","['the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in #AUTHOR_TAG']","['the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in #AUTHOR_TAG']","['second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.', 'apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the sbd task, as we showed in section 3. we tackle capitalized words in a similar fashion as we tackled the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in #AUTHOR_TAG']",5
"['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus ( #AUTHOR_TAG ) and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and santorini 1993 ).', 'the brown corpus represents general english.', 'it contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'the brown corpus is rich in out - of - vocabulary ( unknown ) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'altogether there are about 500 documents in the brown corpus, with an average length of 2, 300 word tokens']","['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus ( #AUTHOR_TAG ) and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and santorini 1993 ).', 'the brown corpus represents general english.', 'it contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'the brown corpus is rich in out - of - vocabulary ( unknown ) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'altogether there are about 500 documents in the brown corpus, with an average length of 2, 300 word tokens']","['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus ( #AUTHOR_TAG ) and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and santorini 1993 ).', 'the brown corpus represents general english.', 'it contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'the brown corpus is rich in out - of - vocabulary ( unknown ) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'altogether there are about 500 documents in the brown corpus, with an average length of 2, 300 word tokens']","['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus ( #AUTHOR_TAG ) and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and santorini 1993 ).', 'the brown corpus represents general english.', 'it contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'the brown corpus is rich in out - of - vocabulary ( unknown ) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'altogether there are about 500 documents in the brown corpus, with an average length of 2, 300 word tokens']",5
"[').', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( #AUTHOR_TAG ).', '']","['classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( #AUTHOR_TAG ).', ""gale, church, and yarowsky's""]","['##ver, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( #AUTHOR_TAG ).', '']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'mani and mac Millan (1995) pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( #AUTHOR_TAG ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ).', 'this is similar to "" one']",0
"['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']",0
"['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( #AUTHOR_TAG ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( #AUTHOR_TAG ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( #AUTHOR_TAG ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( #AUTHOR_TAG ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']",0
"["", brill's [ #AUTHOR_TAG a ], and maxent [ ratnaparkhi 1996""]","["", brill's [ #AUTHOR_TAG a ], and maxent [ ratnaparkhi 1996""]","["", brill's [ #AUTHOR_TAG a ], and maxent [ ratnaparkhi 1996""]","['our markup convention ( section 2 ), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'consequently a pos tagger can naturally treat them similarly to any other ambiguous words.', 'there is, however, one difference in the implementation of such a tagger.', 'normally, a pos tagger operates on text spans that form a sentence.', 'this requires resolving sentence boundaries before tagging.', ""we see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ #AUTHOR_TAG a ], and maxent [ ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which pos information does not depend on the previous and following history.', 'this issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'for instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'at the same time since this token is unambiguous, it is not affected by the history.', 'a trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other']",1
"["", brill's [ brill 1995a ], and maxent [ #AUTHOR_TAG""]","["", brill's [ brill 1995a ], and maxent [ #AUTHOR_TAG""]","["", brill's [ brill 1995a ], and maxent [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence']","['our markup convention ( section 2 ), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'consequently a pos tagger can naturally treat them similarly to any other ambiguous words.', 'there is, however, one difference in the implementation of such a tagger.', 'normally, a pos tagger operates on text spans that form a sentence.', 'this requires resolving sentence boundaries before tagging.', ""we see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ brill 1995a ], and maxent [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which pos information does not depend on the previous and following history.', 'this issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'for instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'at the same time since this token is unambiguous, it is not affected by the history.', 'a trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other']",1
"['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( #AUTHOR_TAG ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( #AUTHOR_TAG ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( #AUTHOR_TAG ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( #AUTHOR_TAG ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['##d systems : rule based and machine learning.', 'the rule - based systems use manually built rules that are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench ( #AUTHOR_TAG ) contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored']","['exist two large classes of sbd systems : rule based and machine learning.', 'the rule - based systems use manually built rules that are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench ( #AUTHOR_TAG ) contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored']","['##d systems : rule based and machine learning.', 'the rule - based systems use manually built rules that are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench ( #AUTHOR_TAG ) contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored']","['exist two large classes of sbd systems : rule based and machine learning.', 'the rule - based systems use manually built rules that are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench ( #AUTHOR_TAG ) contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains']",0
"['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( riley 1989 : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in #AUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( riley 1989 : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in #AUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( riley 1989 : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in #AUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( riley 1989 : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in #AUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']",1
"[""] or brill's [ #AUTHOR_TAG b""]","['only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ baum and petrie 1966 ] or brill's [ #AUTHOR_TAG b""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ baum and petrie 1966 ] or brill's [ #AUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries,""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ baum and petrie 1966 ] or brill's [ #AUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a pos class occur in different ambiguity patterns."", 'counting all possible pos combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'periods as many other closed - class words cannot be successfully covered by such technique']",1
"['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'as #AUTHOR_TAG right']","['##uation of capitalized words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'as #AUTHOR_TAG rightly pointed out, however, ` ` proper nouns and capitalized words are particularly']","['##uation of capitalized words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'as #AUTHOR_TAG right']","['##uation of capitalized words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'as #AUTHOR_TAG rightly pointed out, however, ` ` proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not.', 'estimates from the brown corpus can be misleading.', ""for example, the capitalized word'acts'is found twice in the brown corpus, both times as a proper noun ( in a title )."", ""it would be misleading to infer from this evidence that the word'acts'is always a proper noun.""]",1
"['only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ #AUTHOR_TAG ] or brill's [ brill 1995b""]","['only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ #AUTHOR_TAG ] or brill's [ brill 1995b""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ #AUTHOR_TAG ] or brill's [ brill 1995b""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ #AUTHOR_TAG ] or brill's [ brill 1995b ] ) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a pos class occur in different ambiguity patterns."", 'counting all possible pos combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'periods as many other closed - class words cannot be successfully covered by such technique']",1
"['is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives, infinitive for verbs,']","['is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives, infinitive for verbs, etc..', 'for words that could be normalized to several main forms ( polysemy ), when secondary forms of different words coincided, we retained all the main forms.', '']","[', unlike english, russian is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives, infinitive for verbs,']","[', unlike english, russian is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives, infinitive for verbs, etc..', 'for words that could be normalized to several main forms ( polysemy ), when secondary forms of different words coincided, we retained all the main forms.', 'since the documents in the bbc news corpus were rather short, we applied the cache module, as described in section 11. 1.', 'this allowed us to reuse information across the documents']",5
"['). this is similar to "" one sense per collocation "" idea of #AUTHOR_TAG']","['). this is similar to "" one sense per collocation "" idea of #AUTHOR_TAG']","['). this is similar to "" one sense per collocation "" idea of #AUTHOR_TAG']","['kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term', 'memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model. within certain limits, such a model can adapt itself to changes in word frequencies, depending on', 'the topic of the text passage. the dca system is similar in spirit to such dynamic adaptation : it applies word n -', 'grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. but unlike the cache model, it uses a multipass strategy. Clarkson and Robinson (1997) developed a way of', 'incorporating standard n - grams into the cache model, using mixtures of', ""language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence. in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tag"", '##ger. instead of decaying nonlocal information, we opted for not propagating it from one document for', 'processing of another. for handling very long documents with our method, however, the information decay strategy seems to be', 'the right way to proceed. mani and mac Millan (1995) pointed out that little attention had been paid in the named - entity', 'recognition field to the discourse properties of proper names. they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context', ', advocating text - driven processing rather than reliance on pre - existing lists. the dca outlined in this article also uses nonlocal discourse context and does not heavily rely', 'on pre - existing word lists. it has been applied not only to the identification of proper names, as described in this article', ', but also to their classification ( mikheev, grover, and moens 1998 ). Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( ""', 'one sense per discourse "" ). since then this idea has been applied to several tasks, including word sense disambiguation ( ya', ""##rowsky 1995 ) and named - entity recognition ( cucerzan and yarowsky 1999 ). gale, church, and yarowsky's observation is also used in our dc"", '##a, especially for the identification of abbreviations. in capitalized - word disambiguation, however, we use this assumption with caution and first', 'apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ). this is similar to "" one sense per collocation "" idea of #AUTHOR_TAG']",1
"['., #AUTHOR_TAG ).', 'words in such lists are usually supplemented with morphological and pos information ( which is not required by our method ).', 'we do']","['first list on which our method relies is a list of common words.', 'this list includes common words for a given language, but no supplementary information such as pos or morphological information is required to be present in this list.', 'a variety of such lists for many languages are already available ( e. g., #AUTHOR_TAG ).', 'words in such lists are usually supplemented with morphological and pos information ( which is not required by our method ).', 'we do']","['., #AUTHOR_TAG ).', 'words in such lists are usually supplemented with morphological and pos information ( which is not required by our method ).', 'we do not have to rely on pre - existing resources, however.', 'a list of common words can be easily obtained automatically from a']","['first list on which our method relies is a list of common words.', 'this list includes common words for a given language, but no supplementary information such as pos or morphological information is required to be present in this list.', 'a variety of such lists for many languages are already available ( e. g., #AUTHOR_TAG ).', 'words in such lists are usually supplemented with morphological and pos information ( which is not required by our method ).', 'we do not have to rely on pre - existing resources, however.', 'a list of common words can be easily obtained automatically from a raw ( unannotated in any way ) text collection by simply collecting and counting lowercased words in it.', 'we generated such list from the nyt collection.', 'to account for potential spelling and capitalization errors, we included in the list of common words only words that occurred lower - cased at least three times in the nyt texts.', 'the list of common words that we developed from the nyt collection contained about 15, 000 english words']",0
"['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in Palmer and Hearst (1997) ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in Palmer and Hearst (1997) ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in Palmer and Hearst (1997) ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']","['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in Palmer and Hearst (1997) ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']",1
"['##a and hausman 1998 ).', 'in some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ).', 'the advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'the disadvantage is in the cost of building a wide - coverage set of contextual clues manually or producing annotated training data.', 'also, the contextual']","['the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'named - entity recognition systems usually use sets of complex hand - crafted rules that employ a gazetteer and a local context ( krupa and hausman 1998 ).', 'in some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ).', 'the advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'the disadvantage is in the cost of building a wide - coverage set of contextual clues manually or producing annotated training data.', 'also, the contextual']","['##a and hausman 1998 ).', 'in some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ).', 'the advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'the disadvantage is in the cost of building a wide - coverage set of contextual clues manually or producing annotated training data.', 'also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port']","['the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'named - entity recognition systems usually use sets of complex hand - crafted rules that employ a gazetteer and a local context ( krupa and hausman 1998 ).', 'in some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ).', 'the advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'the disadvantage is in the cost of building a wide - coverage set of contextual clues manually or producing annotated training data.', 'also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port']",0
"['abbreviation ( as opposed to regular word ) these four lists can be acquired completely automatically from raw ( unlabeled ) texts.', 'for the development of these lists we used a collection of texts of about 300, 000 words derived from the new york times ( nyt ) corpus that was supplied as training data for the 7th message understanding conference ( muc - 7 ) ( #AUTHOR_TAG ).', 'we used these texts because the approach described in this article was initially designed to be part of a named - entity recognition system ( mikheev, grover, and moens 1998 )']","['abbreviation ( as opposed to regular word ) these four lists can be acquired completely automatically from raw ( unlabeled ) texts.', 'for the development of these lists we used a collection of texts of about 300, 000 words derived from the new york times ( nyt ) corpus that was supplied as training data for the 7th message understanding conference ( muc - 7 ) ( #AUTHOR_TAG ).', 'we used these texts because the approach described in this article was initially designed to be part of a named - entity recognition system ( mikheev, grover, and moens 1998 )']","['abbreviation ( as opposed to regular word ) these four lists can be acquired completely automatically from raw ( unlabeled ) texts.', 'for the development of these lists we used a collection of texts of about 300, 000 words derived from the new york times ( nyt ) corpus that was supplied as training data for the 7th message understanding conference ( muc - 7 ) ( #AUTHOR_TAG ).', 'we used these texts because the approach described in this article was initially designed to be part of a named - entity recognition system ( mikheev, grover, and moens 1998 )']","['abbreviation ( as opposed to regular word ) these four lists can be acquired completely automatically from raw ( unlabeled ) texts.', 'for the development of these lists we used a collection of texts of about 300, 000 words derived from the new york times ( nyt ) corpus that was supplied as training data for the 7th message understanding conference ( muc - 7 ) ( #AUTHOR_TAG ).', 'we used these texts because the approach described in this article was initially designed to be part of a named - entity recognition system ( mikheev, grover, and moens 1998 ) developed for muc - 7.', 'although the corpus size of 300, 000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on - line sources ( including the internet ) makes this resource cheap to obtain']",5
"['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in #AUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in #AUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in #AUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in #AUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then reused in further processing.', 'this is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'the main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', 'Park and Byrd (2001) recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions ( e. g., united kingdom for uk ).', 'the abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'there is no harm ( apart from the performance issues ) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'candidates then are filtered through a set of known common words and proper names.', 'at the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document']",0
"[', including word sense disambiguation ( #AUTHOR_TAG ) and named']","[', including word sense disambiguation ( #AUTHOR_TAG ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's""]","['##ver, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( #AUTHOR_TAG ) and named']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'mani and mac Millan (1995) pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( #AUTHOR_TAG ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ).', 'this']",0
"[') or sentence splitting.', 'segmenting text into sentences is an important aspect in developing many applications : syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. sentence splitting in most cases is a simple matter : a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'in certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in #AUTHOR_TAG']","['important task of text normalization is sentence boundary disambiguation ( sbd ) or sentence splitting.', 'segmenting text into sentences is an important aspect in developing many applications : syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. sentence splitting in most cases is a simple matter : a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'in certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in #AUTHOR_TAG']","[') or sentence splitting.', 'segmenting text into sentences is an important aspect in developing many applications : syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. sentence splitting in most cases is a simple matter : a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'in certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in #AUTHOR_TAG']","['important task of text normalization is sentence boundary disambiguation ( sbd ) or sentence splitting.', 'segmenting text into sentences is an important aspect in developing many applications : syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. sentence splitting in most cases is a simple matter : a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'in certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in #AUTHOR_TAG']",0
"['texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( palmer and hearst 1997 ) or the pos tagger reported in #AUTHOR_TAG, which do']","['to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( palmer and hearst 1997 ) or the pos tagger reported in #AUTHOR_TAG, which do']","['to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( palmer and hearst 1997 ) or the pos tagger reported in #AUTHOR_TAG, which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']","['all its strong points, there are a number of restrictions to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( palmer and hearst 1997 ) or the pos tagger reported in #AUTHOR_TAG, which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']",1
"['decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', '']","['decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', '']","['##hn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ).', '']",5
"['that dca can be used as a complement to a local - context approach, we combined our main configuration ( evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']","['that dca can be used as a complement to a local - context approach, we combined our main configuration ( evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']","['that dca can be used as a complement to a local - context approach, we combined our main configuration ( evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']","['test our hypothesis that dca can be used as a complement to a local - context approach, we combined our main configuration ( evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries']",5
"['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( #AUTHOR_TAG ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( #AUTHOR_TAG ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( #AUTHOR_TAG ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system ( #AUTHOR_TAG ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in Mikheev (2000).', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( #AUTHOR_TAG ) or the pos tagger reported in Mikheev ( 2000 ), which do']","['to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( #AUTHOR_TAG ) or the pos tagger reported in Mikheev ( 2000 ), which do']","['to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( #AUTHOR_TAG ) or the pos tagger reported in Mikheev ( 2000 ), which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']","['all its strong points, there are a number of restrictions to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( #AUTHOR_TAG ) or the pos tagger reported in Mikheev ( 2000 ), which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']",1
"['##hn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', '#AUTHOR_TAG developed a way of']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', '#AUTHOR_TAG developed a way of']","['##hn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""#AUTHOR_TAG developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last"", 'in our experiments we applied simple linear interpolation to']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de Mori (1998) proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', ""#AUTHOR_TAG developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last"", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'mani and mac Millan (1995) pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams']",2
['of the eagle workbench for linguistic engineering ( #AUTHOR_TAG ) mention'],"['of the eagle workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower - cased in the same document.', 'this heuristic also employs a database of bigrams and unigrams of lower - cased and capitalized words found in unambiguous positions.', 'it is quite similar to our method for capitalized - word disambiguation.', 'the description of the eagle case normalization module provided by baldwin et al. is, however, very brief and provides no performance evaluation or other details']","['description of the eagle workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower - cased in the same document.', 'this heuristic also employs a database of bigrams and unigrams of lower - cased and capitalized words found in unambiguous positions.', 'it is quite similar to our method for capital']","['description of the eagle workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower - cased in the same document.', 'this heuristic also employs a database of bigrams and unigrams of lower - cased and capitalized words found in unambiguous positions.', 'it is quite similar to our method for capitalized - word disambiguation.', 'the description of the eagle case normalization module provided by baldwin et al. is, however, very brief and provides no performance evaluation or other details']",1
"['1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( #AUTHOR_TAG ).', 'machine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( #AUTHOR_TAG ).', 'machine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( #AUTHOR_TAG ).', 'machine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']","['trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( #AUTHOR_TAG ).', 'machine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']",0
"['), and in this article we will use these two terms and the term case normalization interchangeably.', '#AUTHOR_TAG, p. 294 )']","['elephants are....', 'the disambiguation of capitalized words in ambiguous positions leads to the identification of proper names ( or their derivatives ), and in this article we will use these two terms and the term case normalization interchangeably.', '#AUTHOR_TAG, p. 294 )']","['), and in this article we will use these two terms and the term case normalization interchangeably.', '#AUTHOR_TAG, p. 294 )']","['##uation of capitalized words in mixed - case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', 'in mixed - case texts capitalized words usually denote proper names ( names of organizations, locations, people, artifacts, etc. ), but there are special positions in the text where capitalization is expected.', 'such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'capitalized words in these and some other positions present a case of ambiguity : they can stand for proper names, as in white later said...', ', or they can be just capitalized common words, as in white elephants are....', 'the disambiguation of capitalized words in ambiguous positions leads to the identification of proper names ( or their derivatives ), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG, p. 294 ) studied, among other simple text normalization techniques, the effect of case normalization for different words and showed that ` ` sometimes case variants refer to the same thing ( hurricane and hurricane ), sometimes they refer to different things ( continental and continental ) and sometimes they don't refer to much of anything ( e. g., anytime and anytime ).''"", 'obviously these differences arise because some capitalized words stand for proper names ( such as continental, the name of an airline ) and some do not']",0
"['a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions (']","['a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions ( e. g., united kingdom for uk ).', 'the abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'there is no harm ( apart from the performance issues ) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'candidates then are filtered through a set of known common words and proper names.', 'at the same time many good abbreviations and acronyms are']","['a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions (']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then reused in further processing.', 'this is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'the main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', 'this method first applies an "" abbreviation recognizer "" that generates a set of "" candidate abbreviations "" for a document.', 'then for this set of candidates the system tries to find in the text their definitions ( e. g., united kingdom for uk ).', 'the abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'there is no harm ( apart from the performance issues ) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'candidates then are filtered through a set of known common words and proper names.', 'at the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document']",0
"['##d systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'for instance, #AUTHOR_TAG report that the satz system ( decision tree variant ) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16, 000 words.', ""this is a relatively small training set that can be manually marked in a few hours'time."", 'but the error rate ( 1. 5 % ) of the decision tree classifier trained on this small sample was about 50 % higher than that when trained on 6, 000 labeled examples ( 1. 0 % )']","['operation can be generated automatically from a raw corpus and require no human annotation.', 'although some sbd systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'for instance, #AUTHOR_TAG report that the satz system ( decision tree variant ) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16, 000 words.', ""this is a relatively small training set that can be manually marked in a few hours'time."", 'but the error rate ( 1. 5 % ) of the decision tree classifier trained on this small sample was about 50 % higher than that when trained on 6, 000 labeled examples ( 1. 0 % )']","['operation can be generated automatically from a raw corpus and require no human annotation.', 'although some sbd systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'for instance, #AUTHOR_TAG report that the satz system ( decision tree variant ) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16, 000 words.', ""this is a relatively small training set that can be manually marked in a few hours'time."", 'but the error rate ( 1. 5 % ) of the decision tree classifier trained on this small sample was about 50 % higher than that when trained on 6, 000 labeled examples ( 1. 0 % )']","['significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'the four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'although some sbd systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'for instance, #AUTHOR_TAG report that the satz system ( decision tree variant ) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16, 000 words.', ""this is a relatively small training set that can be manually marked in a few hours'time."", 'but the error rate ( 1. 5 % ) of the decision tree classifier trained on this small sample was about 50 % higher than that when trained on 6, 000 labeled examples ( 1. 0 % )']",1
"['#AUTHOR_TAG ). 7 as an example, consider the translation into french of the house collapsed']","['#AUTHOR_TAG ). 7 as an example, consider the translation into french of the house collapsed']","['#AUTHOR_TAG ). 7 as an example, consider the translation into french of the house collapsed']","['translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'in order to calculate a ranking for each tl sentence produced, we multiply the weights of each chunk used in its construction.', 'note that this ensures that greater importance is attributed to longer chunks, as is usual in most ebmt systems ( cfxxx sato and nagao 1990 ; veale and way 1997 ; #AUTHOR_TAG ). 7 as an example, consider the translation into french of the house collapsed']",0
"['the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by #AUTHOR_TAG, Frederking et al. ( 1994 ), and Hogan and Frederking ( 1998 )']","['the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by #AUTHOR_TAG, Frederking et al. ( 1994 ), and Hogan and Frederking ( 1998 )']","['the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by #AUTHOR_TAG, Frederking et al. ( 1994 ), and Hogan and Frederking ( 1998 )']","['translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source, target ) phrasal translation pairs, ( 2 ) the marker lexicon, ( 3 ) the gen11 thanks are due to one of the anonymous reviewers for pointing out that our webmt system, seeded with input from multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by #AUTHOR_TAG, Frederking et al. ( 1994 ), and Hogan and Frederking ( 1998 )']",1
"['using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', '#AUTHOR_TAG attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they']","['using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', '#AUTHOR_TAG attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they']","['##isen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', '#AUTHOR_TAG attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate']","['ebmt systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'there is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'kay and roscheisen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', '#AUTHOR_TAG attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by becker 1975 ) has been used successfully in a number of areas']",0
['## learnability ( #AUTHOR_TAG ) a text generation ('],['## learnability ( #AUTHOR_TAG ) a text generation ( hovy'],['## learnability ( #AUTHOR_TAG ) a text generation ('],"['## learnability ( #AUTHOR_TAG ) a text generation ( hovy 1988 ; milosavljevic, tulloch, and dale 1996 ) a speech generation ( rayner and carter 1997 ) a localization ( sch [UNK] aler 1996']",0
"['##r ( 1996 ), #AUTHOR_TAG, Carl ( 1999 ), and Brown ( 2000 ), inter alia']","['[UNK] uvenir ( 1996 ), #AUTHOR_TAG, Carl ( 1999 ), and Brown ( 2000 ), inter alia']","['.', 'other similar approaches include those of cicekli and g [UNK] uvenir ( 1996 ), #AUTHOR_TAG, Carl ( 1999 ), and Brown ( 2000 ), inter alia']","['distinguishes chunks from "" patterns, "" as we do : his chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm : "" for each pair of chunk pairs using the algorithm described above, the patterns in ( 26 ) are derived from the chunks in ( 25 ) : of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'other similar approaches include those of cicekli and g [UNK] uvenir ( 1996 ), #AUTHOR_TAG, Carl ( 1999 ), and Brown ( 2000 ), inter alia']",0
['## learnability ( zernik and dyer 1987 ) a text generation ( #AUTHOR_TAG ; milosa'],"['## learnability ( zernik and dyer 1987 ) a text generation ( #AUTHOR_TAG ; milosavljevic, tulloch, and dale 1996 ) a speech generation ( rayner and carter 1997 ) a localization ( sch [UNK] aler 1996']",['## learnability ( zernik and dyer 1987 ) a text generation ( #AUTHOR_TAG ; milosa'],"['## learnability ( zernik and dyer 1987 ) a text generation ( #AUTHOR_TAG ; milosavljevic, tulloch, and dale 1996 ) a speech generation ( rayner and carter 1997 ) a localization ( sch [UNK] aler 1996']",0
['##xxx #AUTHOR_TAG ; veale and way 1997 ; carl 1999 ).'],"['an output string.', 'in order to calculate a ranking for each tl sentence produced, we multiply the weights of each chunk used in its construction.', 'note that this ensures that greater importance is attributed to longer chunks, as is usual in most ebmt systems ( cfxxx #AUTHOR_TAG ; veale and way 1997 ; carl 1999 ).']",['##xxx #AUTHOR_TAG ; veale and way 1997 ; carl 1999 ).'],"['translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'in order to calculate a ranking for each tl sentence produced, we multiply the weights of each chunk used in its construction.', 'note that this ensures that greater importance is attributed to longer chunks, as is usual in most ebmt systems ( cfxxx #AUTHOR_TAG ; veale and way 1997 ; carl 1999 ).']",0
"[', some researchers consider them to be little more than a search - and - replace engine, albeit a rather sophisticated one ( #AUTHOR_TAG )']","[', some researchers consider them to be little more than a search - and - replace engine, albeit a rather sophisticated one ( #AUTHOR_TAG )']","[', some researchers consider them to be little more than a search - and - replace engine, albeit a rather sophisticated one ( #AUTHOR_TAG )']","[""quite a short space of time, translation memory ( tm ) systems have become a very useful tool in the translator's armory."", 'tm systems store a set of source, target translation pairs in their databases.', 'if a new input string cannot be found exactly in the translation database, a search is conducted for close ( or "" fuzzy "" ) matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation.', 'from this description, it should be clear that tm systems do not translate : indeed, some researchers consider them to be little more than a search - and - replace engine, albeit a rather sophisticated one ( #AUTHOR_TAG )']",0
"['using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc Keown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly,']","['using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc Keown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly,']","['##isen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc Keown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly,']","['ebmt systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'there is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'kay and roscheisen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc Keown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by becker 1975 ) has been used successfully in a number of areas']",0
"['machine translation ( juola 1994, 1997 ; #AUTHOR_TAG ; gough, way, and hearne']","['into universal grammar ( juola 1998 ) a machine translation ( juola 1994, 1997 ; #AUTHOR_TAG ; gough, way, and hearne']","['machine translation ( juola 1994, 1997 ; #AUTHOR_TAG ; gough, way, and hearne 2002']","['## language learning ( green 1979 ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) a monolingual grammar induction ( juola 1998 ) a grammar optimization ( juola 1994 ) a insights into universal grammar ( juola 1998 ) a machine translation ( juola 1994, 1997 ; #AUTHOR_TAG ; gough, way, and hearne 2002']",0
"['## language learning ( #AUTHOR_TAG ; mori and moeser 1983 ; morgan, meier, and newport']","['## language learning ( #AUTHOR_TAG ; mori and moeser 1983 ; morgan, meier, and newport']","['## language learning ( #AUTHOR_TAG ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) a']","['## language learning ( #AUTHOR_TAG ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) a monolingual grammar induction ( juola 1998 ) a grammar optimization ( juola 1994 ) a insights into universal grammar ( juola 1998 ) a machine translation ( juola 1994, 1997 ; veale and way 1997 ; gough, way, and hearne 2002']",0
"['are due to one of the anonymous reviewers for pointing out that our webmt system, seeded with input from multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by Frederking and Nirenburg ( 1994 ), #AUTHOR_TAG, and Hogan and Frederking ( 1998 )']","['are due to one of the anonymous reviewers for pointing out that our webmt system, seeded with input from multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by Frederking and Nirenburg ( 1994 ), #AUTHOR_TAG, and Hogan and Frederking ( 1998 )']","['are due to one of the anonymous reviewers for pointing out that our webmt system, seeded with input from multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by Frederking and Nirenburg ( 1994 ), #AUTHOR_TAG, and Hogan and Frederking ( 1998 )']","['are due to one of the anonymous reviewers for pointing out that our webmt system, seeded with input from multiple translation systems, with a postvalidation process via the web ( amounting to an n - gram target language model ), in effect forms a multiengine mt system as described by Frederking and Nirenburg ( 1994 ), #AUTHOR_TAG, and Hogan and Frederking ( 1998 )']",1
"['recently, #AUTHOR_TAG have proposed the exploitation of tms at a subsentential level, while carl, way, and sch [UNK] aler ( 2002 ) and sch [UNK] aler, way, and Carl ( 2003 , pages 108 -']","['recently, #AUTHOR_TAG have proposed the exploitation of tms at a subsentential level, while carl, way, and sch [UNK] aler ( 2002 ) and sch [UNK] aler, way, and Carl ( 2003 , pages 108 - -']","['recently, #AUTHOR_TAG have proposed the exploitation of tms at a subsentential level, while carl, way, and sch [UNK] aler ( 2002 ) and sch [UNK] aler, way, and Carl ( 2003 , pages 108 -']","['recently, #AUTHOR_TAG have proposed the exploitation of tms at a subsentential level, while carl, way, and sch [UNK] aler ( 2002 ) and sch [UNK] aler, way, and Carl ( 2003 , pages 108 - - 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment.', 'this, they suggest, may result in a paradigm shift from tm to ebmt via the phrasal lexicon : translators are on the whole wary of mt technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _ source, target _ phrasal segments, and from there they suggest that it is a reasonably short step to enabling an automated solution via the recombination element of ebmt systems such as those described in [ carl and way 2003 ]']",0
"['using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc Keown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly,']","['using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc Keown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly,']","['##isen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc Keown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly,']","['ebmt systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'there is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'kay and roscheisen ( 1993 ) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'fung and mc Keown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of fung and mckeown with different language pairs using the simpler metric of levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent english - greek clauses on the basis of word occurrence and co - occurrence probabilities.', 'the respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'accordingly, they generate lexical correspondences by means of co - occurrence measures and string similarity metrics.', 'more specifically, the notion of the phrasal lexicon ( used first by becker 1975 ) has been used successfully in a number of areas']",0
"[', 1997 ) assumes that words ending in - ed are verbs.', 'however, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'instead, we take advantage of the']","[', 1997 ) assumes that words ending in - ed are verbs.', 'however, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'instead, we take advantage of the']","[', 1997 ) assumes that words ending in - ed are verbs.', 'however, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right - hand sides.', 'that is, for a rule in the penn treebank vp  vbg, np, pp, we are']","[', 1997 ) assumes that words ending in - ed are verbs.', 'however, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right - hand sides.', 'that is, for a rule in the penn treebank vp  vbg, np, pp, we are certain ( if the annotators have done their job correctly ) that the first word in each of the strings corresponding to this right - hand side is a vbg, that is, a present participle.', 'given this information, in such cases we tag such words with the < lex > tag.', 'taking expanding the board to 14 members  augmente le conseila 14 membres as an example, we extract the chunks in ( 24 we ignore here the trivially true lexical chunk "" < quant > 14 : 14.']",1
"['##s.', 'we have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG )']","['or wordlevel lexicons.', 'we have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG )']","['or wordlevel lexicons.', 'we have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG )']","['translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'in order to calculate a ranking for each tl sentence produced, we multiply the weights of each chunk used in its construction.', 'note that this ensures that greater importance is attributed to longer chunks, as is usual in most ebmt systems ( cf.', 'sato and nagao 1990 ; veale and way 1997 ; carl 1999 ). 7', 's an example, consider the translation into french of the house collapsed.', 'assume the conditional probabilities in ( 33 these mistranslations are all caused by boundary friction.', 'each of the translations in ( 37 ) and ( 38 ) would be output with an associated weight and ranked by the system.', 'we would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word - level lexicon into generalized marker chunks.', 'that is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons.', 'we have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG )']",3
['#AUTHOR_TAG ; carl 1999 ).'],['#AUTHOR_TAG ; carl 1999 ).'],['#AUTHOR_TAG ; carl 1999 ).'],"['translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'in order to calculate a ranking for each tl sentence produced, we multiply the weights of each chunk used in its construction.', 'note that this ensures that greater importance is attributed to longer chunks, as is usual in most ebmt systems ( cfxxx sato and nagao 1990 ; #AUTHOR_TAG ; carl 1999 ).']",0
"['required ) correction process based on #AUTHOR_TAG.', 'grefenstette shows that the web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'rather than search for competing candidates, we select the "" best "" translation and have its']","['required ) correction process based on #AUTHOR_TAG.', 'grefenstette shows that the web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'rather than search for competing candidates, we select the "" best "" translation and have its']","['required ) correction process based on #AUTHOR_TAG.', 'grefenstette shows that the web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'rather than search for competing candidates, we select the "" best "" translation and have its morphological variants searched for on - line.', 'in the example above,']","['problem of boundary friction is clearly visible here : we have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural np.', 'however, rather than output this wrong translation directly, we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG.', 'grefenstette shows that the web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'rather than search for competing candidates, we select the "" best "" translation and have its morphological variants searched for on - line.', ""in the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le / la / l'ordinateurs personnels."", 'interestingly, using lycos, and setting the search language to french, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2, 454 times, whereas the others are not found at all.', 'in this case, this translation overrides the highest - ranked translation ( 50 ) and is output as the final translation.', 'in fact, in checking the translations obtained for nps using system combination abc, we noted that 251 nps out of the test set of 500 could be improved.', 'of these 251, 207 ( 82. 5 % ) were improved post hoc via the web, with no improvement for the remaining 43 cases.', 'we consider this to be quite a significant result']",5
"['universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG, 1997 ) conducts some small experiments using his metla system to show the viability of this approach for english a > french and english a > urdu.', 'for the english  french language pair, juola gives results of 61 % correct translation when the system is tested on the training corpus, and 36 % accuracy when it is evaluated with test data.', 'for english  Urdu, Juola (1997, page 213 ) notes that "" the system learned the original training corpus...', 'perfectly and could reproduce it without errors "" ; that is, it scored 100 % accuracy when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in']","['universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG, 1997 ) conducts some small experiments using his metla system to show the viability of this approach for english a > french and english a > urdu.', 'for the english  french language pair, juola gives results of 61 % correct translation when the system is tested on the training corpus, and 36 % accuracy when it is evaluated with test data.', 'for english  Urdu, Juola (1997, page 213 ) notes that "" the system learned the original training corpus...', 'perfectly and could reproduce it without errors "" ; that is, it scored 100 % accuracy when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in']","['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG, 1997 ) conducts some small experiments using his metla system to show the viability of this approach for english a > french and english a > urdu.', 'for the english  french language pair, juola gives results of 61 % correct translation when the system is tested on the training corpus, and 36 % accuracy when it is evaluated with test data.', 'for english  Urdu, Juola (1997, page 213 ) notes that "" the system learned the original training corpus...', 'perfectly and could reproduce it without errors "" ; that is, it scored 100 % accuracy when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in']","['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG, 1997 ) conducts some small experiments using his metla system to show the viability of this approach for english a > french and english a > urdu.', 'for the english  french language pair, juola gives results of 61 % correct translation when the system is tested on the training corpus, and 36 % accuracy when it is evaluated with test data.', 'for english  Urdu, Juola (1997, page 213 ) notes that "" the system learned the original training corpus...', 'perfectly and could reproduce it without errors "" ; that is, it scored 100 % accuracy when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in their gaijin system, Veale and Way (1997) give a result of 63 % accurate translations obtained for english  german on a test set of 791 sentences from coreldraw manuals']",0
[') a grammar optimization ( #AUTHOR_TAG ) a'],[') a grammar optimization ( #AUTHOR_TAG ) a'],['monolingual grammar induction ( juola 1998 ) a grammar optimization ( #AUTHOR_TAG ) a insights into universal grammar ( juola 1998 ) a'],"['## language learning ( green 1979 ; mori and moeser 1983 ; morgan, meier, and newport 1989 ) a monolingual grammar induction ( juola 1998 ) a grammar optimization ( #AUTHOR_TAG ) a insights into universal grammar ( juola 1998 ) a machine translation ( juola 1994, 1997 ; veale and way 1997 ; gough, way, and hearne 2002']",0
"['the marker lexicon following a process found in #AUTHOR_TAG.', ""in block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'in a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment']","['the marker lexicon following a process found in #AUTHOR_TAG.', ""in block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'in a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment']","['the marker lexicon following a process found in #AUTHOR_TAG.', ""in block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'in a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment']","['a final processing stage, we generalize over the marker lexicon following a process found in #AUTHOR_TAG.', ""in block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'in a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment']",5
['further using a methodology based on #AUTHOR_TAG'],['further using a methodology based on #AUTHOR_TAG'],"['a japanese noun - case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'following construction of the marker lexicon, the ( source, target ) chunks are generalized further using a methodology based on #AUTHOR_TAG']","['language learning ( green 1979 ; mori and moeser 1983 ; morgan, meier, and newport 1989 )  monolingual grammar induction ( juola 1998 )  grammar optimization ( juola 1994 )  insights into universal grammar ( juola 1998 )  machine translation ( juola 1994 (Juola , 1997 veale and way 1997 ; gough, way, and hearne 2002 ) with respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", 'the research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.', 'Juola\'s (1994 juola\'s (, 1998 work on grammar optimization and induction shows that context - free grammars can be converted to "" marker - normal form. ""', 'however, marker - normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto - one mapping between a terminal symbol and a word.', 'Nevertheless, Juola (1998, page 23 ) observes that "" a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item ( for example, a word and its case - marking ), can capture this sort of result quite handily. ""', 'work using the marker hypothesis for mt adapts this monolingual mapping for pairs of languages : it is reasonably straightforward to map an english determiner - noun sequence onto a japanese noun - case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'following construction of the marker lexicon, the ( source, target ) chunks are generalized further using a methodology based on #AUTHOR_TAG to permit a limited form of insertion in the translation process.', 'as a byproduct of the chosen methodology, we also derive a standard "" word - level "" translation lexicon.', 'these various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input']",5
"['##in system, #AUTHOR_TAG give a result of 63 % accurate translations obtained for english a > german on a test set of 791 sentences from coreldraw manuals']","['universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', 'Juola (1994 juola (, 1997 conducts some small experiments using his metla system to show the viability of this approach for english  french and english  urdu.', 'for the english  french language pair, juola gives results of 61 % correct translation when the system is tested on the training corpus, and 36 % accuracy when it is evaluated with test data.', 'for english  Urdu, Juola (1997, page 213 ) notes that "" the system learned the original training corpus...', 'perfectly and could reproduce it without errors "" ; that is, it scored 100 % accuracy when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in their gaijin system, #AUTHOR_TAG give a result of 63 % accurate translations obtained for english a > german on a test set of 791 sentences from coreldraw manuals']","['##in system, #AUTHOR_TAG give a result of 63 % accurate translations obtained for english a > german on a test set of 791 sentences from coreldraw manuals']","['that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', 'Juola (1994 juola (, 1997 conducts some small experiments using his metla system to show the viability of this approach for english  french and english  urdu.', 'for the english  french language pair, juola gives results of 61 % correct translation when the system is tested on the training corpus, and 36 % accuracy when it is evaluated with test data.', 'for english  Urdu, Juola (1997, page 213 ) notes that "" the system learned the original training corpus...', 'perfectly and could reproduce it without errors "" ; that is, it scored 100 % accuracy when tested against the training corpus.', 'on novel test sentences, he gives results of 72 % correct translation.', 'in their gaijin system, #AUTHOR_TAG give a result of 63 % accurate translations obtained for english a > german on a test set of 791 sentences from coreldraw manuals']",1
"['to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phr']","['to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f,']","['possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phr']","['syntax / prosody misalignment may be viewed as resulting in part from semantic considerations.', 'both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['psycholinguistic studies of Martin ( 1970 ), #AUTHOR_TAG, Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and Gee and Grosjean ( 1983 ), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and pros']","['psycholinguistic studies of Martin ( 1970 ), #AUTHOR_TAG, Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and Gee and Grosjean ( 1983 ), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [']","['psycholinguistic studies of Martin ( 1970 ), #AUTHOR_TAG, Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and Gee and Grosjean ( 1983 ), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and pros']","['psycholinguistic studies of Martin ( 1970 ), #AUTHOR_TAG, Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and Gee and Grosjean ( 1983 ), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [ the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent.']",0
['#AUTHOR_TAG ;'],['#AUTHOR_TAG ; cahn'],['#AUTHOR_TAG ;'],"['text - to - speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g. allen 1976 ; elowitz et al. 1976 ; #AUTHOR_TAG ; cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and o'Shaughnessy (1989) - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'we believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'textto - speech systems that lack sentence - level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'in constructing the system, we focused on two core questions : ( i ) what kind of parser is needed for the p : rosody rules?', '']",4
"['example, #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for']","['example, #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for']","['example, #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example, #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']",1
"['to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in #AUTHOR_TAG also assume that phr']","['to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f,']","['possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in #AUTHOR_TAG also assume that phr']","['syntax / prosody misalignment may be viewed as resulting in part from semantic considerations.', 'both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",2
"['., #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for']","['some researchers, e. g., #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from Chomsky (1965) to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus']","['., #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g., #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from Chomsky (1965) to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of chomsky and halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'he thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2 - - that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface, ; with prosodic phonology']",0
['correspondent. 4 the most explicit version of this approach is the analysis presented in #AUTHOR_TAG'],['correspondent. 4 the most explicit version of this approach is the analysis presented in #AUTHOR_TAG'],"['occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating ii the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in #AUTHOR_TAG']","['psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating ii the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth g & g )']",1
"['. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', '']","['example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing']","['. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', '']","['syntax / prosody misalignment may be viewed as resulting in part from semantic considerations.', 'both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['rules for phonological word formation are adopted, for the most part, from g & g, #AUTHOR_TAG, and the account of monosyllabic destressing in Selkirk ( 1984 ).', 'thus in our analysis, rules of phonological word formation apply to the']","['rules for phonological word formation are adopted, for the most part, from g & g, #AUTHOR_TAG, and the account of monosyllabic destressing in Selkirk ( 1984 ).', 'thus in our analysis, rules of phonological word formation apply to the non - null terminal nodes in a syntax tree.', 'if the terminal is a content word, i. e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'otherwise the word combines with one or more orthographically']","['rules for phonological word formation are adopted, for the most part, from g & g, #AUTHOR_TAG, and the account of monosyllabic destressing in Selkirk ( 1984 ).', 'thus in our analysis, rules of phonological word formation apply to the non - null terminal nodes in a syntax tree.', 'if the terminal is a content word, i. e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'this is accomplished by']","['rules for phonological word formation are adopted, for the most part, from g & g, #AUTHOR_TAG, and the account of monosyllabic destressing in Selkirk ( 1984 ).', 'thus in our analysis, rules of phonological word formation apply to the non - null terminal nodes in a syntax tree.', 'if the terminal is a content word, i. e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'this is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'function words, e. g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.', 'content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone']",5
"['all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of #AUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser (1981) that the ratio of derived to explicit']","['all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of #AUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser (1981) that the ratio of derived to explicit']","['all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of #AUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser (1981) that the ratio of derived to explicit']","[', there are at least three arguments against iterating pt.', 'first of all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of #AUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8 : 1 ; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh - questions']",4
"['others.', 'we are going to make such a comparison with the theories proposed by j. #AUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w. Kintch ( 1983 ), who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']","['others.', 'we are going to make such a comparison with the theories proposed by j. #AUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w. Kintch ( 1983 ), who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']","['others.', 'we are going to make such a comparison with the theories proposed by j. #AUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w. Kintch ( 1983 ), who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']","['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'we are going to make such a comparison with the theories proposed by j. #AUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w. Kintch ( 1983 ), who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']",1
"['. #AUTHOR_TAG), and']","['prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf. #AUTHOR_TAG), and']","['. #AUTHOR_TAG), and']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf. #AUTHOR_TAG), and their kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abduc - tive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of Berwick (1986).', 'but, ob - viously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
['. #AUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['. #AUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['. #AUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['. #AUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],0
"['#AUTHOR_TAG, p. 67']","['#AUTHOR_TAG, p. 672 )']","['##an 1976 ; cf. also #AUTHOR_TAG, p. 67']","['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by halliday and hasan 1976 ; cf. also #AUTHOR_TAG, p. 672 )']",0
"['##fe 1979, #AUTHOR_TAG, long']","['of the paragraph as a central element of discourse ( e. g. chafe 1979, #AUTHOR_TAG, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']","['. chafe 1979, #AUTHOR_TAG, long']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, #AUTHOR_TAG, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"[', a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG, p. 54']","[', a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence']","[', a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG, p. 54']","[', our definition of coherence may not be restrictive enough : two collections of sentences, one referring to "" black "" ( about black pencils, black pullovers, and black poodles ), the other one about "" death "" ( war, cancer, etc. ), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic "" black + death. ""', ""this problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e. g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]",0
"['describing the logic.', 'this principle of finitism is also assumed by #AUTHOR_TAG, Jackendoff ( 1983 ), Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['describing the logic.', 'this principle of finitism is also assumed by #AUTHOR_TAG, Jackendoff ( 1983 ), Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']","['describing the logic.', 'this principle of finitism is also assumed by #AUTHOR_TAG, Jackendoff ( 1983 ), Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by #AUTHOR_TAG, Jackendoff ( 1983 ), Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
"[""for instance, the discourse representation structures ( drs's ) of #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of Montague (1970) is more sophisticated,']","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of Montague (1970) is more sophisticated,']","[""for instance, the discourse representation structures ( drs's ) of #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of Montague (1970) is more sophisticated,']","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", 'Jackendoff (1983, p. 14) writes "" it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. ""', 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role, too.', '']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role, too.', '']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['to #AUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... (']","['to #AUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... ( ibid. )', 'suddenly ( for hobbs ) becomes coherent.', ""it seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change ; after all, the first two sentences didn't change when the third one was added."", 'on the other hand, this change is easily explained when we treat the first two sentences as a paragraph : if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'and the paragraph obtained by adding the third sentence is coherent.', 'moreover, coherence here is clearly the result of the existence of the topic "" john likes spinach.']","['to #AUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... (']","['to #AUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... ( ibid. )', 'suddenly ( for hobbs ) becomes coherent.', ""it seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change ; after all, the first two sentences didn't change when the third one was added."", 'on the other hand, this change is easily explained when we treat the first two sentences as a paragraph : if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'and the paragraph obtained by adding the third sentence is coherent.', 'moreover, coherence here is clearly the result of the existence of the topic "" john likes spinach.']",1
['#AUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so'],"['instance, relating "" they "" to "" apples "" in the sentence ( cfxxx #AUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so']","['instance, relating "" they "" to "" apples "" in the sentence ( cfxxx #AUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so']","['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( zadrozny 1987a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not come as a surprise that we can now use this apparatus for text / discourse analysis ; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'for instance, relating "" they "" to "" apples "" in the sentence ( cfxxx #AUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so']",0
"[') paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG']","['example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG']","[') paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG']","['example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG']",0
"['#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus,']","['#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus,']","['#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus,']","[', there are at least three arguments against iterating pt.', 'first of all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of Grice (1975 grice (, 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8 : 1 ; furthermore, our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh - questions']",4
"['#AUTHOR_TAG ).', 'brachman et al. 1985 ).', ""krypton's a - box,""]","['#AUTHOR_TAG ).', 'brachman et al. 1985 ).', ""krypton's a - box,""]","['##xxx #AUTHOR_TAG ).', 'brachman et al. 1985 ).', ""krypton's a - box, encoding the object theory as a set of assertions, uses standard first order logic ; the t - box contains information expressed in a frame - based language equivalent to a fragment of fol."", ""however, the distinction between the two parts is purely functional - - that is, characterized in terms of the system's behavior."", 'from the logical point of view, the knowledge base is the union of the two boxes, i. e. a theory, and the entailment is standard.', 'in our system, we also distinguish between the "" definitional "" and factual information, but the "" definitional "" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'moreover, in addition to proposing this structure of r, we have described the two mechanisms for exploiting it, "" coherence "" and "" dominance, "" which are not']","['last point may be seen better if we look at some differences between our system and krypton, which also distinguishes between an object theory and background knowledge ( cfxxx #AUTHOR_TAG ).', 'brachman et al. 1985 ).', ""krypton's a - box, encoding the object theory as a set of assertions, uses standard first order logic ; the t - box contains information expressed in a frame - based language equivalent to a fragment of fol."", ""however, the distinction between the two parts is purely functional - - that is, characterized in terms of the system's behavior."", 'from the logical point of view, the knowledge base is the union of the two boxes, i. e. a theory, and the entailment is standard.', 'in our system, we also distinguish between the "" definitional "" and factual information, but the "" definitional "" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'moreover, in addition to proposing this structure of r, we have described the two mechanisms for exploiting it, "" coherence "" and "" dominance, "" which are not variants of the standard first order entailment, but abduction']",1
"[""is spiritually related to the ` ` abductive inference'' of #AUTHOR_TAG, the ` ` diagnosis from first principles'' of Reiter ( 1987 ), ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of #AUTHOR_TAG, the ` ` diagnosis from first principles'' of Reiter ( 1987 ), ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of #AUTHOR_TAG, the ` ` diagnosis from first principles'' of Reiter ( 1987 ), ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure assumption "" ( ibid. ),', '"" domain circumscription "" ( cf.', 'etherington and mercer 1987 ), and their kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of #AUTHOR_TAG, the ` ` diagnosis from first principles'' of Reiter ( 1987 ), ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
"['#AUTHOR_TAG ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983)""]","['#AUTHOR_TAG ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983)""]","['#AUTHOR_TAG ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems""]","['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event compo - nents.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. moens and steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram - matical constraint ( that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon - - - ibid. )""]",0
"['- real struc - tures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to #AUTHOR_TAG, paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences.']","['e. j. crothers. his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, "" the linguistic - logical notions of consequent and presupposition "" Crothers (1979 : 112 ) have collected convincing evidence of the existence of language chunks - - real struc - tures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to #AUTHOR_TAG, paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences. hinds discusses three major types of paragraphs,']","['- real struc - tures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to #AUTHOR_TAG, paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences.']","['textualist approach to paragraph analysis is exemplified by e. j. crothers. his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, "" the linguistic - logical notions of consequent and presupposition "" Crothers (1979 : 112 ) have collected convincing evidence of the existence of language chunks - - real struc - tures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to #AUTHOR_TAG, paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences. hinds discusses three major types of paragraphs, and their corresponding segment types.', 'the three types are procedural ( how - to ), ex - pository ( essay ), and narrative ( in this case, spontaneous conversation ).', 'for each type, its segments are distinguished by bearing distinct relationships to the paragraph topic ( which is central, but nowhere clearly defined ).', 'segments themselves are composed of clauses and regulated by "" switching "" patterns, such as the question - answer pattern and the remark - reply pattern']",0
"[""is spiritually related to the ` ` abductive inference'' of Reggia ( 1985 ), the ` ` diagnosis from first principles'' of #AUTHOR_TAG, ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of Reggia ( 1985 ), the ` ` diagnosis from first principles'' of #AUTHOR_TAG, ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of Reggia ( 1985 ), the ` ` diagnosis from first principles'' of #AUTHOR_TAG, ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure assumption "" ( ibid. ),', '"" domain circumscription "" ( cf.', 'etherington and mercer 1987 ), and their kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of Reggia ( 1985 ), the ` ` diagnosis from first principles'' of #AUTHOR_TAG, ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
['#AUTHOR_TAG ; patel - schneider 1985'],['#AUTHOR_TAG ; patel - schneider 1985'],['#AUTHOR_TAG ; patel - schneider 1985'],"['in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, ( cfxxx levesque 1984 ; #AUTHOR_TAG ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]",1
"['##xxx #AUTHOR_TAG.', 'Hintikka (1985)']","['l is going to be used only in section 5 for dealing with noun phrase references.', ""this means that natural language expressions such as ` ` a is b,'' ` ` a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx #AUTHOR_TAG."", 'Hintikka (1985)']","['##xxx #AUTHOR_TAG.', 'Hintikka (1985)']","['explicitly stated otherwise, we assume that formulas are expressed in a certain ( formal ) language l without equality ; the extension l ( = ) of l is going to be used only in section 5 for dealing with noun phrase references.', ""this means that natural language expressions such as ` ` a is b,'' ` ` a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx #AUTHOR_TAG."", 'Hintikka (1985)']",1
"['"" all "" in the above definitions ( cf. #AUTHOR_TAG']","['"" all "" in the above definitions ( cf. #AUTHOR_TAG']","['"" all "" in the above definitions ( cf. #AUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system, "" satisfies "" should be probably replaced by "" violates fewest.']","[': the notions of strong provability and strong r + m - abduction can be in - troduced by replacing "" there exists "" by "" all "" in the above definitions ( cf. #AUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system, "" satisfies "" should be probably replaced by "" violates fewest.']",1
"['( semantic feature information, #AUTHOR_TAG ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'Later, Hobbs (1979']","['( semantic feature information, #AUTHOR_TAG ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'Later, Hobbs (1979 hobbs (, 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using "" salience "" in choosing facts from this knowledge base']","['selectional restrictions ( semantic feature information, #AUTHOR_TAG ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'Later, Hobbs (1979 hobbs (, 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using "" salience "" in choosing facts from this knowledge base']","['selectional restrictions ( semantic feature information, #AUTHOR_TAG ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'Later, Hobbs (1979 hobbs (, 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using "" salience "" in choosing facts from this knowledge base']",0
['. g. #AUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time'],"['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. #AUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time']","['. g. #AUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', '']","['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. #AUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram - matical constraint ( that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon - - - ibid. )""]",0
"['#AUTHOR_TAG a, 1987']","['it is the "" highest "" path, fint is the most plausible ( relative to r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx #AUTHOR_TAG a, 1987b ).', 'zadrozny 1987a Zadrozny , 1987 b.', 'another theory, consisting of f ~ = { el, sh2, pl, b2 ~ dl } and s, saying that a space vehicle came into the harbor and caused a disease ~ illness is less plausible according to that ordering.', 'as it turns out, f ~ is never constructed in the process of building an interpretation of a paragraph containing the sentence s, unless assuming fint would lead to a contradiction,']","['#AUTHOR_TAG a, 1987']","['it is the "" highest "" path, fint is the most plausible ( relative to r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx #AUTHOR_TAG a, 1987b ).', 'zadrozny 1987a Zadrozny , 1987 b.', 'another theory, consisting of f ~ = { el, sh2, pl, b2 ~ dl } and s, saying that a space vehicle came into the harbor and caused a disease ~ illness is less plausible according to that ordering.', 'as it turns out, f ~ is never constructed in the process of building an interpretation of a paragraph containing the sentence s, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story']",0
"['##an 1976, #AUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, #AUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']","['##an 1976, #AUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, #AUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['#AUTHOR_TAG ).', 'myc']","['#AUTHOR_TAG ).', 'mycielski 1981 )']","['##xxx #AUTHOR_TAG ).', 'myc']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson - Laird (1983), Jackendoff (1983, Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx #AUTHOR_TAG ).', 'mycielski 1981 )']",0
"['have shown elsewhere ( #AUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( #AUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( #AUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( #AUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - - for example, to disambiguate pp attachment.', 'this means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object - level theory']",2
"['necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification / matching, Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification / matching, Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification / matching, Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification / matching, Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
"['to deal with anaphora.', 'the logical notation of #AUTHOR_TAG is more sophisticated,']","['to deal with anaphora.', 'the logical notation of #AUTHOR_TAG is more sophisticated,']","['to deal with anaphora.', 'the logical notation of #AUTHOR_TAG is more sophisticated,']","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of Kamp (1981), which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of #AUTHOR_TAG is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", 'Jackendoff (1983, p. 14) writes "" it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. ""', 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), #AUTHOR_TAG, Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), #AUTHOR_TAG, Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']","['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), #AUTHOR_TAG, Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), #AUTHOR_TAG, Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
"['.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition #AUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks']","['e. j. crothers.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition #AUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks - - real structures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences.', 'hinds']","['e. j. crothers.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition #AUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks']","['textualist approach to paragraph analysis is exemplified by e. j. crothers.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition #AUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks - - real structures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences.', 'hinds discusses three major types of paragraphs, and their corresponding segment types.', 'the three types are procedural ( how - to ), expository ( essay ), and narrative ( in this case, spontaneous conversation ).', 'for each type, its segments are distinguished by bearing distinct relationships to the paragraph topic ( which is central, but nowhere clearly defined ).', 'segments themselves are composed of clauses and regulated by "" switching "" patterns, such as the question - answer pattern and the remark - reply pattern']",0
"['#AUTHOR_TAG ).', 'woods 1987 )']","['#AUTHOR_TAG ).', 'woods 1987 )']","['##xxx #AUTHOR_TAG ).', 'woods 1987 )']","['the word "" up "" is given its meaning relative to our experience with gravity, it is not free to "" slip "" into its opposite.', '"" up "" means up and not down....', 'we have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model.', 'mothers have a different role than fathers in this model, and thus there is a reason why "" death is the father of beauty "" fails poetically while "" death is the mother of beauty "" succeeds....', 'it is precisely this "" grounding "" of logical predicates in other conceptual structures that we would like to capture.', 'we investigate here only the "" grounding "" in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx #AUTHOR_TAG ).', 'woods 1987 )']",0
"['may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", ""#AUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical']","['may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", ""#AUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical']","['may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", ""#AUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of Kamp (1981), which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", ""#AUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['example of psycholinguistically oriented research work can be found in #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used,']","['example of psycholinguistically oriented research work can be found in #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used,']","['example of psycholinguistically oriented research work can be found in #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used,']","['example of psycholinguistically oriented research work can be found in #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980)']",0
"['needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', '#AUTHOR_TAG, p.']","['needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', '#AUTHOR_TAG, p.']","['needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', '#AUTHOR_TAG, p.']","['demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG, p. 112 ), for example, bemoans the fact that his ` ` theory lacks a world knowledge component, a mental ` encyclopedia,'which could be invoked to generate inferences...''."", 'with respect to that independent source of knowledge, our main contributions are two.', 'first, we identify its possible structure ( a collection of partially ordered theories ) and make formal the choice of a most plausible interpretation.', 'in other words, we recognize it as a separate logical level - - the referential level.', 'second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level']",0
"['##cean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature "" ( #AUTHOR_TAG ), or the metarules of section 5. 2?', 'it seems to us that the answer is no']","['. 1. 1 was the use of a gricean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature "" ( #AUTHOR_TAG ), or the metarules of section 5. 2?', 'it seems to us that the answer is no']","['. 1. 1 was the use of a gricean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature "" ( #AUTHOR_TAG ), or the metarules of section 5. 2?', 'it seems to us that the answer is no']","['. 1. 1 was the use of a gricean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature "" ( #AUTHOR_TAG ), or the metarules of section 5. 2?', 'it seems to us that the answer is no']",0
"['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( #AUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it']","['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( #AUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it']","['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( #AUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it']","['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( #AUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not come as a surprise that we can now use this apparatus for text / discourse analysis ; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'for instance, relating "" they "" to "" apples "" in the sentence ( cf.', 'haugeland 1985 p. 195 ; zadrozny 1987a )']",5
"['is spiritually related to the "" abductive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of #AUTHOR_TAG.', 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abductive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of #AUTHOR_TAG.', 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abductive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of #AUTHOR_TAG.', 'but, obviously, trying to establish precise connections for the metarules or the provability']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure assumption "" ( ibid. ),', '"" domain circumscription "" ( cf.', 'etherington and mercer 1987 ), and their kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abductive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of #AUTHOR_TAG.', 'but, obviously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
"['##e 1979, #AUTHOR_TAG ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our']","['of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, longacre 1979, #AUTHOR_TAG ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']","['##e 1979, #AUTHOR_TAG ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, longacre 1979, #AUTHOR_TAG ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; #AUTHOR_TAG ) or quantifier scoping ( webber 1983 ) must play a role, too.', '']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; #AUTHOR_TAG ) or quantifier scoping ( webber 1983 ) must play a role, too.', '']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; #AUTHOR_TAG ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; #AUTHOR_TAG ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['.', 'some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['.', 'some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']",0
"['#AUTHOR_TAG ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text']","['#AUTHOR_TAG ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text']","['#AUTHOR_TAG ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text']","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ; #AUTHOR_TAG ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]",1
['#AUTHOR_TAG ;'],['#AUTHOR_TAG ;'],['##xxx #AUTHOR_TAG ;'],"['in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, ( cfxxx #AUTHOR_TAG ; frisch 1987 ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]",1
"['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( #AUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', '']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( #AUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', '']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( #AUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( #AUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose ( #AUTHOR_TAG, p.']","[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose ( #AUTHOR_TAG, p.']","[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose ( #AUTHOR_TAG, p.']","[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose ( #AUTHOR_TAG, p. 329 )']",4
"['of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced']","['can also hope for some fine - tuning of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced']","['can also hope for some fine - tuning of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced']","['can also hope for some fine - tuning of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible, but no attempt is made to define only the so - called grammatical strings of a language']",0
"['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]",1
"['##a is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by #AUTHOR_TAG']","['do not claim that gla is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by #AUTHOR_TAG']","['##a is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by #AUTHOR_TAG']","['do not claim that gla is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by #AUTHOR_TAG']",2
"['semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by #AUTHOR_TAG ; cf']","['are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by #AUTHOR_TAG ; cfxxx']","['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by #AUTHOR_TAG ; cf']","['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by #AUTHOR_TAG ; cfxxx also quirk et al. 1972, p. 672 )']",0
"['', '#AUTHOR_TAG, halliday and']","['', '#AUTHOR_TAG, halliday and hasan 1976, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']","['', '#AUTHOR_TAG, halliday and']","['there are other discussions of the paragraph as a central element of discourse ( e. g.', '#AUTHOR_TAG, halliday and hasan 1976, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), Jackendoff ( 1983 ), #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), Jackendoff ( 1983 ), #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']","['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), Jackendoff ( 1983 ), #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), Jackendoff ( 1983 ), #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
"[') paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 )']","['example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 )']","[') paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 )']","['example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 )']",0
"['; #AUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['; #AUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( jensen and binot 1988 ; #AUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( jensen and binot 1988 ; #AUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - - for example, to disambiguate pp attachment.', 'this means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object - level theory']",2
"['as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx #AUTHOR_TAG b ).', 'za']","['an interpretation of a paragraph does not mean finding all of its possible meanings ; the implausible ones should not be computed at all.', 'this viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'we can then later ( section 5. 2 ) define p - models - - a category of models corresponding to paragraphs - - as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx #AUTHOR_TAG b ).', '']","['as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx #AUTHOR_TAG b ).', 'za']","['an interpretation of a paragraph does not mean finding all of its possible meanings ; the implausible ones should not be computed at all.', 'this viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'we can then later ( section 5. 2 ) define p - models - - a category of models corresponding to paragraphs - - as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx #AUTHOR_TAG b ).', 'zadrozny 1987b )']",1
"['##bs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later, #AUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]","['( semantic feature information, hobbs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later, #AUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]","['selectional restrictions ( semantic feature information, hobbs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later, #AUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]","['selectional restrictions ( semantic feature information, hobbs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later, #AUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]",0
"['necessity of this kind of merging of arguments has been recognized before : charniak and mc Dermott ( 1985 ) call it abductive unification / matching, #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : charniak and mc Dermott ( 1985 ) call it abductive unification / matching, #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : charniak and mc Dermott ( 1985 ) call it abductive unification / matching, #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : charniak and mc Dermott ( 1985 ) call it abductive unification / matching, #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
"['look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems""]","['look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems""]","['. g.', 'moens and steedman 1987 ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems""]","['', 'the text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g.', 'moens and steedman 1987 ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', 'extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint ( "" that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon "" - - - ibid. ).', 'however, as noted before, we will use a simplified version of such a logical notation ; we will have only time, event, result, and property as primitives.', 'after these remarks we can begin constructing the model of the example paragraph.', 'we assume that constants are introduced by nps.', 'we have then ( i ) constants s, m, d, i, b, 1347 satisfying : ship ( s ), messina ( m ), disease ( d ), infection ( i ), death ( b ), year ( 1347 )']",0
"[""instance, #AUTHOR_TAG, p. 8 ) says that the sentence ` ` reagan thinks bananas,'' which is otherwise strange, is in fact acceptable if it occurs as an answer to the question ` ` what is kissinger's favorite fruit?''"", 'the pairing of these two sentences may be said to create a small paragraph.', 'our point is that an acceptable structure can be assigned to the utterance "" reagan thinks bananas "" only within the paragraph in which this utterance occurs.', 'we believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two ( or one ) neighboring sentences, will be sufficient for this task.', '']","[""instance, #AUTHOR_TAG, p. 8 ) says that the sentence ` ` reagan thinks bananas,'' which is otherwise strange, is in fact acceptable if it occurs as an answer to the question ` ` what is kissinger's favorite fruit?''"", 'the pairing of these two sentences may be said to create a small paragraph.', 'our point is that an acceptable structure can be assigned to the utterance "" reagan thinks bananas "" only within the paragraph in which this utterance occurs.', 'we believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two ( or one ) neighboring sentences, will be sufficient for this task.', '']","[""instance, #AUTHOR_TAG, p. 8 ) says that the sentence ` ` reagan thinks bananas,'' which is otherwise strange, is in fact acceptable if it occurs as an answer to the question ` ` what is kissinger's favorite fruit?''"", 'the pairing of these two sentences may be said to create a small paragraph.', 'our point is that an acceptable structure can be assigned to the utterance "" reagan thinks bananas "" only within the paragraph in which this utterance occurs.', 'we believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two ( or one ) neighboring sentences, will be sufficient for this task.', '']","['paragraph can be thought of as a grammatical unit in the following sense : it is the discourse unit in which a functional ( or a predicate - argument ) structure can be definitely assigned to sentences / strings.', ""for instance, #AUTHOR_TAG, p. 8 ) says that the sentence ` ` reagan thinks bananas,'' which is otherwise strange, is in fact acceptable if it occurs as an answer to the question ` ` what is kissinger's favorite fruit?''"", 'the pairing of these two sentences may be said to create a small paragraph.', 'our point is that an acceptable structure can be assigned to the utterance "" reagan thinks bananas "" only within the paragraph in which this utterance occurs.', 'we believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two ( or one ) neighboring sentences, will be sufficient for this task.', ""that is, we can ask in the first sentence of a paragraph about kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end."", 'we do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism ( although it may be ) ; rather, it has the grammatical role of providing functional structures that can be assigned to strings']",4
"['##t sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), #AUTHOR_TAG, and Young ( 1989 )']","['of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), #AUTHOR_TAG, and Young ( 1989 )']","['of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), #AUTHOR_TAG, and Young ( 1989 )']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), #AUTHOR_TAG, and Young ( 1989 )']",0
['formula for the test set perplexity ( #AUTHOR_TAG ) is :'],['formula for the test set perplexity ( #AUTHOR_TAG ) is :'],['formula for the test set perplexity ( #AUTHOR_TAG ) is :'],['formula for the test set perplexity ( #AUTHOR_TAG ) is :'],0
"['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( #AUTHOR_TAG ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( #AUTHOR_TAG ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( #AUTHOR_TAG ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( #AUTHOR_TAG ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
['the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG )'],['the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG )'],['the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG )'],"['obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information.', 'their speech was recorded in a simulation mode in which the speech recognition component was excluded.', 'instead, an experimenter in a separate room typed in the utterances as spoken by the subject.', 'subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG )']",5
[''],[''],[''],[''],0
"['make the final decision on whether to accept the proposals.', 'this approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['make the final decision on whether to accept the proposals.', 'this approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","[', one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['far, we have added all semantic filters by hand, and they are implemented in a hard - fail mode, i. e., if the semantic restrictions fail, the node dies.', 'this strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'in principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'there is obviously a great deal more work to be done in this important area']",1
"['is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( #AUTHOR_TAG ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( #AUTHOR_TAG ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['of these systems, tina provides the interface between the recognizer and the application back - end.', 'in this section, i will describe our current interfaces between tina and the recognizer and our future plans in this area.', 'in addition, i will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( zue et al. 1989 ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( #AUTHOR_TAG ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['present, we have available at mit two systems, voyager and atis, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'in both of these systems, tina provides the interface between the recognizer and the application back - end.', 'in this section, i will describe our current interfaces between tina and the recognizer and our future plans in this area.', 'in addition, i will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( zue et al. 1989 ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( #AUTHOR_TAG ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']",5
"['filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements.', 'for instance, the set of complements [ from - place']","['filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements.', 'for instance, the set of complements [ from - place ], [ to - place ], and [ at - time ] are freely ordered following a movement verb such as "" leave. ""', 'thus a flight can "" leave for chicago from boston at nine, "" or, equivalently, "" leave at nine for chicago from boston. ""', 'if these complements are each allowed to follow the other, then in tina an infinite sequence of [ from - place ] s, [ to - place ] s and [ at - time ] s is possible.', 'this is of course unacceptable, but it is straightforward to have each node,']","['filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements.', 'for instance, the set of complements [ from - place']","['filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements.', 'for instance, the set of complements [ from - place ], [ to - place ], and [ at - time ] are freely ordered following a movement verb such as "" leave. ""', 'thus a flight can "" leave for chicago from boston at nine, "" or, equivalently, "" leave at nine for chicago from boston. ""', 'if these complements are each allowed to follow the other, then in tina an infinite sequence of [ from - place ] s, [ to - place ] s and [ at - time ] s is possible.', 'this is of course unacceptable, but it is straightforward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in turn, fail if that bit has already been set.', 'we have found that this strategy, in conjunction with the capability of erasing all semantic bits whenever a new clause is entered ( through the meta level "" detach "" operation mentioned previously ) serves the desired goal of eliminating the unwanted redundancies']",5
"['##t sundial project.', 'representative systems are described in #AUTHOR_TAG, De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and Young ( 1989 )']","['of the esprit sundial project.', 'representative systems are described in #AUTHOR_TAG, De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and Young ( 1989 )']","['of the esprit sundial project.', 'representative systems are described in #AUTHOR_TAG, De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and Young ( 1989 )']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in #AUTHOR_TAG, De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and Young ( 1989 )']",0
"['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( zue et al. 1991 ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( #AUTHOR_TAG )."", 'ultimately we want to']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( zue et al. 1991 ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( #AUTHOR_TAG )."", 'ultimately we want to']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( zue et al. 1991 ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( #AUTHOR_TAG )."", 'ultimately we want to']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( zue et al. 1991 ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( #AUTHOR_TAG )."", ""ultimately we want to incorporate tina's probabilities directly into the a * search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model""]",5
"['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( #AUTHOR_TAG ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( #AUTHOR_TAG ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( #AUTHOR_TAG ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( #AUTHOR_TAG ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
"['in the atis domain ( #AUTHOR_TAG ) represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices such as clause ( for a sentence - level concept, such as request ), predicate ( for a functional operation ), reference ( essentially proper noun ), and qset ( for a set of objects']","['in the atis domain ( #AUTHOR_TAG ) represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices such as clause ( for a sentence - level concept, such as request ), predicate ( for a functional operation ), reference ( essentially proper noun ), and qset ( for a set of objects ).', 'the process of obtaining a completed semantic frame amounts to passing frames along from node to node through the']","['in the atis domain ( #AUTHOR_TAG ) represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices such as clause ( for a sentence - level concept, such as request ), predicate ( for a functional operation ), reference ( essentially proper noun ), and qset ( for a set of objects']","['how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'however, the method we are currently using in the atis domain ( #AUTHOR_TAG ) represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices such as clause ( for a sentence - level concept, such as request ), predicate ( for a functional operation ), reference ( essentially proper noun ), and qset ( for a set of objects ).', 'the process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'each node receives a frame in both a top - down and a bottom - up cycle, and modifies the frame according to specifications based on its broad - class identity ( as one of noun, noun - phrase, predicate, quantifier, etc. ).', 'for example, a [ subject ] is a noun - phrase node with the label "" topic. ""', 'during the top - down cycle, it creates a blank frame and inserts it into a "" topic "" slot in the frame that was handed to it.', 'it passes the blank frame to its children, who will then fill it appropriately, labeling it as a qset or as a reference.', 'it then passes along to the right sibling the same frame that was handed to it from above, with the completed topic slot filled with the information delivered by the children']",3
"['a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( #AUTHOR_TAG et al. 1991 ), is a system for accessing']","['a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( #AUTHOR_TAG et al. 1991 ), is a system for accessing']","['a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( #AUTHOR_TAG et al. 1991 ), is a system for accessing data in the official 80 stephanie seneff tina : a natural language system for spoken language applications airline guide and booking flights.', 'work continues on improving all aspects of these domains. our current research is directed at a number of different remaining issues']","['currently have two application domains that can carry on a spoken dialog with a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( #AUTHOR_TAG et al. 1991 ), is a system for accessing data in the official 80 stephanie seneff tina : a natural language system for spoken language applications airline guide and booking flights.', 'work continues on improving all aspects of these domains. our current research is directed at a number of different remaining issues']",5
"['make the final decision on whether to accept the proposals.', 'this approach resembles the work by #AUTHOR_TAG']","['make the final decision on whether to accept the proposals.', 'this approach resembles the work by #AUTHOR_TAG']","[', one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by #AUTHOR_TAG']","['far, we have added all semantic filters by hand, and they are implemented in a hard - fail mode, i. e., if the semantic restrictions fail, the node dies.', 'this strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'in principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions.', 'the semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'there is obviously a great deal more work to be done in this important area']",1
"['out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model ( #AUTHOR_TAG )']","['is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'this is a problem to be aware of in building grammars from example sentences.', 'in the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model ( #AUTHOR_TAG )']","['appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in "" three hundred and sixteen. ""', 'included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'this is a problem to be aware of in building grammars from example sentences.', 'in the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model ( #AUTHOR_TAG )']","['appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in "" three hundred and sixteen. ""', 'included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'this is a problem to be aware of in building grammars from example sentences.', 'in the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model ( #AUTHOR_TAG )']",0
['( #AUTHOR_TAG ) by'],"['to activators at higher levels of the parse tree.', 'that is to say, the current - focus is a feature, like verb - mode, that is blocked when an [ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded ( #AUTHOR_TAG ) by']","[', like verb - mode, that is blocked when an [ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded ( #AUTHOR_TAG ) by']","['. 5. 1 gaps.', 'the mechanism to deal with gaps resembles in certain respects the hold register idea of atns, but with an important difference, reflecting the design philoso - phy that no node can have access to information outside of its immediate domain.', 'the mechanism involves two slots that are available in the feature vector of each parse node.', 'these are called the current - focus and the float - object, respectively.', 'the current - focus slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'if the float - object slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the float - object.', 'the process of getting into the float - object slot ( which is analogous to the hold register ) requires two steps, executed independently by two different nodes.', 'the first node, the generator, fills the current - focus slot with the subparse returned to it by its children.', 'the second node, the activator, moves the current - focus into the float - object position, for its children, during the top - down cycle.', 'it also requires that the float - object be absorbed somewhere among its descendants by a designated absorber node, a condition that is checked during the bottom - up cycle.', 'the current - focus only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.', 'that is to say, the current - focus is a feature, like verb - mode, that is blocked when an [ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded ( #AUTHOR_TAG ) by its generator.', 'finally, certain blocker nodes block the transfer of the float - object to their children']",0
"['a user.', 'one, the voyager domain ( #AUTHOR_TAG ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one,']","['a user.', 'one, the voyager domain ( #AUTHOR_TAG ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( seneff et al. 1991 ), is a system for accessing']","['a user.', 'one, the voyager domain ( #AUTHOR_TAG ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( seneff et al. 1991 ), is a system for accessing data in the official airline guide and booking flights.', 'work continues on improving all aspects of these domains']","['_ currently have two application domains that can carry on a spoken dialog with a user.', 'one, the voyager domain ( #AUTHOR_TAG ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( seneff et al. 1991 ), is a system for accessing data in the official airline guide and booking flights.', 'work continues on improving all aspects of these domains']",5
"[""example used to illustrate the power of atns ( #AUTHOR_TAG ), ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", 'cases of crossed traces are automatically blocked because the second current - focus gets moved into the float - object position at the time of the second activator, overriding the preexisting float - object set up by the earlier activator.', 'the wrong float - object is available at the position of the first trace, and the parse dies : * ( which books ) / did you ask john ( where ) j bill bought ( ti ) ( tj )?', 'the current - focus slot is not']","[""example used to illustrate the power of atns ( #AUTHOR_TAG ), ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", 'cases of crossed traces are automatically blocked because the second current - focus gets moved into the float - object position at the time of the second activator, overriding the preexisting float - object set up by the earlier activator.', 'the wrong float - object is available at the position of the first trace, and the parse dies : * ( which books ) / did you ask john ( where ) j bill bought ( ti ) ( tj )?', 'the current - focus slot is not']","[""example used to illustrate the power of atns ( #AUTHOR_TAG ), ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", 'cases of crossed traces are automatically blocked because the second current - focus gets moved into the float - object position at the time of the second activator, overriding the preexisting float - object set up by the earlier activator.', 'the wrong float - object is available at the position of the first trace, and the parse dies : * ( which books ) / did you ask john ( where ) j bill bought ( ti ) ( tj )?', 'the current - focus slot is not']","[""example used to illustrate the power of atns ( #AUTHOR_TAG ), ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", 'cases of crossed traces are automatically blocked because the second current - focus gets moved into the float - object position at the time of the second activator, overriding the preexisting float - object set up by the earlier activator.', 'the wrong float - object is available at the position of the first trace, and the parse dies : * ( which books ) / did you ask john ( where ) j bill bought ( ti ) ( tj )?', 'the current - focus slot is not restricted to nodes that represent nouns.', 'some of the generators are adverbial or adjectival parts of speech ( pos ).', 'an absorber checks for agreement in pos before it can accept the float - object as its subparse.', 'as an example, the question, "" ( how oily ) / do you like your salad dressing ( ti )? "" contains a [ q - subject ] "" how oily "" that is an adjective.', 'the absorber [ pred - adjective ] accepts the available float - object as its subparse, but only after confirming that pos is adjective']",1
"['##t sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), #AUTHOR_TAG, Niemann ( 1990 ), and Young ( 1989 )']","['of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), #AUTHOR_TAG, Niemann ( 1990 ), and Young ( 1989 )']","['of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), #AUTHOR_TAG, Niemann ( 1990 ), and Young ( 1989 )']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), #AUTHOR_TAG, Niemann ( 1990 ), and Young ( 1989 )']",0
"['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( #AUTHOR_TAG ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( #AUTHOR_TAG ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( #AUTHOR_TAG ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( #AUTHOR_TAG ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
"['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm ( #AUTHOR_TAG ) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']","['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm ( #AUTHOR_TAG ) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']","['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm ( #AUTHOR_TAG ) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']","['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm ( #AUTHOR_TAG ) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']",5
"['in detail.', 'the recognizer for these systems is the summit system ( #AUTHOR_TAG ), which uses a segmental - based framework']","['in detail.', 'the recognizer for these systems is the summit system ( #AUTHOR_TAG ), which uses a segmental - based framework']","['access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( #AUTHOR_TAG ), which uses a segmental - based framework']","['present, we have available at mit two systems, voyager and atis, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'in both of these systems, tina provides the interface between the recognizer and the application back - end.', 'in this section, i will describe our current interfaces between tina and the recognizer and our future plans in this area.', 'in addition, i will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( #AUTHOR_TAG ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( viterbi 1967 ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']",5
"['section describes how tina handles several issues that are often considered to be part of the task of a parser.', 'these include agreement constraints, semantic restrictions, subject - tagging for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical']","['section describes how tina handles several issues that are often considered to be part of the task of a parser.', 'these include agreement constraints, semantic restrictions, subject - tagging for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical']","['section describes how tina handles several issues that are often considered to be part of the task of a parser.', 'these include agreement constraints, semantic restrictions, subject - tagging for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bre']","['section describes how tina handles several issues that are often considered to be part of the task of a parser.', 'these include agreement constraints, semantic restrictions, subject - tagging for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bresnan 1982, p. 235 ff. ), but it is different from these in that the process of filling the hold register equivalent involves two steps separately initiated by two independent nodes']",1
"['##t sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and #AUTHOR_TAG']","['of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and #AUTHOR_TAG']","['of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and #AUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and #AUTHOR_TAG']",0
"['this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 196""]","['this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 1968, jelinek 1976 )."", 'both the a * and the viterbi search are left - to - right search algorithms.', 'however, the a * search is contrasted with the viterbi search in that the set of active hypotheses take up unequal segments of time.', 'that is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold']","['this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 1968, jelinek 1976 )."", 'both the a * and the viterbi search are left - to - right search algorithms.', 'however, the a * search is contrasted with the viterbi search in that the set of active hypotheses take up unequal segments of time.', 'that is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold']","['we first integrated this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 1968, jelinek 1976 )."", 'both the a * and the viterbi search are left - to - right search algorithms.', 'however, the a * search is contrasted with the viterbi search in that the set of active hypotheses take up unequal segments of time.', 'that is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold']",5
"['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance (""]","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( goodine et al. 1991 )."", ""ultimately we want to incorporate tina'sprobabilities directly""]","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance (""]","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( goodine et al. 1991 )."", ""ultimately we want to incorporate tina'sprobabilities directly into the a * search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model""]",5
"['explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification']","['explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', 'this description can then be given the standard set - theoretical interpretation of King (1989, 1994 )']",0
"['lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'the reader is referred to #AUTHOR_TAG']","['lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'the reader is referred to #AUTHOR_TAG']","['to factor out c in ( c a d1 ) v -.. v ( c a dk ) : we compute c a ( d1 v... v dk ), where the r ) are assumed to contain no further common factors.', 'once we have computed c, we use it to make the extended lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'the reader is referred to #AUTHOR_TAG']","['most specific generalization does not necessarily provide additional constrain - ing information.', 'however, usually it is the case that lexical entries resulting from lexical rule application differ in very few specifications compared to the number of specifica - tions in a base lexical entry.', 'most of the specifications of a lexical entry are assumed to be passed unchanged via the automatically generated frame specification.', 'therefore, after lifting the common information into the extended lexical entry, the out - argument in many cases contains enough information to permit a postponed execution of the interaction predicate.', 'when c is the common information, and d1,..., dk are the definitions of the interaction predicate called, we use distributivity to factor out c in ( c a d1 ) v -.. v ( c a dk ) : we compute c a ( d1 v... v dk ), where the r ) are assumed to contain no further common factors.', 'once we have computed c, we use it to make the extended lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'the reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation']",0
"['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shi']","['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to']","['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shi']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dbrre and eisele 1991 ; d6rre and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['interconnect is represented in figure 19.', '27 #AUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction']","['interconnect is represented in figure 19.', '27 #AUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction']","['way these predicates interconnect is represented in figure 19.', '27 #AUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', '']","['way these predicates interconnect is represented in figure 19.', '27 #AUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', 'since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given']",0
"['##s and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct']","['##s and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van']","['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['example, adopted in the lkb system ( #AUTHOR_TAG ).', '']","['example, adopted in the lkb system ( #AUTHOR_TAG ).', '']","['example, adopted in the lkb system ( #AUTHOR_TAG ).', 'both the input']","['disjunction thus constitutes the base lexicon.', 'the disjuncts in the constraint on derived - word, on the other hand, encode the lexical rules.', 'the in - specification of a lexical rule specifies the in feature, the out - specification, the derived word itself.', 'note that the value of the in feature is of type word and thus also has to satisfy either a base lexical entry or an out - specification of a lexical rule.', 'while this introduces the recursion necessary to permit successive lexical rule application, it also grounds the recursion in a word described by a base lexical entry.', 'contrary to the mlr setup, the dlr formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'this conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as, for example, adopted in the lkb system ( #AUTHOR_TAG ).', 'both the input and output of a lexical rule, i. e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.', 'as a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.', 'the computational treatment of lexical rules that we propose in this paper is essentially a domain - specific refinement of such an approach to lexical rules.', '']",0
"['can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ).', 'given a lexical']","['can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ).', 'given a lexical']","['comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of 29 this improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ).', 'given a lexical entry as in figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.', 'to']","['automata resulting from word class specialization group the lexical entries into natural classes.', 'in case the automata corresponding to two lexical entries are identical, the entries belong to the same natural class.', 'however, each lexical rule application, i. e., each transition in an automaton, calls a frame predicate that can have a large number of defining clauses.', 'intuitively understood, each defining clause of a frame predicate corresponds to a subclass of the class of lexical entries to which a lexical rule can be applied.', 'during word class specialization, though, when the finite - state automaton representing global lexical rule application is pruned with respect to a particular base lexical entry, we know which subclass we are dealing with.', 'for each interaction definition we can therefore check which of the flame clauses are applicable and discard the non - applicable ones.', 'we thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'the elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of 29 this improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ).', 'given a lexical entry as in figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.', 'to eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.', '3 the successive unfolding steps are schematically represented in figure 20']",1
"['##ele and dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation.', '']","['; eisele and dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation.', '']","['##ele and dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']","['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( maxwell and kaplan 1989 ; eisele and dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']",0
"['is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in']","['is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 Hinrichs and Nakazawa (1996) show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']",0
"['preparation ).', '6 the partial - vp topicalization lexical rule proposed by #AUTHOR_TAG, 10 ) is a']","['meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by #AUTHOR_TAG, 10 ) is a']","[', meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
"['#AUTHOR_TAG ).', '5 an in - depth discussion including a comparison of both approaches is provided in calc']","['#AUTHOR_TAG ).', '5 an in - depth discussion including a comparison of both approaches is provided in']","['#AUTHOR_TAG ).', '5 an in - depth discussion including a comparison of both approaches is provided in calc']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ; #AUTHOR_TAG ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by Hinrichs and Nakazawa (1994, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
"['having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG )']","['having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG )']","['having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG )']","['the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'this is so since the lexical rule in figure 2 "" ( like all lexical rules in hpsg ) preserves all properties of the input not mentioned in the rule. ""', '( pollard and sag [ 1994, 314 ], following flickinger [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in ai ( mccarthy and hayes 1969 ), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG )']",0
"['that only the properties changed by a lexical rule need be mentioned.', 'as shown in #AUTHOR_TAG this is a well - motivated convention since it']","['made in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in #AUTHOR_TAG this is a well - motivated convention since it']","['that only the properties changed by a lexical rule need be mentioned.', 'as shown in #AUTHOR_TAG this is a well - motivated convention since']","['computational treatment expanding out the lexicon cannot be used for the increasing number of hpsg analyses that propose lexical rules that would result in an infinite lexicon.', 'most current hpsg analyses of dutch, german, italian, and french fall into that category.', '1 furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run - time.', 'finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'this conflicts with the standard assumption made in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in #AUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries']",4
"['nondeterminism is based on unfold / fold transformation techniques ( #AUTHOR_TAG ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', '']","['nondeterminism is based on unfold / fold transformation techniques ( #AUTHOR_TAG ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', '']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( #AUTHOR_TAG ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', '']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( #AUTHOR_TAG ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']",5
"['#AUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical']","['are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; #AUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical']","['##hemann 1993 ; oliva 1994 ; frank 1994 ; #AUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; #AUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna']","['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['#AUTHOR_TAG ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['#AUTHOR_TAG ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['#AUTHOR_TAG ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; #AUTHOR_TAG ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
['1995 ; #AUTHOR_TAG ) and the'],"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; #AUTHOR_TAG ) and the']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; #AUTHOR_TAG ) and the.', 'lexical rules (']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; #AUTHOR_TAG ) and the.', 'lexical rules ( dlrs ; meurers 1995 ).', '']",0
"['the controll system ( gerdemann and #AUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar']","['the controll system ( gerdemann and #AUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar']","['the controll system ( gerdemann and #AUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar implementing an hpsg analysis covering the so - called aux - flip phenomenon, and partial - vp topicalization in the three clause types of']","['computational treatment of lexical rules as covariation in lexical entries was implemented in prolog by the authors in cooperation with dieter martini for the controll system ( gerdemann and #AUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar implementing an hpsg analysis covering the so - called aux - flip phenomenon, and partial - vp topicalization in the three clause types of german ( hinrichs, meurers, and nakazawa 1994 ).', 'this test grammar includes eight lexical rules ; some serve syntactic purposes, like the partial - vp topicalization lexical rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'our compiler distinguished seven word classes.', 'some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations']",5
"['of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; #AUTHOR_TAG ; calcagno and pollard 1995 ) and the']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; #AUTHOR_TAG ; calcagno and pollard 1995 ) and the']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; #AUTHOR_TAG ; calcagno and pollard 1995 ) and the.', 'lexical rules (']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; #AUTHOR_TAG ; calcagno and pollard 1995 ) and the.', 'lexical rules ( dlrs ; meurers 1995 ).', '']",0
"['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van']","['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['example, in the ale system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical']","['example, in the ale system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical']","['example, in the ale system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile - time.', 'while this provides a front - end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'we mentioned in section 2. 2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'a related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'in the ale system,']","['common computational treatment of lexical rules adopted, for example, in the ale system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile - time.', 'while this provides a front - end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'we mentioned in section 2. 2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'a related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'in the ale system, for example, a depth bound can be specified for this purpose.', 'finally, as shown in section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding']",1
"['logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']","['logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']","['logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']","['logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']",0
"['nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by #AUTHOR_TAG.', '']","['nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by #AUTHOR_TAG.', '']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by #AUTHOR_TAG.', '']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by #AUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']",0
['##b ) or the tfs system ( #AUTHOR_TAG ;'],"['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( #AUTHOR_TAG ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",['##ele 1991 ; done and dorna 1993b ) or the tfs system ( #AUTHOR_TAG ;'],"['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( #AUTHOR_TAG ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['explicit description of the kind shown in figure 1.', ""this description can then be given the standard set - theoretical interpretation of #AUTHOR_TAG, 1994 ). '"", '11 10']","['explicit description of the kind shown in figure 1.', ""this description can then be given the standard set - theoretical interpretation of #AUTHOR_TAG, 1994 ). '"", '11 10']","['a fully explicit description of the kind shown in figure 1.', ""this description can then be given the standard set - theoretical interpretation of #AUTHOR_TAG, 1994 ). '"", '11 10 note that the passivization lexical rule in figure 2 is only intended to illustrate the mechanism.', 'we do not make the linguistic claim that']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', ""this description can then be given the standard set - theoretical interpretation of #AUTHOR_TAG, 1994 ). '"", '11 10 note that the passivization lexical rule in figure 2 is only intended to illustrate the mechanism.', 'we do not make the linguistic claim that passives should be analyzed using such a lexical rule.', 'for space reasons, the synsem feature is abbreviated by its first letter.', 'the traditional ( first i rest ) list notation is used, and the operator  stands for the append relation in the usual way.', '1l Manandhar (1995) proposes to unify these two steps by including an update operator in the the computational treatment we discuss in the rest of the paper follows this setup in that it automatically computes, for each lexical rule specification, the frames necessary to preserve the properties not changed by it.', '12 we will show that the detection and specification of frames and the use of program transformation to advance their integration into the lexicon encoding is one of the key ingredients of the covariation approach to hpsg lexical rules']",0
['#AUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilipp'],"['#AUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical']","['#AUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; #AUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
['#AUTHOR_TAG proposes to unify these two steps by'],['#AUTHOR_TAG proposes to unify these two steps by'],['#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],['#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],0
['constraints ( #AUTHOR_TAG ;'],['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; eisele and dorre 1990 ; griffith'],['constraints ( #AUTHOR_TAG ;'],"['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; eisele and dorre 1990 ; griffith 1996 ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']",0
"['powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']","['powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']","['powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']","['powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']",0
"['properties can be considered an instance of the well - known frame problem in at ( #AUTHOR_TAG ), and we will therefore refer to the specifications left']","['properties can be considered an instance of the well - known frame problem in at ( #AUTHOR_TAG ), and we will therefore refer to the specifications left']","[').', 'this idea of preserving properties can be considered an instance of the well - known frame problem in at ( #AUTHOR_TAG ), and we will therefore refer to the specifications left']","['the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'this is so since the lexical rule in figure 2 "" ( like all lexical rules in hpsg ) preserves all properties of the input not mentioned in the rule. ""', '( pollard and sag [ 1994, 314 ], following flickinger [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in at ( #AUTHOR_TAG ), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( meurers 1994 )']",1
"['is rather straightforward.', 'in fact, one can view the representations as notational variants of one another.', 'each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'using an accumulator passing technique ( #AUTHOR_TAG ), we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']","['is rather straightforward.', 'in fact, one can view the representations as notational variants of one another.', 'each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'using an accumulator passing technique ( #AUTHOR_TAG ), we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']","['a finite - state automaton as definite relations is rather straightforward.', 'in fact, one can view the representations as notational variants of one another.', 'each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'using an accumulator passing technique ( #AUTHOR_TAG ), we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']","['a finite - state automaton as definite relations is rather straightforward.', 'in fact, one can view the representations as notational variants of one another.', 'each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'using an accumulator passing technique ( #AUTHOR_TAG ), we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']",5
"['be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and']","['be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and']","['be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in Meurers (1995).', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']",0
['#AUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilipp'],"['are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; #AUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical']","['as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; #AUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; #AUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['#AUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical']","['#AUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical']","['#AUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; #AUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and #AUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calc']","['attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and #AUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in']","['.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and #AUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calc']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and #AUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by Hinrichs and Nakazawa (1994, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
"['##s and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['##s and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van']","['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', 'de url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb / b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
['example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs'],['example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs'],['a linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs'],"['a linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i. e., changing the prd value of substantive signs from - to +, much like the lexical rule for nps given by Pollard and Sag (1994, p. 360, fn. 20 ).', 'in such a predicative lexical rule ( which we only note as an example and not as a linguistic proposal ) the subtype of the head object undergoing the rule as well as the value of the features only appropriate for the subtypes of substantive either is lost or must be specified by a separate rule for each of the subtypes']",0
"['with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and #AUTHOR_TAG, 1996, 1997b ) for']","['with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and #AUTHOR_TAG, 1996, 1997b ) for']","['with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and #AUTHOR_TAG, 1996, 1997b ) for encoding the main building block of']","['relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding : we show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.', 'the resulting encoding allows the execution of lexical rules on - the - fly, i. e., coroutined with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and #AUTHOR_TAG, 1996, 1997b ) for encoding the main building block of hpsg grammars - - the implicative constraints - - as a logic program']",2
['are captured using lexical underspecification ( #AUTHOR_TAG ; krieger and nerbonne 1992 ;'],['are captured using lexical underspecification ( #AUTHOR_TAG ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994'],"['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; krieger and nerbonne 1992 ;']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['#AUTHOR_TAG ).', 'the lexical']","['#AUTHOR_TAG ).', 'the lexical']","['#AUTHOR_TAG ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; #AUTHOR_TAG ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['##s and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( #AUTHOR_TAG ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( #AUTHOR_TAG ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['##s and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( #AUTHOR_TAG ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van']","['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( #AUTHOR_TAG ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['on the research results reported in #AUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in']","['on the research results reported in #AUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in hpsg.', 'we developed a compiler that takes as its input a set of lexical rules, deduces the nec - essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'the definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries']","['on the research results reported in #AUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in']","['on the research results reported in #AUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in hpsg.', 'we developed a compiler that takes as its input a set of lexical rules, deduces the nec - essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'the definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries']",4
"['example, in lkb ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shi']","['example, in lkb ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to']","['example, in lkb ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shi']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dbrre and eisele 1991 ; d6rre and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by #AUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called frame']","['up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by #AUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called framepredicate, which can have multiple defining clauses.', 'so for the lexical rule 1, the frame specification is taken care']","['ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by #AUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called framepredicate, which can have multiple defining clauses.', 'so for the lexical rule 1, the frame specification is taken care']","['ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by #AUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called framepredicate, which can have multiple defining clauses.', 'so for the lexical rule 1, the frame specification is taken care of by extending the predicate in figure 6 with a call to a frame predicate, as shown in figure 8.']",4
"['a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'consider, for example, the lexical rule in figure 2, which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']",0
['#AUTHOR_TAG ;'],"['#AUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical']",['#AUTHOR_TAG ;'],"['rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; #AUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['a passive lexical rule like the one presented by #AUTHOR_TAG, 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['a passive lexical rule like the one presented by #AUTHOR_TAG, 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['a passive lexical rule like the one presented by #AUTHOR_TAG, 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'consider, for example, the lexical rule in figure 2, which encodes a passive lexical rule like the one presented by #AUTHOR_TAG, 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']",1
[';'],[';'],[';'],"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; calcagno and pollard 1995 ) and the description - level lexical rules ( dlrs ;']",0
"['constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'however, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models,']","['constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'however, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models,']","['task of mention detection is closely related to named entity recognition ( ner ).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'they aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'however, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models,']","['task of mention detection is closely related to named entity recognition ( ner ).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'they aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'however, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e. g.', 'hidden markov model (Rabiner, 1989) or conditional random fields (Sarawagi and Cohen, 2004).', 'the typical bio representation was introduced in Ramshaw and Marcus (1995) ; oc representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities']",0
"[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in #AUTHOR_TAG']","[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in #AUTHOR_TAG']","[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in #AUTHOR_TAG']","[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in #AUTHOR_TAG']",5
['on mention heads ( #AUTHOR_TAG )'],['on mention heads ( #AUTHOR_TAG )'],['on mention heads ( #AUTHOR_TAG )'],"[', phrases in the brackets are mentions and the underlined simple phrases are mention heads.', 'moreover, mention boundaries can be nested ( the boundary of a mention is inside the boundary of another mention ), but mention heads never overlap.', 'this property also simplifies the problem of mention head candidate generation.', 'in the example above, the first "" they "" refers to "" multinational companies investing in china "" and the second "" they "" refers to "" domestic manufacturers, who are also suffering "".', 'in both cases, the mention heads are sufficient to support the decisions : "" they "" refers to "" companies "", and "" they "" refers to "" manufacturers "".', 'in fact, most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG )']",0
"['##ll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and Kuhn (2014).', 'for berkeley system, we use the reported results from #AUTHOR_TAG']","['latest scorer is version v8. 01, but muc, b, ceaf e and conll average scores are not changed.', 'for evaluation on ace - 2004, we convert the system output and gold annotations into conll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and Kuhn (2014).', 'for berkeley system, we use the reported results from #AUTHOR_TAG']","['##ll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and Kuhn (2014).', 'for berkeley system, we use the reported results from #AUTHOR_TAG']","['latest scorer is version v8. 01, but muc, b, ceaf e and conll average scores are not changed.', 'for evaluation on ace - 2004, we convert the system output and gold annotations into conll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and Kuhn (2014).', 'for berkeley system, we use the reported results from #AUTHOR_TAG']",1
"['.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system ( Lee et al. , 2011 ), berkeley system ( #AUTHOR_TAG ) and hotcoref system ( bj [UNK] orkelund and Kuhn , 2014 ).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl']","['on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system ( Lee et al. , 2011 ), berkeley system ( #AUTHOR_TAG ) and hotcoref system ( bj [UNK] orkelund and Kuhn , 2014 ).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl']","['.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system ( Lee et al. , 2011 ), berkeley system ( #AUTHOR_TAG ) and hotcoref system ( bj [UNK] orkelund and Kuhn , 2014 ).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', 'when the cl 3 m coreference system uses gold mentions or heads, we call the system gold ; when it uses predicted mentions or heads, we call the system predicted.', 'the mention head candidate generation module']","['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system ( Lee et al. , 2011 ), berkeley system ( #AUTHOR_TAG ) and hotcoref system ( bj [UNK] orkelund and Kuhn , 2014 ).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', 'when the cl 3 m coreference system uses gold mentions or heads, we call the system gold ; when it uses predicted mentions or heads, we call the system predicted.', 'the mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it h - m - md.', 'we can feed the predicted mentions from h - m - md directly into the mention - pair coref - 9 no parsing information is needed at evaluation time. 10', 'we use gurobi v5. 0. 1 as our ilp solver.', '3 : performance of coreference resolution for all systems on the conll - 2012 dataset.', 'subscripts ( m, h ) indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 9,  1 = 0. 25 and  2 = 0. 2.', 'erence model that we implemented, resulting in a traditional pipelined end - to - end coreference system, namely h - m - coref.', 'we name our new proposed end - to - end coreference resolution system incorporating both the mention head candidate generation module']",1
"['contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ).', 'the ontonotes - 5. 0']","['contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', '']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', '']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']",5
"['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ collins head rules ( #AUTHOR_TAG ) to identify']","['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ collins head rules ( #AUTHOR_TAG ) to identify']","['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ collins head rules ( #AUTHOR_TAG ) to identify']","['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ collins head rules ( #AUTHOR_TAG ) to identify their heads.', 'when these extracted heads do not overlap with gold mention heads, we treat them as negative examples']",5
"['it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  bj [UNK] orkelund and Kuhn , 2014 ).', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', '']","['detection is rarely studied as a stand - alone research problem ( Recasens et al. (2013) is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  bj [UNK] orkelund and Kuhn , 2014 ).', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in table 1.', 'current state - of - the - art systems show a very significant drop in performance when running on system generated mentions.', 'these performance gaps are worrisome, since the real goal of nlp']","['detection is rarely studied as a stand - alone research problem ( Recasens et al. (2013) is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  bj [UNK] orkelund and Kuhn , 2014 ).', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', '']","['detection is rarely studied as a stand - alone research problem ( Recasens et al. (2013) is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  bj [UNK] orkelund and Kuhn , 2014 ).', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in table 1.', 'current state - of - the - art systems show a very significant drop in performance when running on system generated mentions.', 'these performance gaps are worrisome, since the real goal of nlp systems is to process raw data.', '1 : performance gaps between using gold mentions and predicted mentions for three state - of - the - art coreference resolution systems.', 'performance gaps are always larger than 10 %.', ""illinois's system (Chang et al., 2013) is evaluated on conll ( 2012conll (, 2011 ) shared task and ace - 2004 datasets."", 'it reports an average f1 score of muc, b and ceaf e metrics using conll v7. 0 scorer.', ""berkeley's system (Durrett and Klein, 2013) reports the same average score on the conll - 2011 shared task dataset."", ""results of stanford's system (Lee et al., 2011) are for b 3 metric on ace - 2004 dataset."", 'this paper focuses on improving end - to - end coreference performance.', 'we do this by : 1 ) developing a new ilp - based joint learning and inference formulation for coreference and mention head detection.', '2 ) developing a better mention head candidate generation algorithm.', 'importantly, we focus on heads rather than mention boundaries since those can be identified more robustly and used effectively in an end - to - end system.', 'as we show, this results in a dramatic improvement in the quality of the md component and, consequently, a significant reduction in the performance gap between coreference on gold mentions and coreference on raw data']",0
"['in #AUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then']","['in #AUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then']","['in #AUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then']","['on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the bilou - representation as it has advantages over traditional bio - representation, as shown, e. g. in #AUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then transformed into a simple, but constrained, 5 - class classification problem']",4
"[', which is released for the conll - 2012 shared task ( #AUTHOR_TAG ), contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', '']","[', which is released for the conll - 2012 shared task ( #AUTHOR_TAG ), contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', '']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task ( #AUTHOR_TAG ), contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', '']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task ( #AUTHOR_TAG ), contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']",5
"['.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by #AUTHOR_TAG, using constrained latent left - linking model ( cl']","['on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by #AUTHOR_TAG, using constrained latent left - linking model ( cl3m ) as our mention - pair coreference model in the joint framework10.', 'when the cl']","['.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by #AUTHOR_TAG, using constrained latent left - linking model ( cl']","['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by #AUTHOR_TAG, using constrained latent left - linking model ( cl3m ) as our mention - pair coreference model in the joint framework10.', 'when the cl 3 m coreference system uses gold mentions or heads, we call the system gold ; when it uses predicted mentions or heads, we call the system predicted.', 'the mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it h - m - md.', 'we can feed the predicted mentions from h - m - md directly into the mention - pair coref - 9 no parsing information is needed at evaluation time. 10', 'we use gurobi v5. 0. 1 as our ilp solver.', '3 : performance of coreference resolution for all systems on the conll - 2012 dataset.', 'subscripts ( m, h ) indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 9,  1 = 0. 25 and  2 = 0. 2.', 'erence model that we implemented, resulting in a traditional pipelined end - to - end coreference system, namely h - m - coref.', 'we name our new proposed end - to - end coreference resolution system incorporating both the mention head candidate generation module and']",5
['##m ) described in #AUTHOR_TAG in our experiments'],"['##ference resolution has been extensively studied, with several state - of - the - art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013; bjorkelund and Kuhn, 2014;Song et al., 2012).', 'many of the early rule - based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'the introduction of ilp methods has influenced the coreference area too Denis and Baldridge, 2007).', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in #AUTHOR_TAG in our experiments']",['##m ) described in #AUTHOR_TAG in our experiments'],"['##ference resolution has been extensively studied, with several state - of - the - art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013; bjorkelund and Kuhn, 2014;Song et al., 2012).', 'many of the early rule - based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'the introduction of ilp methods has influenced the coreference area too Denis and Baldridge, 2007).', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in #AUTHOR_TAG in our experiments']",5
"['contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ).', 'the ontonotes - 5. 0']","['contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', '']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', '']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']",5
"['details can be found in #AUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in #AUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in #AUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in #AUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']",0
"['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus']","['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus']","['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus']","['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different']",0
"['has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are']","['has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are']","['has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set']","['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', 'when the cl 3 m coreference system uses gold mentions or heads, we call the system gold ; when it uses predicted mentions or heads, we call the system predicted.', 'the mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it h - m - md.', 'we can feed the predicted mentions from h - m - md directly into the mention - pair coref - 9 no parsing information is needed at evaluation time. 10', 'we use gurobi v5. 0. 1 as our ilp solver.', '3 : performance of coreference resolution for all systems on the conll - 2012 dataset.', 'subscripts ( m, h ) indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 9,  1 = 0. 25 and  2 = 0. 2.', 'erence model that we implemented, resulting in a traditional pipelined end - to - end coreference system, namely h - m - coref.', 'we name our new proposed end - to - end coreference resolution system incorporating both the mention head candidate generation module and']",5
"['by the latent left - linking model in #AUTHOR_TAG and the ilp formulation from Chang et al. ( 2011 ).', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and']","['by the latent left - linking model in #AUTHOR_TAG and the ilp formulation from Chang et al. ( 2011 ).', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and']","['section describes our joint coreference resolution and mention head detection framework.', 'our work is inspired by the latent left - linking model in #AUTHOR_TAG and the ilp formulation from Chang et al. ( 2011 ).', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and']","['section describes our joint coreference resolution and mention head detection framework.', 'our work is inspired by the latent left - linking model in #AUTHOR_TAG and the ilp formulation from Chang et al. ( 2011 ).', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and jointly ( 1 ) determines if they are indeed mention heads and ( 2 ) learns a similarity metric between mentions.', 'this is done by simultaneously learning a binary mention head detection classifier and a mention - pair coreference classifier.', 'the mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'by learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'this joint framework aims to improve performance on both mention head detection and on coreference']",5
"[', ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0 ( #AUTHOR_TAG ).', '(Hovy et al., 2006).', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']","[', ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0 ( #AUTHOR_TAG ).', '(Hovy et al., 2006).', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']","['present experiments on the two standard coreference resolution datasets, ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0 ( #AUTHOR_TAG ).', '(Hovy et al., 2006).', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']","['present experiments on the two standard coreference resolution datasets, ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0 ( #AUTHOR_TAG ).', '(Hovy et al., 2006).', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']",5
"['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar ( #AUTHOR_TAG ).', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in  5 ) with probability p del, or a standard ccg category c']","['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar ( #AUTHOR_TAG ).', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in  5 ) with probability p del, or a standard ccg category c']","['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar ( #AUTHOR_TAG ).', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in  5 ) with probability p del, or a standard ccg category c']","['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar ( #AUTHOR_TAG ).', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in  5 ) with probability p del, or a standard ccg category c']",0
"[""' s ccm is an unlabeled bracketing model that""]","[""' s ccm is an unlabeled bracketing model that""]","[""' s ccm is an unlabeled bracketing model that""]","[""' s ccm is an unlabeled bracketing model that generates the span of part - of - speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non - constituent )."", 'they found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', 'while ccm is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels']",0
"['of our model, we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors']","['of our model, we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors']","['this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors']","['this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors on those parameters yields further gains in many scenarios']",5
"['ccgbank ( Hockenmaier and Steedman , 2007 ), which is a transformation of the penn treebank ( #AUTHOR_TAG )']","['ccgbank ( Hockenmaier and Steedman , 2007 ), which is a transformation of the penn treebank ( #AUTHOR_TAG )']","['our evaluation we compared our supertagcontext approach to ( our reimplementation of ) the best - performing model of our previous work (Garrette et al., 2015), which scm extends.', 'we evaluated on the english ccgbank ( Hockenmaier and Steedman , 2007 ), which is a transformation of the penn treebank ( #AUTHOR_TAG )']","['our evaluation we compared our supertagcontext approach to ( our reimplementation of ) the best - performing model of our previous work (Garrette et al., 2015), which scm extends.', 'we evaluated on the english ccgbank ( Hockenmaier and Steedman , 2007 ), which is a transformation of the penn treebank ( #AUTHOR_TAG ) ; the ctbccg ( Tse and Curran , 2010 ) transformation of the penn chinese treebank ( Xue et al. , 2005 ) ; and the ccg - tut corpus ( Bos et al. , 2009 ), built from the tut corpus of italian text ( Bosco et al. , 2000 )']",5
"['corresponds directly to the category transitions used for the hmm supertagger of #AUTHOR_TAG.', ""thus, the right - side context prior mean  rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]","[""right - side context of a non - terminal category - - the probability of generating a category to the right of the current constituent's category - - corresponds directly to the category transitions used for the hmm supertagger of #AUTHOR_TAG."", ""thus, the right - side context prior mean  rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]","['corresponds directly to the category transitions used for the hmm supertagger of #AUTHOR_TAG.', ""thus, the right - side context prior mean  rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]","[""right - side context of a non - terminal category - - the probability of generating a category to the right of the current constituent's category - - corresponds directly to the category transitions used for the hmm supertagger of #AUTHOR_TAG."", ""thus, the right - side context prior mean  rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]",1
"[', a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as #AUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of']","['a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as #AUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of']","[', a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as #AUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of raw data']","['corpus was divided into four distinct data sets : a set from which we extract the tag dictionaries, a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as #AUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of raw data available to the sampler, we supplemented the english data with raw, unannotated newswire sentences from the nyt giga - word 5 corpus (Parker et al., 2011) and supplemented italian with the out - of - domain wacky corpus (Baroni et al., 1999).', 'for english and italian, this allowed us to use 100k raw tokens for training ( chinese uses 62k ).', 'for chinese and italian, for training efficiency, we used only raw sentences that were 50 words or fewer ( note that we did not drop tag dictionary set or test set sentences )']",5
"['by #AUTHOR_TAG, but we do it']","['by #AUTHOR_TAG, but we do it']","['by #AUTHOR_TAG, but we do it']","['evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence : every dependency would be "" wrong "".', 'thus, it is important that we make a best effort to find a parse.', 'to accomplish this, we implemented a parsing backoff strategy.', 'the parser first tries to find a valid parse that has either s dcl or np at its root.', 'if that fails, then it searches for a parse with any root.', 'if no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent ( first with a restricted root set, then without ).', 'this is similar to the "" deletion "" strategy employed by #AUTHOR_TAG, but we do it directly in the grammar.', 'we add unary rules of the form d u for every potential supertag u in the tree.', 'then, at each node spanning exactly two tokens ( but no higher in the tree ), we allow rules t d, v and t v, d.', 'recall that in  3. 1, we stated that d is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.', 'additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents']",1
['this intuition into a bayesian prior can help train a ccg supertagger ( #AUTHOR_TAG )'],['this intuition into a bayesian prior can help train a ccg supertagger ( #AUTHOR_TAG )'],"['further notes that due to the natural associativity of ccg, adjacent categories tend to be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger ( #AUTHOR_TAG )']","['##ridge observed is that, cross - linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.', 'in previous work, we were able to incorporate this preference into a bayesian parsing model, biasing pcfg productions toward sim - pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015).', 'baldridge further notes that due to the natural associativity of ccg, adjacent categories tend to be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger ( #AUTHOR_TAG )']",2
"['_ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ). 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of']","['_ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ). 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly across']","['employ the same procedure as our previous work for setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ). 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of']","['employ the same procedure as our previous work for setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ). 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'these counts are then combined with estimates of the openness of each tag in order to assess its likelihood of appearing with new words']",5
"['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties ( #AUTHOR_TAG )']","['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties ( #AUTHOR_TAG )']","['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties ( #AUTHOR_TAG )']","['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties ( #AUTHOR_TAG )']",4
['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],"['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules for combining with punctuation to the left and right and allow for the merge rule x a x x of #AUTHOR_TAG']",5
"['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non - terminal position spanning words w i through w j1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j1 via any arrangement of productions that is rooted by y ij = t']","['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non - terminal position spanning words w i through w j1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j1 via any arrangement of productions that is rooted by y ij = t']","['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non - terminal position spanning words w i through w j1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j1 via any arrangement of productions that is rooted by y ij = t']","['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non - terminal position spanning words w i through w j1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j1 via any arrangement of productions that is rooted by y ij = t']",5
"['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by #AUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and']","['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by #AUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and']","['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by #AUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and']","['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by #AUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'to efficiently sample new model parameters, we exploit dirichlet - multinomial conjugacy.', 'by repeating these alternating steps and accumu - lating the productions, we obtain an approximation of the required posterior quantities']",5
"['important example is the constituentcontext model ( ccm ) of #AUTHOR_TAG, which was specifically designed to capture the linguistic']","['important example is the constituentcontext model ( ccm ) of #AUTHOR_TAG, which was specifically designed to capture the linguistic']","['important example is the constituentcontext model ( ccm ) of #AUTHOR_TAG, which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regular']","['important example is the constituentcontext model ( ccm ) of #AUTHOR_TAG, which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear.', 'this phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'for example, the part - of - speech ( pos ) sequence adj noun frequently occurs between the tags det and verb.', 'this det - verb context also frequently applies to the single - word sequence noun and to adj adj noun.', 'from this, we might deduce that det - verb is a likely context for a noun phrase.', 'ccm is able to learn which pos contexts are likely, and does so via a probabilistic generative model, providing a statistical, data - driven take on substitutability.', 'however, since there is nothing intrinsic about the pos pair det - verb that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data.', 'Baldridge (2008) observed that unlike opaque, atomic pos labels, the rich structures of combinatory categorial grammar ( ccg ) (Steedman, 2000;Steedman and Baldridge, 2011) categories reflect universal grammatical properties.', 'ccg is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'for example, a category might encode that "" this constituent can combine with a noun phrase to the right ( an object ) and then a noun phrase to the left ( a subject ) to produce a sentence "" instead of simply verb.', 'ccg has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical - expansion algorithms (Bisk and Hockenmaier, 2012; 2013 ) or encoding that information as priors within a bayesian framework (Garrette et al., 2015)']",0
"['##sh operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add']","['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add']","['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add']","['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules for combining with punctuation to the left and right and allow for the merge rule x  x x of Clark and Curran (2007)']",5
"['hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by #AUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by']","['hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by #AUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by']","['hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by #AUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by']","['##boost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by #AUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by discarding those that occurred fewer than 10 times']",1
"['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ).', 'this approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.', 'presumably, this is the reason why dialog systems to date have not used this kind of sentence planning']","['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ).', 'this approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.', 'presumably, this is the reason why dialog systems to date have not used this kind of sentence planning']","['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ).', 'this approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.', 'presumably, this is the reason why dialog systems to date have not used this kind of sentence planning']","['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ).', 'this approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.', 'presumably, this is the reason why dialog systems to date have not used this kind of sentence planning']",0
"[', using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 )']","['input to the surface realizer. our primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 )']","['input to the surface realizer. our primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 )']","['example, consider the required capabilities of a sentence planner for a mixed - initiative spoken dialog system for travel planning : ( d1 ) system1 : welcome.... what airport would you like to fly out of?', 'user2 : i need to go to dallas. system3 : flying to dallas. what departure airport was that? user4 : from newark on september the 1st. system5 : what time would you like to', 'travel on september the 1st to dallas from new', '##ark? utterance system1 requests information about', ""the caller's departure"", 'airport, but in user2, the caller takes the', ""initiative to provide information about her destination. in system3, the system's goal is to implicitly confirm the"", ""destination ( because of the possibility of error in the speech recognition component ), and request information ( for the second time ) of the caller's departure airport"", "". in user4, the caller provides this information but also provides the month and day of travel. given the system's dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has"", 'provided so far, i. e. the departure and destination cities and the month and day information,', ""as well as to request information about the time of travel. the system's representation of its communicative goals for utterance system5 is in figure 1. the job of the sentence planner is to decide among the large number of potential realizations of these communicative goals. some example alternative realizations are in figure"", '2. 2 implicit - confirm ( orig - city : newark ) implicit - confirm (', 'dest - city : dallas ) implicit - confirm ( month : 9 ) implicit - confirm ( day -', 'number : 1 ) request ( depart - time ) in this paper, we present spot, for "" sentence planner, trainable "". we also present a new methodology for automatically training spot on the basis of feedback provided by human judges. in order to train spot, we reconceptualize its task as consisting of two distinct phases. in the first phase, the', 'sentence - plan - generator ( spg ) generates a potentially large sample of possible sentence plans for a', 'given text - plan input. in the second phase, the sentence - plan - ranker ( spr', ') ranks the sample sentence plans, and then selects the top - ranked output to input to the surface realizer. our primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 )']",1
"['a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ), although the']","['a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ), although the']","['a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ), although the various merge operations are, to our knowledge, novel in this form']","['already mentioned, we divide the sentence planning task into two phases.', 'in the first phase, the sentenceplan - generator ( spg ) generates 12 - 20 possible sentence plans for a given input text plan.', ""each speech act is assigned a canonical lexico - structural representation ( called a dsynts - deep syntactic structure ( mel'cuk, 1988 ) )."", 'the sentence plan is a tree recording how these elementary dsynts are combined into larger dsyntss ; the dsynts for the entire input text plan is associated with the root node of the tree.', 'in the second phase, the sentence plan ranker ( spr ) ranks sentence plans generated by the spg, and then selects the top - ranked output as input to the surface realizer, realpro (Lavoie and Rambow, 1997) the research presented here is primarily concerned with creating a trainable spr.', 'a strength of our approach is the ability to use a very simple spg, as we explain below.', 'the basis of our spg is a set of clausecombining operations that incrementally transform a list of elementary predicate - argument representations ( the dsyntss corresponding to elementary speech acts, in our case ) into a single lexico - structural representation, by combining these representations using the following combining operations.', 'examples can be found in figure adjective.', 'this transforms a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ), although the various merge operations are, to our knowledge, novel in this form']",0
"['by (Lavoie and Rambow, 1998).', 'the representations used by Danlos ( 2000 ), Gardent and Webber ( 1998 ), or #AUTHOR_TAG are similar, but do']","['by (Lavoie and Rambow, 1998).', 'the representations used by Danlos ( 2000 ), Gardent and Webber ( 1998 ), or #AUTHOR_TAG are similar, but do']","['.', 'figure 2 shows some of the realizations of alternative sentence plans generated by our spg for utterance sys - 3 the sp - tree is inspired by (Lavoie and Rambow, 1998).', 'the representations used by Danlos ( 2000 ), Gardent and Webber ( 1998 ), or #AUTHOR_TAG are similar, but do not ( always )']","['result of applying the operations is a sentence plan tree ( or sp - tree for short ), which is a binary tree with leaves labeled by all the elementary speech acts from the input text plan, and with its interior nodes labeled with clause - combining operations 3.', 'each node is also associated with a dsynts : the leaves ( which correspond to elementary speech acts from the input text plan ) are linked to a canonical dsynts for that speech act ( by lookup in a hand - crafted dictionary ).', 'the interior nodes are associated with dsyntss by executing their clausecombing operation on their two daughter nodes.', '( a pe - riod node results in a dsynts headed by a period and whose daughters are the two daughter dsyntss. )', 'if a clause combination fails, the sp - tree is discarded ( for example, if we try to create a relative clause of a structure which already contains a period ).', 'as a result, the dsynts for the entire turn is associated with the root node.', 'this dsynts can be sent to realpro, which returns a sentence ( or several sentences, if the dsynts contains period nodes ).', 'the spg is designed in such a way that if a dsynts is associated with the root node, it is a valid structure which can be realized.', 'figure 2 shows some of the realizations of alternative sentence plans generated by our spg for utterance sys - 3 the sp - tree is inspired by (Lavoie and Rambow, 1998).', 'the representations used by Danlos ( 2000 ), Gardent and Webber ( 1998 ), or #AUTHOR_TAG are similar, but do not ( always ) explicitly represent the clause combining operations as labeled nodes.', '4 shows the result of applying the soft - merge operation when args 1 and 2 are implicit confirmations of the origin and destination cities. figure 8 illustrates the relationship between the sp - tree and the dsynts for alternative 8.', 'the labels and arrows show the dsyntss associated with each node in the sp - tree ( in figure 7 ), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'the complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'in our approach, we do not need to encode such constraints.', 'rather, we generate a random sample of possible sentence plans for each text plan, up to a pre - specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution']",0
"['##the algorithm was implemented by the the authors, following the description in #AUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in #AUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in #AUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in #AUTHOR_TAG']",5
"['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system ( #AUTHOR_TAG )']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system ( #AUTHOR_TAG )']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system ( #AUTHOR_TAG )']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system ( #AUTHOR_TAG )']",1
"['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by #AUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by #AUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by #AUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by #AUTHOR_TAG']",5
['a the ica algorithm ( #AUTHOR_TAG )'],['a the ica algorithm ( #AUTHOR_TAG )'],['a the ica algorithm ( #AUTHOR_TAG )'],"[""## the regular tbl, as described in section 2 ; a an improved version of tbl, which makes extensive use of indexes to speed up the rules'update ; a the fasttbl algorithm ; a the ica algorithm ( #AUTHOR_TAG )""]",1
['system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],['system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],['ica system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],['ica system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],0
"['the tree - cut technique described above, our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from wordnet.', 'in this section, we g i v']","['the tree - cut technique described above, our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from wordnet.', 'in this section, we g i v']","['the tree - cut technique described above, our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']","['the tree - cut technique described above, our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']",2
"['is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut (Li and Abe, 1998).', 'in our previous work ( #AUTHOR_TAG ), we applied this method to a small subset of wordnet nouns and showed potential']","['is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut (Li and Abe, 1998).', 'in our previous work ( #AUTHOR_TAG ), we applied this method to a small subset of wordnet nouns and showed potential']","['this paper, we describes a lexicon organized around systematic polysemy.', 'the lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut (Li and Abe, 1998).', 'in our previous work ( #AUTHOR_TAG ), we applied this method to a small subset of wordnet nouns and showed potential applicability.', 'in the current work, we applied the method to all nouns and verbs in wordnet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'we report results of comparing our lexicon with the wordnet cousins as well as the inter - annotator disagreement']","['this paper, we describes a lexicon organized around systematic polysemy.', 'the lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut (Li and Abe, 1998).', 'in our previous work ( #AUTHOR_TAG ), we applied this method to a small subset of wordnet nouns and showed potential applicability.', 'in the current work, we applied the method to all nouns and verbs in wordnet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'we report results of comparing our lexicon with the wordnet cousins as well as the inter - annotator disagreement observed between two semantically annotated corpora : wordnet semcor (Landes et al., 1998) and dso (Ng and Lee, 1996).', 'the results are quite promising : our extraction method discovered 89 % of the wordnet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data']",2
"['order to obtain semantic representations of each word, we apply our previous strategy ( #AUTHOR_TAG ).', 'rather than using a termdocument matrix, we had followed an approach']","['order to obtain semantic representations of each word, we apply our previous strategy ( #AUTHOR_TAG ).', 'rather than using a termdocument matrix, we had followed an approach']","['order to obtain semantic representations of each word, we apply our previous strategy ( #AUTHOR_TAG ).', 'rather than using a termdocument matrix, we had followed an approach akin to that of schutze ( 1993 ), who performed svd on a nx2n term - term matrix.', 'the n here represents the n - 1 most - frequent words as well as a glob position to account for all other words not in the top n - 1.', ""the matrix is structured such that for a given word w's row, the first n columns denote words that - ncs ( [UNK], 1""]","['order to obtain semantic representations of each word, we apply our previous strategy ( #AUTHOR_TAG ).', 'rather than using a termdocument matrix, we had followed an approach akin to that of schutze ( 1993 ), who performed svd on a nx2n term - term matrix.', 'the n here represents the n - 1 most - frequent words as well as a glob position to account for all other words not in the top n - 1.', ""the matrix is structured such that for a given word w's row, the first n columns denote words that - ncs ( [UNK], 1""]",2
"['- Riley and Litman , 2011 a ; #AUTHOR_TAG ), we have also experiment']","['our feature set was drawn primarily from our prior uncertainty detection experiments ( forbes - Riley and Litman , 2011 a ; #AUTHOR_TAG ), we have also experimented with other features,']","['- Riley and Litman , 2011 a ; #AUTHOR_TAG ), we have also experiment']","['train our dise model, we first extracted the set of speech and dialogue features shown in figure 2 from the user turns in our corpus.', 'as shown, the acoustic - prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'the lexical and dialogue features consist of the current dialogue name ( i. e., one of the six physics problems ) and turn number, the current itspoke question\'s name ( e. g., t 3 in figure 1 has a unique identifier ) and depth in the discourse structure ( e. g., an itspoke remediation question after an incorrect user answer would be at one greater depth than the prior question ), a word occurrence vector for the automatically recognized text of the user turn, an automatic ( in ) correctness label, and lastly, the number of user turns since the last correct turn ( "" incorrect runs "" ).', 'we also included two user - based features, gender and pretest score.', 'note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( forbes - Riley and Litman , 2011 a ; #AUTHOR_TAG ), we have also experimented with other features, including state - of - theart acoustic - prosodic features used in the last interspeech challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009 b ) and made freely available in the opensmile toolkit ( Florian et al. , 2010 ).', 'to date, however, these features have only decreased the crossvalidation performance of our models.', '8 while some of our features are tutoring - specific, these have similar counterparts in other applications ( i. e., answer ( in ) correctness corresponds to a more general notion of "" response appropriateness "" in other domains, while pretest score corresponds to the general notion of domain expertise ).', 'moreover, all of our features are fully automatic and available in real - time, so that the model can be directly implemented and deployed.', 'to that end, we now describe the results of our intrinsic and extrinsic evaluations of our dise model, aimed at determining whether it is ready to be evaluated with real users']",2
['have been bilingual stochastic grammars ( #AUTHOR_TAG )'],['have been bilingual stochastic grammars ( #AUTHOR_TAG )'],['have been bilingual stochastic grammars ( #AUTHOR_TAG )'],"['state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation ( Amengual et al. , 2000 ), as have been bilingual stochastic grammars ( #AUTHOR_TAG )']",0
['also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG )'],['also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG )'],['also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG )'],['also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG )'],5
['can be shown ( #AUTHOR_TAG ) that the use of this model with maximum'],['can be shown ( #AUTHOR_TAG ) that the use of this model with maximum'],['can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values. 3 there is no requirement that the components of f represent disjoint or statistically independent events'],['can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values. 3 there is no requirement that the components of f represent disjoint or statistically independent events'],4
['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],0
['statistical technique which has recently become popular for nlp is maximum entropy / minimum divergence ( memd ) modeling ( #AUTHOR_TAG )'],['statistical technique which has recently become popular for nlp is maximum entropy / minimum divergence ( memd ) modeling ( #AUTHOR_TAG )'],['statistical technique which has recently become popular for nlp is maximum entropy / minimum divergence ( memd ) modeling ( #AUTHOR_TAG )'],['statistical technique which has recently become popular for nlp is maximum entropy / minimum divergence ( memd ) modeling ( #AUTHOR_TAG )'],5
"['( #AUTHOR_TAG ) show, lexical']","['( #AUTHOR_TAG ) show, lexical']","['( #AUTHOR_TAG ) show, lexical information improves on np and vp chunking as well']","['( #AUTHOR_TAG ) show, lexical information improves on np and vp chunking as well']",3
"['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by #AUTHOR_TAG, Collins ( 1997 ), and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by #AUTHOR_TAG, Collins ( 1997 ), and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by #AUTHOR_TAG, Collins ( 1997 ), and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by #AUTHOR_TAG, Collins ( 1997 ), and Ratnaparkhi ( 1997 ), and became a common testbed']",1
"['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), #AUTHOR_TAG, Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), #AUTHOR_TAG, Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), #AUTHOR_TAG, Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), #AUTHOR_TAG, Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']",0
"['is not aimed at handling dependencies, which require heavy use of lexical information ( #AUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information ( #AUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information ( #AUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information ( #AUTHOR_TAG, for pp attachment )']",1
['approach for partial parsing was presented by #AUTHOR_TAG'],['approach for partial parsing was presented by #AUTHOR_TAG'],['approach for partial parsing was presented by #AUTHOR_TAG'],['approach for partial parsing was presented by #AUTHOR_TAG'],0
"['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), Collins ( 1997 ), #AUTHOR_TAG, and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), Collins ( 1997 ), #AUTHOR_TAG, and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), Collins ( 1997 ), #AUTHOR_TAG, and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), Collins ( 1997 ), #AUTHOR_TAG, and Sekine ( 1998 ) )']",0
"['., #AUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']","['results are lower than those of full parsers, e. g., #AUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']","['., #AUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']","['results are lower than those of full parsers, e. g., #AUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']",1
"['a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']",3
"['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), #AUTHOR_TAG, Collins ( 1997 ), Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), #AUTHOR_TAG, Collins ( 1997 ), Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), #AUTHOR_TAG, Collins ( 1997 ), Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), #AUTHOR_TAG, Collins ( 1997 ), Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']",0
"['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), Collins ( 1997 ), and #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), Collins ( 1997 ), and #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), Collins ( 1997 ), and #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), Collins ( 1997 ), and #AUTHOR_TAG, and became a common testbed']",1
"['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), #AUTHOR_TAG, and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), #AUTHOR_TAG, and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), #AUTHOR_TAG, and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), #AUTHOR_TAG, and Ratnaparkhi ( 1997 ), and became a common testbed']",1
"['approach to partial parsing was presented by #AUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by #AUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by #AUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by #AUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']",0
"['a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ), the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ), the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ), the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ), the method extends an existing flat shallow - parsing method to handle composite structures']",3
"[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences, #AUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a  b is']","[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences, #AUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a  b is']","[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences, #AUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a  b is']","[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences, #AUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a  b is']",0
"['be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ; #AUTHOR_TAG )']","['be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ; #AUTHOR_TAG )']","['the combined mbl method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'future work will pursue at least two directions for improving the results.', 'first, while semantic information is not available for all adjectives, it is clearly available for some.', 'furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself.', 'since the constraints on adjective ordering in english depend largely on semantic classes, the addition of semantic information to the model']","['the combined mbl method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'future work will pursue at least two directions for improving the results.', 'first, while semantic information is not available for all adjectives, it is clearly available for some.', 'furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself.', 'since the constraints on adjective ordering in english depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']",3
"['simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']","['simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']","['simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']","['simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']",0
"['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'in particular, boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'in particular, boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'in particular, boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'in particular, boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']",3
"['on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a  b.', 'if the re - verse is true, and b, a is found more often than a, b, then b  a.', 'if neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', 'that is, if a  c and c  b, we can conclude that a  b.', 'to take an example from the bnc, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'however, the pairs large, new and new, green occur fairly frequently.', 'therefore, in the face of this evidence we can assign this pair the order large, green, which not coincidently is the correct english word order']","['on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a  b.', 'if the re - verse is true, and b, a is found more often than a, b, then b  a.', 'if neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', 'that is, if a  c and c  b, we can conclude that a  b.', 'to take an example from the bnc, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'however, the pairs large, new and new, green occur fairly frequently.', 'therefore, in the face of this evidence we can assign this pair the order large, green, which not coincidently is the correct english word order']","['way to think of the direct evidence method is to see that it defines a relation  on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a  b.', 'if the re - verse is true, and b, a is found more often than a, b, then b  a.', 'if neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', 'that is, if a  c and c  b, we can conclude that a  b.', 'to take an example from the bnc, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'however, the pairs large, new and new, green occur fairly frequently.', 'therefore, in the face of this evidence we can assign this pair the order large, green, which not coincidently is the correct english word order']","['way to think of the direct evidence method is to see that it defines a relation  on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a  b.', 'if the re - verse is true, and b, a is found more often than a, b, then b  a.', 'if neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', 'that is, if a  c and c  b, we can conclude that a  b.', 'to take an example from the bnc, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'however, the pairs large, new and new, green occur fairly frequently.', 'therefore, in the face of this evidence we can assign this pair the order large, green, which not coincidently is the correct english word order']",0
"['among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator ( #AUTHOR_TAG a ; Langkilde and Knight , 1998 b ), takes advantage of standard statistical techniques by""]","['among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator ( #AUTHOR_TAG a ; Langkilde and Knight , 1998 b ), takes advantage of standard statistical techniques by""]","['among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator ( #AUTHOR_TAG a ; Langkilde and Knight , 1998 b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model."", 'langkilde and knight report that this strategy yields good results for problems like generating verb / object collocations and for selecting the correct morphological form of a word.', 'it also should be straightforwardly']","['problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator ( #AUTHOR_TAG a ; Langkilde and Knight , 1998 b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model."", 'langkilde and knight report that this strategy yields good results for problems like generating verb / object collocations and for selecting the correct morphological form of a word.', 'it also should be straightforwardly applicable to the more specific problem we are addressing here.', 'to determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'this has the advantage of reducing the problem of adjective ordering to the problem of estimating n - gram probabilities, something which is relatively well understood']",5
"['##p.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
"['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand ( #AUTHOR_TAG ) can pay off here, since only part of [UNK] may be needed subsequently.']","['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand ( #AUTHOR_TAG ) can pay off here, since only part of [UNK] may be needed subsequently.']","['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand ( #AUTHOR_TAG ) can pay off here, since only part of [UNK] may be needed subsequently.']","['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand ( #AUTHOR_TAG ) can pay off here, since only part of [UNK] may be needed subsequently.']",0
"['denominator of equation ( 1 ) is the total probability of all accepting paths in x i  f  y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset  must be (  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary']","['denominator of equation ( 1 ) is the total probability of all accepting paths in x i  f  y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset  must be (  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary']","['denominator of equation ( 1 ) is the total probability of all accepting paths in x i  f  y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset  must be (  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations  and  on k. thus  is used to combine arc weights into a path weight and  is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * =  i = 0 k i.', 'the usual finite - state algorithms work if ( k, , , * ) has the structure of a closed semiring. 15', '']","['denominator of equation ( 1 ) is the total probability of all accepting paths in x i  f  y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset  must be (  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations  and  on k. thus  is used to combine arc weights into a path weight and  is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * =  i = 0 k i.', 'the usual finite - state algorithms work if ( k, , , * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring ( r 0, +, , * ). 16', 'our novel weights fall in a novel 14 formal derivation of ( 1 )']",5
"['11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ).', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it push']","['finding t i in a linear number of  and  operations.', 'for hmms ( footnote 11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ).', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs ( more generally, values in v ) forward to the probabilities.', 'this is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together']","['11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ).', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs ( more generally, values in v ) forward to the probabilities.', 'this is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together']","['in many cases of interest, t i is an acyclic graph. 20', ""hen tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of  and  operations."", 'for hmms ( footnote 11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ).', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs ( more generally, values in v ) forward to the probabilities.', 'this is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together']",0
"['availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical']","['availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical nlp.', 'such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and']","['availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical nlp.', 'such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation ( knight and al - Onaizan, 1998).', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical nlp.', 'such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation ( knight and al - Onaizan, 1998).', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
['( #AUTHOR_TAG ) trains only stochastic edit distance'],['( #AUTHOR_TAG ) trains only stochastic edit distance'],['( #AUTHOR_TAG ) trains only stochastic edit distance'],"[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm ( Baum , 1972 ) trains only hidden markov models, while ( #AUTHOR_TAG ) trains only stochastic edit distance']",0
"[') time, and faster approximations ( #AUTHOR_TAG )']","[') time, and faster approximations ( #AUTHOR_TAG )']","[') time, and faster approximations ( #AUTHOR_TAG )']","['therefore reintroduce a backward pass that lets us avoid  and  when computing t i ( so they are needed only to construct t i ).', 'this speedup also works for cyclic graphs and for any v.', 'write w jk as ( p jk, v jk ), and let w 1 jk = ( p 1 jk, v 1 jk ) denote the weight of the edge from j to k. 19 then it can be shown that w 0n = ( p 0n, j, k p 0j v 1 jk p kn ).', 'the forward and backward probabilities, p0j and pkn, can be computed using single - source algebraic path for the simpler semiring ( r, +, x, a ) - - or equivalently, by solving a sparse linear system of equations over r, a much - studied problem at o ( n ) space, o ( nm ) time, and faster approximations ( #AUTHOR_TAG )']",0
"['and apply the well - known kleene - sch [UNK] utzenberger construction ( #AUTHOR_TAG ), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a']","['and apply the well - known kleene - sch [UNK] utzenberger construction ( #AUTHOR_TAG ), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a']","[', express f as an fst and apply the well - known kleene - sch [UNK] utzenberger construction ( #AUTHOR_TAG ), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of']","['##to prove ( 1 ) a ( 3 ), express f as an fst and apply the well - known kleene - sch [UNK] utzenberger construction ( #AUTHOR_TAG ), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', '']",5
"['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'efficient hardware implementation is also possible via chip - level parallelism ( #AUTHOR_TAG )']","['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'efficient hardware implementation is also possible via chip - level parallelism ( #AUTHOR_TAG )']","['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'efficient hardware implementation is also possible via chip - level parallelism ( #AUTHOR_TAG )']","['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'efficient hardware implementation is also possible via chip - level parallelism ( #AUTHOR_TAG )']",3
"['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a   ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given  is j, a dj, a  ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization ( #AUTHOR_TAG b,']","['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a   ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given  is j, a dj, a  ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization ( #AUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a   ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given  is j, a dj, a  ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization ( #AUTHOR_TAG b,']","['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a   ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given  is j, a dj, a  ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization ( #AUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']",1
"['em algorithm ( #AUTHOR_TAG ) can maximize these functions.', 'roughly, the e step guesses hidden']","['em algorithm ( #AUTHOR_TAG ) can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f , which fst paths stand a chance of having been the path used?', '( guessing the path also guesses the exact input and output. )', 'the m step updates  to make those paths more likely.', 'em alternates these steps and converges to a local optimum.', ""the m step's form depends on the parameterization and the e step serves the m step's needs""]","['em algorithm ( #AUTHOR_TAG ) can maximize these functions.', 'roughly, the e step guesses hidden']","['em algorithm ( #AUTHOR_TAG ) can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f , which fst paths stand a chance of having been the path used?', '( guessing the path also guesses the exact input and output. )', 'the m step updates  to make those paths more likely.', 'em alternates these steps and converges to a local optimum.', ""the m step's form depends on the parameterization and the e step serves the m step's needs""]",5
"['by compilation of decision trees ( #AUTHOR_TAG ), ( 4']","['by compilation of decision trees ( #AUTHOR_TAG ), ( 4']","['by compilation of decision trees ( #AUTHOR_TAG ), ( 4']","['defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways : ( 1 ) via fsts as in fig. 1c, ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ), ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem ( #AUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ (']","['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem ( #AUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ ( yi _ _ ), since then the real work is done by an _ - closure step (Mohri, 2002) that implements the all - pairs version of algebraic path, whereas']","['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem ( #AUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ (']","['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem ( #AUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ ( yi _ _ ), since then the real work is done by an _ - closure step (Mohri, 2002) that implements the all - pairs version of algebraic path, whereas all we need is the single - source version.', 'if n and m are the number of states and edges, 19 then both problems are o ( n3 ) in the worst case, but the single - source version can be solved in essentially o ( m ) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981 b ).', 'for a general graph Ti, Tarjan (1981 b ) shows how to partition into hard subgraphs that localize the cyclicity or irreducibility, then run the o ( n3 ) algorithm on each subgraph ( thereby reducing n to as little as 1 ), and recombine the results.', 'the overhead of partitioning and recombining is essentially only o ( m )']",0
['; #AUTHOR_TAG )'],['; #AUTHOR_TAG )'],"['ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( mc Callum et al. , 2000 ; Eisner , 2001 b ; #AUTHOR_TAG )']","[': e , and a : ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( mc Callum et al. , 2000 ; Eisner , 2001 b ; #AUTHOR_TAG )']",0
"['f  ( x i, y i ) where p (  ) is a prior probability.', 'in a log - linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG )']","['f  ( x i, y i ) where p (  ) is a prior probability.', 'in a log - linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG )']","['f  ( x i, y i ) where p (  ) is a prior probability.', 'in a log - linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG )']","['- posterior estimation tries to maximize p (  )  i f  ( x i, y i ) where p (  ) is a prior probability.', 'in a log - linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG )']",5
"['represents a completely hidden state sequence ( cfxxx #AUTHOR_TAG,']","['represents a completely hidden state sequence ( cfxxx #AUTHOR_TAG,']","['represents a completely hidden state sequence ( cfxxx #AUTHOR_TAG,']","['training data we are given a set of observed ( input, output ) pairs, ( xi, yi ).', 'these are assumed to be independent random samples from a joint dis - tribution of the form f _ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx #AUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']",0
"['##p.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( #AUTHOR_TAG ) and""]","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( #AUTHOR_TAG ) and""]","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( knight and al - Onaizan , 1998 )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( knight and al - Onaizan , 1998 )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
"['of o ( yixe ), since then the real work is done by an c - closure step ( #AUTHOR_TAG ) that implements the all - pairs version of algebraic path, whereas']","['of o ( yixe ), since then the real work is done by an c - closure step ( #AUTHOR_TAG ) that implements the all - pairs version of algebraic path, whereas']","['of o ( yixe ), since then the real work is done by an c - closure step ( #AUTHOR_TAG ) that implements the all - pairs version of algebraic path, whereas']","['computing t i is an instance of the well - known algebraic path problem (Lehmann, 1977;Tarjan, 1981 a ).', 'let t i = x i  f  y i.', 'then t i is the total semiring weight w 0n of paths in t i from initial state 0 to final state n ( assumed wlog to be unique and unweighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( cxxi ) of o ( yixe ), since then the real work is done by an c - closure step ( #AUTHOR_TAG ) that implements the all - pairs version of algebraic path, whereas all we need is the single - source version.', 'if n and m are the number of states and edges, 19 then both problems are o ( n 3 ) in the worst case, but the single - source version can be solved in essentially o ( m ) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981 b ).', 'for a general graph t i, Tarjan (1981 b ) shows how to partition into "" hard "" subgraphs that localize the cyclicity or irreducibility, then run the o ( n 3 ) algorithm on each subgraph ( thereby reducing n to as little as 1 ), and recombine the results.', 'the overhead of partitioning and recombining is essentially only o ( m )']",0
"['##s that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias "" ( #AUTHOR_TAG ).', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']","['an easy approach is to normalize the options at each state to make the fst markovian.', 'unfortunately, the result may differ for equivalent fsts that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias "" ( #AUTHOR_TAG ).', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']","['an easy approach is to normalize the options at each state to make the fst markovian.', 'unfortunately, the result may differ for equivalent fsts that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias "" ( #AUTHOR_TAG ).', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']","['an easy approach is to normalize the options at each state to make the fst markovian.', 'unfortunately, the result may differ for equivalent fsts that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias "" ( #AUTHOR_TAG ).', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']",0
"['brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']",2
"['##s ( Nederhof , 2000 ; #AUTHOR_TAG ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pc']","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic (Berstel and Reutenauer, 1988).', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs ( Nederhof , 2000 ; #AUTHOR_TAG ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', 'these are']","['##as that approximate pcfgs ( Nederhof , 2000 ; #AUTHOR_TAG ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic (Berstel and Reutenauer, 1988).', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs ( Nederhof , 2000 ; #AUTHOR_TAG ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]",0
"['by compilation of weighted rewrite rules ( #AUTHOR_TAG ), ( 3']","['by compilation of weighted rewrite rules ( #AUTHOR_TAG ), ( 3']","['by compilation of weighted rewrite rules ( #AUTHOR_TAG ), ( 3']","['defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways : ( 1 ) via fsts as in fig. 1c, ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ), ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['.', 'such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001 b']","['weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001 b']","['.', 'such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001 b']","[': e , and a : ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001 b ; Lafferty et al. , 2001 )']",0
"['##s ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pc']","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic (Berstel and Reutenauer, 1988).', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', 'these are']","['##as that approximate pcfgs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic (Berstel and Reutenauer, 1988).', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]",0
"['kinds of models and training regimens.', 'for example, the forward - backward algorithm ( #AUTHOR_TAG ) trains only hidden markov models,']","['kinds of models and training regimens.', 'for example, the forward - backward algorithm ( #AUTHOR_TAG ) trains only hidden markov models,']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm ( #AUTHOR_TAG ) trains only hidden markov models,']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm ( #AUTHOR_TAG ) trains only hidden markov models, while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance']",0
"['.', 'such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAG b']","['weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAG b']","['.', 'such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAG b']","[': e , and a : ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAG b ; Lafferty et al. , 2001 )']",0
"['0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della #AUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['data and adjusts 0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della #AUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della #AUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['if arc probabilities ( or even , , [UNK],  ) have loglinear parameterization, then the e step must compute c = i ec f ( x i, y i ), where ec ( x, y ) denotes the expected vector of total feature counts along a random path in f  whose ( input, output ) matches ( x, y ).', 'the m step then treats c as fixed, observed data and adjusts 0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della #AUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']",5
"['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ).', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a (  ) ; reading another b (  ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x (  ). 4 to prove ( 1']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ).', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a (  ) ; reading another b (  ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x (  ). 4 to prove ( 1 )  ( 3 ), express f as an fst and apply the well - known kleene - schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ).', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a (  ) ; reading another b (  ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x (  ). 4 to prove ( 1']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ).', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a (  ) ; reading another b (  ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x (  ). 4 to prove ( 1 )  ( 3 ), express f as an fst and apply the well - known kleene - schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', '']",0
"[', refseq #AUTHOR_TAG, and entrez gene [ 13']","[', refseq #AUTHOR_TAG, and entrez gene [ 13']","['( genpept [ 11 ], refseq #AUTHOR_TAG, and entrez gene [ 13']","['system utilizes several large size biological databases including three ncbi databases ( genpept [ 11 ], refseq #AUTHOR_TAG, and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) [ 14 ], and', 'additionally, several model organism databases or nomenclature databases were used.', 'correspondences among records from these databases are identified using the rich cross - reference information provided by the iproclass database of pir [ 14 ].', 'the following provides a brief description of each of the database']",5
"[') #AUTHOR_TAG, rat - - rat genome database ( rg']","[') #AUTHOR_TAG, rat - - rat genome database ( rgd ) [ 21']","[') #AUTHOR_TAG, rat - - rat genome database ( rg']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) #AUTHOR_TAG, rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[') #AUTHOR_TAG, fly flybase [ 19']","['in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) #AUTHOR_TAG, fly flybase [ 19']","[') #AUTHOR_TAG, fly flybase [ 19']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) #AUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[', rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase #AUTHOR_TAG, human nomenclature database ( hugo ) [ 23']","[', rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase #AUTHOR_TAG, human nomenclature database ( hugo ) [ 23']","[') [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase #AUTHOR_TAG, human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase #AUTHOR_TAG, human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"['##s ) has been developed and maintained by national library of medicine ( nlm ) #AUTHOR_TAG.', 'it contains three knowledge']","['the unified medical language system ( umls ) has been developed and maintained by national library of medicine ( nlm ) #AUTHOR_TAG.', 'it contains three knowledge sources : the metathesaurus ( meta ), the specialist lexicon, and the semantic network.', 'the meta provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'the specialist']","['##s ) has been developed and maintained by national library of medicine ( nlm ) #AUTHOR_TAG.', 'it contains three knowledge']","['umls - - the unified medical language system ( umls ) has been developed and maintained by national library of medicine ( nlm ) #AUTHOR_TAG.', 'it contains three knowledge sources : the metathesaurus ( meta ), the specialist lexicon, and the semantic network.', 'the meta provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'the specialist lexicon contains syntactic information for many terms, component words, and english words, including verbs, which do not appear in the meta.', 'the semantic network contains information about the types or categories ( e. g., "" disease or syndrome "", "" virus "" ) to which all meta concepts have been assigned']",0
"['to make the task of managing information recorded in free text more feasible #AUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names,']","['to make the task of managing information recorded in free text more feasible #AUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely : synonymy ( i. e., different terms refer to the same entity ), ambiguity ( i. e., one term is associated with different entities ), and coverage ( i. e., entity terms or entities are not present in']","['the use of computers in storing the explosive amount of biological information, natural language processing ( nlp ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely : synonymy ( i. e., different terms refer to the same entity ), ambiguity ( i. e., one term is associated with different entities ), and coverage ( i. e., entity terms or entities are not present in']","['the use of computers in storing the explosive amount of biological information, natural language processing ( nlp ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely : synonymy ( i. e., different terms refer to the same entity ), ambiguity ( i. e., one term is associated with different entities ), and coverage ( i. e., entity terms or entities are not present in databases or knowledge bases )']",0
"[', online mendelian inheritance in man ( omim ) #AUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26']","[', online mendelian inheritance in man ( omim ) #AUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26']","['hugo ) [ 23 ], online mendelian inheritance in man ( omim ) #AUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) #AUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
['a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm #AUTHOR_TAG'],['a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm #AUTHOR_TAG'],['a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm #AUTHOR_TAG'],"['resources - there are three databases in pir : the protein sequence database ( psd ), iproclass, and pir - nref.', 'psd database includes functionally annotated protein sequences.', 'the iproclass database is a central point for exploration of protein information, which provides summary descriptions of protein family, function and structure for all protein sequences from pir, swiss - prot, and trembl ( now uniprot ).', 'additionally, it links to over 70 biological databases in the world.', 'the pir - nref database is a comprehensive database for sequence searching and protein identification.', 'it contains non - redundant protein sequences from psd, swiss - prot, trembl, refseq, genpept, and pdb.', 'three uniref tables uniref100, uniref90 and uniref50 ) are available for download : uniref100 combines identical sequences and sub - fragments into a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity, respectively, to the representative sequence.', 'ncbi resources - three data sources from ncbi were used in this study : genpept, refseq, and entrez gene.', 'genpept entries are those translated from the genbanknucleotide sequence database.', 'refseq is a comprehensive, integrated, non - redundant set of sequences, including genomic dna, transcript ( rna ), and protein products, for major research organisms.', ""entrez gene provides a unified query environment for genes defined by sequence and / or in ncbi's map viewer."", 'it records gene names, symbols, and many other attributes associated with genes and the products they encode']",5
"[') #AUTHOR_TAG, worm - - wormbase [ 22']","[') [ 20 ], rat - - rat genome database ( rgd ) #AUTHOR_TAG, worm - - wormbase [ 22']","[') #AUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) #AUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"['), psd database from protein information resources ( pir ) #AUTHOR_TAG, and', 'additionally, several model organism']","['), psd database from protein information resources ( pir ) #AUTHOR_TAG, and', 'additionally, several model organism']","['), psd database from protein information resources ( pir ) #AUTHOR_TAG, and', 'additionally, several model organism']","['system utilizes several large size biological databases including three ncbi databases ( genpept [ 11 ], refseq [ 12 ], and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) #AUTHOR_TAG, and', 'additionally, several model organism databases or nomenclature databases were used.', 'correspondences among records from these databases are identified using the rich cross - reference information provided by the iproclass database of pir [ 14 ].', 'the following provides a brief description of each of the database']",5
"[', fly flybase #AUTHOR_TAG, yeast saccharomyces genome database (']","[', fly flybase #AUTHOR_TAG, yeast saccharomyces genome database (']","[') [ 18 ], fly flybase #AUTHOR_TAG, yeast saccharomyces genome database (']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase #AUTHOR_TAG, yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
[') #AUTHOR_TAG'],"[', and enzyme nomenclature database ( ecnum ) #AUTHOR_TAG']",[') #AUTHOR_TAG'],"['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) #AUTHOR_TAG']",5
"['we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work ( #AUTHOR_TAG )']","['we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work ( #AUTHOR_TAG )']","['our experiments, we use naive bayes as the learning algorithm.', 'the knowledge sources we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work ( #AUTHOR_TAG )']","['our experiments, we use naive bayes as the learning algorithm.', 'the knowledge sources we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work ( #AUTHOR_TAG )']",2
"['., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov ( 2002 ) ), #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov ( 2002 ) ), #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city,']","['., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov ( 2002 ) ), #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov ( 2002 ) ), #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. (2005) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. (2004) ) or wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an np ( see Bean and Riloff (2004) )']",0
"['., #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced']","['knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced']","['., #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in']","['ace participants have also adopted a corpus - based approach to sc deter - mination that is investigated as part of the mention detection ( md ) task ( e. g., Florian et al. (2006) ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'un - like them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowl - edge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc clas - sifier ; instead, we use the bbn entity type corpus (Weischedel and Brunstein, 2005), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and an - notated with their scs.', 'this provides us with a train - ing set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",1
"['., Mitkov ( 1998 ), #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in']","[', Mitkov ( 1998 ), #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city,']","['., Mitkov ( 1998 ), #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov ( 1998 ), #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. (2005) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. (2004) ) or wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an np ( see Bean and Riloff (2004) )']",0
"[') we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus ( #AUTHOR_TAG ), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions (']","['as part of the mention detection ( md ) task ( e. g., Florian et al. (2006) ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus ( #AUTHOR_TAG ), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., Luo et al. (2004) ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in']","[') we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus ( #AUTHOR_TAG ), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions (']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g., Florian et al. (2006) ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus ( #AUTHOR_TAG ), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., Luo et al. (2004) ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",5
"['., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see #AUTHOR_TAG ), Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see #AUTHOR_TAG ), Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city,']","['., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see #AUTHOR_TAG ), Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see #AUTHOR_TAG ), Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. (2005) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. (2004) ) or wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an np ( see Bean and Riloff (2004) )']",0
"[') train an svm classifier ( using the libsvm package ( #AUTHOR_TAG ) ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more']","['addition, we train an svm classifier for sc determination by combining the output of five clas - sification methods : dl, 1 - nn, me, nb, and soon et al. s method as described in the introduction, 8 with the goal of examining whether sc classifica - tion accuracy can be improved by combining the output of individual classifiers in a supervised man - ner.', 'specifically, we ( 1 ) use 80 % of the instances generated from the bbn entity type corpus to train the four classifiers ; ( 2 ) apply the four classifiers and soon et al. s method to independently make predictions for the remaining 20 % of the instances ; and ( 3 ) train an svm classifier ( using the libsvm package ( #AUTHOR_TAG ) ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more specifically, let l =']","[') train an svm classifier ( using the libsvm package ( #AUTHOR_TAG ) ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more']","['addition, we train an svm classifier for sc determination by combining the output of five clas - sification methods : dl, 1 - nn, me, nb, and soon et al. s method as described in the introduction, 8 with the goal of examining whether sc classifica - tion accuracy can be improved by combining the output of individual classifiers in a supervised man - ner.', 'specifically, we ( 1 ) use 80 % of the instances generated from the bbn entity type corpus to train the four classifiers ; ( 2 ) apply the four classifiers and soon et al. s method to independently make predictions for the remaining 20 % of the instances ; and ( 3 ) train an svm classifier ( using the libsvm package ( #AUTHOR_TAG ) ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more specifically, let l = i li ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step ( 2 ).', 'to represent i, we generate one feature from each non - empty subset of li']",5
"[""4 ) ne : we use bbn's identifinder ( #AUTHOR_TAG ), a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']","[""4 ) ne : we use bbn's identifinder ( #AUTHOR_TAG ), a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']","[""4 ) ne : we use bbn's identifinder ( #AUTHOR_TAG ), a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']","[""4 ) ne : we use bbn's identifinder ( #AUTHOR_TAG ), a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']",5
"['., Soon et al. ( 2001 ), #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g., Soon et al. ( 2001 ), #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['., Soon et al. ( 2001 ), #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g., Soon et al. ( 2001 ), #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]",0
"[') : we use the dl learner as described in #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","[') : we use the dl learner as described in #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","[') : we use the dl learner as described in #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']",5
"['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer ( #AUTHOR_TAG ), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in']","['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer ( #AUTHOR_TAG ), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in']","['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer ( #AUTHOR_TAG ), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in']","['in sc induction, we use the ace phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer ( #AUTHOR_TAG ), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps automatically extracted by an in - house np chunker and identifinder']",5
"['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following #AUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']","['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following #AUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']","['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following #AUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']","['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following #AUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']",4
"['., #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type (']","['as part of the mention detection ( md ) task ( e. g., #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus (Weischedel and Brunstein, 2005), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a']","['., #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type (']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g., #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus (Weischedel and Brunstein, 2005), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., Luo et al. (2004) ) ; and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",1
"['. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG, as described below']","['baseline coreference system uses the c4. 5 deci - sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG, as described below']","['. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG, as described below']","['baseline coreference system uses the c4. 5 deci - sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG, as described below']",1
"['baseline coreference system uses the c4. 5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work (']","['baseline coreference system uses the c4. 5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, np j, and its closest antecedent, np i ; and a negative instance is created for np j paired with each of the intervening nps, np i + 1, np i + 2,..', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work (']","['baseline coreference system uses the c4. 5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, np j, and its closest antecedent, np i ; and a negative instance is created for np j paired with each of the intervening nps, np i + 1, np i + 2,..', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']",5
"['. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below']","['. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below']",1
"['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer (Vilain et al., 1995), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following #AUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in']","['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer (Vilain et al., 1995), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following #AUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in']","['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer (Vilain et al., 1995), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following #AUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in']","['in sc induction, we use the ace phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer (Vilain et al., 1995), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following #AUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps automatically extracted by an in - house np chunker and identifinder']",5
"['., Poesio et al. ( 2004 ) ) or wikipedia ( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see #AUTHOR_TAG )']","[', Poesio et al. ( 2004 ) ) or wikipedia ( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see #AUTHOR_TAG )']","['., Poesio et al. ( 2004 ) ) or wikipedia ( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see #AUTHOR_TAG )']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. ( 2005 ) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. ( 2004 ) ) or wikipedia ( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see #AUTHOR_TAG )']",0
"['. g., #AUTHOR_TAG, Markert and Nissim ( 2005 ) ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g., #AUTHOR_TAG, Markert and Nissim ( 2005 ) ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['. g., #AUTHOR_TAG, Markert and Nissim ( 2005 ) ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g., #AUTHOR_TAG, Markert and Nissim ( 2005 ) ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]",0
"['. g., #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']","['. g., #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']",5
"[') : we use the dl learner as described in Collins and Singer ( 1999 ), motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","[') : we use the dl learner as described in Collins and Singer ( 1999 ), motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","[') : we use the dl learner as described in Collins and Singer ( 1999 ), motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in Collins and Singer ( 1999 ), motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']",4
"['., Poesio et al. ( 2004 ) ) or wikipedia ( #AUTHOR_TAG ), and the contextual role played by an np ( see Bean and Riloff ( 2004 )']","[', Poesio et al. ( 2004 ) ) or wikipedia ( #AUTHOR_TAG ), and the contextual role played by an np ( see Bean and Riloff ( 2004 )']","['., Poesio et al. ( 2004 ) ) or wikipedia ( #AUTHOR_TAG ), and the contextual role played by an np ( see Bean and Riloff ( 2004 )']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. ( 2005 ) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. ( 2004 ) ) or wikipedia ( #AUTHOR_TAG ), and the contextual role played by an np ( see Bean and Riloff ( 2004 ) )']",0
"[', #AUTHOR_TAG ), their semantic similarity as computed using wordnet (']","[', #AUTHOR_TAG ), their semantic similarity as computed using wordnet (']","[', #AUTHOR_TAG ), their semantic similarity as computed using wordnet (']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. ( 2004 ) ) or wikipedia ( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see Bean and Riloff ( 2004 ) )']",0
['similar nps ( see #AUTHOR_TAG'],['suggests that the sc of an np can be inferred from its distributionally similar nps ( see #AUTHOR_TAG'],"[', we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see #AUTHOR_TAG']","['3 ) verb obj : a verb obj feature is created in a similar fashion as subj verb if np i participates in a verb - object relation.', 'again, this represents our attempt to coarsely model subcategorization.', ""( 4 ) ne : we use bbn's identifinder (Bikel et al., 1999) ( 5 ) wn class : for each keyword w shown in the right column of table 1, we determine whether the head noun of np i is a hyponym of w in wordnet, using only the first wordnet sense of np i. 1 if so, we create a wn class feature with w as its value."", 'these keywords are potentially useful features because some of them are subclasses of the ace scs shown in the left column of table 1, while others appear to be correlated with these ace scs. 2 ( 6 ) induced class : since the first - sense heuristic used in the previous feature may not be accurate in capturing the sc of an np, we employ a corpusbased method for inducing scs that is motivated by research in lexical semantics ( e. g., Hearst (1992) ).', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', 'an example extraction would be < eastern airlines, the carrier >, where the first entry is a proper noun labeled with either one of the seven muc - style ne types 4 or others 5 and the second entry is a common noun.', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see #AUTHOR_TAG a ) ).', ""motivated by this observation, we create for each of np i's ten most semantically similar nps a neigh - bor feature whose value is the surface string of the np."", ""to determine the ten nearest neighbors, we use the semantic similarity values provided by lin's dependency - based thesaurus, which is constructed using a distributional approach combined with an information - theoretic definition of similarity""]",4
"[', #AUTHOR_TAG ).', 'given a']","['by research in lexical semantics ( e. g., #AUTHOR_TAG ).', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract']","[', #AUTHOR_TAG ).', 'given a large, unann']","['3 ) verb obj : a verb obj feature is created in a similar fashion as subj verb if np i participates in a verb - object relation.', 'again, this represents our attempt to coarsely model subcategorization.', ""( 4 ) ne : we use bbn's identifinder (Bikel et al., 1999) ( 5 ) wn class : for each keyword w shown in the right column of table 1, we determine whether the head noun of np i is a hyponym of w in wordnet, using only the first wordnet sense of np i. 1 if so, we create a wn class feature with w as its value."", 'these keywords are potentially useful features because some of them are subclasses of the ace scs shown in the left column of table 1, while others appear to be correlated with these ace scs. 2 ( 6 ) induced class : since the first - sense heuristic used in the previous feature may not be accurate in capturing the sc of an np, we employ a corpusbased method for inducing scs that is motivated by research in lexical semantics ( e. g., #AUTHOR_TAG ).', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', 'an example extraction would be < eastern airlines, the carrier >, where the first entry is a proper noun labeled with either one of the seven muc - style ne types 4 or others 5 and the second entry is a common noun.', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see Lin (1998 a ) ).', ""motivated by this observation, we create for each of np i's ten most semantically similar nps a neigh - bor feature whose value is the surface string of the np."", ""to determine the ten nearest neighbors, we use the semantic similarity values provided by lin's dependency - based thesaurus, which is constructed using a distributional approach combined with an information - theoretic definition of similarity""]",4
"['and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies,']","[', prosody and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies,']","['and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies,']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme ( #AUTHOR_TAG ).', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme ( #AUTHOR_TAG ).', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme ( #AUTHOR_TAG ).', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin attributes has been used, i. e.', 'smile, laughter, scowl, faceother for facial expressions, and nod, jerk, tilt, sideturn, shake, waggle, other for head movements']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme ( #AUTHOR_TAG ).', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin attributes has been used, i. e.', 'smile, laughter, scowl, faceother for facial expressions, and nod, jerk, tilt, sideturn, shake, waggle, other for head movements']",5
"['cues, while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a']","['cues, while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a']","['in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['and eye gaze in interaction with embodied communication agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies,']","[', prosody and eye gaze in interaction with embodied communication agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies,']","['and eye gaze in interaction with embodied communication agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies,']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG'],['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG'],['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG'],"['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"['an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['general, dialogue act, agreement and turn anno - tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']",5
"['dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from']","['cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as']","['in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system ( #AUTHOR_TAG ).', 'we experiment']","['we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system ( #AUTHOR_TAG ).', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our']","['we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system ( #AUTHOR_TAG ).', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our']","['multimodal data we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system ( #AUTHOR_TAG ).', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb ) (Zhang et al., 2005).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']",5
"['##s, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in']","['domain of map - task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']",5
"['and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ).', 'our work is in line with these studies,']","[', prosody and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ).', 'our work is in line with these studies,']","['and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ).', 'our work is in line with these studies,']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annot']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']",0
"['an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG :']","['an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG :']","['an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG :']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc Clave (2000) for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"['have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc Clave (2000) for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"['corpora.', 'for example, Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi - modal corpus']","['corpora.', 'for example, Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi - modal corpus']","['have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi - modal corpus']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc Clave (2000) for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi - modal corpus']",0
"['.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG )."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annot']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG )."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG )."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG )."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']",0
"['.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ), but are still sat - isfactory given the high number of categories provided by the scheme']","['non expert annotators ( one annotator per video ) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ), but are still sat - isfactory given the high number of categories provided by the scheme']","['non expert annotators ( one annotator per video ) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ), but are still sat - isfactory given the high number of categories provided by the scheme']","['head gestures in the danpass data have been coded by non expert annotators ( one annotator per video ) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ), but are still sat - isfactory given the high number of categories provided by the scheme']",1
"['in #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme']","['in #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme']","['in #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",1
"['were obtained using hidden naive bayes ( hnb ) ( #AUTHOR_TAG ).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['were obtained using hidden naive bayes ( hnb ) ( #AUTHOR_TAG ).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['were obtained using hidden naive bayes ( hnb ) ( #AUTHOR_TAG ).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['multimodal data we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system (Witten and Frank, 2005).', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb ) ( #AUTHOR_TAG ).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']",5
"['.', 'both kinds of annotation were carried out using anvil ( #AUTHOR_TAG ).', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging']","['- including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil ( #AUTHOR_TAG ).', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging']","['##ssed ; 7 % of them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil ( #AUTHOR_TAG ).', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging iso 24617']","['already mentioned, all words in danpass are phonetically and prosodically annotated.', 'in the subset of the corpus considered here, 82 % of the feedback expressions bear stress or tone information, and 12 % are unstressed ; 7 % of them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil ( #AUTHOR_TAG ).', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging iso 24617 - 2 standard for semantic annotation of language resources.', 'this subset comprises the categories accept, decline, repeatrephrase and answer.', 'moreover, all feedback expressions were annotated with an agreement feature ( agree, nonagree ) where relevant.', 'finally, the two turn management categories turn - take and turnelicit were also coded']",5
"['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used ( #AUTHOR_TAG )']","['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used ( #AUTHOR_TAG )']","['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used ( #AUTHOR_TAG )']","['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used ( #AUTHOR_TAG )']",5
"['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', 'the diagnoser, based on #AUTHOR_TAG']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', 'the diagnoser, based on #AUTHOR_TAG']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on #AUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008)']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on #AUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008)']",2
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in human']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['to the surface realizer based on the strategy decision, and a fuf / surge ( #AUTHOR_TAG ) generation system to produce']","['to the surface realizer based on the strategy decision, and a fuf / surge ( #AUTHOR_TAG ) generation system to produce']","['to the surface realizer based on the strategy decision, and a fuf / surge ( #AUTHOR_TAG ) generation system to produce']","[""strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer ( e. g., part of the answer to confirm ), is passed to content planning and generation."", 'the system uses a domain - specific content planner to produce input to the surface realizer based on the strategy decision, and a fuf / surge ( #AUTHOR_TAG ) generation system to produce the appropriate text.', 'templates are used to generate some stock phrases such as "" when you are ready, go on to the next slide.']",5
"[""indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","[""indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","[', the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']",3
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in human']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ).', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with']","['between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ).', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with']","['between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ).', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student']","['between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ).', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student']",5
"['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"[""indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG )."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","[""indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG )."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","[', the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG )."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG )."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']",3
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in human']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ).', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp (Hockey et al., 2003) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ).', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp (Hockey et al., 2003) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ).', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp (Hockey et al., 2003) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ).', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp (Hockey et al., 2003) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']",5
"[""then uses a reference resolution approach similar to #AUTHOR_TAG, and an ontology mapping mechanism ( Dzikovska et al. , 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is']","[""then uses a reference resolution approach similar to #AUTHOR_TAG, and an ontology mapping mechanism ( Dzikovska et al. , 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported,']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG, and an ontology mapping mechanism ( Dzikovska et al. , 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG, and an ontology mapping mechanism ( Dzikovska et al. , 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']",1
"['beetle ii system architecture is designed to overcome these limitations ( #AUTHOR_TAG ).', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more""]","['beetle ii system architecture is designed to overcome these limitations ( #AUTHOR_TAG ).', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more""]","['beetle ii system architecture is designed to overcome these limitations ( #AUTHOR_TAG ).', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning""]","['beetle ii system architecture is designed to overcome these limitations ( #AUTHOR_TAG ).', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning""]",0
"[""), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #AUTHOR_TAG']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on Dzikovska et al. (2008 b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #AUTHOR_TAG']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on Dzikovska et al. (2008 b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #AUTHOR_TAG']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on Dzikovska et al. (2008 b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #AUTHOR_TAG']",3
"['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in human']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"[""then uses a reference resolution approach similar to Byron ( 2002 ), and an ontology mapping mechanism ( #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is']","[""then uses a reference resolution approach similar to Byron ( 2002 ), and an ontology mapping mechanism ( #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported,']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ), and an ontology mapping mechanism ( #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ), and an ontology mapping mechanism ( #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']",5
['factors such as student confidence could be considered as well ( #AUTHOR_TAG )'],['factors such as student confidence could be considered as well ( #AUTHOR_TAG )'],['factors such as student confidence could be considered as well ( #AUTHOR_TAG )'],['factors such as student confidence could be considered as well ( #AUTHOR_TAG )'],3
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in human']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in human']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp ( #AUTHOR_TAG ) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp ( #AUTHOR_TAG ) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp ( #AUTHOR_TAG ) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp ( #AUTHOR_TAG ) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']",2
"['use the trips dialogue parser ( #AUTHOR_TAG ) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', 'the contextual']","['use the trips dialogue parser ( #AUTHOR_TAG ) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', 'the contextual']","['use the trips dialogue parser ( #AUTHOR_TAG ) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation']","['use the trips dialogue parser ( #AUTHOR_TAG ) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']",5
"['system uses a knowledge base implemented in the km representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the km representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the km representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the km representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']",5
"['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( Tatu and Moldovan , 2005 ), or adopting transformation - based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( Tatu and Moldovan , 2005 ), or adopting transformation - based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( Tatu and Moldovan , 2005 ), or adopting transformation - based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such information ranges from i ) lexical paraphrases ( textual equivalences between terms ) to ii ) lexical relations preserving entailment between words, and']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( Tatu and Moldovan , 2005 ), or adopting transformation - based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such information ranges from i ) lexical paraphrases ( textual equivalences between terms ) to ii ) lexical relations preserving entailment between words, and iii ) wordlevel similarity / relatedness scores.', 'wordnet, the most widely used resource in te, provides all the three types of information.', 'synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'hypernymy / hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'paths between concepts and glosses can be used to calculate similarity / relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis']",0
"['##t ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ).', 'there are several methods to build phrase']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase']","['##t ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger (Schmid, 1994) for tokenization, and used the giza + + (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit (Koehn et al., 2007).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",0
"[', dirt ( #AUTHOR_TAG ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","[', dirt ( #AUTHOR_TAG ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","[', dirt ( #AUTHOR_TAG ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","[', dirt ( #AUTHOR_TAG ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']",0
"['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( tatu and #AUTHOR_TAG ), or adopting transformation - based']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( tatu and #AUTHOR_TAG ), or adopting transformation - based']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( tatu and #AUTHOR_TAG ), or adopting transformation - based techniques ( Kouleykov and Magnini , 2005 ;  bar - Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such information ranges from i ) lexical paraphrases ( textual equivalences between terms ) to ii']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( tatu and #AUTHOR_TAG ), or adopting transformation - based techniques ( Kouleykov and Magnini , 2005 ;  bar - Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such information ranges from i ) lexical paraphrases ( textual equivalences between terms ) to ii ) lexical relations preserving entailment between words, and iii ) wordlevel similarity / relatedness scores.', 'wordnet, the most widely used resource in te, provides all the three types of information.', 'synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'hypernymy / hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'paths between concepts and glosses can be used to calculate similarity / relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis']",0
"['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by ( #AUTHOR_TAG ).', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent']","['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by ( #AUTHOR_TAG ).', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent']","['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by ( #AUTHOR_TAG ).', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent back to']","['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by ( #AUTHOR_TAG ).', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent back to workers in a new translation job.', 'although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired clte corpus.', 'the validation, carried out by a spanish native speaker on 100 randomly selected pairs after two translation - validation cycles, showed the good quality of the collected material, with only 3 minor "" errors "" consisting in controversial but substantially acceptable translations reflecting regional spanish variations']",5
"['- lingual textual entailment ( clte ) has been proposed by ( #AUTHOR_TAG ) as an extension of textual entailment ( Dagan and Glickman , 2004 ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources (']","['- lingual textual entailment ( clte ) has been proposed by ( #AUTHOR_TAG ) as an extension of textual entailment ( Dagan and Glickman , 2004 ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and']","['- lingual textual entailment ( clte ) has been proposed by ( #AUTHOR_TAG ) as an extension of textual entailment ( Dagan and Glickman , 2004 ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources (']","['- lingual textual entailment ( clte ) has been proposed by ( #AUTHOR_TAG ) as an extension of textual entailment ( Dagan and Glickman , 2004 ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal ex - pressions recognizers and normalizers ) has to con - front, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the ex - isting ones, and the burden of integrating language - specific components into the same cross - lingual ar - chitecture']",0
"[', of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison, we experiment with different sources of multil']","['of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions']","[', of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions']","['paper investigates the idea, still unexplored, of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions']",1
"['for the wmt10 1.', 'we run treetagger ( Schmid , 1994 ) for tokenization, and used the giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the']","['for the wmt10 1.', 'we run treetagger ( Schmid , 1994 ) for tokenization, and used the giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit (Koehn et al., 2007).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase']","['for the wmt10 1.', 'we run treetagger ( Schmid , 1994 ) for tokenization, and used the giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit (Koehn et al., 2007).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger ( Schmid , 1994 ) for tokenization, and used the giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit (Koehn et al., 2007).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",5
"[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( #AUTHOR_TAG ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( #AUTHOR_TAG ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( #AUTHOR_TAG ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( #AUTHOR_TAG ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",4
"['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']",0
"['their relative weights, we trained a support vector machine classifier, svmlight ( #AUTHOR_TAG ), using each score as a feature']","['their relative weights, we trained a support vector machine classifier, svmlight ( #AUTHOR_TAG ), using each score as a feature']","['their relative weights, we trained a support vector machine classifier, svmlight ( #AUTHOR_TAG ), using each score as a feature']","['combine the phrasal matching scores obtained at each n - gram level, and optimize their relative weights, we trained a support vector machine classifier, svmlight ( #AUTHOR_TAG ), using each score as a feature']",5
"[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( #AUTHOR_TAG ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( #AUTHOR_TAG ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( #AUTHOR_TAG ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( #AUTHOR_TAG ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",4
"['mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( #AUTHOR_TAG ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","['mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( #AUTHOR_TAG ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","['mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( #AUTHOR_TAG ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( #AUTHOR_TAG ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']",0
"['aligned with the english wordnet ( e. g. multiwordnet ( #AUTHOR_TAG ) ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards']","['aligned with the english wordnet ( e. g. multiwordnet ( #AUTHOR_TAG ) ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards wikipedia, the cross - lingual links between pages in different languages offer a possibility to extract lexical']","['by wordnet and wikipedia, most of the aforementioned resources are available only for english.', 'multilingual lexical databases aligned with the english wordnet ( e. g. multiwordnet ( #AUTHOR_TAG ) ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards']","['clte we have to face additional and more problematic issues related to : i ) the stronger need of lexical knowledge, and ii ) the limited availability of multilingual lexical resources.', 'as regards the first issue, its worth noting that in the monolingual scenario simple bag of words ( or bag of n - grams ) approaches are per se sufficient to achieve results above baseline.', 'in contrast, their application in the cross - lingual setting is not a viable solution due to the impossibility to perform direct lex - ical matches between texts and hypotheses in different languages.', 'this situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'however, with the only exceptions represented by wordnet and wikipedia, most of the aforementioned resources are available only for english.', 'multilingual lexical databases aligned with the english wordnet ( e. g. multiwordnet ( #AUTHOR_TAG ) ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards wikipedia, the cross - lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for clte.', 'however, due to their relatively small number ( especially for some languages ), bilingual lexicons extracted from wikipedia are still inadequate to provide acceptable coverage.', 'in addition, featuring a bias towards named entities, the information acquired through cross - lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources ( e. g bilingual dictionaries )']",0
"[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( #AUTHOR_TAG ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( #AUTHOR_TAG ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( #AUTHOR_TAG ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( #AUTHOR_TAG ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",4
"[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",0
"['mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']",0
"['- lingual textual entailment ( clte ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of textual entailment ( #AUTHOR_TAG ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources (']","['- lingual textual entailment ( clte ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of textual entailment ( #AUTHOR_TAG ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and']","['- lingual textual entailment ( clte ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of textual entailment ( #AUTHOR_TAG ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources (']","['- lingual textual entailment ( clte ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of textual entailment ( #AUTHOR_TAG ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal expressions recognizers and normalizers ) has to confront, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating language - specific components into the same cross - lingual architecture']",0
"['mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( #AUTHOR_TAG ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( #AUTHOR_TAG ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( #AUTHOR_TAG ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( #AUTHOR_TAG ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']",0
"['parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']","['parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']","[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']",0
"[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( #AUTHOR_TAG ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( #AUTHOR_TAG ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","[') contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( #AUTHOR_TAG ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( #AUTHOR_TAG ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",4
"['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']",0
"[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by ( #AUTHOR_TAG )."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora']","[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by ( #AUTHOR_TAG )."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora']","[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by ( #AUTHOR_TAG )."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora']","[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by ( #AUTHOR_TAG )."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive mt system ( google translate ) in the same scenario']",1
"['use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ).', 'another interesting direction is to investigate the potential of paraphrase patterns ( i.']","['use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ).', 'another interesting direction is to investigate the potential of paraphrase patterns ( i. e.', 'patterns including partof - speech slots ),']","['use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ).', 'another interesting direction is to investigate the potential of paraphrase patterns ( i.']","['future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ).', 'another interesting direction is to investigate the potential of paraphrase patterns ( i. e.', 'patterns including partof - speech slots ), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'on the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'as a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.', '134']",3
"['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by (Kouylekov et al., 2009).', 'in this way we obtained 13760 word pairs']","['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by (Kouylekov et al., 2009).', 'in this way we obtained 13760 word pairs']","['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by (Kouylekov et al., 2009).', 'in this way we obtained 13760 word pairs']","['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by (Kouylekov et al., 2009).', 'in this way we obtained 13760 word pairs']",5
"['of the paraphrase table.', 'second, in line with the findings of ( #AUTHOR_TAG ), the results obtained over the mt - derived corpus are equal to those we achieve over the']","['of the paraphrase table.', 'second, in line with the findings of ( #AUTHOR_TAG ), the results obtained over the mt - derived corpus are equal to those we achieve over the']","['- g row in table 2 ) leads to four main observations.', 'first, we notice that dealing with mt - derived inputs, the optimal pruning threshold changes from 0. 2 to 0. 1, leading to the highest accuracy of 63. 50 %.', 'this suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'second, in line with the findings of ( #AUTHOR_TAG ), the results obtained over the mt - derived corpus are equal to those we achieve over the original rte3 dataset ( i.']","['comparison with the results achieved on monolingual data obtained by automatically translating the spanish hypotheses ( rte3 - g row in table 2 ) leads to four main observations.', 'first, we notice that dealing with mt - derived inputs, the optimal pruning threshold changes from 0. 2 to 0. 1, leading to the highest accuracy of 63. 50 %.', 'this suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'second, in line with the findings of ( #AUTHOR_TAG ), the results obtained over the mt - derived corpus are equal to those we achieve over the original rte3 dataset ( i. e. 63. 50 % ).', '63. 50 % ).', 'third, the accuracy obtained over the clte corpus using combined phrase and paraphrase tables ( 62. 88 %, as reported in table 1 ) is comparable to the best result gained over the automatically translated dataset ( 63. 50 % ).', 'in all the other cases, the use of phrase and paraphrase tables on clte data outperforms the results achieved on the same data after translation.', ""finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62. 12 %, which is lower than the result obtained using only phrase tables on cross - lingual data ( 62. 62 % )."", 'this demonstrates that phrase tables can successfully replace mt systems in the clte task']",1
"['##es toolkit ( #AUTHOR_TAG ).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase']","['for the wmt10 1.', 'we run treetagger (Schmid, 1994) for tokenization, and used the giza + + (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit ( #AUTHOR_TAG ).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase']","['for the wmt10 1.', 'we run treetagger (Schmid, 1994) for tokenization, and used the giza + + (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit ( #AUTHOR_TAG ).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger (Schmid, 1994) for tokenization, and used the giza + + (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit ( #AUTHOR_TAG ).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",5
"['wordnet, the rte literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ).', 'these include, just']","['wordnet, the rte literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ).', 'these include, just']","['wordnet, the rte literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ).', 'these include, just']","['wordnet, the rte literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ).', 'these include, just to mention the most popular ones, dirt (Lin and Pantel, 2001), verbocean (Chklovski and Pantel, 2004), framenet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']",0
"['are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important']","['are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important']","['questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '( see']","['questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '( see sec. 2 for models of morphological organization and access and related experiments )']",0
"['are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we']","['are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we']","['are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them']","['and processing of compound verbs in the mental lexicon compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'the verb v1 is known as pole and v2 is called as vector.', 'for example, "" [UNK] [UNK] "" ( getting up ) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'however, not all v1 + v2 combinations are cvs.', 'for example, expressions like, "" [UNK] [UNK] "" ( take and then go ) and "" [UNK] [UNK] [UNK] "" ( return back ) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as cv.', 'the key question linguists are trying to identify for a long time and debating a lot is whether to consider cvs as a single lexical units or consider them as two separate units.', 'since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'a clear understanding about these phenomena may help us to classify or extract actual cvs from other verb sequences.', 'in order to do so, presently we have applied three different techniques to collect user data.', 'in the first technique, we annotated 4500 v1 + v2 sequences, along with their example sentences, using a group of three linguists ( the expert subjects ).', 'we asked the experts to classify the verb sequences into three classes namely, cv, not a cv and not sure.', 'each linguist has received 2000 verb pairs along with their respective example sentences.', 'out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them to 36 native bangla speakers.', 'we ask each subjects to give a compositionality score of each verb sequences under 1 - 10 point scale, 10 being highly compositional and 1 for noncompositional.', 'we found an agreement of  = 0. 69 among the subjects.', 'we also observe a continuum of compositionality score among the verb sequences.', 'this reflects that it is difficult to classify bangla verb sequences discretely into the classes of cv and not']",5
"['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangl']","['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', '']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in ( #AUTHOR_TAG )']","['speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in ( #AUTHOR_TAG )']","['speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in ( #AUTHOR_TAG )']","['the agreement lies around 0. 79. next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them to 36', 'native bangla speakers. we ask each subjects to give a compositionality score of', 'each verb sequences under 1 - 10 point scale, 10 being highly compositional and 1 for noncompositional. we found an agreement of  =', '0. 69 among the subjects. we also observe a continuum of compositional', ""##ity score among the verb sequences. this reflects that it is difficult to classify bangla verb sequences discretely into the classes of cv and not a cv. we then, compare the compositionality score with that of the expert user's annotation. we found a significant correlation between the expert annotation and the"", 'compositionality score. we observe verb sequences that are annotated as cvs ( like, [UNK] [UNK], [UNK] [UNK], [UNK] [UNK] ) have got low compositionality score ( average score ranges between', '1 - 4 ) on the other hand high compositional values are in general tagged as not a cv ( [UNK] [UNK] ( come and get ), [UNK] [UNK] ( return back )', ', [UNK] [UNK] [UNK] ( kept ), [UNK] [UNK] ( roll on floor ) ). this reflects that verb', 'sequences which are not cv shows high degree of compositionality. in other words non cv verbs', 'can directly interpret from their constituent verbs. this leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be', 'greater than the non - compositional verbs which maps to a single expression of meaning. in order to validate such claim we', 'perform a lexical decision experiment using native bangla speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for english polymorphemic words. however,', 'rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. the reaction time', '( rt ) of each subject is recorded. our preliminarily observation from the rt analysis shows that as per our claim, rt of verb sequences having high compositionality value is significantly higher than the', 'rts for low or noncompositional verbs. this proves our hypothesis that bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon', 'and thus follows the full - listing model whereas compositional verb phrases are individually', 'parsed. however, we do believe that our experiment is composed of', 'a very small set of data and it is premature to conclude anything concrete based only on the current experimental results']",5
"['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangl']","['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', '']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts']",0
"['mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ).', 'on the other hand, morph']","['mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ).', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes are stripped']","['paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ).', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes']","['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'most of the studies are designed to support one of the two mutually exclusive paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ).', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981; mac Kay, 1978).', 'intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988)']",0
"['( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are']","['( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are']","[', and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangl']","['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['respect to this, we apply the different priming and other lexical decision experiments, described in literature ( #AUTHOR_TAG ; bent']","['respect to this, we apply the different priming and other lexical decision experiments, described in literature ( #AUTHOR_TAG ; bentin, s. and Feldman , 1990 )']","['respect to this, we apply the different priming and other lexical decision experiments, described in literature ( #AUTHOR_TAG ; bent']","['respect to this, we apply the different priming and other lexical decision experiments, described in literature ( #AUTHOR_TAG ; bentin, s. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of bangla.', 'our cross - modal and masked priming experiment on bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically / orthographically unrelated.', 'these observations are similar to those reported for english and indicate that derivationally suffixed words in bangla are in general accessed through decomposition of the word into its constituent morphemes.', 'further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of bangla polymorphemic words.', 'our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix']",5
"['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']",5
"[', and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; grainger, et al., 1991']","[', and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; grainger, et al., 1991']","[', and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; grainger, et al., 1991']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; grainger, et al., 1991 ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']",5
"['storage in lexical resources like wordnet ( #AUTHOR_TAG ) and raises the questions like, how to store']","['storage in lexical resources like wordnet ( #AUTHOR_TAG ) and raises the questions like, how to store']","['storage in lexical resources like wordnet ( #AUTHOR_TAG ) and raises the questions like, how to store morphologically complex words, in a lexical resource like wordnet keeping in mind the storage and access efficiency']","['clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language.', 'further, these linguistically important and interesting questions are also highly significant for computational linguistics ( cl ) and natural language processing ( nlp ) applications.', 'their computational significance arises from the issue of their storage in lexical resources like wordnet ( #AUTHOR_TAG ) and raises the questions like, how to store morphologically complex words, in a lexical resource like wordnet keeping in mind the storage and access efficiency']",0
"['like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ).', 'therefore, the findings from experiments in one language cannot be']","['like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ).', 'therefore, the findings from experiments in one language cannot be']","['', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ).', 'therefore, the findings from experiments in one language cannot be generalized']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( marslen - Wilson et al., 2008;Frost et al., 1997;Grainger, et al., 1991;Drews and Zwitserlood, 1995).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ).', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
['model ( #AUTHOR_TAG )'],['follows the morphemic model ( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],"['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'most of the studies are designed to support one of the two mutually exclusive paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon (Bradley, 1980;Butterworth, 1983).', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981; mac Kay, 1978).', 'intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG )']",0
"['on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangl']","['on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', '']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts']",0
"['- Wilson et al. , 2008 ; Frost et al. , 1997 ;  grainger, et al., 1991 ; #AUTHOR_TAG ).', 'however, we do not know of any such investigations for indian languages, which are']","[', and few other languages ( marslen - Wilson et al. , 2008 ; Frost et al. , 1997 ;  grainger, et al., 1991 ; #AUTHOR_TAG ).', 'however, we do not know of any such investigations for indian languages, which are']","['- Wilson et al. , 2008 ; Frost et al. , 1997 ;  grainger, et al., 1991 ; #AUTHOR_TAG ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( marslen - Wilson et al. , 2008 ; Frost et al. , 1997 ;  grainger, et al., 1991 ; #AUTHOR_TAG ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates "" (Dagan et al., 1993).', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the""]","['u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates "" (Dagan et al., 1993).', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the""]","['u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates "" (Dagan et al., 1993).', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']","['##we could just as easily use other symmetric "" association "" measures, such as 2 or the dice coefficient (Smadja, 1992).', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates "" (Dagan et al., 1993).', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",0
"['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g.', '(Macklovitch, 1994;Melamed, 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993;,  computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']",0
"['u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', ""models of translational equivalence that are ignorant of indirect associations have ` ` a tendency... to be confused by collocates'' ( #AUTHOR_TAG )."", 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996 c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the""]","['u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', ""models of translational equivalence that are ignorant of indirect associations have ` ` a tendency... to be confused by collocates'' ( #AUTHOR_TAG )."", 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996 c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the""]","['u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', ""models of translational equivalence that are ignorant of indirect associations have ` ` a tendency... to be confused by collocates'' ( #AUTHOR_TAG )."", 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996 c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']","['##we could just as easily use other symmetric "" association "" measures, such as 2 or the dice coefficient (Smadja, 1992).', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', ""models of translational equivalence that are ignorant of indirect associations have ` ` a tendency... to be confused by collocates'' ( #AUTHOR_TAG )."", 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996 c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",0
"['. ( #AUTHOR_TAG ; Melamed , 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer - assisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g. ( #AUTHOR_TAG ; Melamed , 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']","['. ( #AUTHOR_TAG ; Melamed , 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer - assisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g. ( #AUTHOR_TAG ; Melamed , 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']",0
"['be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ).', 'the']","['be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ).', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'the correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ).', 'the likelihoods in the word - to - word model remain unnormalized, so they do not']","['some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'the correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ).', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']",0
"['. g. (Macklovitch, 1994; Melamed, 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer - assisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g. (Macklovitch, 1994; Melamed, 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']","['. g. (Macklovitch, 1994; Melamed, 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer - assisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g. (Macklovitch, 1994; Melamed, 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']",0
"['. (Macklovitch, 1994; #AUTHOR_TAG b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer - assisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g. (Macklovitch, 1994; #AUTHOR_TAG b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']","['. (Macklovitch, 1994; #AUTHOR_TAG b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer - assisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g. (Macklovitch, 1994; #AUTHOR_TAG b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']",0
"['could just as easily use other symmetric "" association "" measures, such as 02 ( Gale & Church , 1991 ) or the dice coefficient ( #AUTHOR_TAG ).', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk']","['##we could just as easily use other symmetric "" association "" measures, such as 02 ( Gale & Church , 1991 ) or the dice coefficient ( #AUTHOR_TAG ).', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk']","['could just as easily use other symmetric "" association "" measures, such as 02 ( Gale & Church , 1991 ) or the dice coefficient ( #AUTHOR_TAG ).', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk']","['##we could just as easily use other symmetric "" association "" measures, such as 02 ( Gale & Church , 1991 ) or the dice coefficient ( #AUTHOR_TAG ).', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates "" (Dagan et al., 1993).', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996 c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",1
"['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g.', '(Macklovitch, 1994;Melamed, 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993;,  computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']",0
"['Melamed, 1995).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAG a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997),']","['- occurrence with the exception of (Fung, 1998 b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & Hirakawa, 1994;Fung, 1998 a ; Melamed, 1995).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAG a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997),']","['Melamed, 1995).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAG a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997),']","['- occurrence with the exception of (Fung, 1998 b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & Hirakawa, 1994;Fung, 1998 a ; Melamed, 1995).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAG a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;']",0
"['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in ( #AUTHOR_TAG )']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in ( #AUTHOR_TAG )']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in ( #AUTHOR_TAG )']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in ( #AUTHOR_TAG )']",5
"['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",0
"['english weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we']","['model was also used to induce a translation lexicon from a 6200 - word corpus of french / english weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we']","['english weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']","['model was also used to induce a translation lexicon from a 6200 - word corpus of french / english weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']",0
"['##s ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a']","['- occurrence with the exception of (Fung, 1998 b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a ; Melamed, 1995).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997),']","['##s ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a']","['- occurrence with the exception of (Fung, 1998 b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998 a ; Melamed, 1995).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;']",0
"['. g. ( Macklovitch , 1994 ; Melamed , 1996 b ) ), concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ), computerassisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( Church & Hovy , 1993 ), certain machine - assisted translation tools ( e. g. ( Macklovitch , 1994 ; Melamed , 1996 b ) ), concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ), computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']","['. g. ( Macklovitch , 1994 ; Melamed , 1996 b ) ), concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ), computerassisted language learning, corpus linguistics (']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993 a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( Church & Hovy , 1993 ), certain machine - assisted translation tools ( e. g. ( Macklovitch , 1994 ; Melamed , 1996 b ) ), concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ), computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']",0
"[""the definition of a ` ` word'' to include multi - word lexical units ( #AUTHOR_TAG )."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy.', ""another interesting extension is to broaden the definition of a ` ` word'' to include multi - word lexical units ( #AUTHOR_TAG )."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a']","[""the definition of a ` ` word'' to include multi - word lexical units ( #AUTHOR_TAG )."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy.', ""another interesting extension is to broaden the definition of a ` ` word'' to include multi - word lexical units ( #AUTHOR_TAG )."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']",3
"['##s are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following ( #AUTHOR_TAG ) 2.', ""when the l ( u, v ) are re - estimated, the model's hidden""]","['that u and v can be mutual translations.', 'for each co - occurring pair of word types u and v, these likelihoods are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following ( #AUTHOR_TAG ) 2.', ""when the l ( u, v ) are re - estimated, the model's hidden""]","['translation model consists of the hidden parameters a + and a -, and likelihood ratios l ( u, v ).', 'the two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'l ( u, v ) represents the likelihood that u and v can be mutual translations.', 'for each co - occurring pair of word types u and v, these likelihoods are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following ( #AUTHOR_TAG ) 2.', ""when the l ( u, v ) are re - estimated, the model's hidden""]","['translation model consists of the hidden parameters a + and a -, and likelihood ratios l ( u, v ).', 'the two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'l ( u, v ) represents the likelihood that u and v can be mutual translations.', 'for each co - occurring pair of word types u and v, these likelihoods are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following ( #AUTHOR_TAG ) 2.', ""when the l ( u, v ) are re - estimated, the model's hidden parameters come into play""]",5
"['#AUTHOR_TAG ).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other (Gale & Church, 1991; Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997),']","['#AUTHOR_TAG ).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other (Gale & Church, 1991; Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997),']","['#AUTHOR_TAG ).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other (Gale & Church, 1991; Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997),']","['the exception of ( Fung , 1995 b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995 a ; #AUTHOR_TAG ).', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other (Gale & Church, 1991; Melamed, 1996 a ).', 'then, two word tokens ( u, v ) are said to co - occur in the aligned segment pair i if u e si and v e ti.', 'the co - occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997)']",0
"['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",1
"[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm ( #AUTHOR_TAG ), they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr (']","[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm ( #AUTHOR_TAG ), they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr ( datalmodel ) in a non - monotonic fashion.', 'we have adopted the simple heuristic that the model "" has converged "" when this probability stops increasing']","[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm ( #AUTHOR_TAG ), they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr (']","[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm ( #AUTHOR_TAG ), they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr ( datalmodel ) in a non - monotonic fashion.', 'we have adopted the simple heuristic that the model "" has converged "" when this probability stops increasing']",0
"['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy ( #AUTHOR_TAG ).', 'another interesting extension is to broad']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy ( #AUTHOR_TAG ).', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units (Smadja, 1992).', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy ( #AUTHOR_TAG ).', 'another interesting extension is to broad']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy ( #AUTHOR_TAG ).', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units (Smadja, 1992).', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']",3
"['a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( #AUTHOR_TAG )']","['a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( #AUTHOR_TAG )']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( #AUTHOR_TAG )']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( #AUTHOR_TAG )']",0
"['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be']","['l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ).', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",1
"[""##we could just as easily use other symmetric ` ` association'' measures, such as 02 ( #AUTHOR_TAG ) or the dice coefficient ( Smadja , 1992 )."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk']","[""##we could just as easily use other symmetric ` ` association'' measures, such as 02 ( #AUTHOR_TAG ) or the dice coefficient ( Smadja , 1992 )."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk']","[""##we could just as easily use other symmetric ` ` association'' measures, such as 02 ( #AUTHOR_TAG ) or the dice coefficient ( Smadja , 1992 )."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk']","[""##we could just as easily use other symmetric ` ` association'' measures, such as 02 ( #AUTHOR_TAG ) or the dice coefficient ( Smadja , 1992 )."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates "" (Dagan et al., 1993).', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996 c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",1
"['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i - Iovy, 1993), certain machine - assisted translation tools ( e. g.', '(Macklovitch, 1994;Melamed, 1996 b ) ), concordancing for bilingual lexicography (Catizone et al., 1993;,  computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard & Dorr, 1996)']",0
"['be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ).', 'the']","['be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ).', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'the correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ).', 'the likelihoods in the word - to - word model remain unnormalized, so they do not']","['some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'the correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ).', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']",0
"['models ( #AUTHOR_TAG b ).', 'for']","['models ( #AUTHOR_TAG b ).', 'for']","['models ( #AUTHOR_TAG b ).', 'for']","['account for this difference, we can estimate separate values of x + and a - for different ranges of n ( u, v ).', 'similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'just as easily, we can model links that coincide with entries in a pre - existing translation lexicon separately from those that do not.', 'this method of incorporating dictionary information seems simpler than the method proposed by brown et al. for their models ( #AUTHOR_TAG b ).', 'for their models (Brown et al., 1993 b ).', 'when the hidden parameters are conditioned on different link classes, the estimation method does not change ; it is just repeated for each link class']",1
"['english weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we']","['model was also used to induce a translation lexicon from a 6200 - word corpus of french / english weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we']","['english weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']","['model was also used to induce a translation lexicon from a 6200 - word corpus of french / english weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 %.', 'recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ), who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']",1
"['the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from ( #AUTHOR_TAG ), we find that the performance of samt system is similar with the method of']","['the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from ( #AUTHOR_TAG ), we find that the performance of samt system is similar with the method of']","['the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from ( #AUTHOR_TAG ), we find that the performance of samt system is similar with the method of labeling scfg rules']","['the u - trees, we run the gibbs sampler for 1000 iterations on the whole corpus.', 'the sampler uses 1, 087s per iteration, on average, using a single core, 2. 3 ghz intel xeon machine.', 'for the hyperparameters, we set i to 0. 1 and p expand = 1 / 3 to give a preference to the rules with small fragments.', 'we built an s2t translation system with the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from ( #AUTHOR_TAG ), we find that the performance of samt system is similar with the method of labeling scfg rules with pos tags.', 'thus, to be convenient, we only conduct experiments with the samt system']",4
"['nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, #AUTHOR_TAG designed a sampler to infer an']","['nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, #AUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules 7 in terms of frontier nodes (Galley et al., 2004).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold italic nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, #AUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules 7 in terms of frontier nodes (Galley et al., 2004).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold italic nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, #AUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']",4
"['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system ( #AUTHOR_TAG ).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u']","['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system ( #AUTHOR_TAG ).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our']","['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system ( #AUTHOR_TAG ).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system ( #AUTHOR_TAG ).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['into the model more freely and effectively.', '#AUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scf']","['into the model more freely and effectively.', '#AUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scfg )']","['source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scf']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['as s2t ).', 'the system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['as s2t ).', 'the system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']",5
"['source sentences for syntactic pre - reordering.', 'our previous work ( #AUTHOR_TAG ) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u']","['source sentences for syntactic pre - reordering.', 'our previous work ( #AUTHOR_TAG ) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be']","['source sentences for syntactic pre - reordering.', 'our previous work ( #AUTHOR_TAG ) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scf']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work ( #AUTHOR_TAG ) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser (Petrov et al., 2006).', 'then, we binarize the english parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser (Petrov et al., 2006).', 'then, we binarize the english parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser (Petrov et al., 2006).', 'then, we binarize the english parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser (Petrov et al., 2006).', 'then, we binarize the english parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system']",5
"['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011 b )']",0
"['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['##g production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow #AUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']","['base distribution 0 ( | ) p r n is designed to assign prior probabilities to the stsg production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow #AUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']","['base distribution 0 ( | ) p r n is designed to assign prior probabilities to the stsg production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow #AUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']","['base distribution 0 ( | ) p r n is designed to assign prior probabilities to the stsg production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow #AUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']",5
"['effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the scfg rules']","['effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the scfg rules']","['focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the scfg rules']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['nodes ( #AUTHOR_TAG ).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold']","['nodes ( #AUTHOR_TAG ).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold italic nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ).', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'for example, the bold italic nodes with shadows in figure 2 are frontier nodes.', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently, designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']",5
"['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert (Och, 2004) to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach ( #AUTHOR_TAG )']","['evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert (Och, 2004) to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach ( #AUTHOR_TAG )']","['experiments are conducted on chinese - to - english translation.', 'the training data are the fbis corpus with approximately 7. 1 million chinese words and 9. 2 million english words.', 'we obtain the bidirectional word alignment with giza + +, and then adopt the grow - diag - final - and strategy to obtain the final symmetric alignment.', 'we train a 5gram language model on the xinhua portion of the english gigaword corpus and the english part of the training data.', 'for tuning and testing, we use the nist mt 2003 evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert (Och, 2004) to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach ( #AUTHOR_TAG )']","['experiments are conducted on chinese - to - english translation.', 'the training data are the fbis corpus with approximately 7. 1 million chinese words and 9. 2 million english words.', 'we obtain the bidirectional word alignment with giza + +, and then adopt the grow - diag - final - and strategy to obtain the final symmetric alignment.', 'we train a 5gram language model on the xinhua portion of the english gigaword corpus and the english part of the training data.', 'for tuning and testing, we use the nist mt 2003 evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert (Och, 2004) to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach ( #AUTHOR_TAG )']",5
"['as s2t ).', 'the system is implemented based on ( #AUTHOR_TAG ) and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on ( #AUTHOR_TAG ) and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['as s2t ).', 'the system is implemented based on ( #AUTHOR_TAG ) and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on ( #AUTHOR_TAG ) and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules (Galley et al., 2004), and the rules of spmt model 1 (Galley et al., 2006) with phrases up to length l = 5 on the source side']",5
"['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', '#AUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u']","['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', '#AUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our']","['a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', '#AUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', '#AUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do']","['are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do']","['are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['.', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules ( #AUTHOR_TAG ) here to']","['sampler might reinforce the frequent alignment errors ( ae ), which would harm the translation model ( tm ).', 'actually, the frequent aes also greatly impair the conventional tm.', 'besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent aes.', 'thus, compared with the conventional tms, we believe that our final tm would not be worse due to aes.', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules ( #AUTHOR_TAG ) here to']","['.', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules ( #AUTHOR_TAG ) here to']","['sampler might reinforce the frequent alignment errors ( ae ), which would harm the translation model ( tm ).', 'actually, the frequent aes also greatly impair the conventional tm.', 'besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent aes.', 'thus, compared with the conventional tms, we believe that our final tm would not be worse due to aes.', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler']",5
"['as s2t ).', 'the system is implemented based on (Galley et al., 2006) and ).', 'in the system, we extract both the minimal ghkm rules ( #AUTHOR_TAG ), and the rules of spmt model 1 ( Galley et al. , 2006 ) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on (Galley et al., 2006) and ).', 'in the system, we extract both the minimal ghkm rules ( #AUTHOR_TAG ), and the rules of spmt model 1 ( Galley et al. , 2006 ) with phrases up to length l = 5 on the source side']","['as s2t ).', 'the system is implemented based on (Galley et al., 2006) and ).', 'in the system, we extract both the minimal ghkm rules ( #AUTHOR_TAG ), and the rules of spmt model 1 ( Galley et al. , 2006 ) with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on (Galley et al., 2006) and ).', 'in the system, we extract both the minimal ghkm rules ( #AUTHOR_TAG ), and the rules of spmt model 1 ( Galley et al. , 2006 ) with phrases up to length l = 5 on the source side']",5
"['##km algorithm ( #AUTHOR_TAG ), we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i.']","['obviously, towards an s - node for sampling, the two values of o would define two different u - trees.', 'using the ghkm algorithm ( #AUTHOR_TAG ), we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i. e., minimal ghkm translation rules ) of its own.', ""in the two derivations, the stsg rules defined by the two states include the one rooted at the s - node's lowest ancestor frontier node, and the one rooted at the s - node if it is a frontier node."", '']","[', the two values of o would define two different u - trees.', 'using the ghkm algorithm ( #AUTHOR_TAG ), we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i.']","['first gibbs operator, rotate, just works by sampling value of the oparameters, one at a time, and changing the u - tree accordingly.', 'for example, in figure 3 ( a ), the s - node is currently in the left vwdwho : hvdpsohwkhoriwklvqrghdqgli wkhvdpsohgydoxhriolvzhnhhswkhvwuxfwxuh unchanged, i. e., in the left state.', 'otherwise, we change its state to the right state o, and transform the u - tree to figure 3 obviously, towards an s - node for sampling, the two values of o would define two different u - trees.', 'using the ghkm algorithm ( #AUTHOR_TAG ), we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i. e., minimal ghkm translation rules ) of its own.', ""in the two derivations, the stsg rules defined by the two states include the one rooted at the s - node's lowest ancestor frontier node, and the one rooted at the s - node if it is a frontier node."", 'for instance, in figure 3 ( a ), as the s - node is not a frontier node, the left state ( o ) defines only one rule : using these stsg rules, the two derivations are evaluated as follows ( we use the value of o to denote the corresponding stsg derivation )']",5
"['; Quirk et al. , 2005 ; #AUTHOR_TAG, 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['; Quirk et al. , 2005 ; #AUTHOR_TAG, 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG, 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG, 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['#AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 ,  2009 ; #AUTHOR_TAG ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
['mt ( samt ) 11 system ( #AUTHOR_TAG ) respectively'],"['create the baseline system, we use the opensource joshua 4. 0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase - based ( hpb ) system, and a syntax - augmented mt ( samt ) 11 system ( #AUTHOR_TAG ) respectively']","['create the baseline system, we use the opensource joshua 4. 0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase - based ( hpb ) system, and a syntax - augmented mt ( samt ) 11 system ( #AUTHOR_TAG ) respectively']","['create the baseline system, we use the opensource joshua 4. 0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase - based ( hpb ) system, and a syntax - augmented mt ( samt ) 11 system ( #AUTHOR_TAG ) respectively']",5
"['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser ( #AUTHOR_TAG ).', 'then, we binarize the english parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser ( #AUTHOR_TAG ).', 'then, we binarize the english parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser ( #AUTHOR_TAG ).', 'then, we binarize the english parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser ( #AUTHOR_TAG ).', 'then, we binarize the english parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system']",5
"['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation ( #AUTHOR_TAG ; Zhang et al. , 2011 a )']","['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation ( #AUTHOR_TAG ; Zhang et al. , 2011 a )']","['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation ( #AUTHOR_TAG ; Zhang et al. , 2011 a )']","['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation ( #AUTHOR_TAG ; Zhang et al. , 2011 a )']",4
"['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG, 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG, 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011']","['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG, 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']","['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG, 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 ,  2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011 b )']",0
"['training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models ( #AUTHOR_TAG )']","['training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models ( #AUTHOR_TAG )']","['training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models ( #AUTHOR_TAG )']","[', for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of tree - bank resources for training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models ( #AUTHOR_TAG )']",0
"['#AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['#AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['#AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do']","['Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', 'Zollmann and Venugopal (2006) substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( str | frag ) in equation ( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( str | frag ) in equation ( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ), we define p ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar ( #AUTHOR_TAG ) 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']","['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar ( #AUTHOR_TAG ) 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']","['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar ( #AUTHOR_TAG ) 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']","['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar ( #AUTHOR_TAG ) 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']",1
"['effective unsupervised tree structures for tree - based translation models.', '#AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules']","['effective unsupervised tree structures for tree - based translation models.', '#AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules']","['focus on generating effective unsupervised tree structures for tree - based translation models.', '#AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules']","['unsupervised tree structure induction, de Nero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work (Zhai et al., 2012) designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008  utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', 'the obtained scfg is further used in a phrase - based and hierarchical phrase - based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a bayesian method to learn discontinuous scfg rules.', 'this study differs from their work because we concentrate on constructing tree structures for tree - based translation models.', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re - trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', '#AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan (Kobayashi et al., 1992).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search ( #AUTHOR_TAG ) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan (Kobayashi et al., 1992).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search ( #AUTHOR_TAG ) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan (Kobayashi et al., 1992).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search ( #AUTHOR_TAG ) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan (Kobayashi et al., 1992).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search ( #AUTHOR_TAG ) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']",0
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ).', '']","['responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ).', '']","['have been several efforts aimed at developing a domain - independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ).', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no work on domain - independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which wit handles well.', 'therefore incorporating those techniques remains as a future work']","['have been several efforts aimed at developing a domain - independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ).', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no work on domain - independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which wit handles well.', 'therefore incorporating those techniques remains as a future work']",3
"['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG']","['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG']","['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG']","['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG']",1
"['.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan ( #AUTHOR_TAG ).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search (Hirasawa et al., 1998) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan ( #AUTHOR_TAG ).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search (Hirasawa et al., 1998) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan ( #AUTHOR_TAG ).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search (Hirasawa et al., 1998) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti "" (Noda et al., 1998), or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan ( #AUTHOR_TAG ).', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search (Hirasawa et al., 1998) using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']",5
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( #AUTHOR_TAG b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather']","['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( #AUTHOR_TAG b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( Dohsaka et al. , 2000 ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']","['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( #AUTHOR_TAG b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( Dohsaka et al. , 2000 ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( #AUTHOR_TAG b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( Dohsaka et al. , 2000 ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']",2
"['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ).', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ).', 'the language generation module']","['can be covered.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ).', 'the language generation module']","['.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ).', 'the language generation module features common lisp functions, so there is no limitation on the description.', 'some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).', 'it is also possible to build a simple finite - state - model - based dialogue system using wit.', '']","['previous finite - state - model - based toolkits place many severe restrictions on domain descriptions, wit has enough descriptive power to build a variety of dialogue systems.', 'although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.', 'for example, it is possible to represent a discourse stack whose depth is limited.', 'recording some dialogue history is also possible.', 'since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ).', 'the language generation module features common lisp functions, so there is no limitation on the description.', 'some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).', 'it is also possible to build a simple finite - state - model - based dialogue system using wit.', 'states can be represented by dialogue phases in wit']",3
"['is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997)']","['is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997)']","['the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997)']","['the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997)']",0
"['##miation system ( #AUTHOR_TAG ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre -']","['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( Nakano et al. , 1999 b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( #AUTHOR_TAG ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']","['unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( Nakano et al. , 1999 b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( #AUTHOR_TAG ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system ( Nakano et al. , 1999 b ), a video - recording programming system, a schedule management system ( Nakano et al. , 1999 a ), and a weather infomiation system ( #AUTHOR_TAG ).', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']",2
"['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method ( #AUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain -']","['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method ( #AUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain - dependent system specifications into internal knowledge sources so that building systems is easier.', 'although wit requires more domaindependent specifications than finite - state - modelbased toolkits, wit - based systems are capable of taking full advantage of language processing technology.', 'wit has been implemented and used to build several spoken dialogue systems']","['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method ( #AUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain - dependent system specifications into internal knowledge sources so that building systems is easier.', 'although wit requires more domaindependent specifications than finite - state - modelbased toolkits, wit - based systems are capable of taking full advantage of language processing technology.', 'wit has been implemented and used to build several spoken dialogue systems']","['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method ( #AUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain - dependent system specifications into internal knowledge sources so that building systems is easier.', 'although wit requires more domaindependent specifications than finite - state - modelbased toolkits, wit - based systems are capable of taking full advantage of language processing technology.', 'wit has been implemented and used to build several spoken dialogue systems']",5
"['knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ).', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ).', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ).', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ).', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']",5
"['in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']","['in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']","['to dcg (Pereira and Warren, 1980) rules ; they can include logical variables and these variables can be bound when these rules are applied.', 'it is possible to add to the rules constraints that stipulate relationships that must hold among variables ( nakano, 199 i ), but we do not explain these constraints in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']","['roles are similar to dcg (Pereira and Warren, 1980) rules ; they can include logical variables and these variables can be bound when these rules are applied.', 'it is possible to add to the rules constraints that stipulate relationships that must hold among variables ( nakano, 199 i ), but we do not explain these constraints in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']",5
"['this end, several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ).', 'one is the cslu toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']","['this end, several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ).', 'one is the cslu toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']","['this end, several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ).', 'one is the cslu toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']","['this end, several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ).', 'one is the cslu toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']",0
"['##s the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search ) ( #AUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental']","['from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search ) ( #AUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental']","['##s the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search ) ( #AUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental understanding of user utterances that are not segmented into sentences prior to pars - ing by incrementally finding the most plausible sequence of sentences ( or significant utterances in the isss terms ) out of the possible sentence sequences for the input word sequence.', 'isss also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time']","['language understanding : module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search ) ( #AUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental understanding of user utterances that are not segmented into sentences prior to pars - ing by incrementally finding the most plausible sequence of sentences ( or significant utterances in the isss terms ) out of the possible sentence sequences for the input word sequence.', 'isss also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time']",5
"['. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']",1
"['the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by']","['the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', 'the synonym approach is equivalent to changing']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', ""the synonym approach is equivalent to changing all non - zero translation probabilities p ( w ~ [ wy )'s to 1 in our retrieyal function."", 'even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations']",1
"['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test ( #AUTHOR_TAG ) at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']","['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test ( #AUTHOR_TAG ) at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']","['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test ( #AUTHOR_TAG ) at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']","['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test ( #AUTHOR_TAG ) at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']",5
"['##sm ), ( #AUTHOR_TAG ).', 'we believe our approach is computationally']","['third approach to cross - lingual retrieval is to map queries and documents to some intermediate representation, e. g latent semantic indexing ( lsi ) ( Littman et al , 1998 ), or the general vector space model ( gvsm ), ( #AUTHOR_TAG ).', 'we believe our approach is computationally']","['##sm ), ( #AUTHOR_TAG ).', 'we believe our approach is computationally less costly than ( lsi and']","['third approach to cross - lingual retrieval is to map queries and documents to some intermediate representation, e. g latent semantic indexing ( lsi ) ( Littman et al , 1998 ), or the general vector space model ( gvsm ), ( #AUTHOR_TAG ).', 'we believe our approach is computationally less costly than ( lsi and gvsm ) and assumes less resources ( wordnet in Diekema et al., 1999)']",1
['## the transition probability a is 0. 7 using the em algorithm ( #AUTHOR_TAG ) on the trec4 ad - hoc query set'],['## the transition probability a is 0. 7 using the em algorithm ( #AUTHOR_TAG ) on the trec4 ad - hoc query set'],['## the transition probability a is 0. 7 using the em algorithm ( #AUTHOR_TAG ) on the trec4 ad - hoc query set'],['## the transition probability a is 0. 7 using the em algorithm ( #AUTHOR_TAG ) on the trec4 ad - hoc query set'],5
"['.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono - lingual m']",0
"['.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de Jong , 1999 ; #AUTHOR_TAG.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de Jong , 1999 ; #AUTHOR_TAG.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de Jong , 1999 ; #AUTHOR_TAG.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de Jong , 1999 ; #AUTHOR_TAG.', 'Sanderson, 1994  studied the issue of disarnbiguation for mono - lingual ir']",0
"['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ).', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']","['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ).', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']","['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ).', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']","['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ).', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']",1
"['the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros,']","['the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros,']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros,']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery (Callan et al, 1995) synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', ""the synonym approach is equivalent to changing all non - zero translation probabilities p ( w ~ [ wy )'s to 1 in our retrieyal function."", 'even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations']",1
"['this section we compare our approach with two other approaches.', 'one approach is "" simple substitution "", i. e., replacing a query term with all its translations and treating the translated query as a bag of words in mono - lingual retrieval.', 'suppose we have a simple query q = ( a, b ), the translations for a are al, a2, a3, and the translations for b are bl, b2. the translated query would be ( at, a2, a3, b ~, b2 ).', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur ( #AUTHOR_TAG )']","['this section we compare our approach with two other approaches.', 'one approach is "" simple substitution "", i. e., replacing a query term with all its translations and treating the translated query as a bag of words in mono - lingual retrieval.', 'suppose we have a simple query q = ( a, b ), the translations for a are al, a2, a3, and the translations for b are bl, b2. the translated query would be ( at, a2, a3, b ~, b2 ).', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur ( #AUTHOR_TAG )']","['this section we compare our approach with two other approaches.', 'one approach is "" simple substitution "", i. e., replacing a query term with all its translations and treating the translated query as a bag of words in mono - lingual retrieval.', 'suppose we have a simple query q = ( a, b ), the translations for a are al, a2, a3, and the translations for b are bl, b2. the translated query would be ( at, a2, a3, b ~, b2 ).', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur ( #AUTHOR_TAG )']","['this section we compare our approach with two other approaches.', 'one approach is "" simple substitution "", i. e., replacing a query term with all its translations and treating the translated query as a bag of words in mono - lingual retrieval.', 'suppose we have a simple query q = ( a, b ), the translations for a are al, a2, a3, and the translations for b are bl, b2. the translated query would be ( at, a2, a3, b ~, b2 ).', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur ( #AUTHOR_TAG )']",0
"['.', 'each english word has around 1. 5 translations on average.', 'a cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem spanish words.', 'one difference from the treatment of']","['from the internet ( http : / / www. activa. arrakis. es )', 'containing around 22, 000 english words ( 16, 000 english stems ) and processed it similarly.', 'each english word has around 1. 5 translations on average.', 'a cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem spanish words.', 'one difference from the treatment of']","[', 000 english stems ) and processed it similarly.', 'each english word has around 1. 5 translations on average.', 'a cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem spanish words.', 'one difference from the treatment of']","['spanish, we downloaded a bilingual english - spanish lexicon from the internet ( http : / / www. activa. arrakis. es )', 'containing around 22, 000 english words ( 16, 000 english stems ) and processed it similarly.', 'each english word has around 1. 5 translations on average.', 'a cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem spanish words.', 'one difference from the treatment of chinese is to include the english word as one of its own translations in addition to its spanish translations in the lexicon.', 'this is useful for translating proper nouns, which often have identical spellings in english and spanish but are routinely excluded from a lexicon']",5
"['studies which view lr as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']",1
"['studies which view lr as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']","['studies which view lr as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'our work has focused on cross - lingual retrieval']",1
"['technique is automatic discovery of translations from parallel or non - parallel corpora ( #AUTHOR_TAG ).', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora ( #AUTHOR_TAG ).', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora ( #AUTHOR_TAG ).', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora ( #AUTHOR_TAG ).', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus - specific vocabularies']",0
"['#AUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']","['#AUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']","['#AUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']","['#AUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']",5
"['( #AUTHOR_TAG ; Charniak , 1997 a']","['full parsers ( #AUTHOR_TAG ; Charniak , 1997 a']","['full parsers ( #AUTHOR_TAG ; Charniak , 1997 a']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['first is the one used in the chunking competition in conll - 2000 ( tjong kim #AUTHOR_TAG ).', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of    different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are :']","['first is the one used in the chunking competition in conll - 2000 ( tjong kim #AUTHOR_TAG ).', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of    different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are : adjective phrase ( adjp ), adverb phrase ( advp ), conjunction phrase ( conjp ), interjection phrase ( intj ), list marker ( lst ), noun phrase ( np ), preposition phrase ( pp ), particle ( prt ), subordinated clause ( sbar ), unlike coordinated phrase ( ucp ), verb phrase ( vp ).', '( see details']","['first is the one used in the chunking competition in conll - 2000 ( tjong kim #AUTHOR_TAG ).', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of    different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are :']","['first is the one used in the chunking competition in conll - 2000 ( tjong kim #AUTHOR_TAG ).', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of    different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are : adjective phrase ( adjp ), adverb phrase ( advp ), conjunction phrase ( conjp ), interjection phrase ( intj ), list marker ( lst ), noun phrase ( np ), preposition phrase ( pp ), particle ( prt ), subordinated clause ( sbar ), unlike coordinated phrase ( ucp ), verb phrase ( vp ).', '( see details in (Tjong Kim Sang and Buchholz, 2000).']",5
"['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunk']","['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years,']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997 a ; Charniak, 1997 b ; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns - syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000)']",0
"['( Collins , 1997 ; #AUTHOR_TAG a']","['full parsers ( Collins , 1997 ; #AUTHOR_TAG a']","['full parsers ( Collins , 1997 ; #AUTHOR_TAG a']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAG a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['Charniak , 1997 a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 )']","['Charniak , 1997 a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 )']","['Charniak , 1997 a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( Charniak , 1997 b ; Charniak , 1997 a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 )']",0
"['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( Collins , 1996 ; #AUTHOR_TAG ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( Collins , 1996 ; #AUTHOR_TAG ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( Collins , 1996 ; #AUTHOR_TAG ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5 (Collins, 1997)']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( Collins , 1996 ; #AUTHOR_TAG ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5 (Collins, 1997)']",5
"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( #AUTHOR_TAG b']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( #AUTHOR_TAG b']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( #AUTHOR_TAG b']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( #AUTHOR_TAG b ; Charniak , 1997 a ; Collins , 1997 ; Ratnaparkhi , 1997 )']",0
"['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( #AUTHOR_TAG ; Roth , 1998 ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( #AUTHOR_TAG ; Roth , 1998 ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( #AUTHOR_TAG ; Roth , 1998 ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( #AUTHOR_TAG ; Roth , 1998 ) is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunk']","['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years,']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997 a ; Charniak, 1997 b ; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns - syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000)']",0
"['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunk']","['a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years,']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997 a ; Charniak, 1997 b ; Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns - syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000)']",0
"[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ).', 'second, while']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ).', 'second, while']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ).', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', '']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ).', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to produce this level of analysis.', 'this can be augmented later if more information is available.', 'finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low - sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes']",0
"['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim #AUTHOR_TAG ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim #AUTHOR_TAG ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim #AUTHOR_TAG ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim #AUTHOR_TAG ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",5
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['Charniak , 1997 a ; Collins , 1997 ; #AUTHOR_TAG )']","['Charniak , 1997 a ; Collins , 1997 ; #AUTHOR_TAG )']","['Charniak , 1997 a ; Collins , 1997 ; #AUTHOR_TAG )']","[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time ( Charniak , 1997 b ; Charniak , 1997 a ; Collins , 1997 ; #AUTHOR_TAG )']",0
"['shallow parser used is the snow - based cscl parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim #AUTHOR_TAG ) terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']","['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim #AUTHOR_TAG ) terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']","['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim #AUTHOR_TAG ) terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']","['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim #AUTHOR_TAG ) terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']",5
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",2
"['by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further']","['by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further']","['on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further']","['on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint']",0
"['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( #AUTHOR_TAG ; Collins , 1997 ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( #AUTHOR_TAG ; Collins , 1997 ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( #AUTHOR_TAG ; Collins , 1997 ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5 (Collins, 1997)']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins ( #AUTHOR_TAG ; Collins , 1997 ) - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5 (Collins, 1997)']",5
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['shallow parser used is the snow - based cscl parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ).', 'snow (Carleson et al., 1999;Roth, 1998) is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi - class classifier that is specifically']","['shallow parser used is the snow - based cscl parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'snow ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
['be chunked as follows ( tjong kim #AUTHOR_TAG ) : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow'],['be chunked as follows ( tjong kim #AUTHOR_TAG ) : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow'],"['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abneys work (Abney, 1991), who has suggested to chunk sentences to base level phrases.', 'for example, the sentence he reckons the current account deficit will narrow to only $ 1. 8 billion in september.', 'would be chunked as follows ( tjong kim #AUTHOR_TAG ) : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow ] [ pp to ] [ np only $ 1. 8 billion ] [ pp in ] [ np september ]']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abneys work (Abney, 1991), who has suggested to chunk sentences to base level phrases.', 'for example, the sentence he reckons the current account deficit will narrow to only $ 1. 8 billion in september.', 'would be chunked as follows ( tjong kim #AUTHOR_TAG ) : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow ] [ pp to ] [ np only $ 1. 8 billion ] [ pp in ] [ np september ]']",0
"[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ).', 'second, while']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ).', 'second, while']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ).', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', '']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ).', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to produce this level of analysis.', 'this can be augmented later if more information is available.', 'finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low - sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes']",0
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']","['shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; Ratnaparkhi , 1997 ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['Charniak , 1997 b ; #AUTHOR_TAG ), significant progress has been made on the use of statistical learning methods to']","['Charniak , 1997 b ; #AUTHOR_TAG ), significant progress has been made on the use of statistical learning methods to']","['Charniak , 1997 b ; #AUTHOR_TAG ), significant progress has been made on the use of statistical learning methods to']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'a lot of recent work on shallow parsing has been influenced by abney\'s work (Abney, 1991), who has suggested to "" chunk "" sentences to base level phrases.', 'for example, the sentence "" he reckons the current account deficit will narrow to only $ 1. 8 billion in september. ""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000) : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997 a ; Charniak , 1997 b ; #AUTHOR_TAG ), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 )']",0
"['earlier versions of the snow based cscl were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time,']","['earlier versions of the snow based cscl were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",2
"['was done on the penn treebank ( #AUTHOR_TAG ) wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and Buchholz, 2000)']","['was done on the penn treebank ( #AUTHOR_TAG ) wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and Buchholz, 2000)']","['was done on the penn treebank ( #AUTHOR_TAG ) wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and Buchholz, 2000)']","['was done on the penn treebank ( #AUTHOR_TAG ) wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and Buchholz, 2000)']",5
"['hpsg ( #AUTHOR_TAG ).', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and']","['hpsg ( #AUTHOR_TAG ).', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equiva - lence between the original and obtained gram - mars.', 'other works (Kasper et al., 1995; Becker and Lopez, 2000) convert hpsg grammars']","['hpsg ( #AUTHOR_TAG ).', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and']","['##eisi et al. also translated ltag into hpsg ( #AUTHOR_TAG ).', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equiva - lence between the original and obtained gram - mars.', 'other works (Kasper et al., 1995; Becker and Lopez, 2000) convert hpsg grammars into ltag grammars.', 'however, given the greater ex - pressive power of hpsg, it is impossible to con - vert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capac - ity.', 'thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad - vantages']",1
"['have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ), and ones on programming / grammar - development environ - (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for']","['have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ), and ones on programming / grammar - development environ - (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for']","['have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ), and ones on programming / grammar - development environ - (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ), and ones on programming / grammar - development environ - (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['parser ( #AUTHOR_TAG ).', 'this result implies that parsing techniques for hp']","['Research Group, 2001), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser ( #AUTHOR_TAG ).', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the']","['Research Group, 2001), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser ( #AUTHOR_TAG ).', 'this result implies that parsing techniques for hp']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag Research Group, 2001), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser ( #AUTHOR_TAG ).', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",1
"['parser ( #AUTHOR_TAG ), c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord, 1994) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser ( #AUTHOR_TAG ), c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord, 1994) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser ( #AUTHOR_TAG ), c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord, 1994) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser ( #AUTHOR_TAG ), c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2 clearly shows that the hpsg parser is significantly faster than the ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper describes the detailed analysis on the factor of the difference of parsing performance']",1
"[') ( #AUTHOR_TAG ) by a method of grammar conversion.', 'the rental system automatically converts an fb - lt']","[') ( #AUTHOR_TAG ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a']","[') ( #AUTHOR_TAG ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hp']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( vijay - Shanker , 1987 ;  vijay - Shanker and Joshi , 1988 ) and head - driven phrase structure grammar ( hpsg ) ( #AUTHOR_TAG ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the lt']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english (']","['4 : adjunction tions called substitution and adjunction.', 'elementary trees are classified into two types, initial trees and auxiliary trees ( figure 2 ).', 'an elementary tree has at least one leaf node labeled with a terminal symbol called an anchor ( marked with  ).', 'in an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node ( marked with  ).', 'in an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes ( marked with ).', 'substitution replaces a substitution node with another initial tree ( figure 3 ).', 'adjunction grafts an auxiliary tree with the root node and foot node labeled u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag Research Group, 2001).', 'the xtag group (Doran et al., 2000) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and Candito, 2000) has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']","['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']","['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']","['( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']",0
"['##1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and head - driven phrase structure grammar (']","['such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and head - driven phrase structure grammar (']","['##1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and head - driven phrase structure grammar (']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and head - driven phrase structure grammar ( hpsg ) ( Pollard and Sag , 1994 ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
['##ag ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera'],['##ag ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera'],['##ag ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera'],['##ag ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],0
"[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project ( #AUTHOR_TAG ).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project ( #AUTHOR_TAG ).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project ( #AUTHOR_TAG ).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project ( #AUTHOR_TAG ).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']",0
"[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( #AUTHOR_TAG ), which is used in a high - accuracy japanese dependency analyzer ( Kanayama et al. , 2000 )']","['online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( #AUTHOR_TAG ), which is used in a high - accuracy japanese dependency analyzer ( Kanayama et al. , 2000 )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( #AUTHOR_TAG ), which is used in a high - accuracy japanese dependency analyzer ( Kanayama et al. , 2000 )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( #AUTHOR_TAG ), which is used in a high - accuracy japanese dependency analyzer ( Kanayama et al. , 2000 )']",0
"['parse (Tateisi et al., 1998).', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert hp']","['into hpsg lexical entries.', 'the derivation translator module takes hpsg parse (Tateisi et al., 1998).', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some']","['lexical entries.', 'the derivation translator module takes hpsg parse (Tateisi et al., 1998).', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hp']","['1 depicts a brief sketch of the rental system.', 'the system consists of the following four modules : tree converter, type hierarchy extractor, lexicon converter and derivation translator.', 'the tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in section 3. the type hierarchy extractor module extracts the symbols of the node, features, and feature values from the ltag elementary tree templates and lexicon, and construct the type hierarchy from them.', 'the lexicon converter module converts ltag elementary tree templates into hpsg lexical entries.', 'the derivation translator module takes hpsg parse (Tateisi et al., 1998).', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', 'thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages']",1
"[', english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( Mitsuishi et al. , 1998 ), which is used in a high - accuracy japanese dependency analyzer ( #AUTHOR_TAG )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( Mitsuishi et al. , 1998 ), which is used in a high - accuracy japanese dependency analyzer ( #AUTHOR_TAG )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( Mitsuishi et al. , 1998 ), which is used in a high - accuracy japanese dependency analyzer ( #AUTHOR_TAG )']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project (Flickinger, 2000).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese ( Mitsuishi et al. , 1998 ), which is used in a high - accuracy japanese dependency analyzer ( #AUTHOR_TAG )']",0
"['and id grammar rules, each of which is described with typed feature structures ( #AUTHOR_TAG ).', 'a']","['and id grammar rules, each of which is described with typed feature structures ( #AUTHOR_TAG ).', 'a']","['hpsg grammar consists of lexical entries and id grammar rules, each of which is described with typed feature structures ( #AUTHOR_TAG ).', 'a lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'an id grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'figure 6 illustrates an example of bottom - up parsing with an hpsg grammar.', 'first, lexical entries for "" can "" and "" run "" are unified respectively with the daughter feature structures']","['hpsg grammar consists of lexical entries and id grammar rules, each of which is described with typed feature structures ( #AUTHOR_TAG ).', 'a lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'an id grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'figure 6 illustrates an example of bottom - up parsing with an hpsg grammar.', 'first, lexical entries for "" can "" and "" run "" are unified respectively with the daughter feature structures']",0
"['research #AUTHOR_TAG ), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hp']","['english grammar ( the xtag research #AUTHOR_TAG ), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag research #AUTHOR_TAG ), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hp']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag research #AUTHOR_TAG ), which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",5
"['group ( #AUTHOR_TAG ) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', '']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fb - ltag ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag Research Group, 2001).', 'the xtag group ( #AUTHOR_TAG ) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', '']","['Research Group, 2001).', 'the xtag group ( #AUTHOR_TAG ) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', '']","['4 : adjunction tions called substitution and adjunction.', 'elementary trees are classified into two types, initial trees and auxiliary trees ( figure 2 ).', 'an elementary tree has at least one leaf node labeled with a terminal symbol called an anchor ( marked with  ).', 'in an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node ( marked with  ).', 'in an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes ( marked with ).', 'substitution replaces a substitution node with another initial tree ( figure 3 ).', 'adjunction grafts an auxiliary tree with the root node and foot node labeled u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fb - ltag ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag Research Group, 2001).', 'the xtag group ( #AUTHOR_TAG ) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and Candito, 2000) has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['grammar ( #AUTHOR_TAG ).', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the lt']","['1 ) ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) and head - driven phrase structure grammar ( hpsg ) (Pollard and Sag, 1994) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar ( #AUTHOR_TAG ).', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a']","[') (Pollard and Sag, 1994) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar ( #AUTHOR_TAG ).', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hp']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoin - ing grammar ( fb - ltag 1 ) ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) and head - driven phrase structure grammar ( hpsg ) (Pollard and Sag, 1994) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar ( #AUTHOR_TAG ).', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['parser ( #AUTHOR_TAG ), ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hp']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser ( #AUTHOR_TAG ), ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser ( #AUTHOR_TAG ), ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser ( #AUTHOR_TAG ), ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2 clearly shows that the hpsg parser is significantly faster than the ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper describes the detailed analysis on the factor of the difference of parsing performance']",1
"['( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the lt']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english (']","['4 : adjunction tions called substitution and adjunction.', 'elementary trees are classified into two types, initial trees and auxiliary trees ( figure 2 ).', 'an elementary tree has at least one leaf node labeled with a terminal symbol called an anchor ( marked with  ).', 'in an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node ( marked with  ).', 'in an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes ( marked with ).', 'substitution replaces a substitution node with another initial tree ( figure 3 ).', 'adjunction grafts an auxiliary tree with the root node and foot node labeled u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fbltag ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag Research Group, 2001).', 'the xtag group (Doran et al., 2000) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and Candito, 2000) has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['online ( lingo ) project ( #AUTHOR_TAG ).', 'in practical context,']","['online ( lingo ) project ( #AUTHOR_TAG ).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project ( #AUTHOR_TAG ).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project ( #AUTHOR_TAG ).', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project (Kay et al., 1994).', 'our group has developed a wide - coverage hpsg grammar for japanese (Mitsuishi et al., 1998), which is used in a high - accuracy japanese dependency analyzer (Kanayama et al., 2000)']",0
"['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['##sg parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ).', 'we applied our system to the']","['(Makino et al., 1998) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ).', 'we applied our system to the']","['##lfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ).', 'we applied our system to the xtag english grammar (']","['rental system is implemented in lil - fes (Makino et al., 1998) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ).', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus (Marcus et al., 1994) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",0
"['parsing.', 'another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']","['conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']","['are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van Noord, 1994) without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', 'table 2 clearly shows that the hpsg parser is significantly faster than the ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance']",0
"['and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus ( #AUTHOR_TAG ) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']","['and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus ( #AUTHOR_TAG ) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']","['and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus ( #AUTHOR_TAG ) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']","['rental system is implemented in lil - fes (Makino et al., 1998) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system (Nishida et al., 1999;.', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus ( #AUTHOR_TAG ) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",5
"['rental system is implemented in lilfes ( #AUTHOR_TAG ) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient']","['rental system is implemented in lilfes ( #AUTHOR_TAG ) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system (Nishida et al., 1999;.', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the']","['rental system is implemented in lilfes ( #AUTHOR_TAG ) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system (Nishida et al., 1999;.', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the']","['rental system is implemented in lilfes ( #AUTHOR_TAG ) 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system (Nishida et al., 1999;.', 'we applied our system to the xtag english grammar ( the xtag Research Group, 2001) 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus (Marcus et al., 1994) 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",5
"['##1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and head - driven phrase structure grammar (']","['such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and head - driven phrase structure grammar (']","['##1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and head - driven phrase structure grammar (']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and head - driven phrase structure grammar ( hpsg ) ( Pollard and Sag , 1994 ) by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming / grammar - development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ).', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ), and ones on programming / grammar - development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ), and ones on programming / grammar - development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', '']","['enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ), and ones on programming / grammar - development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ), ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ), and ones on programming / grammar - development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
['##sg ( #AUTHOR_TAG ) is the core portion of the rental system'],['to hpsg ( #AUTHOR_TAG ) is the core portion of the rental system'],['##sg ( #AUTHOR_TAG ) is the core portion of the rental system'],['grammar conversion from ltag to hpsg ( #AUTHOR_TAG ) is the core portion of the rental system'],0
"['research #AUTHOR_TAG ).', 'the']","['english grammar, a large - scale grammar for english ( the xtag research #AUTHOR_TAG ).', 'the']","['u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fb - ltag ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag research #AUTHOR_TAG ).', 'the xtag group (Doran et al., 2000) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', '']","['4 : adjunction tions called substitution and adjunction.', 'elementary trees are classified into two types, initial trees and auxiliary trees ( figure 2 ).', 'an elementary tree has at least one leaf node labeled with a terminal symbol called an anchor ( marked with  ).', 'in an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node ( marked with  ).', 'in an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes ( marked with ).', 'substitution replaces a substitution node with another initial tree ( figure 3 ).', 'adjunction grafts an auxiliary tree with the root node and foot node labeled u onto an internal node of another tree with the same symbol u ( figure 4 ).', 'fb - ltag ( vijay - Shanker, 1987; vijay - Shanker and Joshi, 1988) is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag research #AUTHOR_TAG ).', 'the xtag group (Doran et al., 2000) at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and Candito, 2000) has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['research #AUTHOR_TAG ) 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hp']","['english grammar ( the xtag research #AUTHOR_TAG ) 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the']","['on each computational framework and have never been shared among hpsg and ltag communities.', 'we applied our system to the xtag english grammar ( the xtag research #AUTHOR_TAG ) 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hp']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we applied our system to the xtag english grammar ( the xtag research #AUTHOR_TAG ) 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",5
"['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
"['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']","['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG )']",0
"['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance.', 'this has been reported for other languages,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998)']",0
"['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
"['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance.', 'this has been reported for other languages,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto,']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998)']",0
"['#AUTHOR_TAG ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', 'in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (']","['#AUTHOR_TAG ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", 'the']","['#AUTHOR_TAG ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', 'in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (']","['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( Choueka , 1990 ;  j [UNK] appinen and niemist [UNK] o, 1988 ; #AUTHOR_TAG ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", ""the goal is to map all occurring morphological variants to some canonical base forme. g.,'search'in the examples from above""]",0
"['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
"[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method']",1
"['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"['( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']","['( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']","['access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']","['has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 )']",0
"['html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model ( #AUTHOR_TAG ), with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']","['unbiased evaluation of our approach, we used a home - grown search engine ( implemented in the python script language ).', 'it crawls text / html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model ( #AUTHOR_TAG ), with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']","['html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model ( #AUTHOR_TAG ), with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']","['unbiased evaluation of our approach, we used a home - grown search engine ( implemented in the python script language ).', 'it crawls text / html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model ( #AUTHOR_TAG ), with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']",5
"['has been some controversy, at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
['- texts in an ir setting ( #AUTHOR_TAG )'],['is crucial for any attempt to cope adequately with medical free - texts in an ir setting ( #AUTHOR_TAG )'],"['. g., german ), often referred to as neo - classical compounding ( mc - Cray et al., 1988).', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting ( #AUTHOR_TAG )']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding ( mc - Cray et al., 1988).', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting ( #AUTHOR_TAG )']",0
"['thesaurus ( mesh, ( #AUTHOR_TAG )']","['thesaurus ( mesh, ( #AUTHOR_TAG )']","['of our synonym identifiers to a large medical thesaurus ( mesh, ( #AUTHOR_TAG )']","['##izing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'the gain is still not overwhelming.', ""with regard to orthographic normalization, we expected a higher performance benefit because of the well - known spelling problems for german medical terms of latin or greek origin ( such as in'zakum ','cakum ','zaekum ','caekum ','zaecum ','caecum')."", 'for our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'the same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided ( cf.', 'section 3 ).', 'in the layman queries, there were only few latin or greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'however, the experience with medical text retrieval ( especially on medical reports which exhibit a high rate of spelling variations ) shows that orthographic normalization is a desider - whereas the usefulness of subword indexing became evident, we could not provide sufficient evidence for synonym class indexing, so far.', 'however, synonym mapping is still incomplete in the current state of our subword dictionary.', 'a question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'we have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior ( for all layman and expert queries this method retrieved relevant documents, whereas word - based methods failed in 29. 6 % of the layman queries and 8 % of the expert queries, cf. figure 5 ).', 'it would be interesting to evaluate the retrieval effectiveness ( in terms of precision and recall ) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'this will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( mesh, ( #AUTHOR_TAG ) ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies (Hersh et al., 1995)']",3
"['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ).', 'the key for']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those']","['has been some controversy, at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ), about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997)']",0
"['Choueka , 1990 ; #AUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995']","['Choueka , 1990 ; #AUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995']","['Choueka , 1990 ; #AUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995']","['efforts required for performing morphologi - cal analysis vary from language to language.', 'for english, known for its limited number of inflection patterns, lexicon - free general - purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998)']",0
"['of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
"['of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ; Choueka, 1990;Popovic and Willett, 1992; ekmekcioglu et al., 1995 ; Hedlund et al., 2001;Pirkola, 2001).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 )']",0
['think of user - centered comparative studies ( #AUTHOR_TAG )'],['think of user - centered comparative studies ( #AUTHOR_TAG )'],"[', we may think of user - centered comparative studies ( #AUTHOR_TAG )']","['##izing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'the gain is still not overwhelming.', ""with regard to orthographic normalization, we expected a higher performance benefit because of the well - known spelling problems for german medical terms of latin or greek origin ( such as in'zakum ','cakum ','zaekum ','caekum ','zaecum ','caecum')."", 'for our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'the same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided ( cf.', 'section 3 ).', 'in the layman queries, there were only few latin or greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'however, the experience with medical text retrieval ( especially on medical reports which exhibit a high rate of spelling variations ) shows that orthographic normalization is a desider - whereas the usefulness of subword indexing became evident, we could not provide sufficient evidence for synonym class indexing, so far.', 'however, synonym mapping is still incomplete in the current state of our subword dictionary.', 'a question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'we have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior ( for all layman and expert queries this method retrieved relevant documents, whereas word - based methods failed in 29. 6 % of the layman queries and 8 % of the expert queries, cf. figure 5 ).', 'it would be interesting to evaluate the retrieval effectiveness ( in terms of precision and recall ) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'this will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( mesh, ( nlm, 2001 ) ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies ( #AUTHOR_TAG )']",3
"['( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']","['( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']","['access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']","['has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG )']",0
"['( #AUTHOR_TAG ), turns out to be infeasible, at least for german and related languages']","['( #AUTHOR_TAG ), turns out to be infeasible, at least for german and related languages']","['( #AUTHOR_TAG ), turns out to be infeasible, at least for german and related languages']","['one may argue that single - word compounds are quite rare in english ( which is not the case in the medical domain either ), this is certainly not true for german and other basically agglutinative languages known for excessive single - word nominal compounding.', 'this problem becomes even more pressing for technical sublanguages, such as medical german ( e. g., blut druck mess gera _ _ t translates to device for measuring blood pressure ).', 'the problem one faces from an ir point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'hence, enumerating morphological variants in a semi - automatically generated lexicon, such as proposed for french ( #AUTHOR_TAG ), turns out to be infeasible, at least for german and related languages']",0
"['Hedlund et al. , 2001 ; #AUTHOR_TAG ).', 'when it comes to a']","['Hedlund et al. , 2001 ; #AUTHOR_TAG ).', 'when it comes to a']","['Hedlund et al. , 2001 ; #AUTHOR_TAG ).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998)']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1''denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ;  ekmekc [UNK] ioglu et al., 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ).', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998)']",0
['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( #AUTHOR_TAG ; j [UNK]'],"['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( #AUTHOR_TAG ; j [UNK] appinen and niemist [UNK] o,']","['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( #AUTHOR_TAG ; j [UNK] appinen and niemist [UNK] o,']","['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system ( #AUTHOR_TAG ; j [UNK] appinen and niemist [UNK] o, 1988 ; Kraaij and Pohlmann , 1996 ), since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", ""the goal is to map all occurring morphological variants to some canonical base forme. g.,'search'in the examples from above""]",0
"['. g., german ), often referred to as neo - classical compounding ( #AUTHOR_TAG ).', 'while this is simply irrelevant for general']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding ( #AUTHOR_TAG ).', 'while this is simply irrelevant for general - purpose morphological']","['. g., german ), often referred to as neo - classical compounding ( #AUTHOR_TAG ).', 'while this is simply irrelevant for general']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding ( #AUTHOR_TAG ).', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting (Wolff, 1984)']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( #AUTHOR_TAG ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( #AUTHOR_TAG ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( #AUTHOR_TAG ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( #AUTHOR_TAG ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( #AUTHOR_TAG )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( #AUTHOR_TAG )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( #AUTHOR_TAG )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( #AUTHOR_TAG )']",0
"['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and']","['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation.', 'such correspondence is defined in a way that is able to handle some non - standard cases ( e. g.', 'non - projective correspondence )']","['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and']","['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation.', 'such correspondence is defined in a way that is able to handle some non - standard cases ( e. g.', 'non - projective correspondence )']",0
"['crossed dependencies ( #AUTHOR_TAG ).', 'crossed']","['crossed dependencies ( #AUTHOR_TAG ).', 'crossed']","['crossed dependencies ( #AUTHOR_TAG ).', 'crossed dependencies (Tang & Zaharin, 1995)']","['sstc is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'these features are very much desired in the design of an annotation scheme, in particular for the treatment of linguistic phenomena, which are non - standard, e. g. crossed dependencies ( #AUTHOR_TAG ).', 'crossed dependencies (Tang & Zaharin, 1995)']",0
"['string - tree correspondence ( sstc ) was introduced in #AUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is']","['string - tree correspondence ( sstc ) was introduced in #AUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is']","['string - tree correspondence ( sstc ) was introduced in #AUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is expressed by the sub - correspondences']","['this section, we stress on the fact that in order to describe natural language ( nl ) in a natural manner, three distinct components need to be expressed by the linguistic formalisms ; namely, the text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'actually, nl is not only a correspondence between different representation levels, as stressed by mtt postulates, but also a sub - correspondence between them.', 'for instance, between the string in a language and its representation tree structure, it is important to specify the sub - correspondences between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation in nlp.', 'it is well known that many linguistic constructions are not projective ( e. g.', 'scrambling, cross serial dependencies, etc. ).', 'hence, it is very much desired to define the correspondence in a way to be able to handle the non - standard cases ( e. g.', 'non - projective correspondence ), see figure 1.', 'towards this aim, a flexible annotation structure called structured string - tree correspondence ( sstc ) was introduced in #AUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is expressed by the sub - correspondences recorded as part of a sstc']",0
"['.', '#AUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","[', what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""the proposed s - sstc annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages'structures."", 's - sstc very well suited for the construction of a bkb, which is needed for the ebmt applications.', '#AUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","[', what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""the proposed s - sstc annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages'structures."", 's - sstc very well suited for the construction of a bkb, which is needed for the ebmt applications.', '#AUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","[', what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""the proposed s - sstc annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages'structures."", 's - sstc very well suited for the construction of a bkb, which is needed for the ebmt applications.', '#AUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( #AUTHOR_TAG ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( #AUTHOR_TAG ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( #AUTHOR_TAG ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( #AUTHOR_TAG ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( Shieber , 1994 ), ( #AUTHOR_TAG ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this']","['definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( Shieber , 1994 ), ( #AUTHOR_TAG ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this']","['##ille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( Shieber , 1994 ), ( #AUTHOR_TAG ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples (Shieber, 1994)']","['idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'the use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'for example, synchronous tree adjoining grammar ( s - tag ) can be used to relate tags for two different languages, for example, for the purpose of immediate structural translation in machine translation ( abeille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( Shieber , 1994 ), ( #AUTHOR_TAG ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples (Shieber, 1994)']",0
"[') is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ).', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']","[') is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ).', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']","[') is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ).', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']","['the meaning - text theory ( mtt ) 1 point of view, natural language ( nl ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ).', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( #AUTHOR_TAG ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( #AUTHOR_TAG ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( #AUTHOR_TAG ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( #AUTHOR_TAG ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g. #AUTHOR_TAG cases ).', 'Shieber (1994) cases ).', '']","['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g. #AUTHOR_TAG cases ).', 'Shieber (1994) cases ).', '']","['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g. #AUTHOR_TAG cases ).', 'Shieber (1994) cases ).', '']","['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g. #AUTHOR_TAG cases ).', 'Shieber (1994) cases ).', 'due to lack of space we will only brief on some of these non - standard cases without going into the details']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( #AUTHOR_TAG ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( #AUTHOR_TAG ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( #AUTHOR_TAG ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( #AUTHOR_TAG ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ), such as the relation between syntax and semantic']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ), such as the relation between syntax and semantic']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ), such as the relation between syntax and semantic']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ), such as the relation between syntax and semantic']",0
"[').', 'for instance, when building translation units in ebmt approaches ( Richardson et al. , 2001 ), ( Aramaki , 2001 ), ( al Adhaileh & Tang , 1999 ), ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural']","['can be used to license only the linguistically meaningful synchronous correspondences between the two sstcs of the s - sstc ( i. e. between the two languages ).', ""for instance, when building translation units in ebmt approaches ( Richardson et al. , 2001 ), ( Aramaki , 2001 ), ( al Adhaileh & Tang , 1999 ), ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules'extraction from parallel parsed""]","['between the two languages ).', ""for instance, when building translation units in ebmt approaches ( Richardson et al. , 2001 ), ( Aramaki , 2001 ), ( al Adhaileh & Tang , 1999 ), ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules'extraction from parallel parse""]","['are governed by the following constraints :.', 'this means allowing one - to - one, one - to - many and many - to - many, but the mappings do not overlap.', 'note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two sstcs of the s - sstc ( i. e. between the two languages ).', ""for instance, when building translation units in ebmt approaches ( Richardson et al. , 2001 ), ( Aramaki , 2001 ), ( al Adhaileh & Tang , 1999 ), ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules'extraction from parallel parsed""]",0
"['learning ( #AUTHOR_TAG ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( #AUTHOR_TAG ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e.', 'synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG )']",0
"['. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see #AUTHOR_TAG']","['in figure 2, describes how the sstc structure treats some non - standard linguistic phenomena.', 'the particle "" up "" is featurised into the verb "" pick "" and in discontinuous manner ( e. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see #AUTHOR_TAG']","['. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see #AUTHOR_TAG']","['case depicted in figure 2, describes how the sstc structure treats some non - standard linguistic phenomena.', 'the particle "" up "" is featurised into the verb "" pick "" and in discontinuous manner ( e. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see #AUTHOR_TAG']",0
"['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( #AUTHOR_TAG )']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( #AUTHOR_TAG )']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( #AUTHOR_TAG )']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ).', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( Sato , 1991 ), ( #AUTHOR_TAG ), ( al - Adhaileh & Tang , 1999 )']",0
"['languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( #AUTHOR_TAG ), ( Harbusch & Poller , 2000 ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this']","['example, for the purpose of immediate structural translation in machine translation ( abeille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( #AUTHOR_TAG ), ( Harbusch & Poller , 2000 ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this']","['##ille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( #AUTHOR_TAG ), ( Harbusch & Poller , 2000 ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples (Shieber, 1994)']","['idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'the use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'for example, synchronous tree adjoining grammar ( s - tag ) can be used to relate tags for two different languages, for example, for the purpose of immediate structural translation in machine translation ( abeille et al., 1990 ), (Harbusch & Poller,1996), or for relating a syntactic tag and semantic one for the same language (Shieber & Schabes,1990).', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal ( #AUTHOR_TAG ), ( Harbusch & Poller , 2000 ).', 'as a result, Shieber (1994) propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples (Shieber, 1994)']",0
"['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( #AUTHOR_TAG ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( #AUTHOR_TAG ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","[', such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( #AUTHOR_TAG ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ), ( Aramaki et al. , 2001 ), ( Watanabe et al. , 2000 ), ( Meyers et al. , 2000 ), ( Matsumoto et al. , 1993 ), ( kaji et al., 1992 ), and example - base machine translation ebmt3 ( Sato & Nagao , 1990 ), ( #AUTHOR_TAG ), ( Richardson et al. , 2001 ), ( al - Adhaileh & Tang , 1999 )']",0
"['#AUTHOR_TAG.', '']","['#AUTHOR_TAG.', '']","['#AUTHOR_TAG.', '']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
"['##aword corpus ( linguistic data #AUTHOR_TAG ).', 'recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text']","['the english gigaword corpus ( linguistic data #AUTHOR_TAG ).', 'recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","['the english gigaword corpus ( linguistic data #AUTHOR_TAG ).', 'recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus ( linguistic data #AUTHOR_TAG ).', 'recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['final interface we intend to implement is a collection of web services for nlp.', 'a web service provides a remote procedure that can be called using xml based encodings ( xmlrpc or soap ) of function names, arguments and results transmitted via internet protocols such as http.', 'systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with wsdl and uddi.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']",0
"['by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning ( Dagan and Engelson ,1995 ) and co - training ( #AUTHOR_TAG ).', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']","['by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning ( Dagan and Engelson ,1995 ) and co - training ( #AUTHOR_TAG ).', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']","['earlier, there are two main requirements of the system that are covered by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning ( Dagan and Engelson ,1995 ) and co - training ( #AUTHOR_TAG ).', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']","['discussed earlier, there are two main requirements of the system that are covered by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning ( Dagan and Engelson ,1995 ) and co - training ( #AUTHOR_TAG ).', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']",0
"[') for manual annotation ( e. g. general architecture for text engineering ( gate ) ( #AUTHOR_TAG ) and the alembic workbench ( Day et al. , 1997 ) ) as well as nlp tools and resources that can be manipulated from the']","[') for manual annotation ( e. g. general architecture for text engineering ( gate ) ( #AUTHOR_TAG ) and the alembic workbench ( Day et al. , 1997 ) ) as well as nlp tools and resources that can be manipulated from the']","[') for manual annotation ( e. g. general architecture for text engineering ( gate ) ( #AUTHOR_TAG ) and the alembic workbench ( Day et al. , 1997 ) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g. general architecture for text engineering ( gate ) ( #AUTHOR_TAG ) and the alembic workbench ( Day et al. , 1997 ) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['final interface we intend to implement is a collection of web services for nlp.', 'a web service provides a remote procedure that can be called using xml based encodings ( xmlrpc or soap ) of function names, arguments and results transmitted via internet protocols such as http.', 'systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with wsdl and uddi.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ).', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']",0
['an efficient version of the mxpost pos tagger ( #AUTHOR_TAG ) will simply involve composing and config'],"['an efficient version of the mxpost pos tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component,']","[', implementing an efficient version of the mxpost pos tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component,']","['generative programming approach to nlp infrastructure development will allow tools such as sentence boundary detectors, pos taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'for instance, implementing an efficient version of the mxpost pos tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component']",3
"['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt,']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt,']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster training times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster training times when we move to conjugate gradient methods']",4
"['implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ).', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t,']","['implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ).', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t,']","['implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ).', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t, the fastest existing p o s tagger.', 'these tools use a highly optimised g i s imple - mentation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster train - ing times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ).', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t, the fastest existing p o s tagger.', 'these tools use a highly optimised g i s imple - mentation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster train - ing times when we move to conjugate gradient methods']",4
"['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools (Mikheev et al., 1999;Grover et al., 2000) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools (Mikheev et al., 1999;Grover et al., 2000) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ).', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['##aword corpus (Linguistic Data Consortium, 2003).', 'recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","['. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"['will be manually annotated.', 'for example, 10 million words of the american national corpus ( Ide et al. , 2002 ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( #AUTHOR_TAG ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['will be manually annotated.', 'for example, 10 million words of the american national corpus ( Ide et al. , 2002 ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( #AUTHOR_TAG ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus ( Ide et al. , 2002 ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( #AUTHOR_TAG ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus ( Ide et al. , 2002 ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( #AUTHOR_TAG ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']",0
"['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp ( #AUTHOR_TAG )']","['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp ( #AUTHOR_TAG )']","['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp ( #AUTHOR_TAG )']","['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp ( #AUTHOR_TAG )']",2
"['. g. general architecture for text engineering ( gate ) ( Cunningham et al. , 1997 ) and the alembic workbench ( #AUTHOR_TAG ) ) as well as nlp tools and resources that can be manipulated from the']","[') for manual annotation ( e. g. general architecture for text engineering ( gate ) ( Cunningham et al. , 1997 ) and the alembic workbench ( #AUTHOR_TAG ) ) as well as nlp tools and resources that can be manipulated from the']","['. g. general architecture for text engineering ( gate ) ( Cunningham et al. , 1997 ) and the alembic workbench ( #AUTHOR_TAG ) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g. general architecture for text engineering ( gate ) ( Cunningham et al. , 1997 ) and the alembic workbench ( #AUTHOR_TAG ) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"[', gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly config']","[', gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly configurable and']","[', gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly config']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g.', 'general architecture for text engineering ( gate ) (Cunningham et al., 1997) and the alembic workbench (Day et al., 1997) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ).', 'gate goes beyond earlier systems by using a component - based infrastructure (Cunningham, 2000) which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['engineering research on generative programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for']","['engineering research on generative programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for nlp will provide high performance 1 components inspired by generative programming principles']","['engineering research on generative programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for']","['engineering research on generative programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for nlp will provide high performance 1 components inspired by generative programming principles']",0
"['will be manually annotated.', 'for example, 10 million words of the american national corpus ( #AUTHOR_TAG ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( Marcus et al. , 1993 ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['will be manually annotated.', 'for example, 10 million words of the american national corpus ( #AUTHOR_TAG ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( Marcus et al. , 1993 ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus ( #AUTHOR_TAG ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( Marcus et al. , 1993 ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus ( #AUTHOR_TAG ) will have manually corrected pos tags, a tenfold increase over the penn treebank ( Marcus et al. , 1993 ), currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']",0
"['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state,']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state,']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ), and for the dynamic version we will use configuration classes']","['infrastructure will be implemented in c / c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ), and for the dynamic version we will use configuration classes']","['c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ), and for the dynamic version we will use configuration classes']","['infrastructure will be implemented in c / c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ), and for the dynamic version we will use configuration classes']",5
"['##aword corpus (Linguistic Data Consortium, 2003).', 'recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","['. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus (Linguistic Data Consortium, 2003).', 'recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state,']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state,']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['be interchangeable : transformation - based learning ( tbl ) ( #AUTHOR_TAG ) and memory - based learning ( mbl ) ( Daelemans et al. , 2002 ) have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka (Witten and Frank, 1999)']","['be interchangeable : transformation - based learning ( tbl ) ( #AUTHOR_TAG ) and memory - based learning ( mbl ) ( Daelemans et al. , 2002 ) have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka (Witten and Frank, 1999)']","['learning methods should be interchangeable : transformation - based learning ( tbl ) ( #AUTHOR_TAG ) and memory - based learning ( mbl ) ( Daelemans et al. , 2002 ) have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka (Witten and Frank, 1999)']","['learning methods should be interchangeable : transformation - based learning ( tbl ) ( #AUTHOR_TAG ) and memory - based learning ( mbl ) ( Daelemans et al. , 2002 ) have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka (Witten and Frank, 1999)']",4
"['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( #AUTHOR_TAG ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( #AUTHOR_TAG ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( #AUTHOR_TAG ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( #AUTHOR_TAG ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ).', 'an example of using the python tagger interface is shown in figure 1']",0
"['fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing ( #AUTHOR_TAG ).', 'we expect even faster training times when we move to conjugate gradient methods']","['fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing ( #AUTHOR_TAG ).', 'we expect even faster training times when we move to conjugate gradient methods']","['##an and.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing ( #AUTHOR_TAG ).', 'we expect even faster training times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging ( curran and.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing ( #AUTHOR_TAG ).', 'we expect even faster training times when we move to conjugate gradient methods']",5
"['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( #AUTHOR_TAG ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( #AUTHOR_TAG ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( #AUTHOR_TAG ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging ( #AUTHOR_TAG ).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['##tk ) is a package of nlp components implemented in python ( #AUTHOR_TAG ).', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python ( #AUTHOR_TAG ).', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","['##tk ) is a package of nlp components implemented in python ( #AUTHOR_TAG ).', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python ( #AUTHOR_TAG ).', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']",0
"['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt,']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt,']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster training times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing (Chen and Rosenfeld, 1999).', 'we expect even faster training times when we move to conjugate gradient methods']",4
"['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ).', 'an example of using the python tagger interface is shown in figure 1']",0
"['very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit (Ngai and Florian, 2001) which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'the tnt pos tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure ( #AUTHOR_TAG ) which the gui is built on top of.', 'this allows components to be highly config']","['editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure ( #AUTHOR_TAG ) which the gui is built on top of.', 'this allows components to be highly configurable and']","[', gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure ( #AUTHOR_TAG ) which the gui is built on top of.', 'this allows components to be highly config']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g.', 'general architecture for text engineering ( gate ) (Cunningham et al., 1997) and the alembic workbench (Day et al., 1997) ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'gate goes beyond earlier systems by using a component - based infrastructure ( #AUTHOR_TAG ) which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"[', ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a head']","[', ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a head']","[', ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a head driven phrase structure grammar (']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance, ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
"['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).', 'techniques']",0
"['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","[', (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance, (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","[', (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance, (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['a metagrammar in the sense of ( #AUTHOR_TAG ).', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic']","['a metagrammar in the sense of ( #AUTHOR_TAG ).', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic']","['.', 'to address this problem, we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ).', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', '']","['intercategorial synonymic links.', ""a first investigation of anne abeille's tag for french suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward."", 'for instance, as figures 3, 4 and 5 show, the ftag trees assigned on syntactic grounds by anne abeille ftag to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ).', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', 'for instance, there will be a class novn1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'but additionnally there will be semantic classes such as, "" binary predicate of semantic type x "" which will be associated with the relevant syntactic classes for instance, novn1 ( the class of transitive verbs with nominal arguments ), binary npred ( the class of binary predicative nouns ), novsupnn1, the class of support verb constructions taking two nominal arguments.', 'by further associating semantic units ( e. g., "" cost "" ) with the appropriate semantic classes ( e. g., "" binary predicate of semantic type x "" ), we can in this way capture both intra and intercategorial paraphrasing links in a general way']",3
"['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","[', (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance, (Lin and Pantel, 2001) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c. #AUTHOR_TAG ).', 'Johnson et al., 2002).', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c. #AUTHOR_TAG ).', 'Johnson et al., 2002).', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c. #AUTHOR_TAG ).', 'Johnson et al., 2002).', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c. #AUTHOR_TAG ).', 'Johnson et al., 2002).', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']",5
"[', ( #AUTHOR_TAG ) acquire two - argument']","[', ( #AUTHOR_TAG ) acquire two - argument']","[', ( #AUTHOR_TAG ) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance, ( #AUTHOR_TAG ) acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ).', 'techniques']","['domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ).', 'techniques']",3
['test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG )'],['test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG )'],"['( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the par - seval lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG )']","['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? )', 'and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the par - seval lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases reflects the paraphrase typology.', 'in a first phase, we concentrate on simple, non - recursive predicate / argument structure.', 'given such a structure, the construction and annotation of a test item proceeds as follows']",1
"['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file']","['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? ) and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file']","['( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987)']","['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? ) and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987) ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998)']",0
"['language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']","['alternations are partially described in ( saint - Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']","['Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( saint - Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns']",3
"['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction.', 'the meta']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. )']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. ) encoded in the grammar basic units.', 'this specification is then compiled to automatically produce a specific grammar']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. ) encoded in the grammar basic units.', 'this specification is then compiled to automatically produce a specific grammar']",5
['( #AUTHOR_TAG ) can furthermore be resort'],"['( #AUTHOR_TAG ) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( saint - Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables ( #AUTHOR_TAG ) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( saint - Dizier, 1999) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables ( #AUTHOR_TAG ) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']",3
"['( #AUTHOR_TAG ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typ']","['( #AUTHOR_TAG ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases']","['and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases']","['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? )', 'and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the par - seval lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ).', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases reflects the paraphrase typology.', 'in a first phase, we concentrate on simple, non - recursive predicate / argument structure.', 'given such a structure, the construction and annotation of a test item proceeds as follows']",0
"['domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ).', 'techniques']","['domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ).', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ).', 'techniques']",3
['phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip lexical functional grammar ( lfg )'],['phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip lexical functional grammar ( lfg )'],"[', ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip lexical functional grammar ( lfg )']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance, ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
['construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than - - as is more'],['construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than - - as is more'],['construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than - - as is more'],"['construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than - - as is more common in tag - - from the derivation tree.', 'this is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'the association between tree nodes and unification variables encodes the syntax / semantics interface - it specifies which node in the tree provides the value for which variable in the final semantic representation']",0
"[', ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a head']","[', ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a head']","[', ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a head driven phrase structure grammar (']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance, ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
"['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations']","['language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ).', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['shuffling paraphrases, french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs']","['shuffling paraphrases, french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']","['shuffling paraphrases, french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns']",0
"['arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the']","['arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root ""']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root "" and "" term "" interchangeably to refer to canonical forms obtained through this root extraction process']",4
"['arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the']","['arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root ""']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in (El Kourdi, 2004).', 'it compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ), with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root "" and "" term "" interchangeably to refer to canonical forms obtained through this root extraction process']",4
"[') ( #AUTHOR_TAG ), minimum description length principal ( Lang , 1995 ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","[') ( #AUTHOR_TAG ), minimum description length principal ( Lang , 1995 ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","[') ( #AUTHOR_TAG ), minimum description length principal ( Lang , 1995 ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) ( #AUTHOR_TAG ), minimum description length principal ( Lang , 1995 ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['\' s categorizer "" (Sakhr, 2004).', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization (Mitchell, 1997).', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in ( #AUTHOR_TAG ), which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in (Yahyaoui, 2001) by experiment']","['arabic, one automatic categorizer has been reported to have been put under operational use to classify arabic documents ; it is referred to as "" sakhr\'s categorizer "" (Sakhr, 2004).', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization (Mitchell, 1997).', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in ( #AUTHOR_TAG ), which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest arabic site on the web : aljazeera. net']","['\' s categorizer "" (Sakhr, 2004).', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization (Mitchell, 1997).', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in ( #AUTHOR_TAG ), which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in (Yahyaoui, 2001) by experiment']","['arabic, one automatic categorizer has been reported to have been put under operational use to classify arabic documents ; it is referred to as "" sakhr\'s categorizer "" (Sakhr, 2004).', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization (Mitchell, 1997).', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in ( #AUTHOR_TAG ), which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest arabic site on the web : aljazeera. net']",2
"['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in ( #AUTHOR_TAG ).', 'in this work, the average accuracy over all categories is : 68. 78 % in cross validation and 62 % in evaluation set experiments.', 'the corresponding performances in (Yahyaoui, 2001) are 75. 6 % and 50 %, respectively.', 'thus, the overall performance ( including cross validation and evaluation set experiments ) in this work is comparable to that in (Yahyaoui, 2001).', 'this offers some indication that the performance of nb algorithm in classifying arabic documents is not sensitive to the arabic root extraction algorithm.', '']","['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in ( #AUTHOR_TAG ).', 'in this work, the average accuracy over all categories is : 68. 78 % in cross validation and 62 % in evaluation set experiments.', 'the corresponding performances in (Yahyaoui, 2001) are 75. 6 % and 50 %, respectively.', 'thus, the overall performance ( including cross validation and evaluation set experiments ) in this work is comparable to that in (Yahyaoui, 2001).', 'this offers some indication that the performance of nb algorithm in classifying arabic documents is not sensitive to the arabic root extraction algorithm.', '']","['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in ( #AUTHOR_TAG ).', 'in this work, the average accuracy over all categories is : 68. 78 % in cross validation and 62 % in evaluation set experiments.', 'the corresponding performances in (Yahyaoui, 2001) are 75. 6 % and 50 %, respectively.', 'thus, the overall performance ( including cross validation and evaluation set experiments ) in this work is comparable to that in (Yahyaoui, 2001).', 'this offers some indication that the performance of nb algorithm in classifying arabic documents is not sensitive to the arabic root extraction algorithm.', 'future work will be directed']","['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in ( #AUTHOR_TAG ).', 'in this work, the average accuracy over all categories is : 68. 78 % in cross validation and 62 % in evaluation set experiments.', 'the corresponding performances in (Yahyaoui, 2001) are 75. 6 % and 50 %, respectively.', 'thus, the overall performance ( including cross validation and evaluation set experiments ) in this work is comparable to that in (Yahyaoui, 2001).', 'this offers some indication that the performance of nb algorithm in classifying arabic documents is not sensitive to the arabic root extraction algorithm.', 'future work will be directed at experimenting with other root extraction algorithms.', ""further improvement of nb's performance may be effected by using unlabeled documents ; e. g., em has been used successfully for this purpose in ( nigam et al., 200 ), where em has increased the classification accuracy by 30 % for classifying english documents""]",1
"['good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ).', 'more recently, (Sebastiani, 2002) has performed a good survey of document categorization ; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004)']","['good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ).', 'more recently, (Sebastiani, 2002) has performed a good survey of document categorization ; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004)']","['good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ).', 'more recently, (Sebastiani, 2002) has performed a good survey of document categorization ; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004)']","['good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ).', 'more recently, (Sebastiani, 2002) has performed a good survey of document categorization ; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004)']",0
"['will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language ( #AUTHOR_TAG ), and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this']","['will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language ( #AUTHOR_TAG ), and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this']","['will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language ( #AUTHOR_TAG ), and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this processing is not straightforward : it necessitates expert knowledge in arabic language word morphology (']","['arabic, however, the use of stems will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language ( #AUTHOR_TAG ), and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this processing is not straightforward : it necessitates expert knowledge in arabic language word morphology ( al - Shalabi and Evens, 1998).', 'as an example, two close roots ( i. e., roots made of the same letters ), but semantically different, can yield the same infix form thus creating ambiguity']",0
"[') (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","[') (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']",0
"['as a way to cope with such a problem.', 'automatic text ( or document ) categorization attempts to replace and save human effort required in performing manual categorization.', 'it consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'as such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre - specified subject themes.', 'automatic text categorization has been used in search engines, digital library systems, and document management systems ( #AUTHOR_TAG ).', 'such applications have included electronic email filtering, newsgroups classification, and survey']","['as a way to cope with such a problem.', 'automatic text ( or document ) categorization attempts to replace and save human effort required in performing manual categorization.', 'it consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'as such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre - specified subject themes.', 'automatic text categorization has been used in search engines, digital library systems, and document management systems ( #AUTHOR_TAG ).', 'such applications have included electronic email filtering, newsgroups classification, and survey']","['the needs of different end users.', 'to this end, automatic text categorization has emerged as a way to cope with such a problem.', 'automatic text ( or document ) categorization attempts to replace and save human effort required in performing manual categorization.', 'it consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'as such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre - specified subject themes.', 'automatic text categorization has been used in search engines, digital library systems, and document management systems ( #AUTHOR_TAG ).', 'such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'barq']","['the explosive growth of text documents on the web, relevant information retrieval has become a crucial task to satisfy the needs of different end users.', 'to this end, automatic text categorization has emerged as a way to cope with such a problem.', 'automatic text ( or document ) categorization attempts to replace and save human effort required in performing manual categorization.', 'it consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'as such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre - specified subject themes.', 'automatic text categorization has been used in search engines, digital library systems, and document management systems ( #AUTHOR_TAG ).', 'such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).', 'in this paper, nb which is a statistical machine learning algorithm is used to learn to classify non - vocalized 1 arabic web text documents']",0
"[') (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in ( #AUTHOR_TAG ).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in ( #AUTHOR_TAG ).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","[') (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in ( #AUTHOR_TAG ).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in ( #AUTHOR_TAG ).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",1
"['n is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']","['n is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']","['n is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']","['the tf measurement concerns the importance of a term in a given document, idf seeks to measure the relative importance of a term in a collection of documents.', 'the importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'tf is given by tf d, t, and it denotes frequency of term t in document d. idf is given by idf t = log ( n / df t ), where n is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']",4
"[') ( Tzeras and Hartman , 1993 ), minimum description length principal ( #AUTHOR_TAG ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","[') ( Tzeras and Hartman , 1993 ), minimum description length principal ( #AUTHOR_TAG ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","[') ( Tzeras and Hartman , 1993 ), minimum description length principal ( #AUTHOR_TAG ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) ( Tzeras and Hartman , 1993 ), minimum description length principal ( #AUTHOR_TAG ), and the x2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in ( #AUTHOR_TAG ), ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ), respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in ( #AUTHOR_TAG ), ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ), respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in ( #AUTHOR_TAG ), ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ), respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in ( #AUTHOR_TAG ), ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ), respectively']",0
"['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ).', '']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig ) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the  2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between df, ig and the  2 statistic for a term.', 'on the other hand, (Rogati and Yang, 2002) reports the  2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ).', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ), ( Crammer and Singer , 2003 ), and ( Lewis et al. , 2004 )']",0
"['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example, ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example, ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example, ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example, ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']",0
"['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( Oueslati , 1999 ) or, more frequently, on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG,']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( Oueslati , 1999 ) or, more frequently, on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG,']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( Oueslati , 1999 ) or, more frequently, on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG,']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( Oueslati , 1999 ) or, more frequently, on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG, for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']",1
"['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles ( #AUTHOR_TAG ).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles ( #AUTHOR_TAG ).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles ( #AUTHOR_TAG ).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles ( #AUTHOR_TAG ).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']",5
"['number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG,']","['number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG,']","['number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG,']","['number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierentiated']",0
"['on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use, as a starting point, a number of identical characters']",0
"['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles (Lemay et al., 2004).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles (Lemay et al., 2004).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles (Lemay et al., 2004).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles (Lemay et al., 2004).', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']",5
"['on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use, as a starting point, a number of identical characters']",0
"['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993;  kilgarri and Tugwell, 2001,  for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993;  kilgarri and Tugwell, 2001,  for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993;  kilgarri and Tugwell, 2001,  for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist ) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993;  kilgarri and Tugwell, 2001,  for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']",1
"['##res is based on a machine learning technique, inductive logic programming ( ilp ) ( #AUTHOR_TAG ), which infers general morpho - syntactic patterns from a set of examples ( this set is']","['##res is based on a machine learning technique, inductive logic programming ( ilp ) ( #AUTHOR_TAG ), which infers general morpho - syntactic patterns from a set of examples ( this set is']","['##res is based on a machine learning technique, inductive logic programming ( ilp ) ( #AUTHOR_TAG ), which infers general morpho - syntactic patterns from a set of examples ( this set is noted e + hereafter ) and counter -']","['##res is based on a machine learning technique, inductive logic programming ( ilp ) ( #AUTHOR_TAG ), which infers general morpho - syntactic patterns from a set of examples ( this set is noted e + hereafter ) and counter - examples ( e a ) of the elements one wants to acquire and their context.', 'the contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'the acquisition process can be summarized in 3 steps']",0
"[', most strategies are based on ` ` internal or ` ` external methods ( #AUTHOR_TAG ), i. e']","[', most strategies are based on ` ` internal or ` ` external methods ( #AUTHOR_TAG ), i. e. methods that rely on the form of terms or on the information gathered from contexts.', '( in some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process. )', 'the work reported here infers specific semantic relationships based on sets of examples and counterexamples']","[', most strategies are based on ` ` internal or ` ` external methods ( #AUTHOR_TAG ), i. e']","[', most strategies are based on ` ` internal or ` ` external methods ( #AUTHOR_TAG ), i. e. methods that rely on the form of terms or on the information gathered from contexts.', '( in some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process. )', 'the work reported here infers specific semantic relationships based on sets of examples and counterexamples']",1
['on computing that includes collocational information ( #AUTHOR_TAG )'],['on computing that includes collocational information ( #AUTHOR_TAG )'],['on computing that includes collocational information ( #AUTHOR_TAG )'],"['this paper, the method is applied to a french corpus on computing to and noun - verb combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG )']",4
"['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG )']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG )']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG )']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG )']",4
"['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']",0
"[', such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with wordnet relations']","[', such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with wordnet relations']","[', such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with wordnet relations']","['though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'indeed, such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with wordnet relations )']",1
"['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ).', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']",0
"[', contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see ( #AUTHOR_TAG )']","[', contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see ( #AUTHOR_TAG )']","[', contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see ( #AUTHOR_TAG )']","['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ), these patterns allow']",0
"['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in ( #AUTHOR_TAG ).', 'we simply give a short account of its basic principles']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in ( #AUTHOR_TAG ).', 'we simply give a short account of its basic principles herein']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in ( #AUTHOR_TAG ).', 'we simply give a short account of its basic principles']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in ( #AUTHOR_TAG ).', 'we simply give a short account of its basic principles herein']",5
"['number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996,  for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms,']","['number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996,  for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not differentiated']","['number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996,  for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not']","['number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996,  for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not differentiated']",0
"['structure that make arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 )']","['structure that make arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 )']","['', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 )']","['separate the edr task into two parts : a mention detection step, which identifies and classifies all the mentions in a text - and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'in its entirety, the edr task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non - named mentions ( nominal and pronominal ) and the requirement of grouping mentions into entities.', 'this is particularly true for arabic where nominals and pronouns are also attached to the word they modify.', 'in fact, most arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form arabic surface forms ( blank - delimited words ).', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 )']",0
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the fo - cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named (']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the fo - cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named (']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the fo - cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named (']","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the fo - cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g. john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",5
"[', similar to those used for chinese segmentation ( #AUTHOR_TAG ).', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']","['words, we also experimented with models based upon character n - grams, similar to those used for chinese segmentation ( #AUTHOR_TAG ).', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']","['with models based upon character n - grams, similar to those used for chinese segmentation ( #AUTHOR_TAG ).', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']","['addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n - grams, similar to those used for chinese segmentation ( #AUTHOR_TAG ).', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']",1
"['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the #AUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the #AUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the #AUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the #AUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']",5
"['', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard ( #AUTHOR_TAG )']","['', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard ( #AUTHOR_TAG )']","['2 ).', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard ( #AUTHOR_TAG )']","['2 ).', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard ( #AUTHOR_TAG )']",5
"['structure that make arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG )']","['structure that make arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG )']","['', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG )']","['separate the edr task into two parts : a mention detection step, which identifies and classifies all the mentions in a text - and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'in its entirety, the edr task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non - named mentions ( nominal and pronominal ) and the requirement of grouping mentions into entities.', 'this is particularly true for arabic where nominals and pronouns are also attached to the word they modify.', 'in fact, most arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form arabic surface forms ( blank - delimited words ).', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG )']",0
['coreference system system is similar to the bell tree algorithm as described by ( #AUTHOR_TAG )'],['coreference system system is similar to the bell tree algorithm as described by ( #AUTHOR_TAG )'],['coreference system system is similar to the bell tree algorithm as described by ( #AUTHOR_TAG )'],['coreference system system is similar to the bell tree algorithm as described by ( #AUTHOR_TAG )'],1
"['has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different (']","['has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']","['has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different (']","['has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']",0
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ).', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']","['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ).', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']","['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ).', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']","['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ).', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']",0
"['mention sub - type, which is a sub - category of the mention type ( #AUTHOR_TAG ) (']","['mention sub - type, which is a sub - category of the mention type ( #AUTHOR_TAG ) ( e. g. orggovernmental, facilitypath, etc. )']","['mention sub - type, which is a sub - category of the mention type ( #AUTHOR_TAG ) (']","['mention sub - type, which is a sub - category of the mention type ( #AUTHOR_TAG ) ( e. g. orggovernmental, facilitypath, etc. )']",5
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
['demonstrates a technique for segment'],"['demonstrates a technique for segmenting arabic text and uses it as a morphological processing step in machine translation.', 'a trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']","['demonstrates a technique for segmenting arabic text and uses it as a morphological processing step in machine translation.', 'a trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']","['demonstrates a technique for segmenting arabic text and uses it as a morphological processing step in machine translation.', 'a trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']",5
"['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in.', 'both systems are built around from the maximum - entropy technique ( #AUTHOR_TAG ).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in.', 'both systems are built around from the maximum - entropy technique ( #AUTHOR_TAG ).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in.', 'both systems are built around from the maximum - entropy technique ( #AUTHOR_TAG ).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in.', 'both systems are built around from the maximum - entropy technique ( #AUTHOR_TAG ).', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness']",5
"['are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classi cation problem.', '']","['are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classi cation problem.', '']","['are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classi cation problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classi cation problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the pre xes and su _ xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness']",1
"['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ).', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n  1 tokens in the history ( t in + 1,...', 't i1 ) and the forward token n - gram feature will contains the next n  1 tokens ( t i + 1,...', 't i + n1 )']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ).', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n  1 tokens in the history ( t in + 1,...', 't i1 ) and the forward token n - gram feature will contains the next n  1 tokens ( t i + 1,...', 't i + n1 )']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ).', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n  1 tokens in the history ( t in + 1,...', 't i1 ) and the forward token n - gram feature will contains the next n  1 tokens ( t i + 1,...', 't i + n1 )']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ).', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n  1 tokens in the history ( t in + 1,...', 't i1 ) and the forward token n - gram feature will contains the next n  1 tokens ( t i + 1,...', 't i + n1 )']",0
"['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ).', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ).', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ).', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ).', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']",5
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['to improve the dictionary based model.', 'as in ( #AUTHOR_TAG ), we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a']","['to improve the dictionary based model.', 'as in ( #AUTHOR_TAG ), we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a']","['to improve the dictionary based model.', 'as in ( #AUTHOR_TAG ), we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a portion of the arabic gigaword corpus.', 'from this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus']","[', an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'we seeked to exploit this ability to generalize to improve the dictionary based model.', 'as in ( #AUTHOR_TAG ), we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a portion of the arabic gigaword corpus.', 'from this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus']",5
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the #AUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the #AUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the #AUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the #AUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']",5
"['a kneser - ney ( #AUTHOR_TAG ) based backoff language model.', 'differing from (Lee et al., 2003), we have also introduced an explicit model for']","['a kneser - ney ( #AUTHOR_TAG ) based backoff language model.', 'differing from (Lee et al., 2003), we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']","['that accepts characters and produces identifiers corresponding to dictionary entries.', 'the final machine is a trigram language model, specifically a kneser - ney ( #AUTHOR_TAG ) based backoff language model.', 'differing from (Lee et al., 2003), we have also introduced an explicit model for']","['our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.', 'the first machine, illustrated in figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'the second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'the final machine is a trigram language model, specifically a kneser - ney ( #AUTHOR_TAG ) based backoff language model.', 'differing from (Lee et al., 2003), we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']",5
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named']","['recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and""]","['this paper we focus on the entity detection and recognition task ( edr ) for arabic as described in ace 2004 framework ( ace, 2004 ).', ""the edr has close ties to the named entity recognition ( ner ) and coreference resolution tasks, which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ), and have been at the center of evaluations such as : muc - 6, muc - 7, and the conll'02 and conll'03 shared tasks."", 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program ( nist, 2004 ) : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g.', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",0
"['( #AUTHOR_TAG ) for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers']","['out.', 'we test the two systems on both "" true "" and system mentions of the devtest set.', 'true mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'we report results with two metrics : ecm - f and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx ( #AUTHOR_TAG ) for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers']","['this section, we present the coreference results on the devtest defined earlier.', 'first, to see the effect of stem matching features, we compare two coreference systems : one with the stem features, the other with - out.', 'we test the two systems on both "" true "" and system mentions of the devtest set.', 'true mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'we report results with two metrics : ecm - f and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx ( #AUTHOR_TAG ) for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers']","['this section, we present the coreference results on the devtest defined earlier.', 'first, to see the effect of stem matching features, we compare two coreference systems : one with the stem features, the other with - out.', 'we test the two systems on both "" true "" and system mentions of the devtest set.', 'true mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'we report results with two metrics : ecm - f and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx ( #AUTHOR_TAG ) for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers without stem features are listed under \\ base, "" and the results of the coreference system with stem features are listed under \\ base + stem.']",5
"['has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different (']","['has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']","['has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different (']","['has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ).', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']",0
"['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classification problem.', '']","['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ).', 'both systems are built around from the maximum - entropy technique (Berger et al., 1996).', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness']",1
"['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization,""]","['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization,""]","['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization,""]","['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization, etc ), a sub - type ( orgcommercial, orggovernmental, etc ), a mention level ( named, nominal, etc ), and a class ( specific, generic, etc )."", 'detecting the mention boundaries ( set of consecutive tokens ) and their main type is one of the important steps of our mention detection system.', 'the score that the ace community uses ( ace value ) attributes a higher importance ( outlined by its weight ) to the main type compared to other sub - tasks, such as the mention level and the class.', 'hence, to build our mention detection system we spent a lot of effort in improving the rst step : detecting the mention boundary and their main type.', 'in this paper, we report the results in terms of precision, recall, and f - measure3']",5
"['= 1 | e, mk, m ) is an exponential or maximum entropy model ( #AUTHOR_TAG )']","['= 1 | e, mk, m ) is an exponential or maximum entropy model ( #AUTHOR_TAG )']","['= 1 | e, mk, m ) is an exponential or maximum entropy model ( #AUTHOR_TAG )']","['mk is one mention in entity e, and the basic model building block pl ( l = 1 | e, mk, m ) is an exponential or maximum entropy model ( #AUTHOR_TAG )']",5
"['email to participate in the experiment.', 'thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annot']","['email to participate in the experiment.', 'thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', 'their results differed particularly in cases of antonymy or distributionally related pairs.', 'we created a manual']","['email to participate in the experiment.', 'thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', 'their results differed particularly in cases of antonymy or distributionally related pairs.', 'we created a manual']","['developed a web - based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'test subjects were invited via email to participate in the experiment.', 'thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', 'their results differed particularly in cases of antonymy or distributionally related pairs.', 'we created a manual with a detailed introduction to sr stressing the crucial points.', 'the manual was presented to the subjects before the experiment and could be re - accessed at any time.', 'during the experiment, one concept pair at a time was presented to the test subjects in random ordering.', 'subjects had to assign a discrete relatedness value { 0, 1, 2, 3, 4 } to each pair.', ""figure 2 shows the system's gui""]",4
"['', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts ( #AUTHOR_TAG ). 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts ( #AUTHOR_TAG ). 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'it is defined on different kinds of textual units, e. g.', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts ( #AUTHOR_TAG ). 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'it is defined on different kinds of textual units, e. g.', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts ( #AUTHOR_TAG ). 2 linguistic distance between words is inverse to their semantic similarity or relatedness']",0
"['im #AUTHOR_TAG ). table 1 : comparison of previous experiments.', 'r']","['shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im #AUTHOR_TAG ). table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']","['im #AUTHOR_TAG ). table 1 : comparison of previous experiments.', 'r']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im #AUTHOR_TAG ). table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['to #AUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']","['to #AUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']","['to #AUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']","['to #AUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']",0
"['.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and good']","['distribution of averaged human judgments on the whole test set ( see figure 3 ) is almost balanced with a slight underrepresentation of highly related concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and goodenough display an empty horizontal band that could be used to separate related and unrelated pairs.', 'this empty band is not observed here.', 'however, figure 4 shows the distribution of averaged judgments with the highest agreement between annotators ( standard deviation < 0. 8 ).', 'the']","['distribution of averaged human judgments on the whole test set ( see figure 3 ) is almost balanced with a slight underrepresentation of highly related concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and good']","['distribution of averaged human judgments on the whole test set ( see figure 3 ) is almost balanced with a slight underrepresentation of highly related concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and goodenough display an empty horizontal band that could be used to separate related and unrelated pairs.', 'this empty band is not observed here.', 'however, figure 4 shows the distribution of averaged judgments with the highest agreement between annotators ( standard deviation < 0. 8 ).', 'the plot clearly shows an empty horizontal band with no judgments.', 'the connection between averaged judgments and standard deviation is plotted in figure 5']",1
"['shown in #AUTHOR_TAG.', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['shown in #AUTHOR_TAG.', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']","['.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG.', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG.', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by #AUTHOR_TAG with 10 subjects.', 'table']","['38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by #AUTHOR_TAG with 10 subjects.', 'table']","['38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by #AUTHOR_TAG with 10 subjects.', 'table']","['the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by #AUTHOR_TAG with 10 subjects.', 'table']",0
"['by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir #AUTHOR_TAG ) systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]","['by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir #AUTHOR_TAG ) systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]","['extracted word pairs from three different domain - specific corpora ( see table 2 ).', 'this is motivated by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir #AUTHOR_TAG ) systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]","['extracted word pairs from three different domain - specific corpora ( see table 2 ).', 'this is motivated by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir #AUTHOR_TAG ) systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]",4
"['.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['( #AUTHOR_TAG ), the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for']","['implemented a set of filters for word pairs.', 'one group of filters removed unwanted word pairs.', 'word pairs are filtered if they contain at least one word that a ) has less than three letters b ) contains only uppercase letters ( mostly acronyms ) or c ) can be found in a stoplist.', 'another filter enforced a specified fraction of combinations of nouns ( n ), verbs ( v ) and adjectives ( a ) to be present in the result set.', 'we used the following parameters : were noun - noun pairs, 15 % noun - verb pairs and so on.', 'word pairs containing polysemous words are expanded to concept pairs using germanet ( #AUTHOR_TAG ), the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for german']","['.', 'word pairs containing polysemous words are expanded to concept pairs using germanet ( #AUTHOR_TAG ), the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for']","['implemented a set of filters for word pairs.', 'one group of filters removed unwanted word pairs.', 'word pairs are filtered if they contain at least one word that a ) has less than three letters b ) contains only uppercase letters ( mostly acronyms ) or c ) can be found in a stoplist.', 'another filter enforced a specified fraction of combinations of nouns ( n ), verbs ( v ) and adjectives ( a ) to be present in the result set.', 'we used the following parameters : were noun - noun pairs, 15 % noun - verb pairs and so on.', 'word pairs containing polysemous words are expanded to concept pairs using germanet ( #AUTHOR_TAG ), the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for german']",5
"['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric ( #AUTHOR_TAG ). 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']","['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric ( #AUTHOR_TAG ). 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']","['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric ( #AUTHOR_TAG ). 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']","['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric ( #AUTHOR_TAG ). 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']",0
"['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', '#AUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared']","['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', '#AUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared directly.', 'furthermore, we cannot']","['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', '#AUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared']","['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', '#AUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared directly.', 'furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low']",1
"['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'we used the revised experimental setup ( #AUTHOR_TAG ), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'we used the revised experimental setup ( #AUTHOR_TAG ), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'we used the revised experimental setup ( #AUTHOR_TAG ), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'we used the revised experimental setup ( #AUTHOR_TAG ), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']",5
"['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG.', 'we used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG.', 'we used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG.', 'we used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']","['our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG.', 'we used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']",1
"['. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
['##p applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG )'],"['semantic relatedness ( sr ) is defined to cover any kind of lexical or functional association that may exist be - tween two words (Gurevych, 2005). 3', 'dissimilar words can be semantically related, e. g.', 'via functional relationships ( night - dark ) or when they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG )']","['semantic relatedness ( sr ) is defined to cover any kind of lexical or functional association that may exist be - tween two words (Gurevych, 2005). 3', 'dissimilar words can be semantically related, e. g.', 'via functional relationships ( night - dark ) or when they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG )']","['similarity is typically defined via the lexical relations of synonymy ( automobile - car ) and hypernymy ( vehicle - car ), while semantic relatedness ( sr ) is defined to cover any kind of lexical or functional association that may exist be - tween two words (Gurevych, 2005). 3', 'dissimilar words can be semantically related, e. g.', 'via functional relationships ( night - dark ) or when they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG )']",0
"['the seminal work by #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991)']","['the seminal work by #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991)']","['the seminal work by #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991)']","['the seminal work by #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by Resnik (1995) with 10 subjects.', 'table']",0
"['by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ). Lebart and Rajman (2000) argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ). Lebart and Rajman (2000) argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ). Lebart and Rajman (2000) argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ). Lebart and Rajman (2000) argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']",0
"['by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']",0
"['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger (Schmid, 1995).', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme ( #AUTHOR_TAG )""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger (Schmid, 1995).', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme ( #AUTHOR_TAG )""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger (Schmid, 1995).', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme ( #AUTHOR_TAG )""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger (Schmid, 1995).', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme ( #AUTHOR_TAG )""]",5
"['.', '#AUTHOR_TAG']","['not report inter - subject correlation for their larger dataset.', '#AUTHOR_TAG']","['. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter - subject correlation for their larger dataset.', '#AUTHOR_TAG']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter - subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend ( #AUTHOR_TAG ).', 'we removed words which had more than three senses']","['##et contains only a few conceptual glosses.', 'as they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e. g. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend ( #AUTHOR_TAG ).', 'we removed words which had more than three senses']","['. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend ( #AUTHOR_TAG ).', 'we removed words which had more than three senses']","['##et contains only a few conceptual glosses.', 'as they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e. g. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend ( #AUTHOR_TAG ).', 'we removed words which had more than three senses']",5
"['with pairs connected by all kinds of lexical - semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non - classical (']","['with pairs connected by all kinds of lexical - semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']","['with pairs connected by all kinds of lexical - semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non - classical (']","[', manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']",0
"['shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']","['.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger ( #AUTHOR_TAG ).', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme (Salton, 1989)""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger ( #AUTHOR_TAG ).', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme (Salton, 1989)""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger ( #AUTHOR_TAG ).', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme (Salton, 1989)""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger ( #AUTHOR_TAG ).', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme (Salton, 1989)""]",5
"['.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['.', 'dictionary - based ( #AUTHOR_TAG ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( #AUTHOR_TAG ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( #AUTHOR_TAG ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based ( #AUTHOR_TAG ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans ( #AUTHOR_TAG ). 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans ( #AUTHOR_TAG ). 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans ( #AUTHOR_TAG ). 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans ( #AUTHOR_TAG ). 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']",0
"['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced']","['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced']","['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced']","['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non - classical ( i. e.', 'other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']",0
"['dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter - subject correlation for their larger']","['dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter - subject correlation for their larger']","['. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', 'Gurevych (2006)']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', '#AUTHOR_TAG']","['. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', '#AUTHOR_TAG']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', '#AUTHOR_TAG']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r =. 9026. 10', '#AUTHOR_TAG reported a correlation of r =. 9026. 10 the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter - subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholingu']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im Walde and Melinger, 2005) table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"[') may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by #AUTHOR_TAG']","['coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations ( like cooccurrence of words ) and is thus a more complicated task.', 'judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'while most people may agree 10 note that resnik used the averaged correlation coefficient.', 'we computed the summarized correlation coefficient using a fisher z - value transformation.', 'that ( car - vehicle ) are highly related, a strong connection between ( parts - speech ) may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by #AUTHOR_TAG']","['coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations ( like cooccurrence of words ) and is thus a more complicated task.', 'judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'while most people may agree 10 note that resnik used the averaged correlation coefficient.', 'we computed the summarized correlation coefficient using a fisher z - value transformation.', 'that ( car - vehicle ) are highly related, a strong connection between ( parts - speech ) may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by #AUTHOR_TAG']","['coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations ( like cooccurrence of words ) and is thus a more complicated task.', 'judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'while most people may agree 10 note that resnik used the averaged correlation coefficient.', 'we computed the summarized correlation coefficient using a fisher z - value transformation.', 'that ( car - vehicle ) are highly related, a strong connection between ( parts - speech ) may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by #AUTHOR_TAG']",1
"['. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or']","['. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based ( Lesk , 1986 ), ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ), information - based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ).', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
['by #AUTHOR_TAG b ) and'],"['site based corpus annotation - in which the user can specify a web site to annotate  domain based corpus annotation - in which the user specifies a content domain ( with the use of keywords ) to annotate  crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages from a computational linguistic view, the framework will also need to take into account the granularity of the unit ( for example, pos tagging requires sentence - units, but anaphoric annotation needs paragraphs or larger ).', 'secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by #AUTHOR_TAG b ) and shingling techniques']",['by #AUTHOR_TAG b ) and'],"['site based corpus annotation - in which the user can specify a web site to annotate  domain based corpus annotation - in which the user specifies a content domain ( with the use of keywords ) to annotate  crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages from a computational linguistic view, the framework will also need to take into account the granularity of the unit ( for example, pos tagging requires sentence - units, but anaphoric annotation needs paragraphs or larger ).', 'secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by #AUTHOR_TAG b ) and shingling techniques described by Chakrabarti ( 2002 )']",3
"[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]","[""linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]","[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 )""]",0
"['( Kennedy , 1998  : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ).', 'increasingly, corpus researchers are tapping the web to overcome the']","['( Kennedy , 1998  : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ).', 'increasingly, corpus researchers are tapping the web to overcome the']","['( Kennedy , 1998  : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologn']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible ( Kennedy , 1998  : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['for escience ( #AUTHOR_TAG ; Hughes et al , 2004 ).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more']","['of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for escience ( #AUTHOR_TAG ; Hughes et al , 2004 ).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more']","['for escience ( #AUTHOR_TAG ; Hughes et al , 2004 ).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more']","['the areas of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for escience ( #AUTHOR_TAG ; Hughes et al , 2004 ).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet (Shirky, 2001).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', 'examples include seti @ home ( looking for radio evidence of extraterrestrial life ), climateprediction. net ( studying climate change ), predictor @ home ( investigating protein - related diseases ) and einstein @ home ( searching for gravitational signals )']",0
"[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( Kilgarriff , 2003 ; #AUTHOR_TAG )""]","[""linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( Kilgarriff , 2003 ; #AUTHOR_TAG )""]","[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( Kilgarriff , 2003 ; #AUTHOR_TAG )""]","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( Fletcher , 2004 a ) and the linguist's search engine ( Kilgarriff , 2003 ; #AUTHOR_TAG )""]",0
"[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( #AUTHOR_TAG a ) and the linguist's search engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]","[""linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( #AUTHOR_TAG a ) and the linguist's search engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]","[""have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( #AUTHOR_TAG a ) and the linguist's search engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp ( Kehoe and Renouf , 2002 ), kwicfinder ( #AUTHOR_TAG a ) and the linguist's search engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 )""]",0
"['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web']","['example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ).', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to']","['is verifiability and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ).', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to']","['and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ).', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to']","['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ).', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to its database.', 'in contrast, little progress has been made toward annotating sizable sample corpora from the web']",0
"['restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web']","['example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two ap - proaches e. g. automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws4, amal - gam5, connexor6 ) are available, for example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguists search en - gine (Kilgarriff, 2003; Resnik and Elkiss, 2003)']",0
"['data are well documented ( Mair , 2005 ; #AUTHOR_TAG ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a']","['data are well documented ( Mair , 2005 ; #AUTHOR_TAG ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']","['in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998 : 56 ) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ).', 'increasingly, corpus researchers are tapping the web to overcome the']","['( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ).', 'increasingly, corpus researchers are tapping the web to overcome the']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologn']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( #AUTHOR_TAG ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( #AUTHOR_TAG ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( #AUTHOR_TAG ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( #AUTHOR_TAG ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ).', 'it is our intention to implement our distributed corpus annotation framework as a plug - in.', 'this will involve implementing new functionality and integrating this with our existing annotation tools ( such as claws11 ).', 'the development environment is also flexible enough to utilise the boinc platform, and such support will be built into it']",0
"['via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet ( #AUTHOR_TAG ).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', '']","['to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet ( #AUTHOR_TAG ).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', '']","['via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet ( #AUTHOR_TAG ).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', '']","['the areas of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for e - science (Carroll et al., 2005;Hughes et al, 2004).', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet ( #AUTHOR_TAG ).', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', 'examples include seti @ home ( looking for radio evidence of extraterrestrial life ), climateprediction. net ( studying climate change ), predictor @ home ( investigating protein - related diseases ) and einstein @ home ( searching for gravitational signals )']",0
"['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( Hughes and Walkerdine , 2005 ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( Hughes and Walkerdine , 2005 ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( Hughes and Walkerdine , 2005 ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ).', 'it is our intention to implement our distributed corpus annotation framework']","['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding ( Hughes and Walkerdine , 2005 ), distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ).', 'it is our intention to implement our distributed corpus annotation framework as a plugin.', 'this will involve implementing new functionality and integrating this with our existing annotation tools ( such as claws 11 ).', 'the development environment is also flexible enough to utilise the boinc platform, and such support will be built into it']",0
"['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web']","['example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( #AUTHOR_TAG ).', 'studies have used several different methods to mine web']","['example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( #AUTHOR_TAG ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['. g.', 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( #AUTHOR_TAG ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( #AUTHOR_TAG ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['##a - bte, tidy and parcels3 ( #AUTHOR_TAG )']","['with tools like hyppia - bte, tidy and parcels3 ( #AUTHOR_TAG )']","['##a - bte, tidy and parcels3 ( #AUTHOR_TAG )']","['key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'most web - derived corpora have exploited raw text or html pages, so efforts have focussed on boilerplate removal and cleanup of these formats with tools like hyppia - bte, tidy and parcels3 ( #AUTHOR_TAG )']",0
"['data problem ( #AUTHOR_TAG ).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bol']","['data problem ( #AUTHOR_TAG ).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of']","['(Kennedy, 1998 : 56 ) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem ( #AUTHOR_TAG ).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bol']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998 : 56 ) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem ( #AUTHOR_TAG ).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['the new collection onto a queue for the lse annotator to analyse.', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicograph']","['own collections from altavista search engine results.', 'the second method pushes the new collection onto a queue for the lse annotator to analyse.', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicographers to construct dictionary']","['the new collection onto a queue for the lse annotator to analyse.', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicograph']","['real - time "" linguistic analysis of web data at the syntactic level has been piloted by the linguist\'s search engine ( lse ).', 'using this tool, linguists can either perform syntactic searches via parse trees on a pre - analysed web collection of around three million sentences from the internet archive ( www. archive. org )', 'or build their own collections from altavista search engine results.', 'the second method pushes the new collection onto a queue for the lse annotator to analyse.', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicographers to construct dictionary entries requires large pre - annotated corpora.', ""a word sketch is an automatic one - page corpus - derived summary of a word's grammatical and collocational behaviour."", 'word sketches were first used to prepare the macmillan english dictionary for Advanced Learners (2002, edited by michael rundell ).', 'they have also served as the starting point for high - accuracy word sense disambiguation.', 'more recently, the sketch engine was used to develop the new edition of the oxford thesaurus of English (2004, edited by maurice waite )']",0
"['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']","['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']","['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']","['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']",0
"['over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a']","['over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a']","['over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for lingu']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g. automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws4, amal - gam5, connexor6 ) are available, for example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 ,  2004b ) and received a special issue of the journal computational linguistics ( Kilgarriff and Grefenstette , 2003 ).', 'studies have used several different methods to mine web data.', 'Turney (2001) extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguists search en - gine (Kilgarriff, 2003; Resnik and Elkiss, 2003)']",0
"['.', '#AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001 fletcher,, 2004b and received a special issue of the journal computational linguistics (Kilgarriff and Grefenstette, 2003).', 'studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp (Kehoe and Renouf, 2002), kwicfinder (Fletcher, 2004 a ) and the linguist's search engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)""]",0
"['elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by #AUTHOR_TAG']","['techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'the initial solution will be to store the resulting reference 11 http : / / www. comp. lancs. ac. uk / ucrel / claws / corpus in the sketch engine.', 'we will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web - based corpus studies based in general.', 'current practise elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by #AUTHOR_TAG']","['will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'the initial solution will be to store the resulting reference 11 http : / / www. comp. lancs. ac. uk / ucrel / claws / corpus in the sketch engine.', 'we will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web - based corpus studies based in general.', 'current practise elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by #AUTHOR_TAG']","['will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'the initial solution will be to store the resulting reference 11 http : / / www. comp. lancs. ac. uk / ucrel / claws / corpus in the sketch engine.', 'we will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web - based corpus studies based in general.', 'current practise elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by #AUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']",0
"['data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a']","['data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']","['in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998 : 56 ) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem (Keller et al., 2002).', 'this topic generated intense interest at workshops held at the university of heidelberg ( october 2004 ), university of bologna ( january 2005 ), university of birmingham ( july 2005 ) and now in trento in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ).', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
[''],[''],[''],[''],0
"['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001)']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001)']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001)']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001)']",5
"['in academic writing with some success.', 'intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ).', 'storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""however, in our opinion storyspace is a product of its time and in fact it isn't a web application."", 'although it is possible to label links, it lacks a lot of features we need.', 'moreover, no hypertext writing tool']","['intermedia and storyspace.', 'in fact, they were used expecially in academic writing with some success.', 'intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ).', 'storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""however, in our opinion storyspace is a product of its time and in fact it isn't a web application."", 'although it is possible to label links, it lacks a lot of features we need.', 'moreover, no hypertext writing tool']","[', they were used expecially in academic writing with some success.', 'intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ).', 'storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""however, in our opinion storyspace is a product of its time and in fact it isn't a web application."", 'although it is possible to label links, it lacks a lot of features we need.', 'moreover, no hypertext writing tool']","['from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular intermedia and storyspace.', 'in fact, they were used expecially in academic writing with some success.', 'intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ).', 'storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""however, in our opinion storyspace is a product of its time and in fact it isn't a web application."", 'although it is possible to label links, it lacks a lot of features we need.', 'moreover, no hypertext writing tool available is released under an open source licence.', 'we hope that novelle will bridge this gap - we will choose the exact licence when our first public release is ready']",0
"[""the example of #AUTHOR_TAG, we will call the autonomous units of a hypertext lexias ( from ` lexicon'), a word coined by Roland Barthes ( 1970 )."", 'consequently, a hypertext is a set of lexias.', 'in hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'the main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992)']","[""the example of #AUTHOR_TAG, we will call the autonomous units of a hypertext lexias ( from ` lexicon'), a word coined by Roland Barthes ( 1970 )."", 'consequently, a hypertext is a set of lexias.', 'in hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'the main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992)']","[""the example of #AUTHOR_TAG, we will call the autonomous units of a hypertext lexias ( from ` lexicon'), a word coined by Roland Barthes ( 1970 )."", 'consequently, a hypertext is a set of lexias.', 'in hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'the main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992)']","[""the example of #AUTHOR_TAG, we will call the autonomous units of a hypertext lexias ( from ` lexicon'), a word coined by Roland Barthes ( 1970 )."", 'consequently, a hypertext is a set of lexias.', 'in hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'the main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992)']",5
"['the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a ` web page'is more similar to an infinite canvas than a written page ( #AUTHOR_TAG )."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an '""]","['. 1 hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a ` web page'is more similar to an infinite canvas than a written page ( #AUTHOR_TAG )."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco""]","['the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a ` web page'is more similar to an infinite canvas than a written page ( #AUTHOR_TAG )."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 196""]","['. 1 hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a ` web page'is more similar to an infinite canvas than a written page ( #AUTHOR_TAG )."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( mc Neill, 2005) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text?', 'which role is suitable for authors?', 'we have to analyse them before presenting the architecture of novelle']",0
"['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG )']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG )']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG )']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG )']",0
"[""in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author e. g. #AUTHOR_TAG.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind']","[""in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author e. g. #AUTHOR_TAG.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4']","[""in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author e. g. #AUTHOR_TAG.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( mc Neill, 2005).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', ""now they try to collect themselves in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author e. g. #AUTHOR_TAG.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4 for further aspects']",0
"['start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences ( #AUTHOR_TAG ).', 'if nobody claims the document for himself, it will fall in the public domain.', 'the set of lexias in the public domain will form a special document, owned by a special user, called public domain.', 'if the author refuses the permission to create derivative works, i. e. to edit his own lexias, users']","['start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences ( #AUTHOR_TAG ).', 'if nobody claims the document for himself, it will fall in the public domain.', 'the set of lexias in the public domain will form a special document, owned by a special user, called public domain.', 'if the author refuses the permission to create derivative works, i. e. to edit his own lexias, users']","['a user lets others edit some lexias, he has the right to retain or refuse the attribution when other users have edited it.', 'in the first instance, the edited version simply moves ahead the document history.', 'in the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'the lexia will be marked as a derivative work from the original one, and a new document history timeline will start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences ( #AUTHOR_TAG ).', 'if nobody claims the document for himself, it will fall in the public domain.', 'the set of lexias in the public domain will form a special document, owned by a special user, called public domain.', 'if the author refuses the permission to create derivative works, i. e. to edit his own lexias, users']","['a user lets others edit some lexias, he has the right to retain or refuse the attribution when other users have edited it.', 'in the first instance, the edited version simply moves ahead the document history.', 'in the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'the lexia will be marked as a derivative work from the original one, and a new document history timeline will start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences ( #AUTHOR_TAG ).', 'if nobody claims the document for himself, it will fall in the public domain.', 'the set of lexias in the public domain will form a special document, owned by a special user, called public domain.', ""if the author refuses the permission to create derivative works, i. e. to edit his own lexias, users still have the right to comment the author's work."", 'so as to come to terms with this idea, we need a concept invented by Nelson (1992), i. e. transclusion.', ""rather than copy - and - paste contents from a lexia, a user may recall a quotation of the author's lexia and write a comment in the surroundings."", ""in doing so, the link list of the author's lexia will be updated with a special citation link marker, called quotation link ( see later for details )."", ""usually, the quotation will be'frozen ', as in the moment where it was transcluded ( see figure 3 )."", 'consequently the transclusion resembles a copiedand - pasted text chunk, but the link to the original document will always be consistent, i. e. neither it expires nor it returns an error.', 'otherwise the user who has transcluded the quotation may choose to keep updated the links to the original document.', 'this choice has to be made when the transclusion is done.', 'if so, the transcluded quotation will update automatically, following the history timeline of the original document.', 'for example, if the original document changes topic from stars to pentagons, the quotation transcluded will change topic too ( see figure 4 )']",0
"['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design ( #AUTHOR_TAG ), unfortunately blogs have not been designed under a model.', 'so we have tested and compared the most used tools available for blogging : bloggers, wordpress, movabletype and livejournal']","['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design ( #AUTHOR_TAG ), unfortunately blogs have not been designed under a model.', 'so we have tested and compared the most used tools available for blogging : bloggers, wordpress, movabletype and livejournal']","['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design ( #AUTHOR_TAG ), unfortunately blogs have not been designed under a model.', 'so we have tested and compared the most used tools available for blogging : bloggers, wordpress, movabletype and livejournal']","['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design ( #AUTHOR_TAG ), unfortunately blogs have not been designed under a model.', 'so we have tested and compared the most used tools available for blogging : bloggers, wordpress, movabletype and livejournal']",0
"['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences ( #AUTHOR_TAG )']","['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences ( #AUTHOR_TAG )']","['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences ( #AUTHOR_TAG )']","['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences ( #AUTHOR_TAG )']",5
"['speaking, we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]","['speaking, we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]","['speaking, we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]","['speaking, we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]",0
"['takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( #AUTHOR_TAG ).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', 'now they try to collect']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( #AUTHOR_TAG ).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', 'now they try to collect']","['takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( #AUTHOR_TAG ).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', 'now they try to collect']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( #AUTHOR_TAG ).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', ""now they try to collect themselves in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author - e. g.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4 for further aspects']",0
"['- unlike in the previous times ( #AUTHOR_TAG ).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet,']","['was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'terms as chapter, page or foot - note simply become meaningless in the new texts, or they highly change their meaning.', 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - - i. e. literary criticism - - unlike in the previous times ( #AUTHOR_TAG )."", 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet,']","['- unlike in the previous times ( #AUTHOR_TAG ).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet,']","['was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'terms as chapter, page or foot - note simply become meaningless in the new texts, or they highly change their meaning.', 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - - i. e. literary criticism - - unlike in the previous times ( #AUTHOR_TAG )."", 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', 'for example, a web page is more similar to an infinite canvas than a written page ( mc Cloud, 2001).', 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an opera aperta ( open work ), as eco would define it ( 1962 ).', 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( mc Neill, 2005) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text? which role is suitable for authors? we have to analyse them before presenting the architecture of novelle']",0
"['as a new writing space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gut']","['. 1 hypertext as a new writing space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc Cloud, 2001)."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco""]","['. 1 hypertext as a new writing space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gut']","['. 1 hypertext as a new writing space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc Cloud, 2001)."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( mc Neill, 2005) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text?', 'which role is suitable for authors?', 'we have to analyse them before presenting the architecture of novelle']",0
"['##ax is not a technology in itself but a term that refers to the use of a group of technologies together, in particular javascript and xml.', 'in other words ajax is a web development technique for creating interactive web applications using a combination of xhtml and css, document object model ( or dom ), the xmlhttprequest object ( #AUTHOR_TAG )']","['##ax is not a technology in itself but a term that refers to the use of a group of technologies together, in particular javascript and xml.', 'in other words ajax is a web development technique for creating interactive web applications using a combination of xhtml and css, document object model ( or dom ), the xmlhttprequest object ( #AUTHOR_TAG )']","['##ax is not a technology in itself but a term that refers to the use of a group of technologies together, in particular javascript and xml.', 'in other words ajax is a web development technique for creating interactive web applications using a combination of xhtml and css, document object model ( or dom ), the xmlhttprequest object ( #AUTHOR_TAG )']","['##ax is not a technology in itself but a term that refers to the use of a group of technologies together, in particular javascript and xml.', 'in other words ajax is a web development technique for creating interactive web applications using a combination of xhtml and css, document object model ( or dom ), the xmlhttprequest object ( #AUTHOR_TAG )']",0
"[""##t's ideas gave the roots to the assimilation theory by david ausubel."", 'very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'every arc always has a definite direction, i. e. arcs are arrows ( #AUTHOR_TAG )']","[""mapping has been used at least in education for over thirty years, in particular at the cornell university, where piaget's ideas gave the roots to the assimilation theory by david ausubel."", 'very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'every arc always has a definite direction, i. e. arcs are arrows ( #AUTHOR_TAG )']","[""##t's ideas gave the roots to the assimilation theory by david ausubel."", 'very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'every arc always has a definite direction, i. e. arcs are arrows ( #AUTHOR_TAG )']","[""mapping has been used at least in education for over thirty years, in particular at the cornell university, where piaget's ideas gave the roots to the assimilation theory by david ausubel."", 'very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'every arc always has a definite direction, i. e. arcs are arrows ( #AUTHOR_TAG )']",0
['ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],5
"['wikis every document keeps track of its own history : creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'moreover, a sandbox is a temporary view of a document itself i. e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ).', 'figure 1 shows the model.', 'history snapshots of the timeline may be considered as permanent']","['wikis every document keeps track of its own history : creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'moreover, a sandbox is a temporary view of a document itself i. e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ).', 'figure 1 shows the model.', 'history snapshots of the timeline may be considered as permanent views, i. e.']","['wikis every document keeps track of its own history : creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'moreover, a sandbox is a temporary view of a document itself i. e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ).', 'figure 1 shows the model.', 'history snapshots of the timeline may be considered as permanent views, i. e. views with a timestamp.', 'consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'this model will have a strong impact on the role of links and on the underpinning structure of novelle itself']","['wikis every document keeps track of its own history : creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'moreover, a sandbox is a temporary view of a document itself i. e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ).', 'figure 1 shows the model.', 'history snapshots of the timeline may be considered as permanent views, i. e. views with a timestamp.', 'consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'this model will have a strong impact on the role of links and on the underpinning structure of novelle itself']",0
"[').', 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up :']","['. 1 hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc Cloud, 2001)."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up :']","[').', 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up :']","['. 1 hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc Cloud, 2001)."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text?', 'which role is suitable for authors?', 'we have to analyse them before presenting the architecture of novelle']",0
"['and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many "" ( #AUTHOR_TAG )']","['the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many "" ( #AUTHOR_TAG )']","['the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many "" ( #AUTHOR_TAG )']","['the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many "" ( #AUTHOR_TAG )']",0
"['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml ( #AUTHOR_TAG )']","['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml ( #AUTHOR_TAG )']","['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml ( #AUTHOR_TAG )']","['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml ( #AUTHOR_TAG )']",0
"['( Brants et al. , 2002 ) and portuguese ( #AUTHOR_TAG ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['( Brants et al. , 2002 ) and portuguese ( #AUTHOR_TAG ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['( Brants et al. , 2002 ) and portuguese ( #AUTHOR_TAG ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['second observation is that a high proportion of non - projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'this is noticeable for german ( Brants et al. , 2002 ) and portuguese ( #AUTHOR_TAG ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ), dutch ( van der Beek et al. , 2002 ) and slovene ( dezeroski et al., 2006 ), where root precision drops more drastically to about 69 %, 71 % and 41 %, respectively, and root recall is also affected negatively.', 'on the other hand, all three languages behave like high - accuracy languages with respect to attachment score.', 'a very similar pattern is found for spanish ( civit torruella and marti antonin, 2002 ), although this cannot be explained by a high proportion of non - projective structures.', 'one possible explanation in this case may be the fact that dependency graphs in the spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features']",1
"['histories to parser actions (Kudo and Matsumoto, 2002).', 'a graph transformations for recovering nonprojective structures ( #AUTHOR_TAG )']","['histories to parser actions (Kudo and Matsumoto, 2002).', 'a graph transformations for recovering nonprojective structures ( #AUTHOR_TAG )']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', ' history - based feature models for predicting the next parser action (Black et al., 1992).', ' support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', 'a graph transformations for recovering nonprojective structures ( #AUTHOR_TAG )']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', ' history - based feature models for predicting the next parser action (Black et al., 1992).', ' support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', 'a graph transformations for recovering nonprojective structures ( #AUTHOR_TAG )']",5
"['( #AUTHOR_TAG ) and portuguese ( Afonso et al. , 2002 ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['( #AUTHOR_TAG ) and portuguese ( Afonso et al. , 2002 ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['second observation is that a high proportion of non - projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'this is noticeable for german ( #AUTHOR_TAG ) and portuguese ( Afonso et al. , 2002 ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['second observation is that a high proportion of non - projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'this is noticeable for german ( #AUTHOR_TAG ) and portuguese ( Afonso et al. , 2002 ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ), dutch ( van der Beek et al. , 2002 ) and slovene ( dezeroski et al., 2006 ), where root precision drops more drastically to about 69 %, 71 % and 41 %, respectively, and root recall is also affected negatively.', 'on the other hand, all three languages behave like high - accuracy languages with respect to attachment score.', 'a very similar pattern is found for spanish ( civit torruella and marti antonin, 2002 ), although this cannot be explained by a high proportion of non - projective structures.', 'one possible explanation in this case may be the fact that dependency graphs in the spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features']",1
['labeled dependency parsing by #AUTHOR_TAG'],['labeled dependency parsing by #AUTHOR_TAG'],['labeled dependency parsing by #AUTHOR_TAG'],['parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG'],5
"['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( #AUTHOR_TAG ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( #AUTHOR_TAG ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( #AUTHOR_TAG ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( #AUTHOR_TAG ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( #AUTHOR_TAG ),']","['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( #AUTHOR_TAG ),']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( #AUTHOR_TAG ),']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( #AUTHOR_TAG ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['(Nivre, 2006).', 'a history - based feature models for predicting the next parser action ( #AUTHOR_TAG ).', ' support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', ' graph transformations for recovering nonprojective structures']","['(Nivre, 2006).', 'a history - based feature models for predicting the next parser action ( #AUTHOR_TAG ).', ' support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', ' graph transformations for recovering nonprojective structures']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'a history - based feature models for predicting the next parser action ( #AUTHOR_TAG ).', ' support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', ' graph transformations for recovering nonprojective structures']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'a history - based feature models for predicting the next parser action ( #AUTHOR_TAG ).', ' support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', ' graph transformations for recovering nonprojective structures']",5
"['experiments have been performed using maltparser ( #AUTHOR_TAG ), version 0. 4, which is made available together with the suite of programs used for preand post - processing.']","['experiments have been performed using maltparser ( #AUTHOR_TAG ), version 0. 4, which is made available together with the suite of programs used for preand post - processing.']","['experiments have been performed using maltparser ( #AUTHOR_TAG ), version 0. 4, which is made available together with the suite of programs used for preand post - processing.']","['experiments have been performed using maltparser ( #AUTHOR_TAG ), version 0. 4, which is made available together with the suite of programs used for preand post - processing.']",5
"[', we use libsvm ( #AUTHOR_TAG )']","[', we use libsvm ( #AUTHOR_TAG )']","[', we use libsvm ( #AUTHOR_TAG )']","['use support vector machines to predict the next parser action from a feature vector representing the history.', 'more specifically, we use libsvm ( #AUTHOR_TAG ) with a quadratic kernel k ( xz, xj ) = ( - yxt xj + r ) 2 and the built - in one - versus - all strategy for multi - class classification.', 'symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the feats field into its atomic components. 4', 'or some languages, we divide the training data into smaller sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']",5
['##the analysis is reminiscent of the treatment of coordination in the collins parser ( #AUTHOR_TAG )'],['##the analysis is reminiscent of the treatment of coordination in the collins parser ( #AUTHOR_TAG )'],['##the analysis is reminiscent of the treatment of coordination in the collins parser ( #AUTHOR_TAG )'],['##the analysis is reminiscent of the treatment of coordination in the collins parser ( #AUTHOR_TAG )'],1
['to be captured using the pseudoprojective approach of #AUTHOR_TAG'],['to be captured using the pseudoprojective approach of #AUTHOR_TAG'],['are labeled allows non - projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG'],"['the parser only derives projective graphs, the fact that graphs are labeled allows non - projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG']",0
"['sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']","['sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']","['some languages, we divide the training data into smaller sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']","['some languages, we divide the training data into smaller sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']",0
"['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian (Simov et al., 2005;Simov and Osenova, 2003), chinese (Chen et al., 2003), danish (Kromann, 2003), and swedish.', 'japanese ( #AUTHOR_TAG ), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian (Simov et al., 2005;Simov and Osenova, 2003), chinese (Chen et al., 2003), danish (Kromann, 2003), and swedish.', 'japanese ( #AUTHOR_TAG ), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian (Simov et al., 2005;Simov and Osenova, 2003), chinese (Chen et al., 2003), danish (Kromann, 2003), and swedish.', 'japanese ( #AUTHOR_TAG ), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian (Simov et al., 2005;Simov and Osenova, 2003), chinese (Chen et al., 2003), danish (Kromann, 2003), and swedish.', 'japanese ( #AUTHOR_TAG ), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",1
"['', ' history - based feature models for predicting the next parser action (Black et al., 1992).', 'support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ).', ' graph transformations for recovering nonprojective structures']","['', ' history - based feature models for predicting the next parser action (Black et al., 1992).', 'support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ).', ' graph transformations for recovering nonprojective structures']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', ' history - based feature models for predicting the next parser action (Black et al., 1992).', 'support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ).', ' graph transformations for recovering nonprojective structures']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', ' history - based feature models for predicting the next parser action (Black et al., 1992).', 'support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ).', ' graph transformations for recovering nonprojective structures']",5
"['##jic et al., 2004 ; smrz et al., 2002 ) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length ( from about 93 % for length 1 to 67 % for length 2 ).', 'by contrast, turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ).', 'it is noteworthy that arabic and turkish, being "" typological outliers "", show patterns that are different both from each other and from most of the other languages']","['results for arabic ( hajic et al., 2004 ; smrz et al., 2002 ) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length ( from about 93 % for length 1 to 67 % for length 2 ).', 'by contrast, turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ).', 'it is noteworthy that arabic and turkish, being "" typological outliers "", show patterns that are different both from each other and from most of the other languages']","['##jic et al., 2004 ; smrz et al., 2002 ) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length ( from about 93 % for length 1 to 67 % for length 2 ).', 'by contrast, turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ).', 'it is noteworthy that arabic and turkish, being "" typological outliers "", show patterns that are different both from each other and from most of the other languages']","['results for arabic ( hajic et al., 2004 ; smrz et al., 2002 ) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length ( from about 93 % for length 1 to 67 % for length 2 ).', 'by contrast, turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ).', 'it is noteworthy that arabic and turkish, being "" typological outliers "", show patterns that are different both from each other and from most of the other languages']",1
"['.', '#AUTHOR_TAG have previously examined the task of categor']","['the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov']","['provide a foundation for studying the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['from them.', 'for example, #AUTHOR_TAG experiment']","['from them.', 'for example, #AUTHOR_TAG experimented with abstracts and full article texts in the task of']","['question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, #AUTHOR_TAG experiment']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['( corresponding to the hmm states ).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the lda transform matrix, which entails computing the withinand between - covariance matrices of the classes, and using singular value decomposition ( svd ) to compute the eigenvectors of the new space.', 'each sentence']","['( corresponding to the hmm states ).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the lda transform matrix, which entails computing the withinand between - covariance matrices of the classes, and using singular value decomposition ( svd ) to compute the eigenvectors of the new space.', 'each sentence / vector is then mul - tiplied by this matrix, and new hmm models are re - computed from the projected data']","['( corresponding to the hmm states ).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the lda transform matrix, which entails computing the withinand between - covariance matrices of the classes, and using singular value decomposition ( svd ) to compute the eigenvectors of the new space.', 'each sentence / vector is then mul - tiplied by this matrix, and new hmm models are re - computed from the projected']","['an attempt to further boost performance, we employed linear discriminant analysis ( lda ) to find a linear projection of the four - dimensional vec - tors that maximizes the separation of the gaussians ( corresponding to the hmm states ).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the lda transform matrix, which entails computing the withinand between - covariance matrices of the classes, and using singular value decomposition ( svd ) to compute the eigenvectors of the new space.', 'each sentence / vector is then mul - tiplied by this matrix, and new hmm models are re - computed from the projected data']",5
"['- valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( mc']","['content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( mc Keown et al. , 2003 ).', 'we']","['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( mc Keown et al. , 2003 ).', 'we chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']","['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( mc Keown et al. , 2003 ).', 'we chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']",3
"['example, ( Joachims , 1998 ; #AUTHOR_TAG ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['example, ( Joachims , 1998 ; #AUTHOR_TAG ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['example, ( Joachims , 1998 ; #AUTHOR_TAG ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['approaches ( especially svms ) have been shown to be very effective for many supervised classification tasks ; see, for example, ( Joachims , 1998 ; #AUTHOR_TAG ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'in fact, we demonstrate that hmms are competitive with svms, with the added advantage of lower computational complexity.', 'in addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'in the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs']",0
"['and #AUTHOR_TAG, we employed hidden markov models to model the discourse structure of medline abstracts.', 'the four']","['and #AUTHOR_TAG, we employed hidden markov models to model the discourse structure of medline abstracts.', 'the four']","['and #AUTHOR_TAG, we employed hidden markov models to model the discourse structure of medline abstracts.', 'the four states in our hmms correspond to the information that characterizes each section ( "" introduction "", "" methods "", "" results "", and "" conclusions "" ) and state transitions capture the discourse flow from section to section']","['and #AUTHOR_TAG, we employed hidden markov models to model the discourse structure of medline abstracts.', 'the four states in our hmms correspond to the information that characterizes each section ( "" introduction "", "" methods "", "" results "", and "" conclusions "" ) and state transitions capture the discourse flow from section to section']",5
"['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( Mizuta et al. , 2005 ), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['##s mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit ( #AUTHOR_TAG ), which efficiently performs the forward - backward algorithm and baumwelch estimation.', 'for testing, we performed a vit']","['connected graph.', 'the output probabilities were modeled as four - dimensional gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit ( #AUTHOR_TAG ), which efficiently performs the forward - backward algorithm and baumwelch estimation.', 'for testing, we performed a viterbi ( maximum likelihood ) estimation of the label of each test sentence / vector ( also using the htk toolkit )']","['connected graph.', 'the output probabilities were modeled as four - dimensional gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit ( #AUTHOR_TAG ), which efficiently performs the forward - backward algorithm and baumwelch estimation.', 'for testing, we performed a viterbi ( maximum likelihood ) estimation of the label of each test sentence / vector ( also using the htk toolkit )']","['then built a four - state hidden markov model that outputs these four - dimensional vectors.', 'the transition probability matrix of the hmm was initialized with uniform probabilities over a fully connected graph.', 'the output probabilities were modeled as four - dimensional gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit ( #AUTHOR_TAG ), which efficiently performs the forward - backward algorithm and baumwelch estimation.', 'for testing, we performed a viterbi ( maximum likelihood ) estimation of the label of each test sentence / vector ( also using the htk toolkit )']",5
"['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; oreasan, 2001 ).', 'the ability to explicitly identify these sections in']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; oreasan, 2001 ).', 'the ability to explicitly identify these sections in un - structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; oreasan, 2001 ).', 'the ability to explicitly identify these sections in']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; oreasan, 2001 ).', 'the ability to explicitly identify these sections in un - structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'demner - Fushman et al. (2005) found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts']",0
"['entire sentences based on our language models ), as opposed to sequences of terms, as done in ( #AUTHOR_TAG ).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second,']","['entire sentences based on our language models ), as opposed to sequences of terms, as done in ( #AUTHOR_TAG ).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second,']","['interesting aspect of our generative approach is that we model hmm outputs as gaussian vectors ( log probabilities of observing entire sentences based on our language models ), as opposed to sequences of terms, as done in ( #AUTHOR_TAG ).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions allows us to leverage a variety of tools (']","['interesting aspect of our generative approach is that we model hmm outputs as gaussian vectors ( log probabilities of observing entire sentences based on our language models ), as opposed to sequences of terms, as done in ( #AUTHOR_TAG ).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions allows us to leverage a variety of tools ( e. g., lda ) that have been shown to be successful in other fields, such as speech recognition (Evermann et al., 2004)']",1
"['example, ( #AUTHOR_TAG ; Ng and Jordan , 2001 ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['example, ( #AUTHOR_TAG ; Ng and Jordan , 2001 ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['example, ( #AUTHOR_TAG ; Ng and Jordan , 2001 ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['approaches ( especially svms ) have been shown to be very effective for many supervised classification tasks ; see, for example, ( #AUTHOR_TAG ; Ng and Jordan , 2001 ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'in fact, we demonstrate that hmms are competitive with svms, with the added advantage of lower computational complexity.', 'in addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'in the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs']",0
"['of software that leverages this knowledge - - metamap ( #AUTHOR_TAG ) for concept identification and semrep ( Rindflesch and Fiszman , 2003 ) for relation extraction']","['of software that leverages this knowledge - - metamap ( #AUTHOR_TAG ) for concept identification and semrep ( Rindflesch and Fiszman , 2003 ) for relation extraction']","['##s ) ( Lindberg et al. , 1993 ), and the availability of software that leverages this knowledge - - metamap ( #AUTHOR_TAG ) for concept identification and semrep ( Rindflesch and Fiszman , 2003 ) for relation extraction']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) ( Lindberg et al. , 1993 ), and the availability of software that leverages this knowledge - - metamap ( #AUTHOR_TAG ) for concept identification and semrep ( Rindflesch and Fiszman , 2003 ) for relation extraction - - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']",1
"['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', 'information that satisfies']","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a""]","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['rhetorical elements ( mc Keown, 1985;Marcu and Echihabi, 2002).', 'our task is closer to the work of #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( mc Keown, 1985;Marcu and Echihabi, 2002).', 'our task is closer to the work of #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( mc Keown, 1985;Marcu and Echihabi, 2002).', 'our task is closer to the work of #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( mc Keown, 1985;Marcu and Echihabi, 2002).', 'our task is closer to the work of #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']",1
"['is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( demner - Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ).', 'we']","['is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( demner - Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ).', 'we']","['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( demner - Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ).', 'we chose to focus on randomized controlled trials']","['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( demner - Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ).', 'we chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']",3
['( b ) again reproduces the results from #AUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],['( b ) again reproduces the results from #AUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],['( b ) again reproduces the results from #AUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],['( b ) again reproduces the results from #AUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],1
"['( Teufel and Moens , 2000 ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( #AUTHOR_TAG ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( #AUTHOR_TAG ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( #AUTHOR_TAG ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( #AUTHOR_TAG ), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', 'information that satisfies']","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a""]","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( #AUTHOR_TAG ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( #AUTHOR_TAG ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( #AUTHOR_TAG ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( #AUTHOR_TAG ), information extraction ( Mizuta et al. , 2005 ), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['lda ) specifically trained for each of the sections ( one per row in the table ).', 'the table also presents the closest comparable experimental results reported by #AUTHOR_TAG. 1 mcknight and srinivasan ( hence']","['lda ) specifically trained for each of the sections ( one per row in the table ).', 'the table also presents the closest comparable experimental results reported by #AUTHOR_TAG. 1 mcknight and srinivasan ( henceforth, m & s ) created a test collection consisting of 37, 151 rcts from approximately 12 million medline abstracts dated between 1976 and 2001.', 'this collection has significantly more training examples']","['lda ) specifically trained for each of the sections ( one per row in the table ).', 'the table also presents the closest comparable experimental results reported by #AUTHOR_TAG. 1 mcknight and srinivasan ( hence']","['results of our second set of experiments ( with rcts only ) are shown in tables 2 ( a ) and 2 ( b ). table 2 ( a ) reports the multi - way classification error rate ; once again, applying the markov assumption to model discourse transitions improves performance, and using lda further reduces error rate.', 'table 2 ( b ) reports accuracy, precision, recall, and f - measure for four separate binary classifiers ( hmm with lda ) specifically trained for each of the sections ( one per row in the table ).', 'the table also presents the closest comparable experimental results reported by #AUTHOR_TAG. 1 mcknight and srinivasan ( henceforth, m & s ) created a test collection consisting of 37, 151 rcts from approximately 12 million medline abstracts dated between 1976 and 2001.', 'this collection has significantly more training examples than our corpus of 27, 075 abstracts, which could be a source of performance differences.', 'furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.', 'nevertheless, our hmm - based approach is at least competitive with svms, perhaps better in some cases']",1
"['rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']",1
"['( #AUTHOR_TAG ).', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used']","[') ; cfxxx ( #AUTHOR_TAG ).', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used']","[') ; cfxxx ( #AUTHOR_TAG ).', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. ( 2003 ) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cfxxx ( #AUTHOR_TAG ).', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['above ( #AUTHOR_TAG ).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how']","['above ( #AUTHOR_TAG ).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how']","['above ( #AUTHOR_TAG ).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( #AUTHOR_TAG ).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', 'information that satisfies']","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a""]","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['of software that leverages this knowledge - - metamap ( Aronson , 2001 ) for concept identification and semrep ( #AUTHOR_TAG ) for relation extraction']","['of software that leverages this knowledge - - metamap ( Aronson , 2001 ) for concept identification and semrep ( #AUTHOR_TAG ) for relation extraction']","['##s ) ( Lindberg et al. , 1993 ), and the availability of software that leverages this knowledge - - metamap ( Aronson , 2001 ) for concept identification and semrep ( #AUTHOR_TAG ) for relation extraction']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) ( Lindberg et al. , 1993 ), and the availability of software that leverages this knowledge - - metamap ( Aronson , 2001 ) for concept identification and semrep ( #AUTHOR_TAG ) for relation extraction - - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['not the first to employ a generative approach to directly model content, the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison.', 'however, our study differs in several important respects.', 'barzilay and lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'in']","['not the first to employ a generative approach to directly model content, the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison.', 'however, our study differs in several important respects.', 'barzilay and lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'in contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'whereas barzilay and lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'nevertheless, their work bolsters our claims']","['not the first to employ a generative approach to directly model content, the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison.', 'however, our study differs in several important respects.', 'barzilay and lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'in']","['not the first to employ a generative approach to directly model content, the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison.', 'however, our study differs in several important respects.', 'barzilay and lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'in contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'whereas barzilay and lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here']",1
"['. g., lda ) that have been shown to be successful in other fields, such as speech recognition ( #AUTHOR_TAG )']","['allows us to leverage a variety of tools ( e. g., lda ) that have been shown to be successful in other fields, such as speech recognition ( #AUTHOR_TAG )']","['. g., lda ) that have been shown to be successful in other fields, such as speech recognition ( #AUTHOR_TAG )']","['interesting aspect of our generative approach is that we model hmm outputs as gaussian vectors ( log probabilities of observing entire sentences based on our language models ), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions allows us to leverage a variety of tools ( e. g., lda ) that have been shown to be successful in other fields, such as speech recognition ( #AUTHOR_TAG )']",0
"['( #AUTHOR_TAG ), meant for primary school students.', 'the text gradually introduces concepts related to precipitation, and explains them.', 'its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'the system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']","['( #AUTHOR_TAG ), meant for primary school students.', 'the text gradually introduces concepts related to precipitation, and explains them.', 'its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'the system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']","['work with a semi - technical text on meteorological phenomena ( #AUTHOR_TAG ), meant for primary school students.', 'the text gradually introduces concepts related to precipitation, and explains them.', 'its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'the system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']","['work with a semi - technical text on meteorological phenomena ( #AUTHOR_TAG ), meant for primary school students.', 'the text gradually introduces concepts related to precipitation, and explains them.', 'its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'the system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']",5
"['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['process a pair p not encountered previously, the system builds a graph centered on the main element ( often the head ) of p.', ""this idea was inspired by #AUTHOR_TAG, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles ( case relations )."", 'in recent approaches, syntactic']","['process a pair p not encountered previously, the system builds a graph centered on the main element ( often the head ) of p.', ""this idea was inspired by #AUTHOR_TAG, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles ( case relations )."", 'in recent approaches, syntactic']","['process a pair p not encountered previously, the system builds a graph centered on the main element ( often the head ) of p.', ""this idea was inspired by #AUTHOR_TAG, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles ( case relations )."", 'in recent approaches, syntactic information is translated into features which, together with information from framenet, wordnet or verbnet, will be used with ml tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005)']","['process a pair p not encountered previously, the system builds a graph centered on the main element ( often the head ) of p.', ""this idea was inspired by #AUTHOR_TAG, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles ( case relations )."", 'in recent approaches, syntactic information is translated into features which, together with information from framenet, wordnet or verbnet, will be used with ml tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005)']",4
"['arguments.', 'most approaches rely on verbnet ( #AUTHOR_TAG ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']","['arguments.', 'most approaches rely on verbnet ( #AUTHOR_TAG ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']","['arguments.', 'most approaches rely on verbnet ( #AUTHOR_TAG ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet ( #AUTHOR_TAG ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']",0
"['.', 'for each pair of units, the system builds a syntactic graph surrounding the main element ( main clause, head verb, head noun ).', 'it then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'if such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'we have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent ( #AUTHOR_TAG']","['of its arguments, a noun and each of its modifiers.', 'for each pair of units, the system builds a syntactic graph surrounding the main element ( main clause, head verb, head noun ).', 'it then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'if such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'we have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent ( #AUTHOR_TAG']","['.', 'for each pair of units, the system builds a syntactic graph surrounding the main element ( main clause, head verb, head noun ).', 'it then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'if such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'we have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent ( #AUTHOR_TAG']","['work with sentences, clauses, phrases and words, using syntactic structures generated by a parser.', 'our system incrementally processes a text, and extracts pairs of text units : two clauses, a verb and each of its arguments, a noun and each of its modifiers.', 'for each pair of units, the system builds a syntactic graph surrounding the main element ( main clause, head verb, head noun ).', 'it then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'if such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'we have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent ( #AUTHOR_TAG a )']",4
"['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['( #AUTHOR_TAG ).', ""the system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]","['build complex knowledge bases by combining components : events, entities and modifiers ( #AUTHOR_TAG ).', ""the system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]","['( #AUTHOR_TAG ).', ""the system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]","['the rapid knowledge formation project ( rkf ) a support system was developed for domain experts.', 'it helps them build complex knowledge bases by combining components : events, entities and modifiers ( #AUTHOR_TAG ).', ""the system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]",0
"['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['##ini1.', 'he was a grammarian who analysed sanskrit ( #AUTHOR_TAG ).', 'the idea resurface']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century b. c. and the work of panini1.', 'he was a grammarian who analysed sanskrit ( #AUTHOR_TAG ).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965;  fill - more, 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005)']","['##ini1.', 'he was a grammarian who analysed sanskrit ( #AUTHOR_TAG ).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965;  fill - more, 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century b. c. and the work of panini1.', 'he was a grammarian who analysed sanskrit ( #AUTHOR_TAG ).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965;  fill - more, 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005)']",0
"['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']","['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']","['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']",0
"['through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 )']","['modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 )']","['through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 )']","['this work we pursue a well - known and often tacitly assumed line of thinking : connections at the syntactic level reflect connections at the semantic level ( in other words, syntax carries meaning ).', 'anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations (Misra, 1966;Gruber, 1965;Fillmore, 1968).', 'tesniere ( 1959 ), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants - for example, agent or instrument - to such grammatical elements as subject, direct object, indirect object.', 'this idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 )']",0
"['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']","['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']","['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']",0
"['are produced by a parser with good coverage and detailed syntactic information, dipett ( #AUTHOR_TAG ).', 'the parser, written in prolog, implements a classic constituency english grammar from Quirk et al. (1985).', 'pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'a dependency parser would produce a similar output, but dipett also provides verb subcategorization information ( such as, for example, subject - verb - object or subject - verb - objectindirect object ), which we use to select the ( best ) matching syntactic structures']","['are produced by a parser with good coverage and detailed syntactic information, dipett ( #AUTHOR_TAG ).', 'the parser, written in prolog, implements a classic constituency english grammar from Quirk et al. (1985).', 'pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'a dependency parser would produce a similar output, but dipett also provides verb subcategorization information ( such as, for example, subject - verb - object or subject - verb - objectindirect object ), which we use to select the ( best ) matching syntactic structures']","['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information, dipett ( #AUTHOR_TAG ).', 'the parser, written in prolog, implements a classic constituency english grammar from Quirk et al. (1985).', 'pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'a dependency parser would produce a similar output, but dipett also provides verb subcategorization information ( such as, for example, subject - verb - object or subject - verb - objectindirect object ), which we use to select the ( best ) matching syntactic structures']","['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information, dipett ( #AUTHOR_TAG ).', 'the parser, written in prolog, implements a classic constituency english grammar from Quirk et al. (1985).', 'pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'a dependency parser would produce a similar output, but dipett also provides verb subcategorization information ( such as, for example, subject - verb - object or subject - verb - objectindirect object ), which we use to select the ( best ) matching syntactic structures']",5
"['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['##ini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']",0
"['forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['##ini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']",0
"['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'this design idea was adopted from tanka ( #AUTHOR_TAG b ).', 'the only manually encoded knowledge is a dictionary of markers ( subordinators, coordinators, prepositions ).', 'this resource does not affect the syntacticsemantic graph - matching heuristic']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'this design idea was adopted from tanka ( #AUTHOR_TAG b ).', 'the only manually encoded knowledge is a dictionary of markers ( subordinators, coordinators, prepositions ).', 'this resource does not affect the syntacticsemantic graph - matching heuristic']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'this design idea was adopted from tanka ( #AUTHOR_TAG b ).', 'the only manually encoded knowledge is a dictionary of markers ( subordinators, coordinators, prepositions ).', 'this resource does not affect the syntacticsemantic graph - matching heuristic']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'this design idea was adopted from tanka ( #AUTHOR_TAG b ).', 'the only manually encoded knowledge is a dictionary of markers ( subordinators, coordinators, prepositions ).', 'this resource does not affect the syntacticsemantic graph - matching heuristic']",5
"['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']",0
['list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAG'],['list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAG'],['list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAG'],"['list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAG a ).', 'three lists of relations for three syntactic levels - inter - clause, intra - clause ( case ) and nounmodifier relations - were next combined based on syntactic and semantic phenomena.', 'the resulting list is the one used in the experiments we present in this paper.', 'the relations are grouped by general similarity into 6 relation classes ( h denotes the head of a base np, m denotes the modifier ).', 'there is no consensus in the literature on a list of semantic relations that would work in all situations.', 'this is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts.', 'all the relations in the list we use were necessary, and sufficient, for the analysis of the input text']",5
"['( #AUTHOR_TAG ), we tried to adapt the same approach']","['by the success of chunk - based verb reordering lattices on arabicenglish ( #AUTHOR_TAG ), we tried to adapt the same approach']","['by the success of chunk - based verb reordering lattices on arabicenglish ( #AUTHOR_TAG ), we tried to adapt the same approach']","['reordering between german and english is a complex problem.', 'encouraged by the success of chunk - based verb reordering lattices on arabicenglish ( #AUTHOR_TAG ), we tried to adapt the same approach to the german - english language pair.', 'it turned out that there is a larger variety of long reordering patterns in this case.', 'nevertheless, some experiments performed after the official evaluation showed promising results.', 'we plan to pursue this work in several directions : defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice.', 'applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences.', 'finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade - off between decoderdriven short - range and lattice - driven long - range reordering']",4
"['research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ), who marginalize over']","['research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ), who marginalize over']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ), who marginalize over']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ), who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multi - nonterminal grammars']",3
"['research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG, who marginalize over']","['research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG, who marginalize over']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG, who marginalize over']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG, who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multi - nonterminal grammars']",3
"['our prior work ( #AUTHOR_TAG ), we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be']","['our prior work ( #AUTHOR_TAG ), we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be']","['our prior work ( #AUTHOR_TAG ), we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored']","['our prior work ( #AUTHOR_TAG ), we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer - review domain, where the definition of helpfulness is largely influenced by the educational context of peer review.', 'while previously we used the average of two expert - provided ratings as our gold standard of peer - review helpfulness 1, there are other types of helpfulness rating ( e. g.', 'author perceived helpfulness ) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.', 'in fact, we observe that peer - review helpfulness seems to differ not only between students and experts ( example 1 ), but also between types of experts ( example 2 )']",2
"['.', 'while the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'to compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( details of how the average - expert model performs can be found in our prior work ( #AUTHOR_TAG ).']","['algorithms are provided by weka ( http : / / www. cs. waikato. ac. nz / ml / weka / ).', 'provide complementary perspectives.', 'while the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'to compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( details of how the average - expert model performs can be found in our prior work ( #AUTHOR_TAG ).']","[').', 'provide complementary perspectives.', 'while the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'to compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( details of how the average - expert model performs can be found in our prior work ( #AUTHOR_TAG ).']","['both algorithms are provided by weka ( http : / / www. cs. waikato. ac. nz / ml / weka / ).', 'provide complementary perspectives.', 'while the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'to compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( details of how the average - expert model performs can be found in our prior work ( #AUTHOR_TAG ).']",2
"['selecting features for korean, we have to ac - count for relatively free word order (Chung et al., 2010).', 'we follow our previous work ( #AUTHOR_TAG ) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see']","['selecting features for korean, we have to ac - count for relatively free word order (Chung et al., 2010).', 'we follow our previous work ( #AUTHOR_TAG ) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see']","['selecting features for korean, we have to ac - count for relatively free word order (Chung et al., 2010).', 'we follow our previous work ( #AUTHOR_TAG ) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ).', 'each word is broken down']","['selecting features for korean, we have to ac - count for relatively free word order (Chung et al., 2010).', 'we follow our previous work ( #AUTHOR_TAG ) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ).', 'each word is broken down into : stem, affixes, stem pos, and affixes pos.', 'we also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.', 'although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data ( section 3 ) and to provide a basis for a preliminary system ( section 4 )']",2
"['( #AUTHOR_TAG ).', '(Schmitt et al., 2009).', 'from all 4, 832 user turns, 68. 5 % were non - angry, 14. 3 % slightly angry, 5. 0 % very angry and 12. 2 % contained garbage, i. e. nonspeech events.', 'in total, the number of interaction parameters']","['( #AUTHOR_TAG ).', '(Schmitt et al., 2009).', 'from all 4, 832 user turns, 68. 5 % were non - angry, 14. 3 % slightly angry, 5. 0 % very angry and 12. 2 % contained garbage, i. e. nonspeech events.', 'in total, the number of interaction parameters servings as input variables for the model amounts to 52']","['. ( #AUTHOR_TAG ).', '(Schmitt et al., 2009).', 'from all 4, 832 user turns, 68. 5 % were non - angry, 14. 3 % slightly angry, 5. 0 % very angry and 12. 2 % contained garbage, i. e. nonspeech events.', 'in total, the number of interaction parameters servings as input variables for the model amounts to 52']","['same annotation scheme as in our previous work on anger detection has been applied, see e. g. ( #AUTHOR_TAG ).', '(Schmitt et al., 2009).', 'from all 4, 832 user turns, 68. 5 % were non - angry, 14. 3 % slightly angry, 5. 0 % very angry and 12. 2 % contained garbage, i. e. nonspeech events.', 'in total, the number of interaction parameters servings as input variables for the model amounts to 52']",2
"[') each word type has non - zero probability only on a single class.', 'given a one - toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'the brown clustering algorithm works by starting with an initial assignment of word types to classes ( which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus ), and then iteratively selecting the pair of classes to merge that would lead to the highest post - merge log - likelihood, doing so until all classes have been merged.', 'this process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ).', 'there are other similar models of distributional clustering of english words which can be similarly effective (Pereira et al., 1993)']","['the actual word types in the bigram, and ( 3 ) each word type has non - zero probability only on a single class.', 'given a one - toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'the brown clustering algorithm works by starting with an initial assignment of word types to classes ( which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus ), and then iteratively selecting the pair of classes to merge that would lead to the highest post - merge log - likelihood, doing so until all classes have been merged.', 'this process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ).', 'there are other similar models of distributional clustering of english words which can be similarly effective (Pereira et al., 1993)']","[') each word type has non - zero probability only on a single class.', 'given a one - toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'the brown clustering algorithm works by starting with an initial assignment of word types to classes ( which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus ), and then iteratively selecting the pair of classes to merge that would lead to the highest post - merge log - likelihood, doing so until all classes have been merged.', 'this process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ).', 'there are other similar models of distributional clustering of english words which can be similarly effective (Pereira et al., 1993)']","['simple language model that discovers useful embeddings is known as brown clustering (Brown et al., 1992).', 'a brown clustering is a class - based bigram model in which ( 1 ) the probability of a document is the product of the probabilities of its bigrams, ( 2 ) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and ( 3 ) each word type has non - zero probability only on a single class.', 'given a one - toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'the brown clustering algorithm works by starting with an initial assignment of word types to classes ( which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus ), and then iteratively selecting the pair of classes to merge that would lead to the highest post - merge log - likelihood, doing so until all classes have been merged.', 'this process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ).', 'there are other similar models of distributional clustering of english words which can be similarly effective (Pereira et al., 1993)']",4
"['#AUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as']","['#AUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as']","['#AUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as']","['#AUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as seen in section 2. 1, brown clusters are hierarchical, we use features corresponding to prefixes of the path from the root to the leaf for each word type']",5
"['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in #AUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the bilou encoding, where each token can either begin an entity, be inside an entity, be the last token in an entity, be']","['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in #AUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the bilou encoding, where each token can either begin an entity, be inside an entity, be the last token in an entity, be']","['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in #AUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the bilou encoding, where each token can either begin an entity, be inside an entity, be the last token in an entity, be']","['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in #AUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the bilou encoding, where each token can either begin an entity, be inside an entity, be the last token in an entity, be outside an entity, or be the single unique token in an entity']",4
"['by recent work on learning syntactic categories ( #AUTHOR_TAG ), which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models, such as class - based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be']","['by recent work on learning syntactic categories ( #AUTHOR_TAG ), which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models, such as class - based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be']","['capture syntagmatic patterns, we choose in this work standard n - gram language models as the basis for a concrete model implementing our scheme.', 'this choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ), which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models, such as class - based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly']","['hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.', 'it is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (Levin, 1993;Hanks, 2013).', 'this provides grounds to expect that such model has the potential to excel for verbs.', 'to capture syntagmatic patterns, we choose in this work standard n - gram language models as the basis for a concrete model implementing our scheme.', 'this choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ), which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models, such as class - based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme']",4
"['##g work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['##g work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is']","['of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is']","['of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is naturally complementary to these efforts, as their methods require a corpus of in - vocab text to serve as seed sentences']","['motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is naturally complementary to these efforts, as their methods require a corpus of in - vocab text to serve as seed sentences']",4
"['##g work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['##g work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['##g work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan #AUTHOR_TAG ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan #AUTHOR_TAG ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['##g work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan #AUTHOR_TAG ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan #AUTHOR_TAG ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. length of words ( l ) : instead of using the raw length value as a feature, we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean']","['. char - n - grams ( g ) : we start with a character n - gram - based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'following the work of King and Abney (2013), we select character n - grams ( n = 1 to 5 ) and the word as the features in our experiments.', '2. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. length of words ( l ) : instead of using the raw length value as a feature, we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean']","['. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. length of words ( l ) : instead of using the raw length value as a feature, we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '']","['. char - n - grams ( g ) : we start with a character n - gram - based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'following the work of King and Abney (2013), we select character n - grams ( n = 1 to 5 ) and the word as the features in our experiments.', '2. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. length of words ( l ) : instead of using the raw length value as a feature, we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean features to encode capitalization information : whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized']",2
"['available dictionaries in previous experiments.', 'raw length value as a feature, we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean']","['available dictionaries in previous experiments.', 'raw length value as a feature, we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean']","['( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature, we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '']","['. char - n - grams ( g ) : we start with a character n - gram - based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'following the work of King and Abney (2013), we select character n - grams ( n = 1 to 5 ) and the word as the features in our experiments.', '2. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature, we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean features to encode capitalization information : whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized']",2
"['shape - based metric.', ""the only disambiguation metric that we used in our previous work ( #AUTHOR_TAG b ) was the shape - based metric, according to which the ` ` best'' trees are those that are skewed to the right."", 'the explanation for this metric is that text processing is, essentially, a left - to - right process.', 'in many genres, people write texts']","['shape - based metric.', ""the only disambiguation metric that we used in our previous work ( #AUTHOR_TAG b ) was the shape - based metric, according to which the ` ` best'' trees are those that are skewed to the right."", 'the explanation for this metric is that text processing is, essentially, a left - to - right process.', 'in many genres, people write texts']","['shape - based metric.', ""the only disambiguation metric that we used in our previous work ( #AUTHOR_TAG b ) was the shape - based metric, according to which the ` ` best'' trees are those that are skewed to the right."", 'the explanation for this metric is that text processing is, essentially, a left - to - right process.', 'in many genres, people write texts']","['shape - based metric.', ""the only disambiguation metric that we used in our previous work ( #AUTHOR_TAG b ) was the shape - based metric, according to which the ` ` best'' trees are those that are skewed to the right."", 'the explanation for this metric is that text processing is, essentially, a left - to - right process.', 'in many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', 'i the more text writers add, the more they elaborate on the text that went before : as a consequence, incremental discourse building consists mostly of expansion of the tight branches.', 'according to the shape - based metric, we consider that a discourse tree a is "" better "" than another discourse tree b if a is more skewed to the right than b ( see Marcu (1997 c ) for a mathematical formulation of the notion of skewedness )']",2
"['.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english and french.', 'in the case of english - chinese alignment, where there are no cognates shared by the two languages, only the html markup in both texts are taken as cognates.', 'because the html structures of parallel pages are normally similar, the markup was found to be helpful for alignment']",0
"[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus ( #AUTHOR_TAG ).', 'in this paper, we will describe a method which automatically searches for parallel']","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus ( #AUTHOR_TAG ).', 'in this paper, we will describe a method which automatically searches for parallel']","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus ( #AUTHOR_TAG ).', 'in this paper, we will describe a method which automatically searches for parallel texts on the web.', ""we will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in clir""]","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus ( #AUTHOR_TAG ).', 'in this paper, we will describe a method which automatically searches for parallel texts on the web.', ""we will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in clir""]",0
"['.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english and french.', 'in the case of english - chinese alignment, where there are no cognates shared by the two languages, only the html markup in both texts are taken as cognates.', 'because the html structures of parallel pages are normally similar, the markup was found to be helpful for alignment']",0
"['.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in ( #AUTHOR_TAG ).', 'we hope to implement such correspondences in our future research']","['also be incorporated.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in ( #AUTHOR_TAG ).', 'we hope to implement such correspondences in our future research']","['.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in ( #AUTHOR_TAG ).', 'we hope to implement such correspondences in our future research']","['html markups, other criteria may also be incorporated.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in ( #AUTHOR_TAG ).', 'we hope to implement such correspondences in our future research']",3
"['.', 'a number of alignment techniques have been proposed, varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english and french.', 'in the case of english - chinese alignment, where there are no cognates shared by the two languages, only the html markup in both texts are taken as cognates.', 'because the html structures of parallel pages are normally similar, the markup was found to be helpful for alignment']",0
"[""##es is provided by rosa's carmel system ( rosa 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG""]","['first system we have implemented with ape is a prototype atlas - andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.', 'figure 4 shows the architecture of atlas - andes ; any other system built with ape would look similar.', ""robust natural language understanding in atlas - andes is provided by rosa's carmel system ( rosa 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG""]","[""##es is provided by rosa's carmel system ( rosa 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG""]","['first system we have implemented with ape is a prototype atlas - andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.', 'figure 4 shows the architecture of atlas - andes ; any other system built with ape would look similar.', ""robust natural language understanding in atlas - andes is provided by rosa's carmel system ( rosa 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG""]",5
"[""see ( #AUTHOR_TAG ) for how mimic's dialoguelevel knowledge is used to override default prosodic assignments for concept - to - speech generation""]","[""see ( #AUTHOR_TAG ) for how mimic's dialoguelevel knowledge is used to override default prosodic assignments for concept - to - speech generation""]","[""see ( #AUTHOR_TAG ) for how mimic's dialoguelevel knowledge is used to override default prosodic assignments for concept - to - speech generation""]","[""see ( #AUTHOR_TAG ) for how mimic's dialoguelevel knowledge is used to override default prosodic assignments for concept - to - speech generation""]",0
"['#AUTHOR_TAG ) ).', 'to instantiate an attribute,']","['#AUTHOR_TAG ) ).', 'to instantiate an attribute,']","['#AUTHOR_TAG ) ).', 'to instantiate an attribute, mimic adopts the lnfoseek dialogue act to solicit the missing information.', 'in contrast, when mimic has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute ( giveopt']","['strategies employed when mimic has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e. g., ( bennacef et at., 1996 ; #AUTHOR_TAG ) ).', 'to instantiate an attribute, mimic adopts the lnfoseek dialogue act to solicit the missing information.', 'in contrast, when mimic has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute ( giveoptions ).', 'given an invalid query, mimic notifies the user of the failed query and provides an openended prompt when it only has dialogue initiative.', ""when mimic has both initiatives, however, in addition to no - tifyfailure, it suggests an alternative close to the user's original query and provides a limited prompt."", 'finally, when mimic has neither initiative, it simply adopts no - tifyfailure, allowing the user to determine the next discourse goal']",1
"['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative affects the cur - 5an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1']",0
"['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative affects the cur - 5an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1']",0
"['paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user']","['paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user']","['describes the evaluation process and results in further detail ( #AUTHOR_TAG ).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme (Walker et al., 1997), were automatically log']","[""conducted two experiments to evaluate mimic's automatic adaptation capabilities."", 'we compared mimic with two control systems : mimic - si, a system - initiative version of mimic in which the system retains both initiatives throughout the dialogue, and mimic - mi, a nonadaptive mixed - initiative version of mimic that resembles the behavior of many existing dialogue systems.', 'in this section we summarize these experiments and their results.', 'a companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme (Walker et al., 1997), were automatically logged, derived, or manually annotated.', 'in addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response']",2
"['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ), which we leave for future work']","['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ), which we leave for future work']","['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ), which we leave for future work']","['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ), which we leave for future work']",3
"['was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme ( #AUTHOR_TAG ), were']","['was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme ( #AUTHOR_TAG ), were']","['describes the evaluation process and results in further detail ( chu - Carroll and Nickerson, 2000).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme ( #AUTHOR_TAG ), were automatically log']","[""conducted two experiments to evaluate mimic's automatic adaptation capabilities."", 'we compared mimic with two control systems : mimic - si, a system - initiative version of mimic in which the system retains both initiatives throughout the dialogue, and mimic - mi, a nonadaptive mixed - initiative version of mimic that resembles the behavior of many existing dialogue systems.', 'in this section we summarize these experiments and their results.', 'a companion paper describes the evaluation process and results in further detail ( chu - Carroll and Nickerson, 2000).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme ( #AUTHOR_TAG ), were automatically logged, derived, or manually annotated.', 'in addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response']",5
"['##issa project ( #AUTHOR_TAG ).', 'it is intended to both speed the development of slss and to localize the speech -']","['traditional desktop applications - - this is similar to the goals of the melissa project ( #AUTHOR_TAG ).', 'it is intended to both speed the development of slss and to localize the speech - specific code within the application.', 'javox allows']","['enable traditional desktop applications - - this is similar to the goals of the melissa project ( #AUTHOR_TAG ).', 'it is intended to both speed the development of slss and to localize the speech - specific code within the application.', 'javox allows']","['systems to assist in the development of spoken - langnage systems ( slss ) have focused on building stand - alone, customized applications, such as (Sutton et al., 1996) and (Pargellis et al., 1999).', 'the goal of the javox toolkit is to speech - enable traditional desktop applications - - this is similar to the goals of the melissa project ( #AUTHOR_TAG ).', 'it is intended to both speed the development of slss and to localize the speech - specific code within the application.', 'javox allows developers to add speech interfaces to applications at the end of the development process ; slss no longer need to be built from the ground up']",1
"['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints (Ganssier, 1995;Kupiec, 1993; hua chen and chen, 94 ; Fung, 1995;Evans and Zhai, 1996).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ;  lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints (Ganssier, 1995;Kupiec, 1993; hua chen and chen, 94 ; Fung, 1995;Evans and Zhai, 1996).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ;  lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints (Ganssier, 1995;Kupiec, 1993; hua chen and chen, 94 ; Fung, 1995;Evans and Zhai, 1996).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ;  lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints (Ganssier, 1995;Kupiec, 1993; hua chen and chen, 94 ; Fung, 1995;Evans and Zhai, 1996).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ;  lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part - of - speech tags.', ""an excerpt of the association probabilities of a unit model trained considering only the np - sequences is given in table 3. applying this filter ( referred to as jrnp in the following ) to the 39, 093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35, 939 of them ( 92 % )."", '4 : completion results of several translation models, spared : theoretical proportion of characters saved ; ok : number of target units accepted by the user ; good : number of target units that matched the expected whether they were proposed or not ; nu : number of sentences for which no target unit was found by the translation model ; u : number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed']",0
"['; #AUTHOR_TAG ; Russell , 1998 )']","['; #AUTHOR_TAG ; Russell , 1998 )']","['; #AUTHOR_TAG ; Russell , 1998 )']","['relevant units in a text has been explored in many areas of natural language processing.', 'our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus.', 'for sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus.', 'this method allows the efficient retrieval of arbitrary length n - grams ( nagao and mori, 94 ; haruno et al., 96 ; ikehaxa et al., 96 ; #AUTHOR_TAG ; Russell , 1998 )']",0
"['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua chen and chen, 94 ; Fung , 1995 ; Evans and Zhai , 1996 ).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997; lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua chen and chen, 94 ; Fung , 1995 ; Evans and Zhai , 1996 ).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997; lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua chen and chen, 94 ; Fung , 1995 ; Evans and Zhai , 1996 ).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997; lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua chen and chen, 94 ; Fung , 1995 ; Evans and Zhai , 1996 ).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997; lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part - of - speech tags.', ""an excerpt of the association probabilities of a unit model trained considering only the np - sequences is given in table 3. applying this filter ( referred to as jrnp in the following ) to the 39, 093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35, 939 of them ( 92 % )."", '4 : completion results of several translation models, spared : theoretical proportion of characters saved ; ok : number of target units accepted by the user ; good : number of target units that matched the expected whether they were proposed or not ; nu : number of sentences for which no target unit was found by the translation model ; u : number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed']",5
"['speech and language processing architecture is based on that of the sri commandtalk system ( #AUTHOR_TAG ; stent et a., 1999 ).', 'the system comprises a suite of about 20 agents, connected together using the spd open agent architecture ( oaa ; (Martin et al., 1998) ).', 'speech recognition is performed using a version of the nuance recognizer (Nuance, 2000).', 'initial language processing is carried out using the sri gemini system (Dowding et al., 1993), using a domain ~ independent unification grammar and a domain - specific lexicon.', 'the language processing grammar is compiled into a recognition grarnm ~ kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized.', 'output from the initial language - processing step is represented in a version of quasi logical form ( van Eijck and Moore, 1992), and']","['speech and language processing architecture is based on that of the sri commandtalk system ( #AUTHOR_TAG ; stent et a., 1999 ).', 'the system comprises a suite of about 20 agents, connected together using the spd open agent architecture ( oaa ; (Martin et al., 1998) ).', 'speech recognition is performed using a version of the nuance recognizer (Nuance, 2000).', 'initial language processing is carried out using the sri gemini system (Dowding et al., 1993), using a domain ~ independent unification grammar and a domain - specific lexicon.', 'the language processing grammar is compiled into a recognition grarnm ~ kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized.', 'output from the initial language - processing step is represented in a version of quasi logical form ( van Eijck and Moore, 1992), and']","['speech and language processing architecture is based on that of the sri commandtalk system ( #AUTHOR_TAG ; stent et a., 1999 ).', 'the system comprises a suite of about 20 agents, connected together using the spd open agent architecture ( oaa ; (Martin et al., 1998) ).', 'speech recognition is performed using a version of the nuance recognizer (Nuance, 2000).', 'initial language processing is carried out using the sri gemini system (Dowding et al., 1993), using a domain ~ independent unification grammar and a domain - specific lexicon.', 'the language processing grammar is compiled into a recognition grarnm ~ kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized.', 'output from the initial language - processing step is represented in a version of quasi logical form ( van Eijck and Moore, 1992), and']","['speech and language processing architecture is based on that of the sri commandtalk system ( #AUTHOR_TAG ; stent et a., 1999 ).', 'the system comprises a suite of about 20 agents, connected together using the spd open agent architecture ( oaa ; (Martin et al., 1998) ).', 'speech recognition is performed using a version of the nuance recognizer (Nuance, 2000).', 'initial language processing is carried out using the sri gemini system (Dowding et al., 1993), using a domain ~ independent unification grammar and a domain - specific lexicon.', 'the language processing grammar is compiled into a recognition grarnm ~ kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized.', 'output from the initial language - processing step is represented in a version of quasi logical form ( van Eijck and Moore, 1992), and passed in that form to the dialogue manager.', 'we refer to these as linguistic level representations']",5
"['interbot project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'a number of other systems have addressed part of the task.', 'cornmandtalk ( #AUTHOR_TAG ), circuit fix - it shop ( Smith , 1997 ) and trains - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they']","['to semi - antonomous robots include srrs flakey robot (Konolige et al., 1993) and ncarars interbot project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'a number of other systems have addressed part of the task.', 'cornmandtalk ( #AUTHOR_TAG ), circuit fix - it shop ( Smith , 1997 ) and trains - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they']","['basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'as evidence of the importance of this task in the nlp community note that the early, influential system shrdlu (Winograd, 1973) was intended to address just this type of problem.', 'more recent work on spoken language interfaces to semi - antonomous robots include srrs flakey robot (Konolige et al., 1993) and ncarars interbot project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'a number of other systems have addressed part of the task.', 'cornmandtalk ( #AUTHOR_TAG ), circuit fix - it shop ( Smith , 1997 ) and trains - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi -']","['basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'as evidence of the importance of this task in the nlp community note that the early, influential system shrdlu (Winograd, 1973) was intended to address just this type of problem.', 'more recent work on spoken language interfaces to semi - antonomous robots include srrs flakey robot (Konolige et al., 1993) and ncarars interbot project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'a number of other systems have addressed part of the task.', 'cornmandtalk ( #AUTHOR_TAG ), circuit fix - it shop ( Smith , 1997 ) and trains - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi - autonomous agents.', ""jack's moose lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi - autonomous."", 'other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995;Pyre et al., 1995).', 'in most of this and other related work the treatment is some variant of the following.', 'if there is a speech interface, the input speech signal is converted into text.', ""text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command ; this formula is then fed into a command interpreter, which executes the command""]",0
"['toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo #AUTHOR_TAG ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['by an alignment program.', 'this alignment is done on the basis of both length ( gale and church [ 7 ] ) and a notion of cognateness ( simard [ 161 ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo #AUTHOR_TAG ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['##e and church [ 7 ] ) and a notion of cognateness ( simard [ 161 ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo #AUTHOR_TAG ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['.', 'an aligner.', 'after identification of word and sentence boundaries the text is processed into a bi - text by an alignment program.', 'this alignment is done on the basis of both length ( gale and church [ 7 ] ) and a notion of cognateness ( simard [ 161 ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo #AUTHOR_TAG ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']",5
"['##e and church #AUTHOR_TAG ) and a notion of cognateness ( simard [ 16 ] ).', '2']","['by an alignment program.', 'this alignment is done on the basis of both length ( gale and church #AUTHOR_TAG ) and a notion of cognateness ( simard [ 16 ] ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo [ 13 ] ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['##e and church #AUTHOR_TAG ) and a notion of cognateness ( simard [ 16 ] ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo [ 13 ] ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['.', 'an aligner.', 'after identification of word and sentence boundaries the text is processed into a bi - text by an alignment program.', 'this alignment is done on the basis of both length ( gale and church #AUTHOR_TAG ) and a notion of cognateness ( simard [ 16 ] ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo [ 13 ] ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']",5
"['## before indexing the text, we process it with textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ), which performs lemmatization, and discovers proper names and technical terms.', 'we added a new module ( resporator ) which annotates text segments with qa - tokens using pattern matching.', 'thus the']","['## before indexing the text, we process it with textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ), which performs lemmatization, and discovers proper names and technical terms.', 'we added a new module ( resporator ) which annotates text segments with qa - tokens using pattern matching.', 'thus the']","['## before indexing the text, we process it with textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ), which performs lemmatization, and discovers proper names and technical terms.', 'we added a new module ( resporator ) which annotates text segments with qa - tokens using pattern matching.', 'thus the text "" for 5 centuries "" matches the durations pattern "" for : cardinal _ timeperiod "", where : car - dinal is the label for cardinal numbers, and _ timeperiod marks a time expression']","['## before indexing the text, we process it with textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ), which performs lemmatization, and discovers proper names and technical terms.', 'we added a new module ( resporator ) which annotates text segments with qa - tokens using pattern matching.', 'thus the text "" for 5 centuries "" matches the durations pattern "" for : cardinal _ timeperiod "", where : car - dinal is the label for cardinal numbers, and _ timeperiod marks a time expression']",5
['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],"['##ana : morphological analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'we are using a lexicon of approx.', '100000 word stems of german ( #AUTHOR_TAG )']","['##ana : morphological analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'we are using a lexicon of approx.', '100000 word stems of german ( #AUTHOR_TAG )']",5
"['text processing ( #AUTHOR_TAG ).', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build a single multicategorizer except for svm - light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines']","['text processing ( #AUTHOR_TAG ).', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build a single multicategorizer except for svm - light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines']","['preprocessing of text documents is carried out by re - using smes, an information extraction core system for real - world german text processing ( #AUTHOR_TAG ).', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build a single multicategorizer except for svm - light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines']","['preprocessing of text documents is carried out by re - using smes, an information extraction core system for real - world german text processing ( #AUTHOR_TAG ).', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build a single multicategorizer except for svm - light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines includes a text tokenizer, a lexical processor and a chunk parser.', 'the chunk parser itself is subdivided into three components.', 'in the first step, phrasal fragments like general nominal expressions and verb groups are recognized.', 'next, the dependency - based structure of the fragments of each sentence is computed using a set of specific sentence patterns.', 'third, the grammatical functions are determined for each dependency - based structure on the basis of a large subcategorization lexicon.', 'the present application benefits from the high modularity of the usage of the components.', 'thus, it is possible to run only a subset of the components and to tailor their output.', 'the experiments described in section 4 make use of this feature']",5
"['np - based qa system.', 'our implementation of the np - based qa system uses the empire noun phrase finder, which is described in detail in #AUTHOR_TAG.', 'empire identifies base nps']","['np - based qa system.', 'our implementation of the np - based qa system uses the empire noun phrase finder, which is described in detail in #AUTHOR_TAG.', 'empire identifies base nps - - non - recursive noun phrases - - using a very simple algorithm that matches part - of - speech tag sequences based on a learned noun phrase grammar.', 'the approach is able to achieve 94 % precision and recall for base nps derived from the penn treebank wall street journal (Marcus et al., 1993).', 'in the experiments below, the np filter follows the application of the document retrieval and text summarization components.', 'pronoun answer hypotheses are discarded, and the nps are assembled']","['np - based qa system.', 'our implementation of the np - based qa system uses the empire noun phrase finder, which is described in detail in #AUTHOR_TAG.', 'empire identifies base nps']","['np - based qa system.', 'our implementation of the np - based qa system uses the empire noun phrase finder, which is described in detail in #AUTHOR_TAG.', 'empire identifies base nps - - non - recursive noun phrases - - using a very simple algorithm that matches part - of - speech tag sequences based on a learned noun phrase grammar.', 'the approach is able to achieve 94 % precision and recall for base nps derived from the penn treebank wall street journal (Marcus et al., 1993).', 'in the experiments below, the np filter follows the application of the document retrieval and text summarization components.', 'pronoun answer hypotheses are discarded, and the nps are assembled into 50 - byte chunks']",5
"['text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ), we again propose the use of vector space methods from ir, which can be easily']","['next hypothesize that query - dependent text summarization algorithms will improve the performance of the qa system by focusing the system on the most relevant portions of the retrieved documents.', 'the goal for query - dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'although a number of methods for query - dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ), we again propose the use of vector space methods from ir, which can be easily']","['next hypothesize that query - dependent text summarization algorithms will improve the performance of the qa system by focusing the system on the most relevant portions of the retrieved documents.', 'the goal for query - dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'although a number of methods for query - dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ), we again propose the use of vector space methods from ir, which can be easily']","['next hypothesize that query - dependent text summarization algorithms will improve the performance of the qa system by focusing the system on the most relevant portions of the retrieved documents.', 'the goal for query - dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'although a number of methods for query - dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ), we again propose the use of vector space methods from ir, which can be easily extended to the summarization task ( Salton et al. , 1994 )']",1
"['aim of this paper is to give a detailed account of the techniques used in tnt.', 'additionally, we present results of the tagger on the negra corpus (Brants et al., 1999) and the penn treebank (Marcus et al., 1993).', 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in ( #AUTHOR_TAG ).', '']","['aim of this paper is to give a detailed account of the techniques used in tnt.', 'additionally, we present results of the tagger on the negra corpus (Brants et al., 1999) and the penn treebank (Marcus et al., 1993).', 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in ( #AUTHOR_TAG ).', '']","['aim of this paper is to give a detailed account of the techniques used in tnt.', 'additionally, we present results of the tagger on the negra corpus (Brants et al., 1999) and the penn treebank (Marcus et al., 1993).', 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in ( #AUTHOR_TAG ).', '']","['aim of this paper is to give a detailed account of the techniques used in tnt.', 'additionally, we present results of the tagger on the negra corpus (Brants et al., 1999) and the penn treebank (Marcus et al., 1993).', 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in ( #AUTHOR_TAG ).', 'for a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999)']",1
"['of languages.', 'according to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ), and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ), the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']","['of languages.', 'according to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ), and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ), the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']","['and the english corpus.', 'they do so for several other corpora as well.', 'the architecture remains applicable to a large variety of languages.', 'according to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ), and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ), the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']","['have shown that a tagger based on markov models yields state - of - the - art results, despite contrary claims found in the literature.', 'for example, the markov model tagger used in the comparison of ( van Halteren et al., 1998) yielded worse results than all other taggers.', 'in our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor.', 'the rather large amount of freedom was not handled in detail in previous publications : handling of start - and end - of - sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.', 'note that the decisions we made yield good results for both the german and the english corpus.', 'they do so for several other corpora as well.', 'the architecture remains applicable to a large variety of languages.', 'according to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ), and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ), the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']",1
"['similarity measures were examined.', 'the cosine coefficient ( r98 ( s, co, ) ) and dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach ( #AUTHOR_TAG ) is computationally expensive, it does produce more precise segmentation""]","['similarity measures were examined.', 'the cosine coefficient ( r98 ( s, co, ) ) and dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach ( #AUTHOR_TAG ) is computationally expensive, it does produce more precise segmentation""]","['similarity measures were examined.', 'the cosine coefficient ( r98 ( s, co, ) ) and dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach ( #AUTHOR_TAG ) is computationally expensive, it does produce more precise segmentation""]","['similarity measures were examined.', 'the cosine coefficient ( r98 ( s, co, ) ) and dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach ( #AUTHOR_TAG ) is computationally expensive, it does produce more precise segmentation""]",1
"[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies f ( wi, wj ), the transition probability matrix t is computed by equation 10.', 'equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm ( y ) converts a matrix y into a transition matrix, x = 5 was used in the experiment']","[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies f ( wi, wj ), the transition probability matrix t is computed by equation 10.', 'equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm ( y ) converts a matrix y into a transition matrix, x = 5 was used in the experiment']","[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies f ( wi, wj ), the transition probability matrix t is computed by equation 10.', 'equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm ( y ) converts a matrix y into a transition matrix, x = 5 was used in the experiment']","[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies f ( wi, wj ), the transition probability matrix t is computed by equation 10.', 'equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm ( y ) converts a matrix y into a transition matrix, x = 5 was used in the experiment']",2
"['the unification algorithm is at variance with the findings of #AUTHOR_TAG, who report large speed - ups from the elimination of disjunction processing during unification.', 'unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison']","['the unification algorithm is at variance with the findings of #AUTHOR_TAG, who report large speed - ups from the elimination of disjunction processing during unification.', 'unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison']","['the unification algorithm is at variance with the findings of #AUTHOR_TAG, who report large speed - ups from the elimination of disjunction processing during unification.', 'unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison']","['related work, Miyao (1999) describes an ap - proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'although the approach may have potential, the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG, who report large speed - ups from the elimination of disjunction processing during unification.', 'unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison']",1
"['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ).', 'we leave this for future work']","['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ).', 'we leave this for future work']","['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ).', 'we leave this for future work']","['this paper we have provided an original mathematical argument in favour of this thesis.', 'our results hold for bilexical context - free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism ( see section 1 ).', 'we perceive that these results can be extended to other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ).', 'we leave this for future work']",3
"['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ).', 'we leave this for future work']","['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ).', 'we leave this for future work']","['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ).', 'we leave this for future work']","['this paper we have provided an original mathematical argument in favour of this thesis.', 'our results hold for bilexical context - free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism ( see section 1 ).', 'we perceive that these results can be extended to other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ).', 'we leave this for future work']",3
"['is not the best measure to assess segmentation quality, therefore we also conducted experiments using the windowdiff measure as proposed by #AUTHOR_TAG.', 'windowdiff returns 0 in case of a perfect segmentation ; 1 is the worst possible score.', 'however, it only takes into account segment boundaries and disregards segment types.', 'in section 5. 2, we mentioned that loopy bp is not']","['is not the best measure to assess segmentation quality, therefore we also conducted experiments using the windowdiff measure as proposed by #AUTHOR_TAG.', 'windowdiff returns 0 in case of a perfect segmentation ; 1 is the worst possible score.', 'however, it only takes into account segment boundaries and disregards segment types.', 'in section 5. 2, we mentioned that loopy bp is not']","['is not the best measure to assess segmentation quality, therefore we also conducted experiments using the windowdiff measure as proposed by #AUTHOR_TAG.', 'windowdiff returns 0 in case of a perfect segmentation ; 1 is the worst possible score.', 'however, it only takes into account segment boundaries and disregards segment types.', 'in section 5. 2, we mentioned that loopy bp is not']","['is not the best measure to assess segmentation quality, therefore we also conducted experiments using the windowdiff measure as proposed by #AUTHOR_TAG.', 'windowdiff returns 0 in case of a perfect segmentation ; 1 is the worst possible score.', 'however, it only takes into account segment boundaries and disregards segment types.', 'in section 5. 2, we mentioned that loopy bp is not guaranteed to converge in a finite number of iterations.', 'since we optimize pseudolikelihood for parameter estimation, we are not affected by this limitation in the training phase.', 'however, we use loopy bp with a trp schedule during testing, so we must expect to encounter non - convergence for some examples.', 'theoretical results on this topic are discussed by Heskes (2004).', 'we give here an empirical observation of convergence behaviour of loopy bp in our setting ; the maximum number of iterations of the trp schedule was restricted to 1, 000.', 'table 4 shows the percentage of examples converging within this limit and the average number of iterations required by the converging examples, broken down by the different corpora.', 'from these results, we conclude that there is a connection between the quality of the annotation and the convergence behaviour of loopy bp.', ""in practice, even though loopy bp didn't converge for some examples, the solutions after 1, 000 iterations where satisfactory""]",5
"['##onyms : for some relations, there is no contradiction when y 1 and y 2 share a meronym, i. e. "" part of "" relation.', 'for example, in the set born in ( mozart,  ) there is no contradiction between the y values "" salzburg "" and "" austria "", but "" salzburg "" conflicts with "" vienna "".', 'although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ), in practice genuine contradictions between y - values sharing a meronym relationship are extremely rare.', 'we therefore simply assigned contradictions between meronyms a probability close to zero.', 'we used the tipster gazetteer 4 and wordnet to identify meronyms, both of which have high precision but low coverage']","['##onyms : for some relations, there is no contradiction when y 1 and y 2 share a meronym, i. e. "" part of "" relation.', 'for example, in the set born in ( mozart,  ) there is no contradiction between the y values "" salzburg "" and "" austria "", but "" salzburg "" conflicts with "" vienna "".', 'although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ), in practice genuine contradictions between y - values sharing a meronym relationship are extremely rare.', 'we therefore simply assigned contradictions between meronyms a probability close to zero.', 'we used the tipster gazetteer 4 and wordnet to identify meronyms, both of which have high precision but low coverage']","['##onyms : for some relations, there is no contradiction when y 1 and y 2 share a meronym, i. e. "" part of "" relation.', 'for example, in the set born in ( mozart,  ) there is no contradiction between the y values "" salzburg "" and "" austria "", but "" salzburg "" conflicts with "" vienna "".', 'although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ), in practice genuine contradictions between y - values sharing a meronym relationship are extremely rare.', 'we therefore simply assigned contradictions between meronyms a probability close to zero.', 'we used the tipster gazetteer 4 and wordnet to identify meronyms, both of which have high precision but low coverage']","['##onyms : for some relations, there is no contradiction when y 1 and y 2 share a meronym, i. e. "" part of "" relation.', 'for example, in the set born in ( mozart,  ) there is no contradiction between the y values "" salzburg "" and "" austria "", but "" salzburg "" conflicts with "" vienna "".', 'although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ), in practice genuine contradictions between y - values sharing a meronym relationship are extremely rare.', 'we therefore simply assigned contradictions between meronyms a probability close to zero.', 'we used the tipster gazetteer 4 and wordnet to identify meronyms, both of which have high precision but low coverage']",4
"['train our model, we use l - bfgs to locally maximize the log of the objective function ( 1 ) : 15 these are the function words with count  40 in a random sample of 100 documents, and which were associated with the o - i tag transition at more than twice the average rate.', 'we do not use any other lexical  - features that reference x, for fear that they would enable the learner to explain the rationales without changing  as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser ( #AUTHOR_TAG ).', 'then the document has one big parse tree, whose root is doc, with each sentence being a child of doc']","['train our model, we use l - bfgs to locally maximize the log of the objective function ( 1 ) : 15 these are the function words with count  40 in a random sample of 100 documents, and which were associated with the o - i tag transition at more than twice the average rate.', 'we do not use any other lexical  - features that reference x, for fear that they would enable the learner to explain the rationales without changing  as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser ( #AUTHOR_TAG ).', 'then the document has one big parse tree, whose root is doc, with each sentence being a child of doc']","['train our model, we use l - bfgs to locally maximize the log of the objective function ( 1 ) : 15 these are the function words with count  40 in a random sample of 100 documents, and which were associated with the o - i tag transition at more than twice the average rate.', 'we do not use any other lexical  - features that reference x, for fear that they would enable the learner to explain the rationales without changing  as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser ( #AUTHOR_TAG ).', 'then the document has one big parse tree, whose root is doc, with each sentence being a child of doc']","['train our model, we use l - bfgs to locally maximize the log of the objective function ( 1 ) : 15 these are the function words with count  40 in a random sample of 100 documents, and which were associated with the o - i tag transition at more than twice the average rate.', 'we do not use any other lexical  - features that reference x, for fear that they would enable the learner to explain the rationales without changing  as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser ( #AUTHOR_TAG ).', 'then the document has one big parse tree, whose root is doc, with each sentence being a child of doc']",5
"[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of #AUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds ( f0 - - f9 ).', 'all our experiments use f9 as their final blind test set']","[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of #AUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds ( f0 - - f9 ).', 'all our experiments use f9 as their final blind test set']","[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of #AUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds ( f0 - - f9 ).', 'all our experiments use f9 as their final blind test set']","[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of #AUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds ( f0 - - f9 ).', 'all our experiments use f9 as their final blind test set']",2
"['.', 'we collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator.', 'our new generative approach exploits the rationales more effectively than our previous "" masking svm "" approach.', 'it is also more principle']","['.', 'we collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator.', 'our new generative approach exploits the rationales more effectively than our previous "" masking svm "" approach.', 'it is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks']","['to a machine learner by highlighting contextual "" rationales "" for each of his or her annotations (Zaidan et al., 2007).', 'how can one exploit this side information to better learn the desired parameters ?', 'we present a generative model of how a given annotator, knowing the true , stochastically chooses rationales.', 'thus, observing the rationales helps us infer the true .', 'we collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator.', 'our new generative approach exploits the rationales more effectively than our previous "" masking svm "" approach.', 'it is also more principle']","['human annotator can provide hints to a machine learner by highlighting contextual "" rationales "" for each of his or her annotations (Zaidan et al., 2007).', 'how can one exploit this side information to better learn the desired parameters ?', 'we present a generative model of how a given annotator, knowing the true , stochastically chooses rationales.', 'thus, observing the rationales helps us infer the true .', 'we collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator.', 'our new generative approach exploits the rationales more effectively than our previous "" masking svm "" approach.', 'it is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks']",5
"['f (  ) extracts a feature vector from a classified document,  are the corresponding weights of those features, and z  ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ).', '']","['f (  ) extracts a feature vector from a classified document,  are the corresponding weights of those features, and z  ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ).', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count  4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0 otherwise.', 'thus   r 17744, and positive weights in  favor class label y = + 1 and equally discourage y = 1, while negative weights do the opposite.', 'this standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'future work should consider more complex features and how they are signaled by rationales, as discussed in section 3. 2']","['f (  ) extracts a feature vector from a classified document,  are the corresponding weights of those features, and z  ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ).', '']","['f (  ) extracts a feature vector from a classified document,  are the corresponding weights of those features, and z  ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ).', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count  4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0 otherwise.', 'thus   r 17744, and positive weights in  favor class label y = + 1 and equally discourage y = 1, while negative weights do the opposite.', 'this standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'future work should consider more complex features and how they are signaled by rationales, as discussed in section 3. 2']",5
"['##sim from equation ( 1 ).', 'we also test an mi model inspired by Erk (2007) : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', 'we gather similar words using #AUTHOR_TAG']","['et al. ( 1999 ) s similarity - smoothed probability to mi by replacing the empirical pr n | v in equa - tion ( 2 ) with the smoothed prsim from equation ( 1 ).', 'we also test an mi model inspired by Erk (2007) : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', 'we gather similar words using #AUTHOR_TAG']","['et al. ( 1999 ) s similarity - smoothed probability to mi by replacing the empirical pr n | v in equa - tion ( 2 ) with the smoothed prsim from equation ( 1 ).', 'we also test an mi model inspired by Erk (2007) : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', 'we gather similar words using #AUTHOR_TAG']","['first evaluate d s p on disambiguating positives from pseudo - negatives, comparing to recently - proposed systems that also require no manually - compiled resources like wordnet.', 'we convert da - gan et al. ( 1999 ) s similarity - smoothed probability to mi by replacing the empirical pr n | v in equa - tion ( 2 ) with the smoothed prsim from equation ( 1 ).', 'we also test an mi model inspired by Erk (2007) : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', ""we gather similar words using #AUTHOR_TAG a ), mining similar verbs from a comparable - sized parsed corpus, and collecting similar nouns from a broader 10 gb corpus of english text. 4 we also use Keller and Lapata ( 2003 )'s approach to obtaining web - counts""]",5
['by #AUTHOR_TAG. 2this'],['by #AUTHOR_TAG. 2this'],"['by #AUTHOR_TAG. 2this data provides counts for pairs such as "" edwin moses, hurdler "" and "" william farley, industrialist.', 'we have features for all concepts and therefore learn their association with each verb']","['also made use of the person - name / instance pairs automatically extracted by #AUTHOR_TAG. 2this data provides counts for pairs such as "" edwin moses, hurdler "" and "" william farley, industrialist.', 'we have features for all concepts and therefore learn their association with each verb']",5
"[""that even the #AUTHOR_TAG system, built on the world's largest corpus, achieves only 34 % recall ( table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed, but see foot""]","[', we evaluate dsp on a common application of selectional preferences : choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""we study the cases where a 9recall that even the #AUTHOR_TAG system, built on the world's largest corpus, achieves only 34 % recall ( table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed, but see footnote 5 )."", 'pronoun is the direct object of a verb predicate, v.', ""a pronoun's antecedent must obey v's selectional preferences."", 'if we have a better model of sp, we should be able to better select pronoun antecedents']","[""that even the #AUTHOR_TAG system, built on the world's largest corpus, achieves only 34 % recall ( table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed, but see footnote 5 )."", 'pronoun is the direct object of a verb predicate, v.', ""a pronoun's antecedent must obey v's selectional preferences."", 'if we have a better model of sp, we should be able to better select pronoun antecedents']","[', we evaluate dsp on a common application of selectional preferences : choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""we study the cases where a 9recall that even the #AUTHOR_TAG system, built on the world's largest corpus, achieves only 34 % recall ( table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed, but see footnote 5 )."", 'pronoun is the direct object of a verb predicate, v.', ""a pronoun's antecedent must obey v's selectional preferences."", 'if we have a better model of sp, we should be able to better select pronoun antecedents']",1
"['( n | garnish ).', '#AUTHOR_TAG']","['example, place high weight on features like pr ( n | braise ), pr ( n | ration ), and pr ( n | garnish ).', '#AUTHOR_TAG']","['( n | garnish ).', '#AUTHOR_TAG']","['is interesting to inspect the feature weights returned by our system.', 'in particular, the weights on the verb co - occurrence features ( section 3. 3. 1 )', 'provide a high - quality, argument - specific similarityranking of other verb contexts.', 'the dsp parameters for eat, for example, place high weight on features like pr ( n | braise ), pr ( n | ration ), and pr ( n | garnish ).', ""#AUTHOR_TAG a )'s similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ), because these have similar subjects to eat."", 'discriminative, context - specific training seems to yield a better set of similar predicates, e. g. the highest - ranked contexts for dsp cooc on the verb join, 3 lead 1. 42, rejoin 1. 39, form 1. 34, belong to 1. 31, found 1. 31, quit 1. 29, guide 1. 19, induct 1. 19, launch ( subj ) 1. 18, work at 1. 14 give a better sims ( join ) for equation ( 1 ) than the top similarities returned by (Lin, 1998 a other features are also weighted intuitively.', 'note that case is a strong indicator for some arguments, for example the weight on being lower - case is high for become ( 0. 972 ) and eat ( 0. 505 ), but highly negative for accuse ( - 0. 675 ) and embroil ( - 0. 573 ) which often take names of people and organizations']",0
"['from the ldc as ldc2006t13.', 'this collection was generated from approximately 1 trillion tokens of online text.', 'unfortunately, tokens appearing less than 200 times have been mapped to the unk symbol, and only n - grams appearing more than 40 times are included.', 'unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'the similarity - smoothed examples will be undefined if sims ( w ) is empty.', 'also, the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']","['from the ldc as ldc2006t13.', 'this collection was generated from approximately 1 trillion tokens of online text.', 'unfortunately, tokens appearing less than 200 times have been mapped to the unk symbol, and only n - grams appearing more than 40 times are included.', 'unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'the similarity - smoothed examples will be undefined if sims ( w ) is empty.', 'also, the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']","['from the ldc as ldc2006t13.', 'this collection was generated from approximately 1 trillion tokens of online text.', 'unfortunately, tokens appearing less than 200 times have been mapped to the unk symbol, and only n - grams appearing more than 40 times are included.', 'unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'the similarity - smoothed examples will be undefined if sims ( w ) is empty.', 'also, the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']","['available from the ldc as ldc2006t13.', 'this collection was generated from approximately 1 trillion tokens of online text.', 'unfortunately, tokens appearing less than 200 times have been mapped to the unk symbol, and only n - grams appearing more than 40 times are included.', 'unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'the similarity - smoothed examples will be undefined if sims ( w ) is empty.', 'also, the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']",5
"['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'this']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'this']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'this data consists of triples ( v, n, n  ) where v, n is a predicateargument pair observed in the corpus and v, n  has not been']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'this data consists of triples ( v, n, n  ) where v, n is a predicateargument pair observed in the corpus and v, n  has not been observed.', 'the models score correctly if they rank observed ( and thus plausible ) arguments above corresponding unobserved ( and thus likely implausible ) ones.', 'we refer to this as pairwise disambiguation.', 'unlike this task, we classify each predicate - argument pair independently as plausible / implausible.', 'we also use mi rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise.']",1
"['the application of interest has been shown previously by #AUTHOR_TAG.', 'they optimize a few meta']","['the application of interest has been shown previously by #AUTHOR_TAG.', 'they optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric']","['the application of interest has been shown previously by #AUTHOR_TAG.', 'they optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric']","['advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG.', 'they optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric']",1
"['by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by #AUTHOR_TAG']","['by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by #AUTHOR_TAG']","['by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by #AUTHOR_TAG']","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a for a fixed verb, mi is proportional to Keller and Lapata (2003)'s conditional probability scores for pseudodisambiguation of ( v, n, n  ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", 'normalizing by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by #AUTHOR_TAG']",0
"['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'this']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'this']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'this data consists of triples ( v, n, n  ) where v, n is a predicateargument pair observed in the corpus and v, n  has not been']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'this data consists of triples ( v, n, n  ) where v, n is a predicateargument pair observed in the corpus and v, n  has not been observed.', 'the models score correctly if they rank observed ( and thus plausible ) arguments above corresponding unobserved ( and thus likely implausible ) ones.', 'we refer to this as pairwise disambiguation.', 'unlike this task, we classify each predicate - argument pair independently as plausible / implausible.', 'we also use mi rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise.']",1
"['predicate - argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'for example, we might have a class mexican food and learn that the entire class is suitable for eating.', 'usually, the classes are from wordnet ( Miller et al. , 1990 ), although they can also be inferred from clustering ( #AUTHOR_TAG ).', 'Brockmann and Lapata (2003) compare a number of wordnet - based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class - based approaches do not always outperform simple frequency - based models']","['predicate - argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'for example, we might have a class mexican food and learn that the entire class is suitable for eating.', 'usually, the classes are from wordnet ( Miller et al. , 1990 ), although they can also be inferred from clustering ( #AUTHOR_TAG ).', 'Brockmann and Lapata (2003) compare a number of wordnet - based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class - based approaches do not always outperform simple frequency - based models']","['predicate - argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'for example, we might have a class mexican food and learn that the entire class is suitable for eating.', 'usually, the classes are from wordnet ( Miller et al. , 1990 ), although they can also be inferred from clustering ( #AUTHOR_TAG ).', 'Brockmann and Lapata (2003) compare a number of wordnet - based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class - based approaches do not always outperform simple frequency - based models']","['approaches to sps generalize from observed predicate - argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'for example, we might have a class mexican food and learn that the entire class is suitable for eating.', 'usually, the classes are from wordnet ( Miller et al. , 1990 ), although they can also be inferred from clustering ( #AUTHOR_TAG ).', 'Brockmann and Lapata (2003) compare a number of wordnet - based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class - based approaches do not always outperform simple frequency - based models']",0
"['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus,']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'next, we omit verb co - occurrence features for nouns that occur less than 10 times, and instead fire a low - count feature.', 'when we move to a new corpus, previouslyunseen nouns are treated like these low - count training nouns']",1
"['have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ).', 'inferences such as "" [ x wins y ]  [ x plays y ] "" are only valid for certain argu - ments x and y.', 'we follow Pantel et al. (2007) in using']","['have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ).', 'inferences such as "" [ x wins y ]  [ x plays y ] "" are only valid for certain argu - ments x and y.', 'we follow Pantel et al. (2007) in using']","['##al preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ).', 'inferences such as "" [ x wins y ]  [ x plays y ] "" are only valid for certain argu - ments x and y.', 'we follow Pantel et al. (2007) in']","['##al preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ).', 'inferences such as "" [ x wins y ]  [ x plays y ] "" are only valid for certain argu - ments x and y.', 'we follow Pantel et al. (2007) in using automatically - extracted semantic classes to help characterize plausible arguments']",0
"['##s ( w ).', 'Erk ( 2007 ) compared a number of techniques for creating similar - word sets and found that both the jaccard coefficient and #AUTHOR_TAG']","['sim ( v , v ) returns a real - valued similarity between two verbs v  and v ( normalized over all pair similarities in the sum ).', 'in contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross - product of similar pairs.', 'one key issue is how to define the set of similar words, sims ( w ).', 'Erk ( 2007 ) compared a number of techniques for creating similar - word sets and found that both the jaccard coefficient and #AUTHOR_TAG']","['##s ( w ).', 'Erk ( 2007 ) compared a number of techniques for creating similar - word sets and found that both the jaccard coefficient and #AUTHOR_TAG']","['sim ( v , v ) returns a real - valued similarity between two verbs v  and v ( normalized over all pair similarities in the sum ).', 'in contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross - product of similar pairs.', 'one key issue is how to define the set of similar words, sims ( w ).', ""Erk ( 2007 ) compared a number of techniques for creating similar - word sets and found that both the jaccard coefficient and #AUTHOR_TAG a )'s information - theoretic metric work best."", 'similarity - smoothed models are simple to compute, potentially adaptable to new domains, and require no manually - compiled resources such as wordnet']",0
"['the 3 gb aquaint corpus ( Voorhees , 2002 ) using minipar ( #AUTHOR_TAG b ), and collected verb - object and verb - subject frequencies, building an empirical mi model from this data.', 'verbs and nouns were converted to their ( possibly multi - token ) root, and string case was preserved.', 'passive subjects ( the car was bought ) were converted to objects ( bought car ).', 'we set the mi - threshold, , to be 0, and the negative - to - positive ratio, k, to be 2']","['parsed the 3 gb aquaint corpus ( Voorhees , 2002 ) using minipar ( #AUTHOR_TAG b ), and collected verb - object and verb - subject frequencies, building an empirical mi model from this data.', 'verbs and nouns were converted to their ( possibly multi - token ) root, and string case was preserved.', 'passive subjects ( the car was bought ) were converted to objects ( bought car ).', 'we set the mi - threshold, , to be 0, and the negative - to - positive ratio, k, to be 2']","['parsed the 3 gb aquaint corpus ( Voorhees , 2002 ) using minipar ( #AUTHOR_TAG b ), and collected verb - object and verb - subject frequencies, building an empirical mi model from this data.', 'verbs and nouns were converted to their ( possibly multi - token ) root, and string case was preserved.', 'passive subjects ( the car was bought ) were converted to objects ( bought car ).', 'we set the mi - threshold, , to be 0, and the negative - to - positive ratio, k, to be 2']","['parsed the 3 gb aquaint corpus ( Voorhees , 2002 ) using minipar ( #AUTHOR_TAG b ), and collected verb - object and verb - subject frequencies, building an empirical mi model from this data.', 'verbs and nouns were converted to their ( possibly multi - token ) root, and string case was preserved.', 'passive subjects ( the car was bought ) were converted to objects ( bought car ).', 'we set the mi - threshold, , to be 0, and the negative - to - positive ratio, k, to be 2']",5
"['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus,']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'next, we omit verb co - occurrence features for nouns that occur less than 10 times, and instead fire a low - count feature.', 'when we move to a new corpus, previouslyunseen nouns are treated like these low - count training nouns']",1
"['learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'to create the positives, we automatically parse a large corpus, and then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi ) ( #AUTHOR_TAG ).', 'the']","['learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'to create the positives, we automatically parse a large corpus, and then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi ) ( #AUTHOR_TAG ).', 'the']","['learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'to create the positives, we automatically parse a large corpus, and then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi ) ( #AUTHOR_TAG ).', 'the mi between a verb predicate, v, and its object argument, n, is']","['learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'to create the positives, we automatically parse a large corpus, and then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi ) ( #AUTHOR_TAG ).', 'the mi between a verb predicate, v, and its object argument, n, is']",5
"['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1for a fixed verb, mi is proportional to #AUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n [UNK] ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", '']","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1for a fixed verb, mi is proportional to #AUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n [UNK] ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", '']","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1for a fixed verb, mi is proportional to #AUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n [UNK] ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", '']","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1for a fixed verb, mi is proportional to #AUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n [UNK] ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", 'normalizing by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by Pantel et al. (2007)']",4
"['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG )']","['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG )']","['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG )']","['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG )']",1
"[', text t and hypothesis h, and attempt to decide if the meaning of h can be inferred from the meaning of t (Dagan et al., 2005).', ""while many approaches have addressed this problem, our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ), which convert the inputs into logical forms and then attempt to ` prove'h from t plus a set of axioms."", 'for']","['entailment systems are given two textual fragments, text t and hypothesis h, and attempt to decide if the meaning of h can be inferred from the meaning of t (Dagan et al., 2005).', ""while many approaches have addressed this problem, our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ), which convert the inputs into logical forms and then attempt to ` prove'h from t plus a set of axioms."", 'for instance, (Braz et al., 2005) represents t,']","['entailment systems are given two textual fragments, text t and hypothesis h, and attempt to decide if the meaning of h can be inferred from the meaning of t (Dagan et al., 2005).', ""while many approaches have addressed this problem, our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ), which convert the inputs into logical forms and then attempt to ` prove'h from t plus a set of axioms."", 'for']","['entailment systems are given two textual fragments, text t and hypothesis h, and attempt to decide if the meaning of h can be inferred from the meaning of t (Dagan et al., 2005).', ""while many approaches have addressed this problem, our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ), which convert the inputs into logical forms and then attempt to ` prove'h from t plus a set of axioms."", 'for instance, (Braz et al., 2005) represents t, h, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer linear program derived from that representation']",1
"['studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the']","['studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the']","['studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full penn treebank tag set.', 'we ran all our estimators in both conditions here ( thanks to noah smith for supplying us with his tag set )']","['studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full penn treebank tag set.', 'we ran all our estimators in both conditions here ( thanks to noah smith for supplying us with his tag set )']",1
"['.', 'on small data sets all of the bayesian estimators strongly outperform em ( and, to a lesser extent, vb ) with respect to all of our evaluation measures, confirming the results reported in #AUTHOR_TAG.', 'this is perhaps not too surprising, as the bayesian prior plays a comparatively stronger role with a smaller training corpus ( which makes the likelihood term smaller ) and the approximation used by variational bayes is likely to be less accurate on smaller data sets']","['be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'on small data sets all of the bayesian estimators strongly outperform em ( and, to a lesser extent, vb ) with respect to all of our evaluation measures, confirming the results reported in #AUTHOR_TAG.', 'this is perhaps not too surprising, as the bayesian prior plays a comparatively stronger role with a smaller training corpus ( which makes the likelihood term smaller ) and the approximation used by variational bayes is likely to be less accurate on smaller data sets']","['.', 'on small data sets all of the bayesian estimators strongly outperform em ( and, to a lesser extent, vb ) with respect to all of our evaluation measures, confirming the results reported in #AUTHOR_TAG.', 'this is perhaps not too surprising, as the bayesian prior plays a comparatively stronger role with a smaller training corpus ( which makes the likelihood term smaller ) and the approximation used by variational bayes is likely to be less accurate on smaller data sets']","['might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'on small data sets all of the bayesian estimators strongly outperform em ( and, to a lesser extent, vb ) with respect to all of our evaluation measures, confirming the results reported in #AUTHOR_TAG.', 'this is perhaps not too surprising, as the bayesian prior plays a comparatively stronger role with a smaller training corpus ( which makes the likelihood term smaller ) and the approximation used by variational bayes is likely to be less accurate on smaller data sets']",1
"['resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG )']","['resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG )']","['resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG )']","['resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG )']",1
"['mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ).', 'Bunescu and Mooney (2005 a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the']","['mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ).', 'Bunescu and Mooney (2005 a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the']","['mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ).', 'Bunescu and Mooney (2005 a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', '']","['few kernels based on dependency trees have also been proposed.', 'Zelenko et al. (2003) proposed a tree kernel over shallow parse tree representations of sentences.', 'this tree kernel was slightly generalized by Culotta and Sorensen (2004) to compute similarity between two dependency trees.', 'in addition to the words, this kernel also incorporates word classes into the kernel.', 'the kernel is based on counting matching subsequences of children of matching nodes.', 'but as was also noted in (Bunescu and Mooney, 2005 a ), this kernel is opaque i. e. it is not obvious what the implicit features are and the authors do not describe it either.', 'in contrast, our dependency - based word subsequence kernel, which also computes similarity between two dependency trees, is very transparent with the implicit features being simply the dependency paths.', 'their kernel is also very time consuming and in their more general sparse setting it requires o ( mn3 ) time and o ( mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ).', 'Bunescu and Mooney (2005 a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', 'their kernel also uses word classes in addition to the words themselves']",3
"['are various strands of future research.', 'firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'secondly, as ( #AUTHOR_TAG ) show, marginal']","['are various strands of future research.', 'firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'secondly, as ( #AUTHOR_TAG ) show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we plan to build our own decoder ( based on itg ) where different ideas can be']","['are various strands of future research.', 'firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'secondly, as ( #AUTHOR_TAG ) show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we plan to build our own decoder ( based on itg ) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'apart from a new decoder, it will be worthwhile adapting the prior probability in our model']","['are various strands of future research.', 'firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'secondly, as ( #AUTHOR_TAG ) show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we plan to build our own decoder ( based on itg ) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'finally, it would be interesting to study properties of the penalized deleted estimation used in this paper']",3
"['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( #AUTHOR_TAG ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( #AUTHOR_TAG ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( #AUTHOR_TAG ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( #AUTHOR_TAG ), and ( Lewis et al. , 2004 )']",0
"[', conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and #AUTHOR_TAG )']","[', conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and #AUTHOR_TAG )']","[', conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and #AUTHOR_TAG )']","['most similar efforts to ours, mainly ( de Nero et al., 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure.', 'in this work we also start out from a generative model with latent segmentation variables.', 'however, we find out that concentrating the learning effort on smoothing is crucial for good performance.', 'for this, we devise itg - based priors over segmentations and employ a penalized version of deleted estimation working with em at its core.', 'the fact that our results ( at least ) match the heuristic estimates on a reasonably sized data set ( 947k parallel sentence pairs ) is rather encouraging']",1
['more flexible regions - - see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -'],"['more flexible regions - - see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition - - and phonological features and syllable boundaries.', 'indeed, our local log - linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'finally, rather than define a fixed set of feature']",['more flexible regions - - see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -'],"['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string - transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al., 2007).', 'latent variables we wish to consider are an increased number of word classes ; more flexible regions - - see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition - - and phonological features and syllable boundaries.', 'indeed, our local log - linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'finally, rather than define a fixed set of feature templates as in fig. 2, we would like to refine empirically useful features during training, resulting in language - specific backoff patterns and adaptively sized n - gram windows.', 'many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods']",0
"['transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive ( #AUTHOR_TAG ).', 'latent']","['training methods that port well across languages and string - transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive ( #AUTHOR_TAG ).', 'latent']","['transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive ( #AUTHOR_TAG ).', 'latent variables we wish to consider are an increased number of word classes ; more flexible regions - see Petrov et al. (2007) on learning a state transition diagram for acoustic regions in phone recognition - and phonological features and syllable boundaries.', '']","['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string - transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive ( #AUTHOR_TAG ).', 'latent variables we wish to consider are an increased number of word classes ; more flexible regions - see Petrov et al. (2007) on learning a state transition diagram for acoustic regions in phone recognition - and phonological features and syllable boundaries.', 'indeed, our local log - linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'finally, rather than define a fixed set of feature templates as in fig. 2, we would like to refine empirically useful features during training, resulting in language - specific backoff patterns and adaptively sized n - gram windows.', 'many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods']",3
"['methods to improve the similarity estimates.', '#AUTHOR_TAG']","['methods to improve the similarity estimates.', '#AUTHOR_TAG']","['different methods on the propbank dataset.', 'our best performing method showed a significant improvement over the supervised model and over methods previously proposed in the literature.', 'on the full training set the best method performed 2. 33 % better than the fully supervised model, which is a 10. 91 % error reduction.', 'using only 5 % of the training data the best semi - supervised model still achieved 60. 29 %, compared to 40. 49 % by the supervised model, which is an error reduction of 33. 27 %.', 'these results demonstrate that the latent words learned by the lwlm help for this complex information extraction task.', 'furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'we would like to perform experiments on employing this model in other information extraction tasks, such as word sense disambiguation or named entity recognition.', 'the current model uses the context in a very straightforward way, i. e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', '#AUTHOR_TAG']","['have presented the latent words language model and showed how it learns, from unlabeled texts, latent words that capture the meaning of a certain word, depending on the context.', 'we then experimented with different methods to incorporate the latent words for semantic role labeling, and tested different methods on the propbank dataset.', 'our best performing method showed a significant improvement over the supervised model and over methods previously proposed in the literature.', 'on the full training set the best method performed 2. 33 % better than the fully supervised model, which is a 10. 91 % error reduction.', 'using only 5 % of the training data the best semi - supervised model still achieved 60. 29 %, compared to 40. 49 % by the supervised model, which is an error reduction of 33. 27 %.', 'these results demonstrate that the latent words learned by the lwlm help for this complex information extraction task.', 'furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'we would like to perform experiments on employing this model in other information extraction tasks, such as word sense disambiguation or named entity recognition.', 'the current model uses the context in a very straightforward way, i. e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']",0
"['( e. g., #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ), to ranking models for web search']","['( e. g., #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ), to ranking models for web search']","['this paper, we extend two classes of model adaptation methods ( i. e., model interpolation and error - driven learning ), which have been well studied in statistical language modeling for speech and natural language applications ( e. g., #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ), to ranking models for web search applications']","['this paper, we extend two classes of model adaptation methods ( i. e., model interpolation and error - driven learning ), which have been well studied in statistical language modeling for speech and natural language applications ( e. g., #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ), to ranking models for web search applications']",0
"['( #AUTHOR_TAG ), knowledge about implicit']","['( #AUTHOR_TAG ), knowledge about implicit']","['example ( #AUTHOR_TAG ), knowledge about implicit predicates could be potentially useful for a variety of']","['already mentioned in the literature, see for example ( #AUTHOR_TAG ), knowledge about implicit predicates could be potentially useful for a variety of nlp tasks such as language generation, information extraction, question answering or machine translation.', 'many applications of semantic relations in nlp are connected to paraphrasing or query expansion, see for example (Voorhees, 1994).', ""suppose that a search engine or a question answering system receives the query schnelle bombe'quick bomb '."", 'probably, in this case the user is interested in finding information about bombs that explode quickly rather then about bombs in general.', ""knowledge about predicates associated with the noun bombe'bomb'could be used for predicting a set of probable implicit predicates."", 'for generation of the semantically and syntactically correct paraphrases it is sometimes not enough to guess the most probable argument - predicate pairs.', 'information about types of an argument - predicate relation could be helpful, i. e. which semantic and syntactic position does the argument fill in the argument structure of the predicate.', ""for example, compare eine bombe explodiert schnell'a bomb explodes quickly'for schnelle bombe with ein buch schnell lesen / schreiben'to read / write a book quickly'for schnelles buch'quick book '."", 'in the first case the argument bombe fills the subject position, while in the second case buch fills the object position.', 'since framenet contains information about syntactic realization patterns for frame elements, representation of argument - predicate relations in terms of frames directly supports generation of semantically and syntactically correct paraphrases']",0
"['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ).', 'in future work, we plan to investigate additional methods for increasing the']","['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ).', 'in future work, we plan to investigate additional methods for increasing the']","['large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to']","['evaluated methods for self - training high accuracy products of latent variable grammars with large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'it would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'a simple but computationally expensive way to do this would be to parse the data with an sm7 product model']",1
"['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the']","['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the']","['large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to']","['evaluated methods for self - training high accuracy products of latent variable grammars with large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'it would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'a simple but computationally expensive way to do this would be to parse the data with an sm7 product model']",1
"['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the']","['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the']","['large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to']","['evaluated methods for self - training high accuracy products of latent variable grammars with large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'it would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'a simple but computationally expensive way to do this would be to parse the data with an sm7 product model']",1
"['##ntanglement ( #AUTHOR_TAG ).', ""in our corpus we found 175 instances where a participant mentions other participant's name."", ""in addition to these,'subject of the email ','topic - shift cue words'can also be beneficial for a model."", 'as a next step for this research, we will']","[""possibly critical feature is the'mention of names '."", ""in multi - party discussion people usually mention each other's name for the purpose of disentanglement ( #AUTHOR_TAG )."", ""in our corpus we found 175 instances where a participant mentions other participant's name."", ""in addition to these,'subject of the email ','topic - shift cue words'can also be beneficial for a model."", 'as a next step for this research, we will']","['##ntanglement ( #AUTHOR_TAG ).', ""in our corpus we found 175 instances where a participant mentions other participant's name."", ""in addition to these,'subject of the email ','topic - shift cue words'can also be beneficial for a model."", 'as a next step for this research, we will investigate how to exploit these features in our methods.', 'we are also']","[""possibly critical feature is the'mention of names '."", ""in multi - party discussion people usually mention each other's name for the purpose of disentanglement ( #AUTHOR_TAG )."", ""in our corpus we found 175 instances where a participant mentions other participant's name."", ""in addition to these,'subject of the email ','topic - shift cue words'can also be beneficial for a model."", 'as a next step for this research, we will investigate how to exploit these features in our methods.', 'we are also interested in the near future to transfer our approach to other similar domains by hierarchical bayesian multi - task learning and other domain adaptation methods.', 'we plan to work on both synchronous ( e. g., chats, meetings ) and asynchronous ( e. g., blogs ) domains']",0
"[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'we found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes, and that using these prototypes gave state - of - the - art performance on wsj, as well as improvements on nearly all of the non - english corpora.', 'these promising results suggest a new direction for future research : improving pos induction by developing methods targeted']","[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'we found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes, and that using these prototypes gave state - of - the - art performance on wsj, as well as improvements on nearly all of the non - english corpora.', 'these promising results suggest a new direction for future research : improving pos induction by developing methods targeted']","[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'we found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes, and that using these prototypes gave state - of - the - art performance on wsj, as well as improvements on nearly all of the non - english corpora.', 'these promising results suggest a new direction for future research : improving pos induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set']","[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'we found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes, and that using these prototypes gave state - of - the - art performance on wsj, as well as improvements on nearly all of the non - english corpora.', 'these promising results suggest a new direction for future research : improving pos induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set']",0
"['.', 'others include selectional preferences, transitivity ( #AUTHOR_TAG ), mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our understanding about these open ie relation strings.', 'we believe that the general principles developed in this work,']","['run our techniques on a large set of relations to output a first repository of typed functional relations.', 'we release this list for further use by the research community. 2', 'future work : functionality is one of the several properties a relation can possess.', 'others include selectional preferences, transitivity ( #AUTHOR_TAG ), mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our understanding about these open ie relation strings.', 'we believe that the general principles developed in this work,']","['run our techniques on a large set of relations to output a first repository of typed functional relations.', 'we release this list for further use by the research community. 2', 'future work : functionality is one of the several properties a relation can possess.', 'others include selectional preferences, transitivity ( #AUTHOR_TAG ), mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our understanding about these open ie relation strings.', 'we believe that the general principles developed in this work,']","['run our techniques on a large set of relations to output a first repository of typed functional relations.', 'we release this list for further use by the research community. 2', 'future work : functionality is one of the several properties a relation can possess.', 'others include selectional preferences, transitivity ( #AUTHOR_TAG ), mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our understanding about these open ie relation strings.', 'we believe that the general principles developed in this work, for example, connecting the open ie knowledge with an existing knowledge resource, will come in very handy in identifying these other properties']",0
"['phrase table combination.', 'finally, we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ).', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', 'the two tables are then merged as follows : all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'each phrase pair retains its original scores, which are further augmented with 1 - 3 additional feature scores indicating its origin : the first / second / third feature is 1 if the pair came from the first / second / both table ( s ), and 0 otherwise.', 'we experiment using']","['phrase table combination.', 'finally, we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ).', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', 'the two tables are then merged as follows : all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'each phrase pair retains its original scores, which are further augmented with 1 - 3 additional feature scores indicating its origin : the first / second / third feature is 1 if the pair came from the first / second / both table ( s ), and 0 otherwise.', 'we experiment using']","['phrase table combination.', 'finally, we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ).', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', 'the two tables are then merged as follows : all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'each phrase pair retains its original scores, which are further augmented with 1 - 3 additional feature scores indicating its origin : the first / second / third feature is 1 if the pair came from the first / second / both table ( s ), and 0 otherwise.', 'we experiment using']","['phrase table combination.', 'finally, we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ).', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', 'the two tables are then merged as follows : all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'each phrase pair retains its original scores, which are further augmented with 1 - 3 additional feature scores indicating its origin : the first / second / third feature is 1 if the pair came from the first / second / both table ( s ), and 0 otherwise.', 'we experiment using all three, the first two, or the first feature only ; we also try setting the features to 0. 5 instead of 0. this makes the following six combinations ( 0, 00, 000,. 5,. 5. 5,. 5. 5. 5 ) ; on testing, we use the one that achieves the highest bleu score on the development set']",5
"['or with very little adaptation, which works well for very closely related languages.', 'for example, our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experiment']","['or with very little adaptation, which works well for very closely related languages.', 'for example, our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi - text for a resource - poor language ( indonesian or spanish, pretending that spanish is resource - poor ) with a much larger bi - text for a related resource - rich language ( malay or portuguese ) ; the target language of all bi - texts was english.', 'however, our previous work did not attempt language adaptation, except for very simple transliteration for portuguese - spanish that ignored context entirely ; since it could not substitute one word for a completely different word, it did not help much for malay - indonesian, which use unified spelling.', 'still, once we have language - adapted the large bi - text, it makes sense to try to combine it further with the small bi - text ; thus,']","['or with very little adaptation, which works well for very closely related languages.', 'for example, our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experiment']","['third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'for example, our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi - text for a resource - poor language ( indonesian or spanish, pretending that spanish is resource - poor ) with a much larger bi - text for a related resource - rich language ( malay or portuguese ) ; the target language of all bi - texts was english.', 'however, our previous work did not attempt language adaptation, except for very simple transliteration for portuguese - spanish that ignored context entirely ; since it could not substitute one word for a completely different word, it did not help much for malay - indonesian, which use unified spelling.', 'still, once we have language - adapted the large bi - text, it makes sense to try to combine it further with the small bi - text ; thus, below we will directly compare and combine these two approaches']",1
"['mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""however, we do include a measure we call productivity to indicate the algorithm's completeness."", 'it is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'tab. 2 shows the evaluation results.', 'we reach our best precision by using the vp - fragment heuristics, which is still more productive than the lcs method.', 'the grammatical filter gives us a higher precision compared to the purely alignment - based approaches.', 'enhancing the system with coreference resolution raises the score even further.', 'we cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'however, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work : one state - of - theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0. 67.', 'we found the same number using our previous approach ( #AUTHOR_TAG ), which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']","['mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""however, we do include a measure we call productivity to indicate the algorithm's completeness."", 'it is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'tab. 2 shows the evaluation results.', 'we reach our best precision by using the vp - fragment heuristics, which is still more productive than the lcs method.', 'the grammatical filter gives us a higher precision compared to the purely alignment - based approaches.', 'enhancing the system with coreference resolution raises the score even further.', 'we cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'however, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work : one state - of - theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0. 67.', 'we found the same number using our previous approach ( #AUTHOR_TAG ), which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']","['mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""however, we do include a measure we call productivity to indicate the algorithm's completeness."", 'it is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'tab. 2 shows the evaluation results.', 'we reach our best precision by using the vp - fragment heuristics, which is still more productive than the lcs method.', 'the grammatical filter gives us a higher precision compared to the purely alignment - based approaches.', 'enhancing the system with coreference resolution raises the score even further.', 'we cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'however, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work : one state - of - theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0. 67.', 'we found the same number using our previous approach ( #AUTHOR_TAG ), which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']","['mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""however, we do include a measure we call productivity to indicate the algorithm's completeness."", 'it is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'tab. 2 shows the evaluation results.', 'we reach our best precision by using the vp - fragment heuristics, which is still more productive than the lcs method.', 'the grammatical filter gives us a higher precision compared to the purely alignment - based approaches.', 'enhancing the system with coreference resolution raises the score even further.', 'we cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'however, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work : one state - of - theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0. 67.', 'we found the same number using our previous approach ( #AUTHOR_TAG ), which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']",2
"['( #AUTHOR_TAG ).', 'in this earlier work, we focused on event structures and their possible']","['( #AUTHOR_TAG ).', 'in this earlier work, we focused on event structures and their possible']","['take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ).', 'in this earlier work, we focused on event structures and their possible realizations in natural language.', 'the corpus used in those experiments were short crowd - sourced descriptions of everyday tasks written in bullet point style.', 'we aligned them with a hand - crafted similarity measure that was specifically designed for this text type.', 'in this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'the current approach uses a domainindependent similarity measure instead of a specific hand - crafted similarity score and is thus']","['take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ).', 'in this earlier work, we focused on event structures and their possible realizations in natural language.', 'the corpus used in those experiments were short crowd - sourced descriptions of everyday tasks written in bullet point style.', 'we aligned them with a hand - crafted similarity measure that was specifically designed for this text type.', 'in this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'the current approach uses a domainindependent similarity measure instead of a specific hand - crafted similarity score and is thus applicable to standard texts']",2
"['.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'our own work ( #AUTHOR_TAG )']","['like recognizing textual entailment (Dinu and Wang, 2009).', 'the research on general paraphrase fragment extraction at the sub - sentential level is mainly based on phrase pair extraction techniques from the mt literature.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'our own work ( #AUTHOR_TAG )']","['like recognizing textual entailment (Dinu and Wang, 2009).', 'the research on general paraphrase fragment extraction at the sub - sentential level is mainly based on phrase pair extraction techniques from the mt literature.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'our own work ( #AUTHOR_TAG )']","['an applicational point of view, sentential paraphrases are difficult to use in other nlp tasks.', 'at the phrasal level, interchangeable patterns (Shinyama et al., 2002;Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted.', 'in both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e. g., named entities ( ne ) or content words.', 'they are quite successful in ne - centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like recognizing textual entailment (Dinu and Wang, 2009).', 'the research on general paraphrase fragment extraction at the sub - sentential level is mainly based on phrase pair extraction techniques from the mt literature.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora.', 'our current approach also uses word - word alignment, however, we use syntactic dependency trees to compute grammatical fragments.', 'our use of dependency trees is inspired by the constituent - tree - based experiments of callison - Burch (2008)']",2
"['with the candidate fragment elements, we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a ( para - ) phrase.', 'we extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (']","['with the candidate fragment elements, we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a ( para - ) phrase.', 'we extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments ( sec.', '5. 3 ).', 'finally, we filter']","['with the candidate fragment elements, we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a ( para - ) phrase.', 'we extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (']","['with the candidate fragment elements, we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a ( para - ) phrase.', 'we extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments ( sec.', '5. 3 ).', 'finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs']",2
['to ( #AUTHOR_TAG'],['to ( #AUTHOR_TAG'],['to ( #AUTHOR_TAG'],"['to ( #AUTHOR_TAG a ), our summarization system is, which consists of three key components : an initial sentence pre - selection module to select some important sentence candidates ; the above compression model to generate n - best compressions for each sentence ; and then an ilp summarization method to select the best summary sentences from the multiple compressed sentences']",1
"['approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of chinese to english machine translation and found that there is no correlation between sentence length and mt quality.', 'rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple english sentences.', 'this prior work was carried']","['approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of chinese to english machine translation and found that there is no correlation between sentence length and mt quality.', 'rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple english sentences.', 'this prior work was carried']","['approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of chinese to english machine translation and found that there is no correlation between sentence length and mt quality.', 'rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple english sentences.', 'this prior work was carried']","['approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of chinese to english machine translation and found that there is no correlation between sentence length and mt quality.', 'rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple english sentences.', 'this prior work was carried over a dataset containing a single reference translation for each chinese sentence.', 'in the work presented in this paper, we strengthen our findings by examining multiple reference translations for each chinese sentence.', 'we define heavy sentences based on agreement of translator choices and reader preferences']",1
"['the tectogrammatical parsing of czech, the analytical tree structure is converted into the tectogrammatical one.', 'these automatic transformations are based on linguistic rules ( #AUTHOR_TAG ).', 'subsequently, tectogrammatical functors are assigned by the c4. 5 classifier ( 2abokrtsk9 et al., 2002 )']","['the tectogrammatical parsing of czech, the analytical tree structure is converted into the tectogrammatical one.', 'these automatic transformations are based on linguistic rules ( #AUTHOR_TAG ).', 'subsequently, tectogrammatical functors are assigned by the c4. 5 classifier ( 2abokrtsk9 et al., 2002 )']","['the tectogrammatical parsing of czech, the analytical tree structure is converted into the tectogrammatical one.', 'these automatic transformations are based on linguistic rules ( #AUTHOR_TAG ).', 'subsequently, tectogrammatical functors are assigned by the c4. 5 classifier ( 2abokrtsk9 et al., 2002 )']","['the tectogrammatical parsing of czech, the analytical tree structure is converted into the tectogrammatical one.', 'these automatic transformations are based on linguistic rules ( #AUTHOR_TAG ).', 'subsequently, tectogrammatical functors are assigned by the c4. 5 classifier ( 2abokrtsk9 et al., 2002 )']",5
"['for czech, parser i ( Hajie et al. , 1998 ) and parser ii ( #AUTHOR_TAG ).', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']","['for czech, parser i ( Hajie et al. , 1998 ) and parser ii ( #AUTHOR_TAG ).', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']","['for czech, parser i ( Hajie et al. , 1998 ) and parser ii ( #AUTHOR_TAG ).', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']","['analytical parsing of czech runs in two steps : the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.', 'we carried out two parallel experiments with two parsers available for czech, parser i ( Hajie et al. , 1998 ) and parser ii ( #AUTHOR_TAG ).', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']",5
"['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see #AUTHOR_TAG ) on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', 'as a result, the entry / translation pairs seen in the parallel corpus of wsj become more probable.', 'for entry / translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'the translation is "" giza + + se - lected "" if its probability is higher than a threshold, which is in our case set to 0. 10']","['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see #AUTHOR_TAG ) on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', 'as a result, the entry / translation pairs seen in the parallel corpus of wsj become more probable.', 'for entry / translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'the translation is "" giza + + se - lected "" if its probability is higher than a threshold, which is in our case set to 0. 10']","['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see #AUTHOR_TAG ) on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', 'as a result, the entry / translation pairs seen in the parallel corpus of wsj become more probable.', 'for entry / translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'the translation is "" giza + + se - lected "" if its probability is higher than a threshold, which is in our case set to 0. 10']","['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see #AUTHOR_TAG ) on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', 'as a result, the entry / translation pairs seen in the parallel corpus of wsj become more probable.', 'for entry / translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'the translation is "" giza + + se - lected "" if its probability is higher than a threshold, which is in our case set to 0. 10']",5
"['7.', 'for the evaluation of the results we use the bleu score ( #AUTHOR_TAG ).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite']","['describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score ( #AUTHOR_TAG ).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite']","['its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score ( #AUTHOR_TAG ).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( al - Onaizan et al., 1999;Och and Ney, 2000;Germann et al., 2001), trained on the same parallel corpus']","[', we summarize resources available for the experiments ( section 2 ).', 'section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score ( #AUTHOR_TAG ).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( al - Onaizan et al., 1999;Och and Ney, 2000;Germann et al., 2001), trained on the same parallel corpus']",5
"['7.', 'for the evaluation of the results we use the bleu score (Papineni et al., 2001).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ), trained on the same parallel corpus']","['describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score (Papineni et al., 2001).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ), trained on the same parallel corpus']","['its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score (Papineni et al., 2001).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ), trained on the same parallel corpus']","[', we summarize resources available for the experiments ( section 2 ).', 'section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score (Papineni et al., 2001).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ), trained on the same parallel corpus']",1
"[""evaluated our translations with ibm's bleu evaluation metric ( #AUTHOR_TAG ), using the same evaluation method and reference retranslations that were used for evaluation at hlt workshop 2002 at clsp (""]","[""evaluated our translations with ibm's bleu evaluation metric ( #AUTHOR_TAG ), using the same evaluation method and reference retranslations that were used for evaluation at hlt workshop 2002 at clsp ( haji 6 et al., 2002 )."", 'we used four reference retranslations of 490 sentences selected from the wsj sections 22, 23, and 24, which were themselves used as the fifth reference.', 'the evaluation method used is to hold']","[""evaluated our translations with ibm's bleu evaluation metric ( #AUTHOR_TAG ), using the same evaluation method and reference retranslations that were used for evaluation at hlt workshop 2002 at clsp (""]","[""evaluated our translations with ibm's bleu evaluation metric ( #AUTHOR_TAG ), using the same evaluation method and reference retranslations that were used for evaluation at hlt workshop 2002 at clsp ( haji 6 et al., 2002 )."", 'we used four reference retranslations of 490 sentences selected from the wsj sections 22, 23, and 24, which were themselves used as the fifth reference.', 'the evaluation method used is to hold out each reference in turn and evaluate it against the remaining four, averaging the five bleu scores']",5
"['from the maximum entropy training on the transformed corpus.', 'for the baseline lexicon, we observed an average of 5. 82 catalan translation candidates per english word and 6. 16 spanish translation candidates.', 'these numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy : there, we have an average of 4. 20 for catalan and 4. 46 for spanish.', 'especially for ( nominative ) english pronouns ( which have many verbs as translation candidates in the baseline lexicon ), the number of translation candidates was substantially scaled down by a factor around 4. this shows that our method was successful in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model ( #AUTHOR_TAG ).', 'a description of the system can be found in (Tillmann and Ney, 2002).', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate for the translation into catal']","['from the maximum entropy training on the transformed corpus.', 'for the baseline lexicon, we observed an average of 5. 82 catalan translation candidates per english word and 6. 16 spanish translation candidates.', 'these numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy : there, we have an average of 4. 20 for catalan and 4. 46 for spanish.', 'especially for ( nominative ) english pronouns ( which have many verbs as translation candidates in the baseline lexicon ), the number of translation candidates was substantially scaled down by a factor around 4. this shows that our method was successful in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model ( #AUTHOR_TAG ).', 'a description of the system can be found in (Tillmann and Ney, 2002).', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate for the translation into catalan.', 'this change is consistent across both error rates, the wer and 100 - bleu.', 'for']","['from the maximum entropy training on the transformed corpus.', 'for the baseline lexicon, we observed an average of 5. 82 catalan translation candidates per english word and 6. 16 spanish translation candidates.', 'these numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy : there, we have an average of 4. 20 for catalan and 4. 46 for spanish.', 'especially for ( nominative ) english pronouns ( which have many verbs as translation candidates in the baseline lexicon ), the number of translation candidates was substantially scaled down by a factor around 4. this shows that our method was successful in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model ( #AUTHOR_TAG ).', 'a description of the system can be found in (Tillmann and Ney, 2002).', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate for the translation into catal']","['compared the two statistical lexica obtained from the baseline system and from the maximum entropy training on the transformed corpus.', 'for the baseline lexicon, we observed an average of 5. 82 catalan translation candidates per english word and 6. 16 spanish translation candidates.', 'these numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy : there, we have an average of 4. 20 for catalan and 4. 46 for spanish.', 'especially for ( nominative ) english pronouns ( which have many verbs as translation candidates in the baseline lexicon ), the number of translation candidates was substantially scaled down by a factor around 4. this shows that our method was successful in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model ( #AUTHOR_TAG ).', 'a description of the system can be found in (Tillmann and Ney, 2002).', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate for the translation into catalan.', 'this change is consistent across both error rates, the wer and 100 - bleu.', 'for translations from english into spanish, the improvement is less substantial.', 'a reason for this might be that the spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher : 1. 57 for spanish versus 1. 53 for catalan4.', 'this makes it more difficult for the system to choose the correct inflection when generating a spanish sentence.', 'we assume that the extension of our approach to other word classes than verbs will yield a quality gain for translations into spanish']",5
"['maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'the distribution is required to satisfy constraints, which represent facts known from the data.', 'these']","['maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'the distribution is required to satisfy constraints, which represent facts known from the data.', 'these']","['maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'the distribution is required to satisfy constraints, which represent facts known from the data.', 'these constraints are expressed on the basis of feature functions hu, ( s, t )']","['maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'the distribution is required to satisfy constraints, which represent facts known from the data.', 'these constraints are expressed on the basis of feature functions hu, ( s, t )']",5
"['example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'if necessary, the inverse of these transformations will be applied to the generated output string.', 'in the work presented here, we restrict ourselves to transforming only one language of the two : the source, which has the less inflected morphology.', 'for descriptions of smt systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']",0
"['instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 )']","['instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 )']","['instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 )']","['a = { am } is the set of model parameters with one weight a, for each feature function hm.', 'for an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 )']",0
"['example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'if necessary, the inverse of these transformations will be applied to the generated output string.', 'in the work presented here, we restrict ourselves to transforming only one language of the two : the source, which has the less inflected morphology.', 'for descriptions of smt systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']",0
"['to using a global model like crfs, our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAG c']","['to using a global model like crfs, our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAG c']","['to using a global model like crfs, our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAG c']","['to using a global model like crfs, our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAG c ) reported the best results over the evaluated corpora of bakeoff - 2 until now7.', 'though those results are slightly better than the results here, we still see that the results of character - level dependency parsing approach ( scheme e ) are comparable to those state - of - the - art ones on each evaluated corpus']",1
"['the representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an']","['the representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an']","['the representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an unrestricted context - free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""for this purpose, mohri and pereira's representation offers little advantage""]","['the representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an unrestricted context - free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""for this purpose, mohri and pereira's representation offers little advantage""]",1
"['#AUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this,']","['#AUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this,']","['#AUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this,']","['restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context - free language, and therefore a subset approximation results.', 'this idea was proposed by krauwer and des Tombe ( 1981 ), Langendoen and Langsam ( 1987 ), and Pulman ( 1986 ), and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method']",0
"['##ase the method of #AUTHOR_TAG as follows : first, we construct the']","['rephrase the method of #AUTHOR_TAG as follows : first, we construct the']","['rephrase the method of #AUTHOR_TAG as follows : first, we construct the approximating finite automaton according to the unparameterized rtn method above.', 'then an additional mechanism is introduced that ensures for each rule a - - ~ x1 .. xm separately that the list of visits to the states qo,..  ']","['rephrase the method of #AUTHOR_TAG as follows : first, we construct the approximating finite automaton according to the unparameterized rtn method above.', 'then an additional mechanism is introduced that ensures for each rule a - - ~ x1 .. xm separately that the list of visits to the states qo,..   qm satisfies some reasonable criteria : a visit to qi, with 0 < i < m, should be followed by one to qi + l or q0.', 'the latter option amounts to a nested incarnation of the rule.', 'there is a complementary condition for what should precede a visit to qi, with 0 < i < m']",5
"['by #AUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings al... an such that  for each substring v = ai + l... ai + n ( 0 < i < n - - n ) we have a - - + * w']","['by #AUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings al... an such that  for each substring v = ai + l... ai + n ( 0 < i < n - - n ) we have a - - + * wvy, for some w and y']","['by #AUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings al... an such that  for each substring v = ai + l... ai + n ( 0 < i < n - - n ) we have a - - + * wvy, for some w and y']","['method can be generalized, inspired by #AUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings al... an such that  for each substring v = ai + l... ai + n ( 0 < i < n - - n ) we have a - - + * wvy, for some w and y']",0
['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],0
"['is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in #AUTHOR_TAG']","['is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in #AUTHOR_TAG']","['is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in #AUTHOR_TAG']","['10 ( b ) is no longer possible, since no nonterminal in the transformed grammar would contain 1 in its superscript. because of the demonstrated increase of the counter f, this transformation is guaranteed to remove self - embedding from the grammar.', 'however, it is not as selective as the transformation we saw before, in the sense that it may also block subderivations that are not of the form a - - * * ~ afl.', 'consider for example the subderivation from figure10, but replacing the lower occurrence of s by any other nonterminal c that is mutually recursive with s, a, and b. such a subderivation s - - - * * b c c d a would also be blocked by choosing d = 0.', 'in general, increasing d allows more of such derivations that are not of the form a ~ "" o ~ afl but also allows more derivations that are of that form. the reason for considering this transformation rather than any other that eliminates self - embedding is purely pragmatic : of the many variants we have tried that yield nontrivial subset approximations, this transformation has the lowest complexity in terms of the sizes of intermediate structures and of the resulting finite automata. in the actual implementation, we have integrated the grammar transformation and the construction of the finite automaton, which avoids reanalysis of the grammar to determine the partition of mutually recursive nonterminals after transformation.', 'this integration makes use, for example, of the fact that for fixed ni and fixed f, the set of nonterminals of the form a, f, with a c ni, is ( potentially ) mutually right - recursive. a set of such nonterminals can therefore be treated as the corresponding case from figure2, assuming the value right. the full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in #AUTHOR_TAG']",1
"['an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by Ainsworth ( 1973 ), mc Ilroy ( 1973 ), Elovitz et al. ( 1976 ), Hurmicutt ( 1976 ), and #AUTHOR_TAG']","['an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by Ainsworth ( 1973 ), mc Ilroy ( 1973 ), Elovitz et al. ( 1976 ), Hurmicutt ( 1976 ), and #AUTHOR_TAG']","['an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by Ainsworth ( 1973 ), mc Ilroy ( 1973 ), Elovitz et al. ( 1976 ), Hurmicutt ( 1976 ), and #AUTHOR_TAG']","['states that the letter substring b with left context a and right context c receives the pronunciation ( i. e., phoneme substring ) d. such rules can also be straightforwardly cast in the if... then form commonly featured in high - level programming languages and employed in expert, knowledge - based systems technology.', 'they also constitute a formal model of universal computation ( post 1943 ).', 'conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by Ainsworth ( 1973 ), mc Ilroy ( 1973 ), Elovitz et al. ( 1976 ), Hurmicutt ( 1976 ), and #AUTHOR_TAG']",0
"[', #AUTHOR_TAG']","[', #AUTHOR_TAG']","[', #AUTHOR_TAG']","['is also conceivable that data - driven techniques can actually outperform traditional rules.', 'however, this possibility is not usually given much credence.', ""for instance, #AUTHOR_TAG recently wrote : ` ` to our knowledge, learning algorithms, although promising, have not ( yet ) reached the level of rule sets developed by humans'' ( p. 520 )."", '520 ).', 'Dutoit (1997) takes this further, stating "" such training - based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores "" ( p.', '115, note 14 )']",0
"['the work of #AUTHOR_TAG, which considers computer - based pronunciation by analogy but does not']","['the work of #AUTHOR_TAG, which considers computer - based pronunciation by analogy but does not']","['and Nusbaum (1986, 1991 ).', 'see also the work of #AUTHOR_TAG, which considers computer - based pronunciation by analogy but does not']","['##unciation by analogy ( pba ) is a data - driven technique for the automatic phonemization of text, originally proposed as a model of reading, e. g., by Glushko (1979) and Kay and Marcel (1981).', 'it was first proposed for tts applications over a decade ago by dedina and Nusbaum (1986, 1991 ).', 'see also the work of #AUTHOR_TAG, which considers computer - based pronunciation by analogy but does not mention the possible application to text - to - speech synthesis.', 'as detailed by Damper (1995) and Damper and Eastmond (1997), pba shares many similarities with the artificial intelligence paradigms variously called case - based, memory - based, or instance - based reasoning as applied to letter - to - phoneme conversion ( stanfill and waltz 1986 ; lehnert 1987 ; stanfill 1987 Stanfill , 1988 golding 1991 ; golding and rosenbloom 1991 ; van den bosch and daelemans 1993 )']",0
"['., #AUTHOR_TAG ; van']","['including some applications in computational linguistics ( e. g., #AUTHOR_TAG ; van halteren, zavrel, and daelemans 1998 ) and speech technology (']","['., #AUTHOR_TAG ; van halteren, zavrel, and']","[', the above characterization is very wide ranging.', 'consequently, fusion has been applied to a wide variety of pattern recognition and decision theoretic problems - - using a plethora of theories, techniques, and tools - - including some applications in computational linguistics ( e. g., #AUTHOR_TAG ; van halteren, zavrel, and daelemans 1998 ) and speech technology ( e. g., bowles and damper 1989 ; romary and pierre11989 ).', 'according to Abbott (1999, 290 ), "" while the reasons [ that ] combining models works so well are not rigorously understood, there is ample evidence that improvements over single models are typical....', 'a strong case can be made for combining models across algorithm families as a means of providing uncorrelated output estimates. ""', 'our purpose in this paper is to study and exploit such fusion by model ( or strategy ) combination as a way of achieving performance gains in pba']",0
"['kumano and hirakawa 1994 ; #AUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']","['gale and church 1991 ; fung 1995 ; kumano and hirakawa 1994 ; #AUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']","['##e and church 1991 ; fung 1995 ; kumano and hirakawa 1994 ; #AUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']","['researchers have proposed greedy algorithms for estimating nonprobabilistic word - to - word translation models, also known as translation lexicons ( e. g., catizone, russell, and warwick 1989 ; gale and church 1991 ; fung 1995 ; kumano and hirakawa 1994 ; #AUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']",0
"['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in table 1']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in table 1']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in table 1']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in table 1']",0
"['probability distribution trans (. 1, ~ ) is a word - to - word translation model.', 'unlike the models proposed by #AUTHOR_TAG b ), this model is symmetric, because both word bags are generated together from a joint probability distribution.', ""brown and his colleagues'models, reviewed in section 4. 3, generate one half of the bitext given the other hall so they are represented by conditional probability""]","['probability distribution trans (. 1, ~ ) is a word - to - word translation model.', 'unlike the models proposed by #AUTHOR_TAG b ), this model is symmetric, because both word bags are generated together from a joint probability distribution.', ""brown and his colleagues'models, reviewed in section 4. 3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'a sequenceto - sequence translation model can be obtained from a word - to - word translation model by combining equation 11 with order']","['probability distribution trans (. 1, ~ ) is a word - to - word translation model.', 'unlike the models proposed by #AUTHOR_TAG b ), this model is symmetric, because both word bags are generated together from a joint probability distribution.', ""brown and his colleagues'models, reviewed in section 4. 3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'a sequenceto - sequence translation model can be obtained from a word - to - word translation model by combining equation 11 with order information as in equation 8']","['probability distribution trans (. 1, ~ ) is a word - to - word translation model.', 'unlike the models proposed by #AUTHOR_TAG b ), this model is symmetric, because both word bags are generated together from a joint probability distribution.', ""brown and his colleagues'models, reviewed in section 4. 3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'a sequenceto - sequence translation model can be obtained from a word - to - word translation model by combining equation 11 with order information as in equation 8']",1
"['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag ( #AUTHOR_TAG ), then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the same!"", 'method b exploits this']","['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag ( #AUTHOR_TAG ), then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the same!"", 'method b exploits this']","['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag ( #AUTHOR_TAG ), then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the same!"", 'method b exploits this property under the hypothesis that "" one sense per collocation "" holds for translational collocations.', 'this hypothesis implies that if u and v are possible mutual translations, and a token u co - occurs with a token v in the bitext, then with very high probability the pair ( u, v ) was generated from the same concept and should be linked.', 'to test this hypothesis, i ran one iteration of method a']","['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag ( #AUTHOR_TAG ), then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the same!"", 'method b exploits this property under the hypothesis that "" one sense per collocation "" holds for translational collocations.', 'this hypothesis implies that if u and v are possible mutual translations, and a token u co - occurs with a token v in the bitext, then with very high probability the pair ( u, v ) was generated from the same concept and should be linked.', 'to test this hypothesis, i ran one iteration of method a on 300, 000 aligned sentence pairs from the canadian hansards bitext.', 'i then plotted the links ( u, v ) ratio ~ for several values of cooc ( u, v ) in figure 2. the curves show that the ratio links ( u, v ) cooc ( u, v ) tends to be either very high or very low.', 'this bimodality is not an artifact of the competitive linking process, because in the first iteration, linking decisions are based only on the initial similarity metric']",5
"['informal experiments described elsewhere ( #AUTHOR_TAG ), i found that the g2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( #AUTHOR_TAG ), i found that the g2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( #AUTHOR_TAG ), i found that the g2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( #AUTHOR_TAG ), i found that the g2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02.', 'let the cells of the contingency table be named as follows']",2
"['., #AUTHOR_TAG ), a']","['## cross - language information retrieval ( e. g., mccarley 1999 ), a multilingual document filtering ( e. g., oard 1997 ), a computer - assisted language learning ( e. g., #AUTHOR_TAG ), a certain machine - assisted translation tools ( e. g.,']","['., #AUTHOR_TAG ), a']","['## cross - language information retrieval ( e. g., mccarley 1999 ), a multilingual document filtering ( e. g., oard 1997 ), a computer - assisted language learning ( e. g., #AUTHOR_TAG ), a certain machine - assisted translation tools ( e. g., macklovitch 1994 ; melamed 1996a ), a concordancing for bilingual lexicography ( e. g., catizone, russell, and warwick 1989 ; gale and church 1991 )']",0
"[""this situation, #AUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most']","[""this situation, #AUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most']","[""this situation, #AUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most probable assignment ama ~ is the maximum a posteriori ( map ) assignment']","[""this situation, #AUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most probable assignment ama ~ is the maximum a posteriori ( map ) assignment']",4
"['##xxx #AUTHOR_TAG ).', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class : b ( links ( u, v ) [ cooc ( u, v ), a + ) scorec ( u, vlz = class ( u, v ) ) =']","['in an on - line bilingual dictionary separately from those that do not ( cfxxx #AUTHOR_TAG ).', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class : b ( links ( u, v ) [ cooc ( u, v ), a + ) scorec ( u, vlz = class ( u, v ) ) =']","['##xxx #AUTHOR_TAG ).', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class : b ( links ( u, v ) [ cooc ( u, v ), a + ) scorec ( u, vlz = class ( u, v ) ) = log b ( links ( u, v ) [ cooc ( u, v ), a z ) "" ( 37 ) section 6. 1. 1 describes the link classes used in the experiments below']","['method b, the estimation of the auxiliary parameters a + and a - depends only on the overall distribution of co - occurrence counts and link frequencies.', 'all word pairs that co - occur the same number of times and are linked the same number of times are assigned the same score.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( catizone, russell, and warwick 1989 ).', 'to account for these differences, we can estimate separate values of a + and a - for different ranges of cooc ( u, v ).', 'similarly, the auxiliary parameters can be conditioned on the linked parts of speech.', 'a kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts.', 'just as easily, we can model link types that coincide with entries in an on - line bilingual dictionary separately from those that do not ( cfxxx #AUTHOR_TAG ).', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class : b ( links ( u, v ) [ cooc ( u, v ), a + ) scorec ( u, vlz = class ( u, v ) ) = log b ( links ( u, v ) [ cooc ( u, v ), a z ) "" ( 37 ) section 6. 1. 1 describes the link classes used in the experiments below']",5
"["". white and o'connell 1993 ) or using relative metrics, such as perplexity with respect to other models ( #AUTHOR_TAG b )."", 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly']","[""now, translation models have been evaluated either subjectively ( e. g. white and o'connell 1993 ) or using relative metrics, such as perplexity with respect to other models ( #AUTHOR_TAG b )."", 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly 16, 000 corresponding words between on - line versions of the bible in french and english.', 'this bitext was selected to facilitate widespread use and standardization ( see melamed [ 1998c ]']","["". white and o'connell 1993 ) or using relative metrics, such as perplexity with respect to other models ( #AUTHOR_TAG b )."", 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly']","[""now, translation models have been evaluated either subjectively ( e. g. white and o'connell 1993 ) or using relative metrics, such as perplexity with respect to other models ( #AUTHOR_TAG b )."", 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly 16, 000 corresponding words between on - line versions of the bible in french and english.', 'this bitext was selected to facilitate widespread use and standardization ( see melamed [ 1998c ] for details ).', 'the entire bible bitext comprised 29, 614 verse pairs, of which 250 verse pairs were hand - linked using a specially developed annotation tool.', 'the annotation style guide ( melamed 1998b ) was based on the intuitions of the annotators, so it was not biased towards any particular translation model.', 'the annotation was replicated five times by seven different annotators']",1
"['##amed 1995 ), i found that the g2 statistic suggested by #AUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( melamed 1995 ), i found that the g2 statistic suggested by #AUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['##amed 1995 ), i found that the g2 statistic suggested by #AUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( melamed 1995 ), i found that the g2 statistic suggested by #AUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']",0
"['##ille et al. 1990 ; shieber 1994 ; #AUTHOR_TAG ), lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ;']","['above equation holds regardless of how we represent concepts.', 'there are many plausible representations, such as pairs of trees from synchronous tree adjoining grammars ( abeille et al. 1990 ; shieber 1994 ; #AUTHOR_TAG ), lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ; vossen']","['##ille et al. 1990 ; shieber 1994 ; #AUTHOR_TAG ), lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ;']","['above equation holds regardless of how we represent concepts.', 'there are many plausible representations, such as pairs of trees from synchronous tree adjoining grammars ( abeille et al. 1990 ; shieber 1994 ; #AUTHOR_TAG ), lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ; vossen 1998 ).', 'of course, for a representation to be used, a method must exist for estimating its distribution in data']",0
"['., #AUTHOR_TAG ), a']","['cross - language information retrieval ( e. g., #AUTHOR_TAG ), a multilingual document filtering ( e. g., oard 1997 ), a computer - assisted language learning ( e. g., nerbonne et al. 1997 ), a certain machine - assisted translation tools ( e. g.,']","['., #AUTHOR_TAG ), a']","['## cross - language information retrieval ( e. g., #AUTHOR_TAG ), a multilingual document filtering ( e. g., oard 1997 ), a computer - assisted language learning ( e. g., nerbonne et al. 1997 ), a certain machine - assisted translation tools ( e. g., macklovitch 1994 ; melamed 1996a ), a concordancing for bilingual lexicography ( e. g., catizone, russell, and warwick 1989 ; gale and church 1991 )']",0
"[""##los's book ( #AUTHOR_TAG )."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or performing linguistic']","[""other such cases are described in danlos's book ( #AUTHOR_TAG )."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or performing linguistic']","[""##los's book ( #AUTHOR_TAG )."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or performing linguistic optimizations ( such as using pronouns instead of longer referring expressions']","[""other such cases are described in danlos's book ( #AUTHOR_TAG )."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or performing linguistic optimizations ( such as using pronouns instead of longer referring expressions whenever possible ) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'this is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module']",0
['##g systems ( #AUTHOR_TAG )'],"['these arguments, most applied nlg systems use a pipelined architecture ; indeed, a pipeline was used in every one of the systems surveyed by Reiter (1994) and Paiva (1998).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems ( #AUTHOR_TAG )']",['##los and other pipeline critics do not seem to be a major problem in current applied nlg systems ( #AUTHOR_TAG )'],"['these arguments, most applied nlg systems use a pipelined architecture ; indeed, a pipeline was used in every one of the systems surveyed by Reiter (1994) and Paiva (1998).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems ( #AUTHOR_TAG )']",0
"[', a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by']","[', a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems ( mittal et al. 1998 )']","[', a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by']","['these arguments, most applied nlg systems use a pipelined architecture ; indeed, a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems ( mittal et al. 1998 )']",0
"['##g, the upper model ( #AUTHOR_TAG ).', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see']","['is used to mediate and map between a language - independent domain model and a language - dependent ontology widely used in nlg, the upper model ( #AUTHOR_TAG ).', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see']","['##g, the upper model ( #AUTHOR_TAG ).', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see']","['first i found chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms.', 'however, at a second, more - careful, reading, everything falls into place.', ""the resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993)."", 'the lexicon is used to mediate and map between a language - independent domain model and a language - dependent ontology widely used in nlg, the upper model ( #AUTHOR_TAG ).', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see for example nirenburg and levin [ 1992 ], dorr and voss [ 1993 ], and di eugenio [ 1998 ] ), stede is among the few who make effective use of those two levels in a complex system']",0
"["", ` ` Johnson ( 1970 ) demonstrated that the context - sensitive machinery of spe... [ could ] be replaced by a much simpler one, based on finite - state transducers ( fsts ) ; the same conclusion was reached independently by kaplan and kay, whose work remained an underground classic until it was finally published in #AUTHOR_TAG.''"", ""these works inspired koskenniemi's two - level system,""]","["", ` ` Johnson ( 1970 ) demonstrated that the context - sensitive machinery of spe... [ could ] be replaced by a much simpler one, based on finite - state transducers ( fsts ) ; the same conclusion was reached independently by kaplan and kay, whose work remained an underground classic until it was finally published in #AUTHOR_TAG.''"", ""these works inspired koskenniemi's two - level system,""]","[""##e 1968 ), kornai points out, ` ` Johnson ( 1970 ) demonstrated that the context - sensitive machinery of spe... [ could ] be replaced by a much simpler one, based on finite - state transducers ( fsts ) ; the same conclusion was reached independently by kaplan and kay, whose work remained an underground classic until it was finally published in #AUTHOR_TAG.''"", ""these works inspired koskenniemi's two - level system,""]","[""after the publication of the sound pattern of english ( chomsky and halle 1968 ), kornai points out, ` ` Johnson ( 1970 ) demonstrated that the context - sensitive machinery of spe... [ could ] be replaced by a much simpler one, based on finite - state transducers ( fsts ) ; the same conclusion was reached independently by kaplan and kay, whose work remained an underground classic until it was finally published in #AUTHOR_TAG.''"", ""these works inspired koskenniemi's two - level system, and the xerox rule compiler ( dalrymple et al. 1987 )."", 'both are now dominant tools in the fields of computational phonology and morphology, as exemplified by tateno et al. ( chapter 6 ), "" the japanese lexical transducer based on stem - suffix style forms "" and kim and jang ( chapter 7 ), "" acquiring rules for reducing morphological ambiguity from pos tagged corpus in korean. ""', 'the latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part - of - speech tagged corpora']",0
"['this is always possible under an appropriate definition of "" simple constraints "" ( e. g., #AUTHOR_TAG']","['this is always possible under an appropriate definition of "" simple constraints "" ( e. g., #AUTHOR_TAG']","['simple surface - level constraints that partially mask one another. 1 whether this is always possible under an appropriate definition of "" simple constraints "" ( e. g., #AUTHOR_TAG b ) is of course an empirical question']","['therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface - level constraints that partially mask one another. 1 whether this is always possible under an appropriate definition of "" simple constraints "" ( e. g., #AUTHOR_TAG b ) is of course an empirical question']",0
"[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar for stylistic revision of parsed sentences.', 'the tension between']","[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar for stylistic revision of parsed sentences.', 'the tension between']","[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar for stylistic revision of parsed sentences.', ""the tension between preserving the original author's text ( faithfulness to the underlying form ) and making it readable in various ways ( well - formedness ) is right up ot's alley."", 'the same applies to document layout : i have often wished i could write ot - style tex macros ~ third, even in statistical corpus - based nlp, estimating a full gibbs distribution is not always']","[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar for stylistic revision of parsed sentences.', ""the tension between preserving the original author's text ( faithfulness to the underlying form ) and making it readable in various ways ( well - formedness ) is right up ot's alley."", 'the same applies to document layout : i have often wished i could write ot - style tex macros ~ third, even in statistical corpus - based nlp, estimating a full gibbs distribution is not always feasible.', 'even if strict ranking is not quite accurate, sparse data or the complexity of parameter estimation may make it easier to learn a good ot grammar than a good arbitrary gibbs model.', ""a well - known example is Yarowsky's (1996) work on word sense disambiguation using decision lists ( a kind of ot grammar )."", 'although decision lists are not very powerful because of their simple output space, they have the characteristic ot property that each generalization partially masks lower - ranked generalizations']",0
"['), another restricted class of gibbs distributions used in speech recognition or part - of - speech tagging.', ""just like ot grammars, hmm viterbi decoders are functions that pick the optimal output from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context ( #AUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms,']","['example, consider the relevance to hidden markov models ( hmms ), another restricted class of gibbs distributions used in speech recognition or part - of - speech tagging.', ""just like ot grammars, hmm viterbi decoders are functions that pick the optimal output from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context ( #AUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms,']","['), another restricted class of gibbs distributions used in speech recognition or part - of - speech tagging.', ""just like ot grammars, hmm viterbi decoders are functions that pick the optimal output from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context ( #AUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms, ot generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'when is this appropriate?', 'it seems to me that there are three possible uses']","['example, consider the relevance to hidden markov models ( hmms ), another restricted class of gibbs distributions used in speech recognition or part - of - speech tagging.', ""just like ot grammars, hmm viterbi decoders are functions that pick the optimal output from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context ( #AUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms, ot generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'when is this appropriate?', 'it seems to me that there are three possible uses']",0
"['original question concerned the extent to which recall and precision are influenced by the size of the window.', 'it turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest - frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'it is common practice in information retrieval to discard the lowest - frequency words a priori as nonsignificant ( rijsbergen 1979 ).', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", '#AUTHOR_TAG use mutual information to identify collocations, a method']","['original question concerned the extent to which recall and precision are influenced by the size of the window.', 'it turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest - frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'it is common practice in information retrieval to discard the lowest - frequency words a priori as nonsignificant ( rijsbergen 1979 ).', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", '#AUTHOR_TAG use mutual information to identify collocations, a method']","['original question concerned the extent to which recall and precision are influenced by the size of the window.', 'it turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest - frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'it is common practice in information retrieval to discard the lowest - frequency words a priori as nonsignificant ( rijsbergen 1979 ).', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", '#AUTHOR_TAG use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five']","['original question concerned the extent to which recall and precision are influenced by the size of the window.', 'it turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest - frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'it is common practice in information retrieval to discard the lowest - frequency words a priori as nonsignificant ( rijsbergen 1979 ).', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", '#AUTHOR_TAG use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five']",0
"[""reasonable results with both g2 and fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure ( #AUTHOR_TAG )""]","[""reasonable results with both g2 and fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure ( #AUTHOR_TAG )""]","['/ 139 = 46. 0 %', 'and 79 / 170 = 46. 7 %,', ""respectively, using fisher's exact test."", ""note that this technique is optimal for the extraction of the lowest - frequency words, leading to identical performance for g 2 and fisher's exact test for these words."", ""for the higherfrequency words, fisher's exact test leads to a slightly better recall with the same precision scores ( 0. 31 for both tests )."", ""while we have observed reasonable results with both g2 and fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure ( #AUTHOR_TAG )""]","[""that we want to retain dis legomena with a 2 - 0 distribution, we proceed to compute the corresponding significance levels for both g 2 and fisher's exact test by equations 1 and 2. the critical x 2 value for g 2 equals 3. 65, the critical p for fisher's exact test is 0. 161."", 'the extraction results for both tests as measured by f are 0. 31 and 0. 33, respectively.', 'this procedure allows us to extract 64 / 139 = 46. 0 % of the lowfrequency words and 66 / 170 - ~ 38. 8 % of the high - frequency words using g 2, and 64 / 139 = 46. 0 %', 'and 79 / 170 = 46. 7 %,', ""respectively, using fisher's exact test."", ""note that this technique is optimal for the extraction of the lowest - frequency words, leading to identical performance for g 2 and fisher's exact test for these words."", ""for the higherfrequency words, fisher's exact test leads to a slightly better recall with the same precision scores ( 0. 31 for both tests )."", ""while we have observed reasonable results with both g2 and fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure ( #AUTHOR_TAG )""]",0
"['is the average case for a specific grammar.', 'specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time - consuming behavior of the algorithm never happens for this grammar.', 'average, since it can happen that the grammar does admit hard - toparse sentences that are not used ( or at least not frequently used ) in the real corpus.', 'for example, #AUTHOR_TAG proves that chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not context - free, which implies that chinese is not a context - free language and thus might parse in exponential worst - case time.', 'do such arguments - - no doubt important for mathematical linguistics']","['is the average case for a specific grammar.', 'specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time - consuming behavior of the algorithm never happens for this grammar.', 'average, since it can happen that the grammar does admit hard - toparse sentences that are not used ( or at least not frequently used ) in the real corpus.', 'for example, #AUTHOR_TAG proves that chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not context - free, which implies that chinese is not a context - free language and thus might parse in exponential worst - case time.', 'do such arguments - - no doubt important for mathematical']","[', the complexity of a grammar class is measured by the worst case : a grammar class has a complexity x if there exists some grammar in this class such that there exists an infinite series of long - enough sentences that parse in time x by this grammar.', 'however, what matters in engineering practice is the average case for a specific grammar.', 'specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time - consuming behavior of the algorithm never happens for this grammar.', 'average, since it can happen that the grammar does admit hard - toparse sentences that are not used ( or at least not frequently used ) in the real corpus.', 'for example, #AUTHOR_TAG proves that chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not context - free, which implies that chinese is not a context - free language and thus might parse in exponential worst - case time.', 'do such arguments - - no doubt important for mathematical linguistics - - have any direct consequences for an engineering linguistics?', 'even if a chinese grammar includes a non - context - flee rule for parsing such numerals, how frequently will it be activated?', 'does it imply impossibility of processing real chinese texts in reasonable time?', 'clearly, the average time for a specific grammar cannot be calculated in such a mathematically elegant way as the worst - case complexity of a grammar class ; for the time being, the only practical way to compare the complexity of natural language processing formalisms is the hard one - - building real - world systems and comparing their efficiency and coverage']","[', the complexity of a grammar class is measured by the worst case : a grammar class has a complexity x if there exists some grammar in this class such that there exists an infinite series of long - enough sentences that parse in time x by this grammar.', 'however, what matters in engineering practice is the average case for a specific grammar.', 'specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time - consuming behavior of the algorithm never happens for this grammar.', 'average, since it can happen that the grammar does admit hard - toparse sentences that are not used ( or at least not frequently used ) in the real corpus.', 'for example, #AUTHOR_TAG proves that chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not context - free, which implies that chinese is not a context - free language and thus might parse in exponential worst - case time.', 'do such arguments - - no doubt important for mathematical linguistics - - have any direct consequences for an engineering linguistics?', 'even if a chinese grammar includes a non - context - flee rule for parsing such numerals, how frequently will it be activated?', 'does it imply impossibility of processing real chinese texts in reasonable time?', 'clearly, the average time for a specific grammar cannot be calculated in such a mathematically elegant way as the worst - case complexity of a grammar class ; for the time being, the only practical way to compare the complexity of natural language processing formalisms is the hard one - - building real - world systems and comparing their efficiency and coverage']",0
"['in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ).', 'this parser produces trees to represent the structure of the sentences that compose the text.', 'however, it is set to "" skip "" or surrender attempts to parse clauses after reaching a time - out threshold.', 'when the parser skips, it notes that in the parse tree.', 'the measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis - level style markers']","['in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ).', 'this parser produces trees to represent the structure of the sentences that compose the text.', 'however, it is set to "" skip "" or surrender attempts to parse clauses after reaching a time - out threshold.', 'when the parser skips, it notes that in the parse tree.', 'the measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis - level style markers']","['. g., part - of - speech taggers, parsers, etc. ) can provide similar measures.', 'the appropriate analysis - level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'for example, some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ).', 'this parser produces trees to represent the structure of the sentences that compose the text.', 'however, it is set to "" skip "" or surrender attempts to parse clauses after reaching a time - out threshold.', 'when the parser skips, it notes that in the parse tree.', 'the measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis - level style markers']","['particular analysis - level style markers can be calculated only when this specific computational tool is utilized.', 'however, the scbd is a general - purpose tool and was not designed for providing stylistic information exclusively.', 'thus, any nlp tool ( e. g., part - of - speech taggers, parsers, etc. ) can provide similar measures.', 'the appropriate analysis - level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'for example, some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ).', 'this parser produces trees to represent the structure of the sentences that compose the text.', 'however, it is set to "" skip "" or surrender attempts to parse clauses after reaching a time - out threshold.', 'when the parser skips, it notes that in the parse tree.', 'the measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis - level style markers']",0
['by researchers in automatic text genre detection ( #AUTHOR_TAG b ;'],['by researchers in automatic text genre detection ( #AUTHOR_TAG b ; karlgren and cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables'],"['cx is the covariance matrix of x.', 'using this classification method we can also derive the probability that a case belongs to a particular group ( i. e., posterior probabilities ), which is roughly proportional to the mahalanobis distance from that group centroid.', 'discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAG b ;']","['cx is the covariance matrix of x.', 'using this classification method we can also derive the probability that a case belongs to a particular group ( i. e., posterior probabilities ), which is roughly proportional to the mahalanobis distance from that group centroid.', 'discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAG b ; karlgren and cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']",0
"['algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']","['algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']","['algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']","['algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']",4
"['automatic annotation of nouns and verbs in the corpus has been done by matching them with the wordnet database files.', 'before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in #AUTHOR_TAG, page 75 ).', 'to lemmatize the words we used morpha, a lemmatizer developed by john a. carroll and freely available at the address : http : / / www. informatics. susx. ac. uk. / research / nlp / carroll / morph. html.', 'upon simple']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the wordnet database files.', 'before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in #AUTHOR_TAG, page 75 ).', 'to lemmatize the words we used morpha, a lemmatizer developed by john a. carroll and freely available at the address : http : / / www. informatics. susx. ac. uk. / research / nlp / carroll / morph. html.', 'upon simple observation, it showed a better performance than the frequently used porter stemmer for this task']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the wordnet database files.', 'before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in #AUTHOR_TAG, page 75 ).', 'to lemmatize the words we used morpha, a lemmatizer developed by john a. carroll and freely available at the address : http : / / www. informatics. susx. ac. uk. / research / nlp / carroll / morph. html.', 'upon simple observation, it showed a better performance than the frequently used porter stemmer for this task']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the wordnet database files.', 'before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in #AUTHOR_TAG, page 75 ).', 'to lemmatize the words we used morpha, a lemmatizer developed by john a. carroll and freely available at the address : http : / / www. informatics. susx. ac. uk. / research / nlp / carroll / morph. html.', 'upon simple observation, it showed a better performance than the frequently used porter stemmer for this task']",4
"['the generic beam - search algorithm and the generalized perceptron.', 'then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'we give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'for the segmentation task, we also compare our beam - search framework with alternative decoding algorithms including an exact dynamic - programming method, showing that the beam - search method is significantly faster with comparable accuracy.', 'for the joint segmentation and pos - tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAG']","['the generic beam - search algorithm and the generalized perceptron.', 'then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'we give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'for the segmentation task, we also compare our beam - search framework with alternative decoding algorithms including an exact dynamic - programming method, showing that the beam - search method is significantly faster with comparable accuracy.', 'for the joint segmentation and pos - tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAG']","['the generic beam - search algorithm and the generalized perceptron.', 'then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'we give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'for the segmentation task, we also compare our beam - search framework with alternative decoding algorithms including an exact dynamic - programming method, showing that the beam - search method is significantly faster with comparable accuracy.', 'for the joint segmentation and pos - tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAG']","['section 2 we describe our general framework of the generic beam - search algorithm and the generalized perceptron.', 'then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'we give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'for the segmentation task, we also compare our beam - search framework with alternative decoding algorithms including an exact dynamic - programming method, showing that the beam - search method is significantly faster with comparable accuracy.', 'for the joint segmentation and pos - tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAG a ), while being more than an order of magnitude faster']",1
"['( #AUTHOR_TAG ), we assessed the importance of various implicit argument feature']","['( #AUTHOR_TAG ), we assessed the importance of various implicit argument feature']","['( #AUTHOR_TAG ), we assessed the importance of various implicit argument feature groups by conducting feature ablation tests.', 'in each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'we summarize the findings of this study in this section']","['( #AUTHOR_TAG ), we assessed the importance of various implicit argument feature groups by conducting feature ablation tests.', 'in each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'we summarize the findings of this study in this section']",2
"['into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'this evaluation set - up is an improvement versus the one we previously reported ( #AUTHOR_TAG ), in which fixed partitions were used for training, development, and testing']","['into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'this evaluation set - up is an improvement versus the one we previously reported ( #AUTHOR_TAG ), in which fixed partitions were used for training, development, and testing']","['order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'this evaluation set - up is an improvement versus the one we previously reported ( #AUTHOR_TAG ), in which fixed partitions were used for training, development, and testing']","['order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'this evaluation set - up is an improvement versus the one we previously reported ( #AUTHOR_TAG ), in which fixed partitions were used for training, development, and testing']",2
"['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'ccgbank ( #AUTHOR_TAG ) is used to train the model.', 'for each training sentence, the corresponding ccgbank derivation together with all its sub - derivations are treated as gold - standard hypotheses.', 'all other']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'ccgbank ( #AUTHOR_TAG ) is used to train the model.', 'for each training sentence, the corresponding ccgbank derivation together with all its sub - derivations are treated as gold - standard hypotheses.', 'all other']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'ccgbank ( #AUTHOR_TAG ) is used to train the model.', 'for each training sentence, the corresponding ccgbank derivation together with all its sub - derivations are treated as gold - standard hypotheses.', 'all other hypotheses that can be constructed from the same bag of words are non - gold hypotheses.', 'from the generation perspective this assumption is too strong, because sentences can have multiple orderings ( with multiple derivations ) that are both gram - matical and fluent.', 'nevertheless, it is the most feasible choice given the training data available']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'ccgbank ( #AUTHOR_TAG ) is used to train the model.', 'for each training sentence, the corresponding ccgbank derivation together with all its sub - derivations are treated as gold - standard hypotheses.', 'all other hypotheses that can be constructed from the same bag of words are non - gold hypotheses.', 'from the generation perspective this assumption is too strong, because sentences can have multiple orderings ( with multiple derivations ) that are both gram - matical and fluent.', 'nevertheless, it is the most feasible choice given the training data available']",5
"['is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers ( #AUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase - based mt decoding ( koehn 2010 ).', 'however, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'we will also']","['is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers ( #AUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase - based mt decoding ( koehn 2010 ).', 'however, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'we will also']","[', we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted hypotheses.', 'when a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses.', 'the data structure for accepted hypotheses is similar to that used for best - first parsing ( caraballo and charniak 1998 ), and we adopt the term chart for this structure.', 'however, note there are important differences to the parsing problem.', 'first, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers ( #AUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase - based mt decoding ( koehn 2010 ).', 'however, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'we will also']","['our formulation of the word ordering problem, a hypothesis is a phrase or sentence together with its ccg derivation.', 'hypotheses are constructed bottom - up : starting from single words, smaller phrases are combined into larger ones according to ccg rules.', 'to allow the combination of hypotheses, we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted hypotheses.', 'when a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses.', 'the data structure for accepted hypotheses is similar to that used for best - first parsing ( caraballo and charniak 1998 ), and we adopt the term chart for this structure.', 'however, note there are important differences to the parsing problem.', 'first, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers ( #AUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase - based mt decoding ( koehn 2010 ).', 'however, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'we will also investigate the possibility of applying dynamic - programming - style pruning to the chart']",1
"['.', 'for this experiment, the post - edit data from the lig corpus were randomly split into 3 subsets : pe - train ( 6, 881 sentences ), pe - dev, and pe - test ( 2, 000 sentences each ).', 'pe - train was used for our online learning experiments.', ""pe - test was held out for testing the algorithms'progress on unseen data."", 'pe - dev was used to obtain w * to define the utility model.', 'this was done by mert optimization ( #AUTHOR_TAG ) towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to improve smt performance over any algorithm that has access to full']","['experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'we select feedback of varying grade by directly inspecting the optimal w *, thus this feedback is idealized.', 'however, the experiment also has a realistic background since we show that  - informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized ter, and that learning from weak and strong feedback leads to convergence in ter on test data.', 'for this experiment, the post - edit data from the lig corpus were randomly split into 3 subsets : pe - train ( 6, 881 sentences ), pe - dev, and pe - test ( 2, 000 sentences each ).', 'pe - train was used for our online learning experiments.', ""pe - test was held out for testing the algorithms'progress on unseen data."", 'pe - dev was used to obtain w * to define the utility model.', 'this was done by mert optimization ( #AUTHOR_TAG ) towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to improve smt performance over any algorithm that has access to full']","['experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'we select feedback of varying grade by directly inspecting the optimal w *, thus this feedback is idealized.', 'however, the experiment also has a realistic background since we show that  - informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized ter, and that learning from weak and strong feedback leads to convergence in ter on test data.', 'for this experiment, the post - edit data from the lig corpus were randomly split into 3 subsets : pe - train ( 6, 881 sentences ), pe - dev, and pe - test ( 2, 000 sentences each ).', 'pe - train was used for our online learning experiments.', ""pe - test was held out for testing the algorithms'progress on unseen data."", 'pe - dev was used to obtain w * to define the utility model.', 'this was done by mert optimization ( #AUTHOR_TAG ) towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to improve smt performance over any algorithm that has access to full information to compute w *.', 'rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'the feedback data in this experiment were generated by searching the n - best list']","['experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'we select feedback of varying grade by directly inspecting the optimal w *, thus this feedback is idealized.', 'however, the experiment also has a realistic background since we show that  - informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized ter, and that learning from weak and strong feedback leads to convergence in ter on test data.', 'for this experiment, the post - edit data from the lig corpus were randomly split into 3 subsets : pe - train ( 6, 881 sentences ), pe - dev, and pe - test ( 2, 000 sentences each ).', 'pe - train was used for our online learning experiments.', ""pe - test was held out for testing the algorithms'progress on unseen data."", 'pe - dev was used to obtain w * to define the utility model.', 'this was done by mert optimization ( #AUTHOR_TAG ) towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to improve smt performance over any algorithm that has access to full information to compute w *.', 'rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'the feedback data in this experiment were generated by searching the n - best list for translations that are  - informative at   { 0. 1, 0. 5, 1. 0 } ( with possible non - zero slack ).', 'this is achieved by scanning the n - best list output for every input x t and returning the firsty t = y t that satisfies equation ( 2 ). 5 this setting can be thought of as an idealized scenario where a user picks translations from the n - best list that are considered improvements under the optimal w *']",5
"[', perceptron - type algorithms are often applied in a batch learning scenario, i. e., the algorithm is applied for k epochs to a training sample of size t and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ).', 'the']","[', perceptron - type algorithms are often applied in a batch learning scenario, i. e., the algorithm is applied for k epochs to a training sample of size t and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ).', 'the']","[', perceptron - type algorithms are often applied in a batch learning scenario, i. e., the algorithm is applied for k epochs to a training sample of size t and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ).', 'the difference to the online learning scenario is that we treat the multi - epoch algorithm as an empirical risk minimizer that selects a final weight vector w t, k whose expected loss on unseen data we would like to bound.', 'we assume that the algorithm is fed with a sequence of examples x 1,...', ', x t, and at each epoch k = 1,...', ', k it makes a prediction y t, k.', 'the correct label is y * t.', 'for k = 1,...', ', k and t = 1,...', ', t, let t, k = u ( x t, y * t )  u ( x t, y t, k ), and denote by  t, k and  t, k the distance at epoch k for example t, and the slack at epoch k for example t, respectively.', 'finally, we denote']","['for online - to - batch conversion.', 'in practice, perceptron - type algorithms are often applied in a batch learning scenario, i. e., the algorithm is applied for k epochs to a training sample of size t and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ).', 'the difference to the online learning scenario is that we treat the multi - epoch algorithm as an empirical risk minimizer that selects a final weight vector w t, k whose expected loss on unseen data we would like to bound.', 'we assume that the algorithm is fed with a sequence of examples x 1,...', ', x t, and at each epoch k = 1,...', ', k it makes a prediction y t, k.', 'the correct label is y * t.', 'for k = 1,...', ', k and t = 1,...', ', t, let t, k = u ( x t, y * t )  u ( x t, y t, k ), and denote by  t, k and  t, k the distance at epoch k for example t, and the slack at epoch k for example t, respectively.', 'finally, we denote by d t, k = t t = 1  2 t, k, and by w t, k the final weight vector returned after k epochs.', 'we state a condition of convergence']",1
"['phrase - based decoder ( #AUTHOR_TAG ) with dense features.', 'we replicated a similar mos']","['used the lig corpus 3 which consists of 10, 881 tuples of french - english post - edits (Potet et al., 2012).', 'the corpus is a subset of the newscommentary dataset provided at wmt 4 and contains input french sentences, mt outputs, postedited outputs and english references.', 'to prepare smt outputs for post - editing, the creators of the corpus used their own wmt10 system ( Potet et al. , 2010 ), based on the moses phrase - based decoder ( #AUTHOR_TAG ) with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit (Heafield, 2011) on news. en', '']","['used the lig corpus 3 which consists of 10, 881 tuples of french - english post - edits (Potet et al., 2012).', 'the corpus is a subset of the newscommentary dataset provided at wmt 4 and contains input french sentences, mt outputs, postedited outputs and english references.', 'to prepare smt outputs for post - editing, the creators of the corpus used their own wmt10 system ( Potet et al. , 2010 ), based on the moses phrase - based decoder ( #AUTHOR_TAG ) with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit (Heafield, 2011) on news. en', '']","['used the lig corpus 3 which consists of 10, 881 tuples of french - english post - edits (Potet et al., 2012).', 'the corpus is a subset of the newscommentary dataset provided at wmt 4 and contains input french sentences, mt outputs, postedited outputs and english references.', 'to prepare smt outputs for post - editing, the creators of the corpus used their own wmt10 system ( Potet et al. , 2010 ), based on the moses phrase - based decoder ( #AUTHOR_TAG ) with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit (Heafield, 2011) on news. en', 'data ( 48. 65m', 'sentences, 1. 13b tokens ), pre - processed with the tools from the cdec toolkit (Dyer et al., 2010).', 'perceptron cycling theorem (Block and Levin, 1970;Gelfand et al., 2010) should suffice to show a similar bound.', 'parallel data ( europarl + news - comm, 1. 64m sentences ) were similarly pre - processed and aligned with fast align (Dyer et al., 2013).', 'in all experiments, training is started with the moses default weights.', 'the size of the n - best list, where used, was set to 1, 000.', 'irrespective of the use of re - scaling in perceptron training, a constant learning rate of 5 was used for learning from simulated feedback, and 10 4 for learning from surrogate translations']",5
"['contrast, a single statistical model allows one to maintain a single table ( #AUTHOR_TAG )']","['contrast, a single statistical model allows one to maintain a single table ( #AUTHOR_TAG )']","['contrast, a single statistical model allows one to maintain a single table ( #AUTHOR_TAG )']","['contrast, a single statistical model allows one to maintain a single table ( #AUTHOR_TAG )']",0
['##n uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],['##n uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],['##n uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],['##n uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],0
"[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain ( #AUTHOR_TAG )""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain ( #AUTHOR_TAG )""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain ( #AUTHOR_TAG )""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain ( #AUTHOR_TAG )""]",0
"['such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAG a )']","['such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAG a )']","['has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAG a )']","['has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAG a )']",0
['the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in ( #AUTHOR_TAG b )'],"['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in ( #AUTHOR_TAG b )']","['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in ( #AUTHOR_TAG b )']","['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in ( #AUTHOR_TAG b )']",0
"[""addition to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG )""]","[""addition to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG )""]","[""addition to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG )""]","[""addition to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG )""]",0
"['what follows we explain the properties of the model by applying it to a small number of adjective - noun combinations taken from the lexical semantics literature.', 'table 1 gives the interpretations of eight adjective - noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ).', 'table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections ( v is the most likely interpretation, v 2 is the second most likely interpretation, etc. )']","['what follows we explain the properties of the model by applying it to a small number of adjective - noun combinations taken from the lexical semantics literature.', 'table 1 gives the interpretations of eight adjective - noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ).', 'table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections ( v is the most likely interpretation, v 2 is the second most likely interpretation, etc. )']","['what follows we explain the properties of the model by applying it to a small number of adjective - noun combinations taken from the lexical semantics literature.', 'table 1 gives the interpretations of eight adjective - noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ).', 'table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections ( v is the most likely interpretation, v 2 is the second most likely interpretation, etc. )']","['what follows we explain the properties of the model by applying it to a small number of adjective - noun combinations taken from the lexical semantics literature.', 'table 1 gives the interpretations of eight adjective - noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ).', 'table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections ( v is the most likely interpretation, v 2 is the second most likely interpretation, etc. )']",5
"['., the noun or noun class they modify ( see #AUTHOR_TAG and the references']","['recent work in lexical semantics has been concerned with accounting for regular polysemy, i. e., the regular and predictable sense alternations certain classes of words are subject to.', 'adjectives, more than other categories, are a striking example of regular polysemy since they are able to take on different meanings depending on their context, viz., the noun or noun class they modify ( see #AUTHOR_TAG and the references']","['., the noun or noun class they modify ( see #AUTHOR_TAG and the references']","['recent work in lexical semantics has been concerned with accounting for regular polysemy, i. e., the regular and predictable sense alternations certain classes of words are subject to.', 'adjectives, more than other categories, are a striking example of regular polysemy since they are able to take on different meanings depending on their context, viz., the noun or noun class they modify ( see #AUTHOR_TAG and the references therein )']",0
"['a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ).', 'from these we randomly']","['a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ).', 'from these we randomly']","['a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ).', 'from these we randomly sampled nine adjectives ( difficult, easy, fast, good, hard, right, safe, slow, wrong ).', 'these adjectives had to be unambiguous with respect to their part - of - speech : each adjective was unambiguously tagged as "" adjective "" 98. 6 % of the time, measured as the number of different part - of - speech tags assigned to the word in the bnc.', 'we identified adjective - noun pairs using gsearch (Corley et al., 2000), a chart parser which detects syntactic patterns in a tagged corpus by exploiting a userspecified context free grammar and a syntactic query.', 'gsearch was run on a lemmatized version of the bnc so as to compile a comprehensive corpus count of all nouns occurring in a modifier - head relationship with each of the nine adjectives.', 'from the syntactic analysis provided by we used the model']","['chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the bnc.', 'we chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ).', 'from these we randomly sampled nine adjectives ( difficult, easy, fast, good, hard, right, safe, slow, wrong ).', 'these adjectives had to be unambiguous with respect to their part - of - speech : each adjective was unambiguously tagged as "" adjective "" 98. 6 % of the time, measured as the number of different part - of - speech tags assigned to the word in the bnc.', 'we identified adjective - noun pairs using gsearch (Corley et al., 2000), a chart parser which detects syntactic patterns in a tagged corpus by exploiting a userspecified context free grammar and a syntactic query.', 'gsearch was run on a lemmatized version of the bnc so as to compile a comprehensive corpus count of all nouns occurring in a modifier - head relationship with each of the nine adjectives.', 'from the syntactic analysis provided by we used the model outlined in section 2 to derive meanings for the 90 adjective - noun combinations.', 'we employed no threshold on the frequencies f ( a, v ) and f ( rel, v, n ).', 'in order to obtain the frequency f ( a, v ) the adjective was mapped to its corresponding adverb.', 'in particular, good was mapped to good and well, fast to fast, easy to easily, hard to hard, right to rightly and right, safe to safely and safe, slow to slowly and slow and wrong to wrongly and wrong.', 'the adverbial function of the adjective difficult is expressed only periphrastically ( i. e., in a difficult manner, with difficulty ).', 'as a result, the frequency f ( difficult, v ) was estimated only on the basis of infinitival constructions ( see ( 17 ) ).', 'we estimated the probability p ( a, n, v, rel ) for each adjective - noun pair by varying both the terms v and rel']",5
"[': a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enum']","['a difficult language is a language that is difficult to learn, speak, or write.', 'adjectives like good allow either verb - subject or verb - object interpretations : a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the']","['a difficult language is a language that is difficult to learn, speak, or write.', 'adjectives like good allow either verb - subject or verb - object interpretations : a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify.', 'pustejovsky treats nouns as having a qualia structure as part of']","['2 ) a. easy problem b. difficult language c. good cook d. good soup adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'the meaning of adjective - noun combinations like those in ( 1 ) and ( 2 ) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', 'for example, an easy problem is "" a problem that is easy to solve "" or "" a problem that one can solve easily "".', 'in order to account for the meaning of these combinations Vendler (1968, 92 ) points out that "" in most cases not one verb, but a family of verbs is needed "".', 'vendler further observes that the noun figuring in an adjective - noun combination is usually the subject or object of the paraphrasing verb.', 'although fast usually triggers a verb - subject interpretation ( see ( 1 ) ), easy and difficult trigger verb - object interpretations ( see ( 2a, b ) ).', 'an easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', 'adjectives like good allow either verb - subject or verb - object interpretations : a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify.', 'pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'for example, the telic ( purpose ) role of the qualia structure for problem has a value equivalent to solve.', 'when the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective - noun combination receives the interpretation a problem that is easy to solve.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'furthermore, in cases where more than one interpretations are provided ( see Vendler (1968) ), no information is given with respect to the likelihood of these interpretations.', 'outof context, the number of interpretations for fast scientist is virtually unlimited, yet some interpretations are more likely than others : fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly']",0
"['have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']",0
['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],0
['definitions of predicates may be found in ( #AUTHOR_TAG )'],['definitions of predicates may be found in ( #AUTHOR_TAG )'],['definitions of predicates may be found in ( #AUTHOR_TAG )'],['definitions of predicates may be found in ( #AUTHOR_TAG )'],0
"['appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over - generalisation compared with those given in #AUTHOR_TAG']","['appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over - generalisation compared with those given in #AUTHOR_TAG']","['appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over - generalisation compared with those given in #AUTHOR_TAG']","['appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over - generalisation compared with those given in #AUTHOR_TAG']",1
['task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 )'],['task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 )'],['task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 )'],['task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 )'],1
"['problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important ( #AUTHOR_TAG )']","['problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important ( #AUTHOR_TAG )']","['problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important ( #AUTHOR_TAG )']","['problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important ( #AUTHOR_TAG )']",4
"[', #AUTHOR_TAG claims that the log - likelihood chisquared statistic (']","[', #AUTHOR_TAG claims that the log - likelihood chisquared statistic ( g2 ) is more appropriate for corpus - based nlp']","[', #AUTHOR_TAG claims that the log - likelihood chisquared statistic (']","[', #AUTHOR_TAG claims that the log - likelihood chisquared statistic ( g2 ) is more appropriate for corpus - based nlp']",4
['task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG'],1
"['x2 statistic is performing at least as well as g2, throwing doubt on the claim by #AUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']","['x2 statistic is performing at least as well as g2, throwing doubt on the claim by #AUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']","['x2 statistic is performing at least as well as g2, throwing doubt on the claim by #AUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']","['x2 statistic is performing at least as well as g2, throwing doubt on the claim by #AUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']",1
"['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple - mented in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]",1
"['escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is, in fact, encoded syntactically.', 'they describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'this present framework builds on the work of']","['escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is, in fact, encoded syntactically.', 'they describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'this present framework builds on the work of hale and keyser, but in addition to advancing a more refined theory of verbal argument structure, i also describe a computational implementation']","['observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is, in fact, encoded syntactically.', 'they describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'this present framework builds on the work of']","['observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is, in fact, encoded syntactically.', 'they describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'this present framework builds on the work of hale and keyser, but in addition to advancing a more refined theory of verbal argument structure, i also describe a computational implementation']",0
"['is compatible with verbal roots expressing activity.', 'it projects a functional head, voice ( #AUTHOR_TAG ), whose specifier is the external argument.', 'lexical']","['is compatible with verbal roots expressing activity.', 'it projects a functional head, voice ( #AUTHOR_TAG ), whose specifier is the external argument.', 'lexical']","['is compatible with verbal roots expressing activity.', 'it projects a functional head, voice ( #AUTHOR_TAG ), whose specifier is the external argument.', 'lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a  expression']","['light verb v do licenses an atelic non - inchoative event, and is compatible with verbal roots expressing activity.', 'it projects a functional head, voice ( #AUTHOR_TAG ), whose specifier is the external argument.', 'lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a  expression']",0
"['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple - mented in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]",1
"['this paper, i present a computational implementation of distributed morphology ( #AUTHOR_TAG ), a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']","['this paper, i present a computational implementation of distributed morphology ( #AUTHOR_TAG ), a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']","['this paper, i present a computational implementation of distributed morphology ( #AUTHOR_TAG ), a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']","['this paper, i present a computational implementation of distributed morphology ( #AUTHOR_TAG ), a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']",5
"['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( Baker et al. , 1998 ) and propbank ( #AUTHOR_TAG )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( Baker et al. , 1998 ) and propbank ( #AUTHOR_TAG )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( Baker et al. , 1998 ) and propbank ( #AUTHOR_TAG )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( Baker et al. , 1998 ) and propbank ( #AUTHOR_TAG )""]",0
"['( #AUTHOR_TAG ), i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","['( #AUTHOR_TAG ), i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","['( #AUTHOR_TAG ), i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","['( #AUTHOR_TAG ), i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']",2
"['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ), attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']",0
"['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']",0
"['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles']",0
"['hence are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']","['hence are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']","['hence are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']","[""know run believe walk accomplishments achievements paint a picture recognize make a chair find under vendler's classification, activities and states both depict situations that are inherently temporally unbounded ( atelic ) ; states denote static situations, whereas activities denote on - going dynamic situations."", 'accomplishments and achievements both express a change of state, and hence are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']",0
"['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles']",0
"['the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by #AUTHOR_TAG']","['the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by #AUTHOR_TAG']","['the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by #AUTHOR_TAG']","['that the only difference between flat. adj and flatten. v is the suffix - en, it must be the source of inchoativity and contribute the change of state reading that distinguishes the verb from the adjective.', 'here, we have evidence that derivational affixes affect the semantic representation of lexical items, that is, fragments of event structure are directly associated with derivational morphemes.', 'we have the following situation : in this case, the complete event structure of a word can be compositionally derived from its component morphemes.', ""this framework, where the ` ` semantic load'' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by #AUTHOR_TAG a ), among many others."", 'note that such an approach is no longer lexicalist : each lexical item does not fully encode its associated syntactic and semantic structures.', 'rather, meanings are composed from component morphemes']",0
"['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]","['theory of verbal argument structure can be imple - mented in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]",1
"['in the syntactic structure, as so - called light verbs.', 'here, i adopt the model proposed by #AUTHOR_TAG and decompose lexical']","['in the syntactic structure, as so - called light verbs.', 'here, i adopt the model proposed by #AUTHOR_TAG and decompose lexical']","['the non - lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so - called light verbs.', 'here, i adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs']","['the non - lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so - called light verbs.', 'here, i adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots.', 'verbalizing heads introduce relevant eventive interpretations in the syntax, and correspond to ( assumed ) universal primitives of the human cognitive system.', 'on the other hand, verbal roots represent abstract ( categoryless ) concepts and basically correspond to open - class items drawn from encyclopedic knowledge.', 'i assume an inventory of three verbalizing heads, each corresponding to an aforementioned primitive']",5
"['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( #AUTHOR_TAG ) and propbank ( Kingsbury et al. , 2002 )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( #AUTHOR_TAG ) and propbank ( Kingsbury et al. , 2002 )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( #AUTHOR_TAG ) and propbank ( Kingsbury et al. , 2002 )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( #AUTHOR_TAG ) and propbank ( Kingsbury et al. , 2002 )""]",0
"['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']",0
"['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ), attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']",0
"['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']",0
['has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to'],['has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to'],['has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to'],"['has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to his paper for a description of the parsing algorithm.', 'i have adapted it for my needs and developed grammar fragments that reflect my non - lexicalist semantic framework.', 'as an example, a simplified derivation of the sentence "" the tire flattened. "" is shown in figure 1']",2
"['typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ),']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ),']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ),']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ), possibly arranged in an inheritance hierarchy.', 'the argument structure and syntax - tosemantics mapping would then only need to be specified once for each verb class.', 'in addition, lexical rules could be formulated to derive certain alternations from more basic forms']",1
"['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; rappaport #AUTHOR_TAG ).', 'consider the following example']","['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; rappaport #AUTHOR_TAG ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; rappaport #AUTHOR_TAG ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; rappaport #AUTHOR_TAG ).', 'consider the following example']",0
"[""corresponding to vendler's event classes ( #AUTHOR_TAG ) : (""]","[""corresponding to vendler's event classes ( #AUTHOR_TAG ) : (""]","['##s, the activity of sweeping the floor and its result, the state of the floor being clean.', ""a more recent approach, advocated by Rappaport Hovav and Levin ( 1998 ), describes a basic set of event templates corresponding to vendler's event classes ( #AUTHOR_TAG ) : (""]","['##ty breaks the event described by ( 2 ) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""a more recent approach, advocated by Rappaport Hovav and Levin ( 1998 ), describes a basic set of event templates corresponding to vendler's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x act < manner > ] ( activity ) b. [ x < state > ] ( state ) c. [ become [ x < state > ] ] ( achievement ) d. [ x cause [ become [ x < state > ] ] ] ( accomplishment""]",0
"['are converted into the surface structure format via language generation tools.', '#AUTHOR_TAG b ) and Topkara et al. ( 2006']","['are converted into the surface structure format via language generation tools.', '#AUTHOR_TAG b ) and Topkara et al. ( 2006']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. (2005), Meral et al. (2007, Murphy (2001), Murphy and Vogel (2007) and Topkara et al. (2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', '#AUTHOR_TAG b ) and Topkara et al. ( 2006']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. (2005), Meral et al. (2007, Murphy (2001), Murphy and Vogel (2007) and Topkara et al. (2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', '#AUTHOR_TAG b ) and Topkara et al. ( 2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['.', 'the first lexical substitution method was proposed by #AUTHOR_TAG.', 'later works, such as Atallah et al. (2001 a ), Bolshakov (2004), Taskiran et al. (2006  and Topkara et al. (2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioriti']","['synonyms.', 'the first lexical substitution method was proposed by #AUTHOR_TAG.', 'later works, such as Atallah et al. (2001 a ), Bolshakov (2004), Taskiran et al. (2006  and Topkara et al. (2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['.', 'the first lexical substitution method was proposed by #AUTHOR_TAG.', 'later works, such as Atallah et al. (2001 a ), Bolshakov (2004), Taskiran et al. (2006  and Topkara et al. (2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by #AUTHOR_TAG.', 'later works, such as Atallah et al. (2001 a ), Bolshakov (2004), Taskiran et al. (2006  and Topkara et al. (2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), #AUTHOR_TAG and Topkara et al. ( 2006']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), #AUTHOR_TAG and Topkara et al. ( 2006']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), #AUTHOR_TAG and Topkara et al. ( 2006']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), #AUTHOR_TAG and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as #AUTHOR_TAG a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioriti']","['words with their synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as #AUTHOR_TAG a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n - gram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as #AUTHOR_TAG a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n - gram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as #AUTHOR_TAG a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n - gram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioriti']","['synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), #AUTHOR_TAG, Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), #AUTHOR_TAG, Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), #AUTHOR_TAG, Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), #AUTHOR_TAG, Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), #AUTHOR_TAG, Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioriti']","['synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), #AUTHOR_TAG, Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), #AUTHOR_TAG, Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), #AUTHOR_TAG, Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['google n - gram data was collected by google research for statistical language modelling, and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ), and contains english n - grams and their observed frequency counts, for counts of at least 40.', 'the striking feature of the n - gram corpus is the large number of n - grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of english text on publicly accessible web pages collected in january 2006.', 'for example, the 5 - gram phrase the part that you were has a count of 103.', 'the compressed']","['google n - gram data was collected by google research for statistical language modelling, and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ), and contains english n - grams and their observed frequency counts, for counts of at least 40.', 'the striking feature of the n - gram corpus is the large number of n - grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of english text on publicly accessible web pages collected in january 2006.', 'for example, the 5 - gram phrase the part that you were has a count of 103.', 'the compressed']","['google n - gram data was collected by google research for statistical language modelling, and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ), and contains english n - grams and their observed frequency counts, for counts of at least 40.', 'the striking feature of the n - gram corpus is the large number of n - grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of english text on publicly accessible web pages collected in january 2006.', 'for example, the 5 - gram phrase the part that you were has a count of 103.', 'the compressed']","['google n - gram data was collected by google research for statistical language modelling, and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ), and contains english n - grams and their observed frequency counts, for counts of at least 40.', 'the striking feature of the n - gram corpus is the large number of n - grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of english text on publicly accessible web pages collected in january 2006.', 'for example, the 5 - gram phrase the part that you were has a count of 103.', 'the compressed data is around 24 gb on disk']",0
"['( #AUTHOR_TAG ).', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in order to identify it,']","['( #AUTHOR_TAG ).', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in order to identify it,']","['is concerned with hiding information in some cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ).', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in order to identify it,']","['##ganography is concerned with hiding information in some cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ).', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in order to identify it, for example for the purposes of copyright.', 'here the changes may be known to an observer, and the task is to make the changes in such a way that the watermark cannot easily be removed']",0
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and #AUTHOR_TAG']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and #AUTHOR_TAG']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and #AUTHOR_TAG']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and #AUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['##p technology.', 'it requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation ( tmr ) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition,']","['semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state - ofthe - art for nlp technology.', 'it requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation ( tmr ) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition,']","['##p technology.', 'it requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation ( tmr ) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition,']","['semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state - ofthe - art for nlp technology.', 'it requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation ( tmr ) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition, with the idea that some presuppositional information can be removed without changing the meaning of a sentence']",0
"['sets of possible paraphrases.', 'each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'the examples show that, while some of the paraphrases are of a high quality, some are not.', 'for example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'for example, year end is an unsuitable paraphrase for the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context.', 'section 4 describes our method for determining if a paraphrase is suitable in a given context']","['sets of possible paraphrases.', 'each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'the examples show that, while some of the paraphrases are of a high quality, some are not.', 'for example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'for example, year end is an unsuitable paraphrase for the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context.', 'section 4 describes our method for determining if a paraphrase is suitable in a given context']","['sets of possible paraphrases.', 'each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'the examples show that, while some of the paraphrases are of a high quality, some are not.', 'for example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'for example, year end is an unsuitable paraphrase for the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context.', 'section 4 describes our method for determining if a paraphrase is suitable in a given context']","['1 gives summary statistics of the paraphrase dictionary and its coverage on section 00 of the penn treebank.', 'the length of the extracted n - gram phrases ranges from unigrams to five - grams.', 'the coverage figure gives the percentage of sentences which have at least one phrase in the dictionary.', 'the coverage is important for us because it determines the payload capacity of the embedding method described in section 5. original phrase paraphrases the end of this year later this year the end of the year year end a number of people some of my colleagues differences the european peoples party the ppe group dictionary is a mapping from phrases to sets of possible paraphrases.', 'each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'the examples show that, while some of the paraphrases are of a high quality, some are not.', 'for example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'for example, year end is an unsuitable paraphrase for the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context.', 'section 4 describes our method for determining if a paraphrase is suitable in a given context']",0
"['cover text used for our experiments consists of newspaper sentences from section 00 of the penn treebank (Marcus et al., 1993).', 'hence we require possible paraphrases for phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in #AUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']","['cover text used for our experiments consists of newspaper sentences from section 00 of the penn treebank (Marcus et al., 1993).', 'hence we require possible paraphrases for phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in #AUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']","['cover text used for our experiments consists of newspaper sentences from section 00 of the penn treebank (Marcus et al., 1993).', 'hence we require possible paraphrases for phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in #AUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']","['cover text used for our experiments consists of newspaper sentences from section 00 of the penn treebank (Marcus et al., 1993).', 'hence we require possible paraphrases for phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in #AUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']",5
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG, Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG, Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG, Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG, Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['the amount of white space between words ( #AUTHOR_TAG ).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in callison - Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words ( #AUTHOR_TAG ).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in callison - Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the']","['the amount of white space between words ( #AUTHOR_TAG ).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in callison - Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words ( #AUTHOR_TAG ).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in callison - Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text']",1
"['the amount of white space between words (Por et al., 2008).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words (Por et al., 2008).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the']","['the amount of white space between words (Por et al., 2008).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words (Por et al., 2008).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text']",5
"[', in which linguistic properties of a text are modified to hide information, is small compared with other media ( #AUTHOR_TAG ).', 'the likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an']","['observer. 1', 'key question for any steganography system is the choice of cover medium.', 'given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'however, the literature on linguistic steganography, in which linguistic properties of a text are modified to hide information, is small compared with other media ( #AUTHOR_TAG ).', 'the likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'language has the property that even small local changes to a text, e. g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'hence finding linguistic']","['being perceptable by a human observer. 1', 'key question for any steganography system is the choice of cover medium.', 'given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'however, the literature on linguistic steganography, in which linguistic properties of a text are modified to hide information, is small compared with other media ( #AUTHOR_TAG ).', 'the likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'language has the property that even small local changes to a text, e. g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'hence finding linguistic transformations which can be applied reliably and often is a challenging problem for linguistic steganography']","['is a large literature on image steganography and watermarking, in which images are modified to encode a hidden message or watermark.', 'image stegosystems exploit the redundancy in an image representation together with limitations of the human visual system.', 'for example, a standard image stegosystem uses the least - significant - bit ( lsb ) substitution technique.', 'since the difference between 11111111 and 11111110 in the value for red / green / blue intensity is likely to be undetectable by the human eye, the lsb can be used to hide information other than colour, without being perceptable by a human observer. 1', 'key question for any steganography system is the choice of cover medium.', 'given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'however, the literature on linguistic steganography, in which linguistic properties of a text are modified to hide information, is small compared with other media ( #AUTHOR_TAG ).', 'the likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'language has the property that even small local changes to a text, e. g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'hence finding linguistic transformations which can be applied reliably and often is a challenging problem for linguistic steganography']",0
"['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the #AUTHOR_TAG ccg parser to analyse the sentence before and after paraphrasing.', 'combinatory categorial grammar ( ccg ) is a lexicalised grammar formalism, in which ccg lexical categories - typically expressing subcategorisation information - are assigned to each word in a sentence.', 'the grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'if there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'hence the grammar check is at the word, rather than derivation, level ; however, ccg lexical categories contain a large amount of syntactic information which this method is able to exploit']","['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the #AUTHOR_TAG ccg parser to analyse the sentence before and after paraphrasing.', 'combinatory categorial grammar ( ccg ) is a lexicalised grammar formalism, in which ccg lexical categories - typically expressing subcategorisation information - are assigned to each word in a sentence.', 'the grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'if there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'hence the grammar check is at the word, rather than derivation, level ; however, ccg lexical categories contain a large amount of syntactic information which this method is able to exploit']","['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the #AUTHOR_TAG ccg parser to analyse the sentence before and after paraphrasing.', 'combinatory categorial grammar ( ccg ) is a lexicalised grammar formalism, in which ccg lexical categories - typically expressing subcategorisation information - are assigned to each word in a sentence.', 'the grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'if there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'hence the grammar check is at the word, rather than derivation, level ; however, ccg lexical categories contain a large amount of syntactic information which this method is able to exploit']","['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the #AUTHOR_TAG ccg parser to analyse the sentence before and after paraphrasing.', 'combinatory categorial grammar ( ccg ) is a lexicalised grammar formalism, in which ccg lexical categories - typically expressing subcategorisation information - are assigned to each word in a sentence.', 'the grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'if there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'hence the grammar check is at the word, rather than derivation, level ; however, ccg lexical categories contain a large amount of syntactic information which this method is able to exploit']",5
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), #AUTHOR_TAG, Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), #AUTHOR_TAG, Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), #AUTHOR_TAG, Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), #AUTHOR_TAG, Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ), we applied our approach to token']","['our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ), we applied our approach to tokenized arabic and our da - msa transfer component used feature transfer rules only.', 'we did not use a language model to pick the best path ; instead we kept the ambiguity in the lattice and passed it to our smt system.', 'in contrast, in this paper, we run elissa on untokenized arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the']","['our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ), we applied our approach to token']","['our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ), we applied our approach to tokenized arabic and our da - msa transfer component used feature transfer rules only.', 'we did not use a language model to pick the best path ; instead we kept the ambiguity in the lattice and passed it to our smt system.', 'in contrast, in this paper, we run elissa on untokenized arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the generated msa lattice through a language model.', ""certain aspects of our approach are similar to Riesa and Yarowsky (2006)'s, in that we use morphological analysis for da to help da - english mt ; but unlike them, we use a rule - based approach to model da morphology""]",1
"['##m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + ( #AUTHOR_TAG ).', 'phrase translations of up to 10 words are extracted in the']","['use the open - source moses toolkit (Koehn et al., 2007) to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + ( #AUTHOR_TAG ).', 'phrase translations of up to 10 words are extracted in the']","['##m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + ( #AUTHOR_TAG ).', 'phrase translations of up to 10 words are extracted in the moses phrase table.', 'the language model for our system is trained on the english side of the bitext']","['use the open - source moses toolkit (Koehn et al., 2007) to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + ( #AUTHOR_TAG ).', 'phrase translations of up to 10 words are extracted in the moses phrase table.', 'the language model for our system is trained on the english side of the bitext augmented with english gigaword (Graff and Cieri, 2003).', 'we use a 5 - gram language model with modified kneser - ney smoothing.', 'feature weights are tuned to maximize bleu on the nist mteval 2006 test set using minimum error rate training (Och, 2003).', 'this is only done on the baseline systems.', 'the english data is tokenized using simple punctuation - based rules.', 'the arabic side is segmented according to the arabic treebank ( atb ) tokenization scheme (Maamouri et al., 2004) using the mada + tokan morphological analyzer and tokenizer v3. 1 (Habash and Rambow, 2005;Roth et al., 2008).', 'the arabic text is also alif / ya normalized.', 'mada - produced arabic lemmas are used for word alignment']",5
"['low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'comparing the best performer to the oov selection mode system shows that translating low frequency in - vocabulary dialectal words and phrases to their msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in #AUTHOR_TAG']","['low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'comparing the best performer to the oov selection mode system shows that translating low frequency in - vocabulary dialectal words and phrases to their msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in #AUTHOR_TAG']","['low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'comparing the best performer to the oov selection mode system shows that translating low frequency in - vocabulary dialectal words and phrases to their msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in #AUTHOR_TAG']","['the last system group, phrase + word - based selection, phrase - based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'phrase - based trans - lation is also added to word - based translation.', 'results show that selecting and translating phrases improve the three best performers of word - based selection.', 'the best performer, shown in the last raw, suggests using phrase - based selection and restricted word - based selection.', 'the restriction is to include oov words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'comparing the best performer to the oov selection mode system shows that translating low frequency in - vocabulary dialectal words and phrases to their msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in #AUTHOR_TAG']",1
['use the open - source moses toolkit ( #AUTHOR_TAG ) to build a phrase - based smt system trained on mostly msa data ('],"['use the open - source moses toolkit ( #AUTHOR_TAG ) to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + (Och and Ney, 2003).', 'phrase translations of up to 10 words are extracted in the']",['use the open - source moses toolkit ( #AUTHOR_TAG ) to build a phrase - based smt system trained on mostly msa data ('],"['use the open - source moses toolkit ( #AUTHOR_TAG ) to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + (Och and Ney, 2003).', 'phrase translations of up to 10 words are extracted in the moses phrase table.', 'the language model for our system is trained on the english side of the bitext augmented with english gigaword (Graff and Cieri, 2003).', 'we use a 5 - gram language model with modified kneser - ney smoothing.', 'feature weights are tuned to maximize bleu on the nist mteval 2006 test set using minimum error rate training (Och, 2003).', 'this is only done on the baseline systems.', 'the english data is tokenized using simple punctuation - based rules.', 'the arabic side is segmented according to the arabic treebank ( atb ) tokenization scheme (Maamouri et al., 2004) using the mada + tokan morphological analyzer and tokenizer v3. 1 (Habash and Rambow, 2005;Roth et al., 2008).', 'the arabic text is also alif / ya normalized.', 'mada - produced arabic lemmas are used for word alignment']",5
"['are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; ']","['are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; ']","[', speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing ( see, e. g., tanenhaus et al. 1995 ).', 'this sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well - formed utterance.', 'few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ;  shriberg, bear, & Dowding , 1992 )']","[""implicit assumptions of psychological and computational theories that ignore disfluencies must be either that people aren't disfluent, or that disfluencies make processing more difficult, and so theories of fluent speech processing should be developed before the research agenda turns to disfluent speech processing."", 'the first assumption is clearly false ; disfluency rates in spontaneous speech are estimated by Fox Tree (1995) and by Bortfeld, Leon, Bloom, Schober, and Brennan (2000) to be about 6 disfluencies per 100 words, not including silent pauses.', 'the rate is lower for speech to machines (Oviatt, 1995;Shriberg, 1996), due in part to utterance length ; that is, disfluency rates are higher in longer utterances, where planning is more difficult, and utterances addressed to machines tend to be shorter than those addressed to people, often because dialogue interfaces are designed to take on more initiative.', 'the average speaker may believe, quite rightly, that machines are imperfect speech processors, and plan their utterances to machines more carefully.', 'the good news is that speakers can adapt to machines ; the bad news is that they do so by recruiting limited cognitive resources that could otherwise be focused on the task itself.', 'as for the second assumption, if the goal is to eventually process unrestricted, natural human speech, then committing to an early and exclusive focus on processing fluent utterances is risky.', 'in humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing ( see, e. g., tanenhaus et al. 1995 ).', 'this sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well - formed utterance.', 'few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ;  shriberg, bear, & Dowding , 1992 )']",0
"['to our previous work ( Chan and Ng , 2005 b ), we used the supervised wsd approach described in ( #AUTHOR_TAG ) for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of - the - art accuracy.', 'all accuracies reported in our experiments are micro - averages over all test examples']","['to our previous work ( Chan and Ng , 2005 b ), we used the supervised wsd approach described in ( #AUTHOR_TAG ) for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of - the - art accuracy.', 'all accuracies reported in our experiments are micro - averages over all test examples']","['to our previous work ( Chan and Ng , 2005 b ), we used the supervised wsd approach described in ( #AUTHOR_TAG ) for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of - the - art accuracy.', 'all accuracies reported in our experiments are micro - averages over all test examples']","['to our previous work ( Chan and Ng , 2005 b ), we used the supervised wsd approach described in ( #AUTHOR_TAG ) for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of - the - art accuracy.', 'all accuracies reported in our experiments are micro - averages over all test examples']",5
"['the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow #AUTHOR_TAG and split the sentences']","['our system is completely data - driven.', 'however, the comparison is indirect because our partitions of the ctb corpus are different.', 'Shi and Wang (2007) also chunked the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow #AUTHOR_TAG and split the sentences']","['the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow #AUTHOR_TAG and split the sentences']","['overall tagging accuracy of our joint model was comparable to but less than the joint model of Shi and Wang (2007).', 'despite the higher accuracy improvement from the baseline, the joint system did not give higher overall accuracy.', 'one likely reason is that Shi and Wang (2007) included knowledge about special characters and semantic knowledge from web corpora ( which may explain the higher baseline accuracy ), while our system is completely data - driven.', 'however, the comparison is indirect because our partitions of the ctb corpus are different.', 'Shi and Wang (2007) also chunked the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison']",5
"['built a two - stage baseline system, using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron pos tagging model from Collins ( 2002 ).', 'we use baseline system to refer to the system which performs segmentation first, followed by pos tagging ( using the single - best segmentation )']","['built a two - stage baseline system, using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron pos tagging model from Collins ( 2002 ).', 'we use baseline system to refer to the system which performs segmentation first, followed by pos tagging ( using the single - best segmentation ) ; baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only ; and baseline postagger to refer to the collins tagger which performs pos tagging only ( given segmentation ).', 'the features used by the baseline segmentor are shown in table 1.', 'the features used by the pos tagger, some of which are different to those from Collins (2002) and are']","['built a two - stage baseline system, using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron pos tagging model from Collins ( 2002 ).', 'we use baseline system to refer to the system which performs segmentation first, followed by pos tagging ( using the single - best segmentation ) ; baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only ; and baseline postagger to refer to the collins tagger which performs pos tagging only ( given segmentation ).', 'the features used by the baseline segmentor are shown in table 1.', 'the features used by the pos tagger, some of which are different to those from Collins (2002) and are']","['built a two - stage baseline system, using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron pos tagging model from Collins ( 2002 ).', 'we use baseline system to refer to the system which performs segmentation first, followed by pos tagging ( using the single - best segmentation ) ; baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only ; and baseline postagger to refer to the collins tagger which performs pos tagging only ( given segmentation ).', 'the features used by the baseline segmentor are shown in table 1.', 'the features used by the pos tagger, some of which are different to those from Collins (2002) and are specific to chinese, are shown in table 2']",2
"['.', 'our hdp extension is also inspired from the bayesian model proposed by #AUTHOR_TAG.', 'however, their model is strictly customized for entity coreference resolution, and therefore,']","['present an extension of the hierarchical dirichlet process ( hdp ) model which is able to represent each observable object ( i. e., event mention ) by a finite number of feature types l.', 'our hdp extension is also inspired from the bayesian model proposed by #AUTHOR_TAG.', 'however, their model is strictly customized for entity coreference resolution, and therefore,']","['present an extension of the hierarchical dirichlet process ( hdp ) model which is able to represent each observable object ( i. e., event mention ) by a finite number of feature types l.', 'our hdp extension is also inspired from the bayesian model proposed by #AUTHOR_TAG.', 'however, their model is strictly customized for entity coreference resolution, and therefore, extending it']","['present an extension of the hierarchical dirichlet process ( hdp ) model which is able to represent each observable object ( i. e., event mention ) by a finite number of feature types l.', 'our hdp extension is also inspired from the bayesian model proposed by #AUTHOR_TAG.', 'however, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008)']",4
"['by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class. we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class. we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']",4
"['by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class.', 'we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class.', 'we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']",4
"['recently, an alignment selection approach was proposed in ( #AUTHOR_TAG ), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'there is']","['recently, an alignment selection approach was proposed in ( #AUTHOR_TAG ), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'there is']","['recently, an alignment selection approach was proposed in ( #AUTHOR_TAG ), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'there is no need for a pre - determined threshold as used in (Huang, 2009).', 'also, we utilize various knowledge sources to enrich the alignments instead of using different aligners.', 'our strategy is to divers']","['recently, an alignment selection approach was proposed in ( #AUTHOR_TAG ), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'there is no need for a pre - determined threshold as used in (Huang, 2009).', 'also, we utilize various knowledge sources to enrich the alignments instead of using different aligners.', 'our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low - resource languages']",1
"[""by ( #AUTHOR_TAG ), we split one phrase type into several subsymbols, which contain category information of current constituent's parent."", 'for example, an np immediately dominated by a s, will be substituted by nps.', 'this strategy severely increases the number of chunk types and make it hard to train chunking models.', 'to shrink this number, we linguistically use a cluster of ctb phrasal types, which was introduced in.', 'the column chunk 2 illustrates this definition.', '']","[""by ( #AUTHOR_TAG ), we split one phrase type into several subsymbols, which contain category information of current constituent's parent."", 'for example, an np immediately dominated by a s, will be substituted by nps.', 'this strategy severely increases the number of chunk types and make it hard to train chunking models.', 'to shrink this number, we linguistically use a cluster of ctb phrasal types, which was introduced in.', 'the column chunk 2 illustrates this definition.', 'e. g., nps implicitly represents subject']","[""by ( #AUTHOR_TAG ), we split one phrase type into several subsymbols, which contain category information of current constituent's parent."", 'for example, an np immediately dominated by a s, will be substituted by nps.', 'this strategy severely increases the number of chunk types and make it hard to train chunking models.', 'to shrink this number, we linguistically use a cluster of ctb phrasal types, which was introduced in.', 'the column chunk 2 illustrates this definition.', '']","['introduce two types of chunks.', 'the first is simply the phrase type, such as np, pp, of current chunk.', 'the column chunk 1 illustrates this kind of chunk type definition.', 'the second is more complicated.', ""inspired by ( #AUTHOR_TAG ), we split one phrase type into several subsymbols, which contain category information of current constituent's parent."", 'for example, an np immediately dominated by a s, will be substituted by nps.', 'this strategy severely increases the number of chunk types and make it hard to train chunking models.', 'to shrink this number, we linguistically use a cluster of ctb phrasal types, which was introduced in.', 'the column chunk 2 illustrates this definition.', 'e. g., nps implicitly represents subject while npvp represents object']",4
"['by the trigger - based approach ( #AUTHOR_TAG ).', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'however, as the size of the corpus increases, the maximum entropy model will become larger.', 'similarly, in (Shen et al., 2009), context language model is proposed for better rule selection.', 'taking the dependency edge as condition, our approach is very different from previous approaches of exploring context']","['by the trigger - based approach ( #AUTHOR_TAG ).', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'however, as the size of the corpus increases, the maximum entropy model will become larger.', 'similarly, in (Shen et al., 2009), context language model is proposed for better rule selection.', 'taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information']","['feature of head word trigger which we apply to the log - linear model is motivated by the trigger - based approach ( #AUTHOR_TAG ).', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'however, as the size of the corpus increases, the maximum entropy model will become larger.', 'similarly, in (Shen et al., 2009), context language model is proposed for better rule selection.', 'taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information']","['feature of head word trigger which we apply to the log - linear model is motivated by the trigger - based approach ( #AUTHOR_TAG ).', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'however, as the size of the corpus increases, the maximum entropy model will become larger.', 'similarly, in (Shen et al., 2009), context language model is proposed for better rule selection.', 'taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information']",4
"['sentiment - analysis work in different domains has considered inter - document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter - document references in the form of hyperlinks ( #AUTHOR_TAG )']","['sentiment - analysis work in different domains has considered inter - document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter - document references in the form of hyperlinks ( #AUTHOR_TAG )']","['sentiment - analysis work in different domains has considered inter - document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter - document references in the form of hyperlinks ( #AUTHOR_TAG )']","['sentiment - analysis work in different domains has considered inter - document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter - document references in the form of hyperlinks ( #AUTHOR_TAG )']",0
"['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ).', 'an']","['the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ).', 'an']","['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ).', 'an']","['the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ).', 'an']","['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and #AUTHOR_TAG.', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and #AUTHOR_TAG.', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and #AUTHOR_TAG.', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and #AUTHOR_TAG.', 'Zhu (2005) maintains a survey of this area']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['sophisticated approaches have been proposed ( #AUTHOR_TAG ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ).', 'also relevant is work on the gen - eral problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)']","['sophisticated approaches have been proposed ( #AUTHOR_TAG ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ).', 'also relevant is work on the gen - eral problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)']","['sophisticated approaches have been proposed ( #AUTHOR_TAG ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ).', 'also relevant is work on the gen - eral problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)']","['sophisticated approaches have been proposed ( #AUTHOR_TAG ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ).', 'also relevant is work on the gen - eral problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)']",0
"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 )']",0
"['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['##xxx #AUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for']","['sentiment - polarity classifiers proposed in the recent literature categorize each document in - dependently. a few others incorporate various measures of inter - document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'many interesting opinion - oriented docu - ments, however, can be linked through certain re - lationships that occur in the context of evaluative discussions.', 'for example, we may find textual4 evidence of a high likelihood of agreement be - tween two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see Mullen and Malouf (2006) but cfxxx #AUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for ex - ample, we can easily categorize a complicated ( or overly terse ) document if we find within it indica - tions of agreement with a clearly positive text']","['##xxx #AUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for']","['sentiment - polarity classifiers proposed in the recent literature categorize each document in - dependently. a few others incorporate various measures of inter - document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'many interesting opinion - oriented docu - ments, however, can be linked through certain re - lationships that occur in the context of evaluative discussions.', 'for example, we may find textual4 evidence of a high likelihood of agreement be - tween two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see Mullen and Malouf (2006) but cfxxx #AUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for ex - ample, we can easily categorize a complicated ( or overly terse ) document if we find within it indica - tions of agreement with a clearly positive text']",0
"['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ).', 'an']","['the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ).', 'an']","['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['our experiments, we employed the well - known classifier svmlight to obtain individual - document classification scores, treating y as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis ( #AUTHOR_TAG ), the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for each speech segment s was based on the signed distanceds fromthevectorrepresent']","['our experiments, we employed the well - known classifier svmlight to obtain individual - document classification scores, treating y as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis ( #AUTHOR_TAG ), the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained svm decision plane : ds 234s d s 234s ds 234s def ds = + 234s']","['our experiments, we employed the well - known classifier svmlight to obtain individual - document classification scores, treating y as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis ( #AUTHOR_TAG ), the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for each speech segment s was based on the signed distanceds fromthevectorrepresent']","['our experiments, we employed the well - known classifier svmlight to obtain individual - document classification scores, treating y as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis ( #AUTHOR_TAG ), the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained svm decision plane : ds 234s d s 234s ds 234s def ds = + 234s 2 ind s ; y where 34s is the standard deviation of d s over all speech segments s in the debate in question, and def ind s ; n = ind s ; y']",5
"['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG )']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994) ; see Esuli (2006) for an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )""]",0
"['has considered inter - document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyper - links (Agrawal et al., 2003)']","['has considered inter - document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyper - links (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyper - links (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyper - links (Agrawal et al., 2003)']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 )']",3
"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( #AUTHOR_TAG ), and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( #AUTHOR_TAG ), and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( #AUTHOR_TAG ), and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( #AUTHOR_TAG ), and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 )']",0
"['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual 4 evidence of a high likelihood of agreement be - 4 because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem - specific information.', 'for example, although most votes in our corpus were almost completely along party lines ( and despite the fact that sameparty information is easily incorporated via the methods we propose ), we did not use party - affiliation data.', ""indeed, in other settings ( e. g., a movie - discussion listserv ) one may not be able to determine the participants'political leanings, and such information may not lead to significantly improved results even if it were available""]",0
"['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual 4 evidence of a high likelihood of agreement be - 4 because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem - specific information.', 'for example, although most votes in our corpus were almost completely along party lines ( and despite the fact that sameparty information is easily incorporated via the methods we propose ), we did not use party - affiliation data.', ""indeed, in other settings ( e. g., a movie - discussion listserv ) one may not be able to determine the participants'political leanings, and such information may not lead to significantly improved results even if it were available""]",0
"['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual 4 evidence of a high likelihood of agreement be - 4 because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem - specific information.', 'for example, although most votes in our corpus were almost completely along party lines ( and despite the fact that sameparty information is easily incorporated via the methods we propose ), we did not use party - affiliation data.', ""indeed, in other settings ( e. g., a movie - discussion listserv ) one may not be able to determine the participants'political leanings, and such information may not lead to significantly improved results even if it were available""]",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed ( Hillard et al. , 2003 ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ).', 'also relevant is work on the general problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002)']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed ( Hillard et al. , 2003 ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ).', 'also relevant is work on the general problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002)']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed ( Hillard et al. , 2003 ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ).', 'also relevant is work on the general problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002)']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed ( Hillard et al. , 2003 ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ).', 'also relevant is work on the general problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002)']",0
"['in the nlp literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['has been previously observed and exploited in the nlp literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision']",1
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"["": electronic rulemaking ( erulemaking ) initiatives involving the ` ` electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may ` ` [ alter ] the citizen - government relationship'' ( #AUTHOR_TAG )."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have on politics 2, or at least on political journalism 3.', 'regardless of whether one views such claims as clear - sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text,']","["": electronic rulemaking ( erulemaking ) initiatives involving the ` ` electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may ` ` [ alter ] the citizen - government relationship'' ( #AUTHOR_TAG )."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have on politics 2, or at least on political journalism 3.', 'regardless of whether one views such claims as clear - sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text,']","["": electronic rulemaking ( erulemaking ) initiatives involving the ` ` electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may ` ` [ alter ] the citizen - government relationship'' ( #AUTHOR_TAG )."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have on politics 2, or at least on political journalism 3.', 'regardless of whether one views such claims as clear - sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text,']","[""the united states, for example, governmental bodies are providing and soliciting political documents via the internet, with lofty goals in mind : electronic rulemaking ( erulemaking ) initiatives involving the ` ` electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may ` ` [ alter ] the citizen - government relationship'' ( #AUTHOR_TAG )."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have on politics 2, or at least on political journalism 3.', 'regardless of whether one views such claims as clear - sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process']",0
"['distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994) ; see Esuli (2006) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']",0
"['early papers on graph - based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), kondor and Lafferty (2002), and Joachims (2003).', '#AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), kondor and Lafferty (2002), and Joachims (2003).', '#AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), kondor and Lafferty (2002), and Joachims (2003).', '#AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), kondor and Lafferty (2002), and Joachims (2003).', '#AUTHOR_TAG maintains a survey of this area']",0
"['has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']",0
"['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), #AUTHOR_TAG, and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), #AUTHOR_TAG, and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), #AUTHOR_TAG, and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), #AUTHOR_TAG, and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003)']",0
"['in the nlp literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['has been previously observed and exploited in the nlp literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision']",1
"['by #AUTHOR_TAG, integrates both']","['by #AUTHOR_TAG, integrates both']","['by #AUTHOR_TAG, integrates both perspectives,']","['support / oppose classification problem can be approached through the use of standard classifiers such as support vector machines ( svms ), which consider each text unit in isolation.', 'as discussed in section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'our classification framework, directly inspired by #AUTHOR_TAG, integrates both perspectives, optimizing its labeling of speech segments based on both individual speech - segment classification scores and preferences for groups of speech segments to receive the same label.', 'in this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships']",5
"['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']",0
"['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), Sack ( 1994 ), and #AUTHOR_TAG ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), Sack ( 1994 ), and #AUTHOR_TAG ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), Sack ( 1994 ), and #AUTHOR_TAG ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), Sack ( 1994 ), and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003)']",0
"['early papers on graph - based semisupervised learning include #AUTHOR_TAG, Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include #AUTHOR_TAG, Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include #AUTHOR_TAG, Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include #AUTHOR_TAG, Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']",0
"['in future work.', 'relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests and satisfactions thereof ) to classify messages,']","['in future work.', 'relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests and satisfactions thereof ) to classify messages,']","['currently do not have an efficient means to encode disagreement information as hard constraints ; we plan to investigate incorporating such information in future work.', 'relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests and satisfactions thereof ) to classify messages,']","['currently do not have an efficient means to encode disagreement information as hard constraints ; we plan to investigate incorporating such information in future work.', 'relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests and satisfactions thereof ) to classify messages, and thus also explicitly exploit the structure of conversations']",0
"[""##ative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'people are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ).', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']","[""##ative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'people are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ).', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']","[""##ative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'people are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ).', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']","[""##ative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'people are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ).', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']",0
"['in the nlp literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['has been previously observed and exploited in the nlp literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision']",1
"['distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994) ; see Esuli (2006) for an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )""]",0
"['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), #AUTHOR_TAG, Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), #AUTHOR_TAG, Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), #AUTHOR_TAG, Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), #AUTHOR_TAG, Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']",0
"['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994) ; see Esuli (2006) for an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )""]",0
"['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes #AUTHOR_TAG, Hearst ( 1992 ), Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes #AUTHOR_TAG, Hearst ( 1992 ), Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes #AUTHOR_TAG, Hearst ( 1992 ), Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes #AUTHOR_TAG, Hearst ( 1992 ), Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003)']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), #AUTHOR_TAG, and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), #AUTHOR_TAG, and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), #AUTHOR_TAG, and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), #AUTHOR_TAG, and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']",0
"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG )']",0
"['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']",3
"['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['! ) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfxxx Agrawal et al. ( 2003 ) ).', 'agreement evidence can be a powerful aid in our classification task :']","['example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfxxx Agrawal et al. ( 2003 ) ).', 'agreement evidence can be a powerful aid in our classification task :']","['example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfxxx Agrawal et al. ( 2003 ) ).', 'agreement evidence can be a powerful aid in our classification task :']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfxxx Agrawal et al. ( 2003 ) ).', 'agreement evidence can be a powerful aid in our classification task : for example, we can easily categorize a complicated ( or overly terse ) document if we find within it indications of agreement with a clearly positive text']",0
"['has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']",0
"['determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), #AUTHOR_TAG, Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), #AUTHOR_TAG, Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), #AUTHOR_TAG, Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), #AUTHOR_TAG, Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003)']",0
"['have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'when the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'for example, when using bleu, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in ( #AUTHOR_TAG )']","['have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'when the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'for example, when using bleu, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in ( #AUTHOR_TAG )']","['have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'when the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'for example, when using bleu, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in ( #AUTHOR_TAG )']","['have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'when the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'for example, when using bleu, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in ( #AUTHOR_TAG )']",3
"['. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ).', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'however, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'therefore, the use of the syntactic / semantic kernels, i.']","['( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ).', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'however, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'therefore, the use of the syntactic / semantic kernels, i. e. (Bloehdorn and Moschitti, 2007 a ; Bloehdorn and Moschitti, 2007 b ), to syntactically contextualize word similarities may improve the reranker accuracy.', '( ii ) the latter can be further boosted by studying complex structural kernels,']","['. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ).', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'however, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'therefore, the use of the syntactic / semantic kernels, i. e. (Bloehdorn and Moschitti, 2007 a ; Bloehdorn and Moschitti, 2007 b ), to syntactically contextualize word similarities may improve the reranker accuracy.', '(']","['flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness, e. g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ).', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'however, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'therefore, the use of the syntactic / semantic kernels, i. e. (Bloehdorn and Moschitti, 2007 a ; Bloehdorn and Moschitti, 2007 b ), to syntactically contextualize word similarities may improve the reranker accuracy.', '( ii ) the latter can be further boosted by studying complex structural kernels, e. g.', '(Moschitti, 2008;Nguyen et al., 2009;Dinarelli et al., 2009).', '( iii ) more specific predicate argument structures such those proposed in framenet, e. g.', '(Baker et al., 1998;Giuglea and Moschitti, 2004;Giuglea and Moschitti, 2006;Johansson and Nugues, 2008 b ) may be useful to characterize the opinion holder and the sentence semantic context.', 'finally, while the strategy based on reranking resulted in a significant performance boost, it remains to be seen whether a higher accuracy can be achieved by developing a more sophisticated inference algorithm based on dynamic programming.', 'however, while the development of such an algorithm is an interesting problem, it will not necessarily result in a more usable system - when using a reranker, it is easy to trade accuracy for efficiency']",0
"['right column in table 1 shows the scores if ( using the product - of - ranks algorithm ) four source languages are taken into account in parallel.', 'as can be seen, with an average score of 51. 8 the improvement over the english only variant ( 50. 6 ) is minimal.', 'this contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'this is not the case here, where we try to improve on a score of around 50 for english.', 'remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'as this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'therefore, perhaps we should take it as a success that the product - of - ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non - english languages was probably mostly detrimental']","['right column in table 1 shows the scores if ( using the product - of - ranks algorithm ) four source languages are taken into account in parallel.', 'as can be seen, with an average score of 51. 8 the improvement over the english only variant ( 50. 6 ) is minimal.', 'this contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'this is not the case here, where we try to improve on a score of around 50 for english.', 'remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'as this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'therefore, perhaps we should take it as a success that the product - of - ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non - english languages was probably mostly detrimental']","['right column in table 1 shows the scores if ( using the product - of - ranks algorithm ) four source languages are taken into account in parallel.', 'as can be seen, with an average score of 51. 8 the improvement over the english only variant ( 50. 6 ) is minimal.', 'this contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'this is not the case here, where we try to improve on a score of around 50 for english.', 'remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'as this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'therefore, perhaps we should take it as a success that the product - of - ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non - english languages was probably mostly detrimental']","['right column in table 1 shows the scores if ( using the product - of - ranks algorithm ) four source languages are taken into account in parallel.', 'as can be seen, with an average score of 51. 8 the improvement over the english only variant ( 50. 6 ) is minimal.', 'this contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'this is not the case here, where we try to improve on a score of around 50 for english.', 'remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'as this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'therefore, perhaps we should take it as a success that the product - of - ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non - english languages was probably mostly detrimental']",1
['#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.'],"['#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i. e. the words occurring in a particular word equation ) within the association vector of a translation candidate, and by multiplying these ranks.', 'so for each candidate we obtain a product of ranks.', 'we then assume that the candidate with the smallest product will be the best translation. 3', '']",['#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.'],"['far, we always computed translations to single source words.', 'however, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product - of - ranks algorithm.', 'as suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i. e. the words occurring in a particular word equation ) within the association vector of a translation candidate, and by multiplying these ranks.', 'so for each candidate we obtain a product of ranks.', 'we then assume that the candidate with the smallest product will be the best translation. 3', 'et us illustrate this by an example : if the given words are the variants of the word nervous in english, french, german, and spanish, i. e. nervous, nerveux, nervos, and nervioso, and if we want to find out their translation into italian, we would look at the association vectors of each word in our italian target vocabulary.', 'the association strengths in these vectors need to be inversely sorted, and in each of them we will look up the positions of our four given words.', 'then for each vector we compute the product of the four ranks, and finally sort the italian vocabulary according to these products.', 'we would then expect that the correct italian translation, namely nervoso, ends up in the first position, i. e. has the smallest value for its product of ranks']",4
"['#AUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a']","['#AUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a']","['#AUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora.', 'we were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair.', 'for example, it is more']","['#AUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora.', 'we were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair.', 'for example, it is more promising to look at occurrences of english words in a german corpus rather than the other way around.', 'because of the special status of english it is also advisable to use it as a pivot wherever possible']",1
"['we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG.', 'an important component of our future effort will be to evaluate whether automatically']","['we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG.', 'an important component of our future effort will be to evaluate whether automatically']","['from text.', 'in future work, we will focus on mapping text ( in monologue form ) to dialogue.', 'for this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form.', 'for automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG.', 'an important component of our future effort will be to evaluate whether automatically generating dialogues from naturally - occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue']","['current work has focussed on high - level mapping rules which can be used both for generation from databases and knowledge representations and also for generation from text.', 'in future work, we will focus on mapping text ( in monologue form ) to dialogue.', 'for this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form.', 'for automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG.', 'an important component of our future effort will be to evaluate whether automatically generating dialogues from naturally - occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue']",3
"['.', 'the methods exhibited the expected result on only one of the six corpora, calbc cii, where there is a clear advantage for gazetteer over internal and a further clear advantage for simstring over gazetteer.', 'disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'although we have not been successful in figure 6 : learning curve for super grec figure 7 : learning curve for epi proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'it may be that our notion of distance to lexical resource entries is too naive.', 'a possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG.', 'this']","['to have fair coverage for simstring and gazetteer features.', 'while an advantage over internal was observed for super grec, simstring features showed no benefit over gazetteer features.', 'the methods exhibited the expected result on only one of the six corpora, calbc cii, where there is a clear advantage for gazetteer over internal and a further clear advantage for simstring over gazetteer.', 'disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'although we have not been successful in figure 6 : learning curve for super grec figure 7 : learning curve for epi proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'it may be that our notion of distance to lexical resource entries is too naive.', 'a possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG.', 'this']","['##a, genia and id we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'by contrast, for super grec there are several distinct classes for which we expected lexical resources to have fair coverage for simstring and gazetteer features.', 'while an advantage over internal was observed for super grec, simstring features showed no benefit over gazetteer features.', 'the methods exhibited the expected result on only one of the six corpora, calbc cii, where there is a clear advantage for gazetteer over internal and a further clear advantage for simstring over gazetteer.', 'disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'although we have not been successful in figure 6 : learning curve for super grec figure 7 : learning curve for epi proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'it may be that our notion of distance to lexical resource entries is too naive.', 'a possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG.', 'this would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']","['we expected to see clear benefits from both using gazetteers and simstring features, our exper - iments returned negative results for the majority of the corpora.', 'for nlpba, genia and id we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'by contrast, for super grec there are several distinct classes for which we expected lexical resources to have fair coverage for simstring and gazetteer features.', 'while an advantage over internal was observed for super grec, simstring features showed no benefit over gazetteer features.', 'the methods exhibited the expected result on only one of the six corpora, calbc cii, where there is a clear advantage for gazetteer over internal and a further clear advantage for simstring over gazetteer.', 'disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'although we have not been successful in figure 6 : learning curve for super grec figure 7 : learning curve for epi proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'it may be that our notion of distance to lexical resource entries is too naive.', 'a possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG.', 'this would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']",3
"['article classifier is a discriminative model that draws on the state - of - the - art approach described in #AUTHOR_TAG.', 'the model makes use of the averaged perceptron ( ap ) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'the article module uses the pos and chunker output to generate some of its features and candidates ( likely contexts for missing articles )']","['article classifier is a discriminative model that draws on the state - of - the - art approach described in #AUTHOR_TAG.', 'the model makes use of the averaged perceptron ( ap ) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'the article module uses the pos and chunker output to generate some of its features and candidates ( likely contexts for missing articles )']","['article classifier is a discriminative model that draws on the state - of - the - art approach described in #AUTHOR_TAG.', 'the model makes use of the averaged perceptron ( ap ) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'the article module uses the pos and chunker output to generate some of its features and candidates ( likely contexts for missing articles )']","['article classifier is a discriminative model that draws on the state - of - the - art approach described in #AUTHOR_TAG.', 'the model makes use of the averaged perceptron ( ap ) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'the article module uses the pos and chunker output to generate some of its features and candidates ( likely contexts for missing articles )']",5
"['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine - learning methods on error correction tasks ( #AUTHOR_TAG ).', 'thus, the classifiers trained on the learner data make use of a discriminative model.', 'because the google corpus does not contain complete sentences but only n - gram counts of length up to five, training a discriminative model is not desirable, and we thus use nb (']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine - learning methods on error correction tasks ( #AUTHOR_TAG ).', 'thus, the classifiers trained on the learner data make use of a discriminative model.', 'because the google corpus does not contain complete sentences but only n - gram counts of length up to five, training a discriminative model is not desirable, and we thus use nb ( details in Rozovskaya and Roth (2011) )']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine - learning methods on error correction tasks ( #AUTHOR_TAG ).', 'thus, the classifiers trained on the learner data make use of a discriminative model.', 'because the google corpus does not contain complete sentences but only n - gram counts of length up to five, training a discriminative model is not desirable, and we thus use nb (']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine - learning methods on error correction tasks ( #AUTHOR_TAG ).', 'thus, the classifiers trained on the learner data make use of a discriminative model.', 'because the google corpus does not contain complete sentences but only n - gram counts of length up to five, training a discriminative model is not desirable, and we thus use nb ( details in Rozovskaya and Roth (2011) )']",4
"['line of research that is correlated with ours is recognition of agreement / disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']",1
"['by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG )']","['by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG )']","['by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG )']","['first row of the table 6 represents the baseline results.', 'though the precision is high for agreement category, the recall is quite low and that results in a poor overall f1 measure.', ""this shows that even though markers like'agree'or'disagree'for the next set of experiments we used a supervised machine learning approach for the two - way classification ( agree / disagree )."", 'we use support vector machines ( svm ) as our machine - learning algorithm for classification as implemented in weka (Hall et al., 2009) and ran 10 - fold cross validation.', 'as a svm baseline, we first use all unigrams in callout and target as features ( table 6, row 2 ).', 'we notice that the recall improves significantly when compared with the rule - based method.', 'to further improve the classification accuracy, we use mutual information ( mi ) to select the words in the callouts and targets that are likely to be associated with the categories agree and disagree, respectively.', 'specifically, we sort each word based on its mi value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'the feature vector includes only words present in the mi list.', 'compared to the all unigrams baseline, the mi - based unigrams improve the f1 by 4 % ( agree ) and 2 % ( disagree ) ( table 6 ).', 'the mi approach discovers the words that are highly associated with agree / disagree categories and these words turn to be useful features for classification.', 'in addition, we consider several types of lexical features ( lexf ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG )']",4
"['line of research that is correlated with ours is recognition of agreement / disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']",1
"['by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 )']","['by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 )']","['by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 )']","['first row of the table 6 represents the baseline results.', 'though the precision is high for agreement category, the recall is quite low and that results in a poor overall f1 measure.', ""this shows that even though markers like'agree'or'disagree'for the next set of experiments we used a supervised machine learning approach for the two - way classification ( agree / disagree )."", 'we use support vector machines ( svm ) as our machine - learning algorithm for classification as implemented in weka (Hall et al., 2009) and ran 10 - fold cross validation.', 'as a svm baseline, we first use all unigrams in callout and target as features ( table 6, row 2 ).', 'we notice that the recall improves significantly when compared with the rule - based method.', 'to further improve the classification accuracy, we use mutual information ( mi ) to select the words in the callouts and targets that are likely to be associated with the categories agree and disagree, respectively.', 'specifically, we sort each word based on its mi value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'the feature vector includes only words present in the mi list.', 'compared to the all unigrams baseline, the mi - based unigrams improve the f1 by 4 % ( agree ) and 2 % ( disagree ) ( table 6 ).', 'the mi approach discovers the words that are highly associated with agree / disagree categories and these words turn to be useful features for classification.', 'in addition, we consider several types of lexical features ( lexf ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 )']",4
