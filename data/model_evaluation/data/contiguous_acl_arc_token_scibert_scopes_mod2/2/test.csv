token_context,word_context,seg_context,sent_cotext,label
"['be employed ( #AUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are']","['be employed ( #AUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are']","['from the ldoce coding system.', 'ideally, to distinguish between raising and equi verbs, a number of syntactic criteria should be employed ( #AUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit']","['small experiment demonstrates a number of points.', 'firstly, it seems reasonable to conclude that the assignment of individual codes to verbs is on the whole relatively accurate in ldoce.', 'of the 139 verbs tested, we only found code omissions in 10 cases.', 'secondly though, when we consider the interaction between the assignments of codes and word sense classification, ldoce appears less reliable.', 'this is the primary source of error in the case of the object raising rule.', 'thirdly, it seems clear that the object raising rule is straining the limits of what can be reliably extracted from the ldoce coding system.', 'ideally, to distinguish between raising and equi verbs, a number of syntactic criteria should be employed ( #AUTHOR_TAG : 460 ff. )', 'however, only two of these criteria are explicit in the coding system']",3
"['entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information ( #AUTHOR_TAG ).', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']","['entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information ( #AUTHOR_TAG ).', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']","['the master ldoce file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'in addition to headwords, dictionary search through the pronunciation field is available ; Carter (1987) has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure (Huttenlocher and Zue, 1983).', 'in addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information ( #AUTHOR_TAG ).', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']","['the master ldoce file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'in addition to headwords, dictionary search through the pronunciation field is available ; Carter (1987) has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure (Huttenlocher and Zue, 1983).', 'in addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information ( #AUTHOR_TAG ).', 'independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample']",0
"['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ).', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']","['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ).', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']","['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ).', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']","['tested the classification of verbs into semantic types using a verb list of 139 pre - classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ).', 'figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system']",5
"['can be retrieved.', 'as #AUTHOR_TAG points out, given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'the use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'for reasons of efficiency and']","['can be retrieved.', 'as #AUTHOR_TAG points out, given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'the use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'for reasons of efficiency and']","['can be retrieved.', 'as #AUTHOR_TAG points out, given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'the use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'for reasons of efficiency and']","['series of systems in cambridge are implemented in lisp running under unixtm.', 'they all make use of an efficient dictionary access system which services requests for s - expression entries made by client pro - grams.', 'a dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'as #AUTHOR_TAG points out, given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'the use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'for reasons of efficiency and flexibility of customisation, namely the use of ldoce by different client programs and from different lisp and / or prolog systems, the dictionary access system is implemented in the programming language c and makes use of the inter - process communication facilities provided by the unix operating system.', 'to the lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as lisp function calls']",0
"['make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable diction']","['example, functional unification grammar ( fug ) (Kay, 1984 a ), patr - ii (Shieber, 1984) - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial']","['make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser']","['developments in linguistics, and especially on grammatical theory - - for example, generalised phrase structure grammar ( gpsg ) (Gazdar et al., 1985), lexical functional grammar ( lfg ) (Kaplan and Bresnan, 1982) - - and on natural language parsing frameworks for example, functional unification grammar ( fug ) (Kay, 1984 a ), patr - ii (Shieber, 1984) - - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'these developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'real - time parsing imposes stringent requirements on a dictionary support environment ; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'the research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general - purpose, wide coverage morphological and syntactic analyser for english.', 'one motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser']",0
['. #AUTHOR_TAG ; elowitz et al. 1976 ; luce et al. 1983 ; cahn'],['#AUTHOR_TAG ; elowitz et al. 1976 ; luce et al. 1983 ; cahn'],"['. #AUTHOR_TAG ; elowitz et al. 1976 ; luce et al. 1983 ; cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and o '""]","['text - to - speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g. #AUTHOR_TAG ; elowitz et al. 1976 ; luce et al. 1983 ; cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and o'Shaughnessy (1989) - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'we believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'textto - speech systems that lack sentence - level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'in constructing the system, we focused on two core questions : ( i ) what kind of parser is needed for the p : rosody rules?', '']",4
"['previous work ( bachenko et al. 1986 ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( #AUTHOR_TAG ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate']","['previous work ( bachenko et al. 1986 ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( #AUTHOR_TAG ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate']","['previous work ( bachenko et al. 1986 ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( #AUTHOR_TAG ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate']","['previous work ( bachenko et al. 1986 ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( #AUTHOR_TAG ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries']",0
"['. allen 1976 ; elowitz et al. 1976 ; luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', 'and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation']","['allen 1976 ; elowitz et al. 1976 ; luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and o'Shaughnessy (1989) - - - the""]","['. allen 1976 ; elowitz et al. 1976 ; luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and o'Shaughnessy (1989) - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'we believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'textto - speech systems that lack sentence - level phrasing must take a conservative approach to pitch']","['text - to - speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g. allen 1976 ; elowitz et al. 1976 ; luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and o'Shaughnessy (1989) - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'we believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'textto - speech systems that lack sentence - level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'in constructing the system, we focused on two core questions : ( i ) what kind of parser is needed for the p : rosody rules?', '']",4
"['have built an experimental text - to - speech system that uses our analysis of prosody to generate phrase boundaries for the olive - - liberman synthesizer ( #AUTHOR_TAG ).', 'two concerns motivated our implementation.', 'first, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a']","['have built an experimental text - to - speech system that uses our analysis of prosody to generate phrase boundaries for the olive - - liberman synthesizer ( #AUTHOR_TAG ).', 'two concerns motivated our implementation.', 'first, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a']","['have built an experimental text - to - speech system that uses our analysis of prosody to generate phrase boundaries for the olive - - liberman synthesizer ( #AUTHOR_TAG ).', 'two concerns motivated our implementation.', 'first, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences.', 'second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text - to - speech synthesizer']","['have built an experimental text - to - speech system that uses our analysis of prosody to generate phrase boundaries for the olive - - liberman synthesizer ( #AUTHOR_TAG ).', 'two concerns motivated our implementation.', 'first, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences.', 'second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text - to - speech synthesizer']",5
"['observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in']","['observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in']","['observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues']","['syntax / prosody misalignment may be viewed as resulting in part from semantic considerations.', 'both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['has led some researchers, e. g., cooper and paccia - Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra, ; e.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from #AUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']","['has led some researchers, e. g., cooper and paccia - Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra, ; e.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from #AUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g., cooper and paccia - Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra, ; e.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from #AUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g., cooper and paccia - Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra, ; e.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from #AUTHOR_TAG, are frequently cited.', '( square brackets mark off the np constituents that contain embed - ded sentences.']",0
"['previous work ( #AUTHOR_TAG ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( olive and liberman 1985 ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate']","['previous work ( #AUTHOR_TAG ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( olive and liberman 1985 ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate']","['previous work ( #AUTHOR_TAG ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( olive and liberman 1985 ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate']","['previous work ( #AUTHOR_TAG ), we described an experimental text - to - speech system that determined prosodic phrasing for the olive - - liberman synthesizer ( olive and liberman 1985 ).', 'the system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'while we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'this paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'like the earlier version, it has been implemented in a text - to - speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries']",2
"[', in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of chomsky and halle, but']","['in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of chomsky and halle, but']","['.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of chomsky and halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', '']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g., cooper and paccia - Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra, ; e.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from Chomsky (1965) to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of chomsky and halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'he thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2 - - that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface, ; with prosodic phonology']",0
"['psycholinguistic studies of Martin ( 1970 ), Allen ( 1975 ), Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well']","['psycholinguistic studies of Martin ( 1970 ), Allen ( 1975 ), Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [ the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent.']","['psycholinguistic studies of Martin ( 1970 ), Allen ( 1975 ), Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well']","['psycholinguistic studies of Martin ( 1970 ), Allen ( 1975 ), Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and #AUTHOR_TAG, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [ the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent.']",0
"[', suggests the existence of a discourse - neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG, which "" are often overwhelmed by the chiaroscuro of highlight and background in discourse, but retain the status of null - hypothesis patterns that emerge when there is no good reason to take some other option "" ( p. 251 ).', 'this approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing']","['work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse - neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG, which "" are often overwhelmed by the chiaroscuro of highlight and background in discourse, but retain the status of null - hypothesis patterns that emerge when there is no good reason to take some other option "" ( p. 251 ).', 'this approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing']","[', suggests the existence of a discourse - neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG, which "" are often overwhelmed by the chiaroscuro of highlight and background in discourse, but retain the status of null - hypothesis patterns that emerge when there is no good reason to take some other option "" ( p. 251 ).', 'this approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing']","['work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse - neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG, which "" are often overwhelmed by the chiaroscuro of highlight and background in discourse, but retain the status of null - hypothesis patterns that emerge when there is no good reason to take some other option "" ( p. 251 ).', 'this approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing']",1
"['.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'in #AUTHOR_TAG, this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus']","['each embedded sente, nce to the node dominating it have been posited.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'in #AUTHOR_TAG, this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus']","['.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'in #AUTHOR_TAG, this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g., cooper and paccia - Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra, ; e.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from Chomsky (1965) to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'in #AUTHOR_TAG, this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of chomsky and halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'he thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2 - - that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface, ; with prosodic phonology']",0
"['tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a']","['tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']",1
"['to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f,']","['to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f,']","['to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f,']","['syntax / prosody misalignment may be viewed as resulting in part from semantic considerations.', 'both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['psycholinguistic studies of Martin ( 1970 ), #AUTHOR_TAG, Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and Gee and Grosjean ( 1983 ), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well']","['psycholinguistic studies of Martin ( 1970 ), #AUTHOR_TAG, Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and Gee and Grosjean ( 1983 ), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [ the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent.']","['psycholinguistic studies of Martin ( 1970 ), #AUTHOR_TAG, Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and Gee and Grosjean ( 1983 ), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well']","['psycholinguistic studies of Martin ( 1970 ), #AUTHOR_TAG, Hillinger et al. ( 1976 ), Grosjean et al. ( 1979 ), Dommergues and Grosjean ( 1983 ), and Gee and Grosjean ( 1983 ), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating i [ the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent.']",0
['#AUTHOR_TAG ; cahn'],['#AUTHOR_TAG ; cahn'],"['#AUTHOR_TAG ; cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and o'Shaughnessy (1989) - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'we believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'textto - speech systems that lack sentence - level phrasing must take a conservative approach to pitch']","['text - to - speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'many investigators ( e. g.', 'many investigators ( e. g. allen 1976 ; elowitz et al. 1976 ; #AUTHOR_TAG ; cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech.', ""and while researchers in text - tospeech synthesis have adopted a variety of approaches to prosodic phrase generation - - from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and o'Shaughnessy (1989) - - - the generation of appropriate prosodic phrasing in unres ~ tricted text has remained a problem."", 'as we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'we believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'textto - speech systems that lack sentence - level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'in constructing the system, we focused on two core questions : ( i ) what kind of parser is needed for the p : rosody rules?', '']",4
"['may be more descriptively suitable for some aspects of prosody ( for example, #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for']","['may be more descriptively suitable for some aspects of prosody ( for example, #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example, #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for']","['g & g, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'an alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'although a grid may be more descriptively suitable for some aspects of prosody ( for example, #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing']",1
"['claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f,']","['claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f,']","['to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f,']","['syntax / prosody misalignment may be viewed as resulting in part from semantic considerations.', 'both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",2
"['some researchers, e. g., #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from Chomsky (1965) to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posit']","['some researchers, e. g., #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from Chomsky (1965) to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g., #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from Chomsky (1965) to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posit']","['it comes to sentence - level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'this observation has led some researchers, e. g., #AUTHOR_TAG, to claim a direct mapping between the syntactic phrase and the prosodic phrase.', ""however, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'for example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'sentences like 12, from Chomsky (1965) to account for such mismatches, "" readjustment rules "" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'the result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12 ] "" ( p.', '372 ).', 'thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of chomsky and halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'he thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2 - - that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface, ; with prosodic phonology']",0
"[', is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating ii the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in #AUTHOR_TAG']","['psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating ii the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in #AUTHOR_TAG']","[', is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating ii the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in #AUTHOR_TAG']","['psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'for example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i. e., the data reveal both x ( vy ) and ( xv ) y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length ; for example, the main prosodic phrase break corresponds to the subject - predicate boundary in waiters who remember well ] [ serve orders correctly.', 'discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in chickens were eating ii the remaining green vegetables, where the subject - predicate boundary finds no prosodic correspondent. 4 the most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth g & g )']",1
"['#AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', '']","['#AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', '']","['#AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', '']","['syntax / prosody misalignment may be viewed as resulting in part from semantic considerations.', 'both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co - occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate - argument structure.', 'the problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'in 17a - f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'all of the sentences have the same x ( vy ) pattern even though y is a complement in the first case ( thefirst serious attempt ) and an adjunct in the others.', '( the complement in 17a and the adjuncts in 17b - f are italicized. )', 'the relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'this may be the reason that word count and syllable count play a prominent role in prosodic phrasing ( see section 2. 1. 3. ).', 'to our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing']",0
"['rules for phonological word formation are adopted, for the most part, from g & g, #AUTHOR_TAG, and the account of monosyllabic destressing in Selkirk ( 1984 ).', 'thus in our analysis, rules of phonological word formation apply to the']","['rules for phonological word formation are adopted, for the most part, from g & g, #AUTHOR_TAG, and the account of monosyllabic destressing in Selkirk ( 1984 ).', 'thus in our analysis, rules of phonological word formation apply to the non - null terminal nodes in a syntax tree.', 'if the terminal is a content word, i. e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'otherwise the word combines with one or more orthographically']","['rules for phonological word formation are adopted, for the most part, from g & g, #AUTHOR_TAG, and the account of monosyllabic destressing in Selkirk ( 1984 ).', 'thus in our analysis, rules of phonological word formation apply to the non - null terminal nodes in a syntax tree.', 'if the terminal is a content word, i. e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'this is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'function words, e. g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are']","['rules for phonological word formation are adopted, for the most part, from g & g, #AUTHOR_TAG, and the account of monosyllabic destressing in Selkirk ( 1984 ).', 'thus in our analysis, rules of phonological word formation apply to the non - null terminal nodes in a syntax tree.', 'if the terminal is a content word, i. e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'this is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'function words, e. g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.', 'content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone']",5
"['all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of #AUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser (1981) that the ratio of derived to explicit']","['all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of #AUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser (1981) that the ratio of derived to explicit']","['all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of #AUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser (1981) that the ratio of derived to explicit']","[', there are at least three arguments against iterating pt.', 'first of all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of #AUTHOR_TAG, 1978 ), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8 : 1 ; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh - questions']",4
"['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'we are going to make such a comparison with the theories proposed by j. #AUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w. Kintch ( 1983 ), who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']","['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'we are going to make such a comparison with the theories proposed by j. #AUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w. Kintch ( 1983 ), who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']","['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'we are going to make such a comparison with the theories proposed by j. #AUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w. Kintch ( 1983 ), who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']","['this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'we are going to make such a comparison with the theories proposed by j. #AUTHOR_TAG, 1982 ) that represent a more computationally oriented approach to coherence, and those of t. a. van dijk and w. Kintch ( 1983 ), who are more interested in addressing psychological and cognitive aspects of discourse coherence.', 'the quoted works seem to be good representatives for each of the directions ; they also point to related literature']",1
"['##sson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf. #AUTHOR_TAG), and']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf. #AUTHOR_TAG), and']","['##sson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf. #AUTHOR_TAG), and']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure as - sumption "" ( ibid. ), "" domain circumscription "" ( cf. #AUTHOR_TAG), and their kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abduc - tive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of Berwick (1986).', 'but, ob - viously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
['. #AUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['. #AUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['. #AUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],['. #AUTHOR_TAG discussed sentences of the form * this is a chair but you can sit on it'],0
"['#AUTHOR_TAG, p. 672 )']","['#AUTHOR_TAG, p. 672 )']","['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by halliday and hasan 1976 ; cf. also #AUTHOR_TAG, p. 672 )']","['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by halliday and hasan 1976 ; cf. also #AUTHOR_TAG, p. 672 )']",0
"['of the paragraph as a central element of discourse ( e. g. chafe 1979, #AUTHOR_TAG, long']","['of the paragraph as a central element of discourse ( e. g. chafe 1979, #AUTHOR_TAG, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, #AUTHOR_TAG, long']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, #AUTHOR_TAG, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"[', a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence']","[', a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence']","[', a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence']","[', our definition of coherence may not be restrictive enough : two collections of sentences, one referring to "" black "" ( about black pencils, black pullovers, and black poodles ), the other one about "" death "" ( war, cancer, etc. ), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic "" black + death. ""', ""this problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e. g., ` ` colorless green ideas...'' ), while before the advent of chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG, p. 546 ) gives 10 definitions of a sentence""]",0
"['describing the logic.', 'this principle of finitism is also assumed by #AUTHOR_TAG, Jackendoff ( 1983 ), Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['describing the logic.', 'this principle of finitism is also assumed by #AUTHOR_TAG, Jackendoff ( 1983 ), Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']","['describing the logic.', 'this principle of finitism is also assumed by #AUTHOR_TAG, Jackendoff ( 1983 ), Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by #AUTHOR_TAG, Jackendoff ( 1983 ), Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
"[""for instance, the discourse representation structures ( drs's ) of #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english,""]","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english,""]","[""for instance, the discourse representation structures ( drs's ) of #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english,""]","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of #AUTHOR_TAG, which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", 'Jackendoff (1983, p. 14) writes "" it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. ""', 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['to #AUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... ( ibid. )', 'suddenly ( for hobbs ) becomes coherent.', ""it seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change ; after all, the first two sentences didn't change when the third one was added."", 'on the other hand, this change is easily explained when we treat the first two sentences as a paragraph : if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'and the paragraph obtained by adding the third sentence is coherent.', 'moreover, coherence here is clearly the result of the existence of the topic "" john likes spinach.']","['to #AUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... ( ibid. )', 'suddenly ( for hobbs ) becomes coherent.', ""it seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change ; after all, the first two sentences didn't change when the third one was added."", 'on the other hand, this change is easily explained when we treat the first two sentences as a paragraph : if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'and the paragraph obtained by adding the third sentence is coherent.', 'moreover, coherence here is clearly the result of the existence of the topic "" john likes spinach.']","['to #AUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... ( ibid. )', 'suddenly ( for hobbs ) becomes coherent.', ""it seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change ; after all, the first two sentences didn't change when the third one was added."", 'on the other hand, this change is easily explained when we treat the first two sentences as a paragraph : if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'and the paragraph obtained by adding the third sentence is coherent.', 'moreover, coherence here is clearly the result of the existence of the topic "" john likes spinach.']","['to #AUTHOR_TAG, p. 67 ), these two sentences are incoherent.', 'however, the same fragment, augmented with the third sentence mary told him yesterday that the french spinach crop failed and turkey is the only country... ( ibid. )', 'suddenly ( for hobbs ) becomes coherent.', ""it seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change ; after all, the first two sentences didn't change when the third one was added."", 'on the other hand, this change is easily explained when we treat the first two sentences as a paragraph : if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'and the paragraph obtained by adding the third sentence is coherent.', 'moreover, coherence here is clearly the result of the existence of the topic "" john likes spinach.']",1
['#AUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so'],['#AUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so'],['##xxx #AUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so'],"['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( zadrozny 1987a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not come as a surprise that we can now use this apparatus for text / discourse analysis ; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'for instance, relating "" they "" to "" apples "" in the sentence ( cfxxx #AUTHOR_TAG p. 195 ; zadrozny 1987a ) : we bought the boys apples because they were so']",0
"['##es found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG']","['example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG']","['##es found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG']","['example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG']",0
"['#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus,']","['#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus,']","['#AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus,']","[', there are at least three arguments against iterating pt.', 'first of all, iteration would increase the complexity of building a model of a paragraph ; infinite iteration would almost certainly make impossible such a construction in real time.', 'secondly, the cooperative principle of Grice (1975 grice (, 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'finally, it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8 : 1 ; furthermore, our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh - questions']",4
"['#AUTHOR_TAG ).', 'brachman et al. 1985 ).', ""krypton's a - box, encoding the object theory as a set of assertions, uses standard first order logic ; the t - box contains information expressed in a frame - based language equivalent to a fragment of fol."", ""however, the distinction between the two parts is purely functional - - that is, characterized in terms of the system's behavior."", 'from the logical point of view, the knowledge base is the union of the two boxes, i. e. a theory, and the entailment is standard.', 'in our system, we also distinguish between the "" definitional "" and factual']","['#AUTHOR_TAG ).', 'brachman et al. 1985 ).', ""krypton's a - box, encoding the object theory as a set of assertions, uses standard first order logic ; the t - box contains information expressed in a frame - based language equivalent to a fragment of fol."", ""however, the distinction between the two parts is purely functional - - that is, characterized in terms of the system's behavior."", 'from the logical point of view, the knowledge base is the union of the two boxes, i. e. a theory, and the entailment is standard.', 'in our system, we also distinguish between the "" definitional "" and factual information, but the "" definitional "" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'moreover, in addition to proposing this structure of r, we have described the two mechanisms for exploiting it, "" coherence "" and "" dominance, "" which are not']","['##xxx #AUTHOR_TAG ).', 'brachman et al. 1985 ).', ""krypton's a - box, encoding the object theory as a set of assertions, uses standard first order logic ; the t - box contains information expressed in a frame - based language equivalent to a fragment of fol."", ""however, the distinction between the two parts is purely functional - - that is, characterized in terms of the system's behavior."", 'from the logical point of view, the knowledge base is the union of the two boxes, i. e. a theory, and the entailment is standard.', 'in our system, we also distinguish between the "" definitional "" and factual information, but the "" definitional "" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'moreover, in addition to proposing this structure of r, we have described the two mechanisms for exploiting it, "" coherence "" and "" dominance, "" which are not']","['last point may be seen better if we look at some differences between our system and krypton, which also distinguishes between an object theory and background knowledge ( cfxxx #AUTHOR_TAG ).', 'brachman et al. 1985 ).', ""krypton's a - box, encoding the object theory as a set of assertions, uses standard first order logic ; the t - box contains information expressed in a frame - based language equivalent to a fragment of fol."", ""however, the distinction between the two parts is purely functional - - that is, characterized in terms of the system's behavior."", 'from the logical point of view, the knowledge base is the union of the two boxes, i. e. a theory, and the entailment is standard.', 'in our system, we also distinguish between the "" definitional "" and factual information, but the "" definitional "" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'moreover, in addition to proposing this structure of r, we have described the two mechanisms for exploiting it, "" coherence "" and "" dominance, "" which are not variants of the standard first order entailment, but abduction']",1
"[""is spiritually related to the ` ` abductive inference'' of #AUTHOR_TAG, the ` ` diagnosis from first principles'' of Reiter ( 1987 ), ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of #AUTHOR_TAG, the ` ` diagnosis from first principles'' of Reiter ( 1987 ), ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of #AUTHOR_TAG, the ` ` diagnosis from first principles'' of Reiter ( 1987 ), ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure assumption "" ( ibid. ),', '"" domain circumscription "" ( cf.', 'etherington and mercer 1987 ), and their kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of #AUTHOR_TAG, the ` ` diagnosis from first principles'' of Reiter ( 1987 ), ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
"['##n 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', 'extending and revis']","['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event compo - nents.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. moens and steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems""]","['##n 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', 'extending and revis']","['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event compo - nents.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. moens and steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram - matical constraint ( that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon - - - ibid. )""]",0
"['- real struc - tures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to #AUTHOR_TAG, paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences.']","['e. j. crothers. his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, "" the linguistic - logical notions of consequent and presupposition "" Crothers (1979 : 112 ) have collected convincing evidence of the existence of language chunks - - real struc - tures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to #AUTHOR_TAG, paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences. hinds discusses three major types of paragraphs,']","['- real struc - tures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to #AUTHOR_TAG, paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences.']","['textualist approach to paragraph analysis is exemplified by e. j. crothers. his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, "" the linguistic - logical notions of consequent and presupposition "" Crothers (1979 : 112 ) have collected convincing evidence of the existence of language chunks - - real struc - tures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to #AUTHOR_TAG, paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences. hinds discusses three major types of paragraphs, and their corresponding segment types.', 'the three types are procedural ( how - to ), ex - pository ( essay ), and narrative ( in this case, spontaneous conversation ).', 'for each type, its segments are distinguished by bearing distinct relationships to the paragraph topic ( which is central, but nowhere clearly defined ).', 'segments themselves are composed of clauses and regulated by "" switching "" patterns, such as the question - answer pattern and the remark - reply pattern']",0
"[""is spiritually related to the ` ` abductive inference'' of Reggia ( 1985 ), the ` ` diagnosis from first principles'' of #AUTHOR_TAG, ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of Reggia ( 1985 ), the ` ` diagnosis from first principles'' of #AUTHOR_TAG, ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of Reggia ( 1985 ), the ` ` diagnosis from first principles'' of #AUTHOR_TAG, ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure assumption "" ( ibid. ),', '"" domain circumscription "" ( cf.', 'etherington and mercer 1987 ), and their kin.', ""similarly, the notion of r + m - abduction is spiritually related to the ` ` abductive inference'' of Reggia ( 1985 ), the ` ` diagnosis from first principles'' of #AUTHOR_TAG, ` ` explainability'' of Poole ( 1988 ), and the subset principle of Berwick ( 1986 )."", 'but, obviously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
['levesque 1984 ; #AUTHOR_TAG ; patel - schneider 1985'],['levesque 1984 ; #AUTHOR_TAG ; patel - schneider 1985'],['##xxx levesque 1984 ; #AUTHOR_TAG ; patel - schneider 1985'],"['in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, ( cfxxx levesque 1984 ; #AUTHOR_TAG ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]",1
"['#AUTHOR_TAG.', 'Hintikka (1985)']","['#AUTHOR_TAG.', 'Hintikka (1985)']","['##xxx #AUTHOR_TAG.', 'Hintikka (1985)']","['explicitly stated otherwise, we assume that formulas are expressed in a certain ( formal ) language l without equality ; the extension l ( = ) of l is going to be used only in section 5 for dealing with noun phrase references.', ""this means that natural language expressions such as ` ` a is b,'' ` ` a is the same as b,'' etc. are not directly represented by logical equality ; similarly, ` ` not'' is often not treated as logical negation ; cfxxx #AUTHOR_TAG."", 'Hintikka (1985)']",1
"['can be in - troduced by replacing "" there exists "" by "" all "" in the above definitions ( cf. #AUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system, "" satisfies "" should be probably replaced by "" violates fewest.']","[': the notions of strong provability and strong r + m - abduction can be in - troduced by replacing "" there exists "" by "" all "" in the above definitions ( cf. #AUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system, "" satisfies "" should be probably replaced by "" violates fewest.']","[': the notions of strong provability and strong r + m - abduction can be in - troduced by replacing "" there exists "" by "" all "" in the above definitions ( cf. #AUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system, "" satisfies "" should be probably replaced by "" violates fewest.']","[': the notions of strong provability and strong r + m - abduction can be in - troduced by replacing "" there exists "" by "" all "" in the above definitions ( cf. #AUTHOR_TAG b ).', 'we will have, however, no need for "" strong "" notions in this paper.', 'also, in a practical system, "" satisfies "" should be probably replaced by "" violates fewest.']",1
"['selectional restrictions ( semantic feature information, #AUTHOR_TAG ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'Later, Hobbs (1979']","['selectional restrictions ( semantic feature information, #AUTHOR_TAG ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'Later, Hobbs (1979 hobbs (, 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using "" salience "" in choosing facts from this knowledge base']","['selectional restrictions ( semantic feature information, #AUTHOR_TAG ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'Later, Hobbs (1979 hobbs (, 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using "" salience "" in choosing facts from this knowledge base']","['selectional restrictions ( semantic feature information, #AUTHOR_TAG ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', 'Later, Hobbs (1979 hobbs (, 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using "" salience "" in choosing facts from this knowledge base']",0
"['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. #AUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983)""]","['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. #AUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983)""]","['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. #AUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism""]","['text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g. #AUTHOR_TAG ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram - matical constraint ( that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon - - - ibid. )""]",0
"['it is the "" highest "" path, fint is the most plausible ( relative to r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx #AUTHOR_TAG a, 1987b ).', 'zadrozny 1987a Zadrozny , 1987 b.', 'another theory, consisting of f ~ = { el, sh2, pl, b2 ~ dl } and s, saying that a space vehicle came into the harbor and caused a disease ~ illness is less plausible according to that ordering.', 'as it turns out, f ~ is never constructed in the process of building an interpretation of a paragraph containing the sentence s, unless assuming fint would lead to a contradiction,']","['it is the "" highest "" path, fint is the most plausible ( relative to r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx #AUTHOR_TAG a, 1987b ).', 'zadrozny 1987a Zadrozny , 1987 b.', 'another theory, consisting of f ~ = { el, sh2, pl, b2 ~ dl } and s, saying that a space vehicle came into the harbor and caused a disease ~ illness is less plausible according to that ordering.', 'as it turns out, f ~ is never constructed in the process of building an interpretation of a paragraph containing the sentence s, unless assuming fint would lead to a contradiction,']","['it is the "" highest "" path, fint is the most plausible ( relative to r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx #AUTHOR_TAG a, 1987b ).', 'zadrozny 1987a Zadrozny , 1987 b.', 'another theory, consisting of f ~ = { el, sh2, pl, b2 ~ dl } and s, saying that a space vehicle came into the harbor and caused a disease ~ illness is less plausible according to that ordering.', 'as it turns out, f ~ is never constructed in the process of building an interpretation of a paragraph containing the sentence s, unless assuming fint would lead to a contradiction,']","['it is the "" highest "" path, fint is the most plausible ( relative to r ) interpretation of the words that appear in the sentence.', 'because it is also consistent, it will be chosen as a best interpretation of s, ( cfxxx #AUTHOR_TAG a, 1987b ).', 'zadrozny 1987a Zadrozny , 1987 b.', 'another theory, consisting of f ~ = { el, sh2, pl, b2 ~ dl } and s, saying that a space vehicle came into the harbor and caused a disease ~ illness is less plausible according to that ordering.', 'as it turns out, f ~ is never constructed in the process of building an interpretation of a paragraph containing the sentence s, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story']",0
"['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, #AUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, #AUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, #AUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, #AUTHOR_TAG, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['#AUTHOR_TAG ).', 'myc']","['#AUTHOR_TAG ).', 'mycielski 1981 )']","['##xxx #AUTHOR_TAG ).', 'myc']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson - Laird (1983), Jackendoff (1983, Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cfxxx #AUTHOR_TAG ).', 'mycielski 1981 )']",0
"['( #AUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['( #AUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( #AUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( #AUTHOR_TAG ; zadrozny 1987a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - - for example, to disambiguate pp attachment.', 'this means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object - level theory']",2
"['necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification / matching, Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification / matching, Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification / matching, Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification / matching, Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
"[""for instance, the discourse representation structures ( drs's ) of Kamp (1981), which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of #AUTHOR_TAG is more sophisticated,']","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of Kamp (1981), which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of #AUTHOR_TAG is more sophisticated,']","[""for instance, the discourse representation structures ( drs's ) of Kamp (1981), which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of #AUTHOR_TAG is more sophisticated,']","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of Kamp (1981), which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of #AUTHOR_TAG is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", 'Jackendoff (1983, p. 14) writes "" it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. ""', 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), #AUTHOR_TAG, Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), #AUTHOR_TAG, Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']","['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), #AUTHOR_TAG, Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), #AUTHOR_TAG, Kamp ( 1981 ), and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
"['.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition #AUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks']","['e. j. crothers.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition #AUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks - - real structures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences.', 'hinds discusses three major types of paragraphs,']","['e. j. crothers.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition #AUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks']","['textualist approach to paragraph analysis is exemplified by e. j. crothers.', 'his work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'he lists, classifies, and discusses various types of inference, by which he means, generally, ` ` the linguistic - logical notions of consequent and presupposition #AUTHOR_TAG : 112 ) have collected convincing evidence of the existence of language chunks - - real structures, not just orthographic conventions - - that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature ( like sentences ).', 'these chunks are sometimes called "" episodes, "" and sometimes "" paragraphs. ""', 'according to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'paragraphs therefore give hierarchical structure to sentences.', 'hinds discusses three major types of paragraphs, and their corresponding segment types.', 'the three types are procedural ( how - to ), expository ( essay ), and narrative ( in this case, spontaneous conversation ).', 'for each type, its segments are distinguished by bearing distinct relationships to the paragraph topic ( which is central, but nowhere clearly defined ).', 'segments themselves are composed of clauses and regulated by "" switching "" patterns, such as the question - answer pattern and the remark - reply pattern']",0
"['#AUTHOR_TAG ).', 'woods 1987 )']","['#AUTHOR_TAG ).', 'woods 1987 )']","['##xxx #AUTHOR_TAG ).', 'woods 1987 )']","['the word "" up "" is given its meaning relative to our experience with gravity, it is not free to "" slip "" into its opposite.', '"" up "" means up and not down....', 'we have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model.', 'mothers have a different role than fathers in this model, and thus there is a reason why "" death is the father of beauty "" fails poetically while "" death is the mother of beauty "" succeeds....', 'it is precisely this "" grounding "" of logical predicates in other conceptual structures that we would like to capture.', 'we investigate here only the "" grounding "" in logical theories.', 'however, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfxxx #AUTHOR_TAG ).', 'woods 1987 )']",0
"['may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", ""#AUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical']","['may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", ""#AUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical']","['may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", ""#AUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']","['assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'its details are not important for our aim of giving a semantic interpretation of paragraphs ; the main theses of our theory do not depend on a logical notation.', 'so we will use a very simple formalism, like the one above, resembling the standard first order language.', ""but, obviously, there are other possibilities - - for instance, the discourse representation structures ( drs's ) of Kamp (1981), which have been used to translate a subset of english into logical formulas, to model text ( identified with a list of sentences ), to analyze a fragment of english, and to deal with anaphora."", 'the logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an english grammar."", ""#AUTHOR_TAG, p. 14 ) writes ` ` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.''"", 'therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'it will also be a model for our simplified logical notation ( cf.', 'section 5 ).', 'we can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences']",1
"['example of psycholinguistically oriented research work can be found in #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used,']","['example of psycholinguistically oriented research work can be found in #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used,']","['example of psycholinguistically oriented research work can be found in #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used,']","['example of psycholinguistically oriented research work can be found in #AUTHOR_TAG.', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980)']",0
"['needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', '#AUTHOR_TAG, p. 112 ),']","['needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', '#AUTHOR_TAG, p. 112 ),']","['needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', '#AUTHOR_TAG, p. 112 ),']","['demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '( other reference works could be treated as additional sources of world knowledge. )', 'this type of consultation uses existing natural language texts as a referential level for processing purposes.', 'it is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG, p. 112 ), for example, bemoans the fact that his ` ` theory lacks a world knowledge component, a mental ` encyclopedia,'which could be invoked to generate inferences...''."", 'with respect to that independent source of knowledge, our main contributions are two.', 'first, we identify its possible structure ( a collection of partially ordered theories ) and make formal the choice of a most plausible interpretation.', 'in other words, we recognize it as a separate logical level - - the referential level.', 'second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level']",0
"['. 1. 1 was the use of a gricean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature "" ( #AUTHOR_TAG ), or the metarules of section 5. 2?', 'it seems to us that the answer is no']","['. 1. 1 was the use of a gricean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature "" ( #AUTHOR_TAG ), or the metarules of section 5. 2?', 'it seems to us that the answer is no']","['. 1. 1 was the use of a gricean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature "" ( #AUTHOR_TAG ), or the metarules of section 5. 2?', 'it seems to us that the answer is no']","['. 1. 1 was the use of a gricean maxim necessary?', 'can one deal effectivelywith the problem of reference without axiomatized gricean maxims, for instance by using only "" petty conversational implicature "" ( #AUTHOR_TAG ), or the metarules of section 5. 2?', 'it seems to us that the answer is no']",0
"['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( #AUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it']","['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( #AUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it']","['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( #AUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it']","['adopt the three - level semantics as a formal tool for the analysis of paragraphs.', 'this semantics was constructed ( #AUTHOR_TAG a, 1987b ) as a formal framework for default and commonsense reasoning.', 'it should not come as a surprise that we can now use this apparatus for text / discourse analysis ; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'for instance, relating "" they "" to "" apples "" in the sentence ( cf.', 'haugeland 1985 p. 195 ; zadrozny 1987a )']",5
"['is spiritually related to the "" abductive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of #AUTHOR_TAG.', 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abductive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of #AUTHOR_TAG.', 'but, obviously, trying to establish precise connections for the metarules or the provability']","['kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abductive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of #AUTHOR_TAG.', 'but, obviously, trying to establish precise connections for the metarules or the provability']","['techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of "" unique - name assumption "" ( genesereth and nilsson 1987 ), "" domain closure assumption "" ( ibid. ),', '"" domain circumscription "" ( cf.', 'etherington and mercer 1987 ), and their kin.', 'similarly, the notion of r + m - abduction is spiritually related to the "" abductive inference "" of Reggia (1985), the "" diagnosis from first principles "" of Reiter (1987), "" explainability "" of Poole (1988), and the subset principle of #AUTHOR_TAG.', 'but, obviously, trying to establish precise connections for the metarules or the provability and the r + m - abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'these connections are being examined elsewhere ( zadrozny forthcoming )']",1
"['##e 1979, #AUTHOR_TAG ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our']","['of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, longacre 1979, #AUTHOR_TAG ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']","['##e 1979, #AUTHOR_TAG ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our']","['there are other discussions of the paragraph as a central element of discourse ( e. g. chafe 1979, halliday and hasan 1976, longacre 1979, #AUTHOR_TAG ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; #AUTHOR_TAG ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; #AUTHOR_TAG ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; #AUTHOR_TAG ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( grosz 1977, 1978 ; #AUTHOR_TAG ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"['.', 'some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['.', 'some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']","['referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG, pp. 7 - 8 ) :... semantics is constrained by our models of ourselves and our worlds.', 'we have models of up and down that are based by the way our bodies actually function']",0
"['#AUTHOR_TAG ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text']","['#AUTHOR_TAG ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text']","['#AUTHOR_TAG ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text']","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( hirst 1987 ; #AUTHOR_TAG ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]",1
['#AUTHOR_TAG ;'],['#AUTHOR_TAG ;'],['##xxx #AUTHOR_TAG ;'],"['in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, ( cfxxx #AUTHOR_TAG ; frisch 1987 ; patel - schneider 1985 ).', 'levesque 1984 ; frisch 1987 ; patel - schneider 1985 ).', ""but we won't pursue this topic further here""]",1
"['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( #AUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( #AUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( #AUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']","['have no doubts that various other metarules will be necessary ; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'they are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model ( that is the set of entities that appear in it ).', 'other factors, such as the role of focus ( #AUTHOR_TAG, 1978 ; sidner 1983 ) or quantifier scoping ( webber 1983 ) must play a role, too.', 'determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself']",0
"[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose ( #AUTHOR_TAG, p. 329 )']","[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose ( #AUTHOR_TAG, p. 329 )']","[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose ( #AUTHOR_TAG, p. 329 )']","[': in our translation from english to logic we are assuming that "" it "" is anaphoric ( with the pronoun following the element that it refers to ), not cataphoric ( the other way around ).', 'this means that the "" it "" that brought the disease in p1 will not be considered to refer to the infection "" i "" or the death "" d "" in p3.', 'this strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in english prose ( #AUTHOR_TAG, p. 329 )']",4
"['of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible, but no attempt is made to define only the so - called grammatical strings of a language']","['can also hope for some fine - tuning of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible, but no attempt is made to define only the so - called grammatical strings of a language']","['can also hope for some fine - tuning of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible, but no attempt is made to define only the so - called grammatical strings of a language']","['can also hope for some fine - tuning of the notion of topic, which would prevent many offensive examples.', 'this approach is taken in computational syntactic grammars ( e. g.', '#AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible, but no attempt is made to define only the so - called grammatical strings of a language']",0
"['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]","['idea of using preferences among theories is new, hence it was described in more detail.', ""` ` coherence,'' as outlined above, can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; charniak 1983 ), with one difference : the activation spreads to theories that share a predicate, not through the is - a hierarchy, and is limited to elementary facts about predicates appearing in the text""]",1
"['##a is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by #AUTHOR_TAG']","['do not claim that gla is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by #AUTHOR_TAG']","['##a is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by #AUTHOR_TAG']","['do not claim that gla is the best or unique way of expressing the rule "" assume that the writer did not say too much. ""', 'rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'we shall see this in the next example : two sentences, regarded as a fragment of paragraph, are a variation on a theme by #AUTHOR_TAG']",2
"['that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by #AUTHOR_TAG ; cf']","['are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by #AUTHOR_TAG ; cfxxx']","['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by #AUTHOR_TAG ; cf']","['##s are function words - - like conjunctions and some adverbs - - that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '"" and, "" "" or, "" and "" but "" are the three main coordinating connectives in english.', 'however, "" but "" does not behave quite like the other two - - semantically, "" but "" signals a contradiction, and in this role it seems to have three subfunctions :..', 'opposition ( called "" adversative "" or "" contrary - to - expectation "" by #AUTHOR_TAG ; cfxxx also quirk et al. 1972, p. 672 )']",0
"['', '#AUTHOR_TAG, halliday and']","['', '#AUTHOR_TAG, halliday and hasan 1976, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']","['', '#AUTHOR_TAG, halliday and']","['there are other discussions of the paragraph as a central element of discourse ( e. g.', '#AUTHOR_TAG, halliday and hasan 1976, longacre 1979, haberlandt et al. 1980 ), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'our interest, however, lies precisely in that area']",1
"['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), Jackendoff ( 1983 ), #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), Jackendoff ( 1983 ), #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']","['describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), Jackendoff ( 1983 ), #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'myc']","['logical notions that we are going to consider, such as theory or model, will be finitary.', 'for example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'therefore these notions, and all other constructs we are going to define ( axioms, metarules, definitions etc. ) are computational, although usually we will not provide explicit algorithms for computing them.', 'the issues of control are not so important for us at this point ; we restrict ourselves to describing the logic.', 'this principle of finitism is also assumed by johnson - Laird ( 1983 ), Jackendoff ( 1983 ), #AUTHOR_TAG, and implicitly or explicitly by almost all researchers in computational linguistics.', 'as a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis ( cf.', 'mycielski 1981 )']",1
"['( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 )']","['example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 )']","['##es found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 )']","['example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'these authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'bond and hayes found three major formal devices that are used, by readers, to identify a paragraph : ( 1 ) the repetition of content words ( nouns, verbs, adjectives, adverbs ) ; ( 2 ) pronoun reference ; and ( 3 ) paragraph length, as determined by spatial and / or sentence - count information.', 'other psycholing - uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 )']",0
"['have shown elsewhere ( jensen and binot 1988 ; #AUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( jensen and binot 1988 ; #AUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( jensen and binot 1988 ; #AUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - -']","['have shown elsewhere ( jensen and binot 1988 ; #AUTHOR_TAG a, 1987b ) that natural language programs, such as on - line grammars and dictionaries, can be used as referential levels for commonsense reasoning - - for example, to disambiguate pp attachment.', 'this means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object - level theory']",2
"['a category of models corresponding to paragraphs - - as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx #AUTHOR_TAG b ).', 'za']","['an interpretation of a paragraph does not mean finding all of its possible meanings ; the implausible ones should not be computed at all.', 'this viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'we can then later ( section 5. 2 ) define p - models - - a category of models corresponding to paragraphs - - as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx #AUTHOR_TAG b ).', '']","['a category of models corresponding to paragraphs - - as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx #AUTHOR_TAG b ).', 'za']","['an interpretation of a paragraph does not mean finding all of its possible meanings ; the implausible ones should not be computed at all.', 'this viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'we can then later ( section 5. 2 ) define p - models - - a category of models corresponding to paragraphs - - as models of coherent theories that satisfy all metalevel conditions.', 'the partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'this immediate information may be insufficient to decide the truth of certain predicates.', 'it would seem therefore that the iteration of the pt operation to form a closure is needed ( cfxxx #AUTHOR_TAG b ).', 'zadrozny 1987b )']",1
"['##bs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later, #AUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]","['selectional restrictions ( semantic feature information, hobbs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later, #AUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]","['selectional restrictions ( semantic feature information, hobbs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later, #AUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]","['selectional restrictions ( semantic feature information, hobbs 1977 ) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""later, #AUTHOR_TAG, 1982 ) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ` ` salience'' in choosing facts from this knowledge base""]",0
"['necessity of this kind of merging of arguments has been recognized before : charniak and mc Dermott ( 1985 ) call it abductive unification / matching, #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : charniak and mc Dermott ( 1985 ) call it abductive unification / matching, #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : charniak and mc Dermott ( 1985 ) call it abductive unification / matching, #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']","['necessity of this kind of merging of arguments has been recognized before : charniak and mc Dermott ( 1985 ) call it abductive unification / matching, #AUTHOR_TAG, 1979 ) refers to such operations using the terms knitting or petty conversational implicature']",0
"['to logic.', 'the text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g.', 'moens and steedman 1987 ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems""]","['to logic.', 'the text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g.', 'moens and steedman 1987 ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems""]","['to logic.', 'the text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g.', 'moens and steedman 1987 ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', ""extending and revising Jackendoff's (1983) formalism seems""]","['. 1. 1', 'translation to logic.', 'the text concerns events happening in time.', 'naturally, we will use a logical notation in which formulas may have temporal and event components.', 'we assume that any formal interpretation of time will agree with the intuitive one.', 'so it is not necessary now to present a formal semantics here.', 'the reader may consult recent papers on this subject ( e. g.', 'moens and steedman 1987 ; webber 1987 ) to see what a formal interpretation of events in time might look like.', 'since sentences can refer to events described by other sentences, we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator.', 'extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint ( "" that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon "" - - - ibid. ).', 'however, as noted before, we will use a simplified version of such a logical notation ; we will have only time, event, result, and property as primitives.', 'after these remarks we can begin constructing the model of the example paragraph.', 'we assume that constants are introduced by nps.', 'we have then ( i ) constants s, m, d, i, b, 1347 satisfying : ship ( s ), messina ( m ), disease ( d ), infection ( i ), death ( b ), year ( 1347 )']",0
"[""instance, #AUTHOR_TAG, p. 8 ) says that the sentence ` ` reagan thinks bananas,'' which is otherwise strange, is in fact acceptable if it occurs as an answer to the question ` ` what is kissinger's favorite fruit?''"", 'the pairing of these two sentences may be said to create a small paragraph.', 'our point is that an acceptable structure can be assigned to the utterance "" reagan thinks bananas "" only within the paragraph in which this utterance occurs.', 'we believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two ( or one ) neighboring sentences, will be sufficient for this task.', '']","[""instance, #AUTHOR_TAG, p. 8 ) says that the sentence ` ` reagan thinks bananas,'' which is otherwise strange, is in fact acceptable if it occurs as an answer to the question ` ` what is kissinger's favorite fruit?''"", 'the pairing of these two sentences may be said to create a small paragraph.', 'our point is that an acceptable structure can be assigned to the utterance "" reagan thinks bananas "" only within the paragraph in which this utterance occurs.', 'we believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two ( or one ) neighboring sentences, will be sufficient for this task.', '']","[""instance, #AUTHOR_TAG, p. 8 ) says that the sentence ` ` reagan thinks bananas,'' which is otherwise strange, is in fact acceptable if it occurs as an answer to the question ` ` what is kissinger's favorite fruit?''"", 'the pairing of these two sentences may be said to create a small paragraph.', 'our point is that an acceptable structure can be assigned to the utterance "" reagan thinks bananas "" only within the paragraph in which this utterance occurs.', 'we believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two ( or one ) neighboring sentences, will be sufficient for this task.', '']","['paragraph can be thought of as a grammatical unit in the following sense : it is the discourse unit in which a functional ( or a predicate - argument ) structure can be definitely assigned to sentences / strings.', ""for instance, #AUTHOR_TAG, p. 8 ) says that the sentence ` ` reagan thinks bananas,'' which is otherwise strange, is in fact acceptable if it occurs as an answer to the question ` ` what is kissinger's favorite fruit?''"", 'the pairing of these two sentences may be said to create a small paragraph.', 'our point is that an acceptable structure can be assigned to the utterance "" reagan thinks bananas "" only within the paragraph in which this utterance occurs.', 'we believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two ( or one ) neighboring sentences, will be sufficient for this task.', ""that is, we can ask in the first sentence of a paragraph about kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end."", 'we do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism ( although it may be ) ; rather, it has the grammatical role of providing functional structures that can be assigned to strings']",4
"[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), #AUTHOR_TAG, and Young ( 1989 )']","['example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), #AUTHOR_TAG, and Young ( 1989 )']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), #AUTHOR_TAG, and Young ( 1989 )']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), #AUTHOR_TAG, and Young ( 1989 )']",0
['formula for the test set perplexity ( #AUTHOR_TAG ) is :'],['formula for the test set perplexity ( #AUTHOR_TAG ) is :'],['formula for the test set perplexity ( #AUTHOR_TAG ) is :'],['formula for the test set perplexity ( #AUTHOR_TAG ) is :'],0
"['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( #AUTHOR_TAG ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( #AUTHOR_TAG ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( #AUTHOR_TAG ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( #AUTHOR_TAG ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
['the computer ( #AUTHOR_TAG )'],['the computer ( #AUTHOR_TAG )'],['the computer ( #AUTHOR_TAG )'],"['obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information.', 'their speech was recorded in a simulation mode in which the speech recognition component was excluded.', 'instead, an experimenter in a separate room typed in the utterances as spoken by the subject.', 'subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG )']",5
[''],[''],[''],[''],0
"['make the final decision on whether to accept the proposals.', 'this approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['make the final decision on whether to accept the proposals.', 'this approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","[', one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass']","['far, we have added all semantic filters by hand, and they are implemented in a hard - fail mode, i. e., if the semantic restrictions fail, the node dies.', 'this strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'in principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions.', 'the semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'there is obviously a great deal more work to be done in this important area']",1
"['systems, voyager and atis, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'in both of these systems, tina provides the interface between the recognizer and the application back - end.', 'in this section, i will describe our current interfaces between tina and the recognizer and our future plans in this area.', 'in addition, i will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( zue et al. 1989 ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( #AUTHOR_TAG ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['systems, voyager and atis, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'in both of these systems, tina provides the interface between the recognizer and the application back - end.', 'in this section, i will describe our current interfaces between tina and the recognizer and our future plans in this area.', 'in addition, i will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( zue et al. 1989 ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( #AUTHOR_TAG ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['present, we have available at mit two systems, voyager and atis, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'in both of these systems, tina provides the interface between the recognizer and the application back - end.', 'in this section, i will describe our current interfaces between tina and the recognizer and our future plans in this area.', 'in addition, i will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( zue et al. 1989 ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( #AUTHOR_TAG ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['present, we have available at mit two systems, voyager and atis, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'in both of these systems, tina provides the interface between the recognizer and the application back - end.', 'in this section, i will describe our current interfaces between tina and the recognizer and our future plans in this area.', 'in addition, i will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( zue et al. 1989 ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( #AUTHOR_TAG ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']",5
"['multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements.', 'for instance, the set of complements [ from - place']","['multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements.', 'for instance, the set of complements [ from - place ], [ to - place ], and [ at - time ] are freely ordered following a movement verb such as "" leave. ""', 'thus a flight can "" leave for chicago from boston at nine, "" or, equivalently, "" leave at nine for chicago from boston. ""', 'if these complements are each allowed to follow the other, then in tina an infinite sequence of [ from - place ] s, [ to - place ] s and [ at - time ] s is possible.', 'this is of course unacceptable, but it is straightforward to have each node,']","['multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements.', 'for instance, the set of complements [ from - place']","['filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements.', 'for instance, the set of complements [ from - place ], [ to - place ], and [ at - time ] are freely ordered following a movement verb such as "" leave. ""', 'thus a flight can "" leave for chicago from boston at nine, "" or, equivalently, "" leave at nine for chicago from boston. ""', 'if these complements are each allowed to follow the other, then in tina an infinite sequence of [ from - place ] s, [ to - place ] s and [ at - time ] s is possible.', 'this is of course unacceptable, but it is straightforward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in turn, fail if that bit has already been set.', 'we have found that this strategy, in conjunction with the capability of erasing all semantic bits whenever a new clause is entered ( through the meta level "" detach "" operation mentioned previously ) serves the desired goal of eliminating the unwanted redundancies']",5
"[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in #AUTHOR_TAG, De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and Young ( 1989 )']","['example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in #AUTHOR_TAG, De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and Young ( 1989 )']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in #AUTHOR_TAG, De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and Young ( 1989 )']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in #AUTHOR_TAG, De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and Young ( 1989 )']",0
"['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( zue et al. 1991 ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( #AUTHOR_TAG )."", 'ultimately we want to']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( zue et al. 1991 ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( #AUTHOR_TAG )."", 'ultimately we want to']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( zue et al. 1991 ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( #AUTHOR_TAG )."", 'ultimately we want to']","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( zue et al. 1991 ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina's probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( #AUTHOR_TAG )."", ""ultimately we want to incorporate tina's probabilities directly into the a * search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model""]",5
"['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( #AUTHOR_TAG ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( #AUTHOR_TAG ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( #AUTHOR_TAG ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( #AUTHOR_TAG ).', 'the second version ( rm ) concerns the resource management task ( pallett 1989 ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
"['how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'however, the method we are currently using in the atis domain ( #AUTHOR_TAG ) represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices such as clause ( for a sentence - level concept, such as request ), predicate ( for a functional operation ), reference ( essentially proper noun ), and qset ( for a set of objects']","['how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'however, the method we are currently using in the atis domain ( #AUTHOR_TAG ) represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices such as clause ( for a sentence - level concept, such as request ), predicate ( for a functional operation ), reference ( essentially proper noun ), and qset ( for a set of objects ).', 'the process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'each node receives a frame in both a top - down and a bottom - up cycle, and modifies the frame according to specifications based on its broad - class identity ( as one of noun, noun - phrase, predicate, quantifier, etc. ).', 'for example, a [ subject ] is a noun - phrase node with the label "" topic. ""', 'during the top - down cycle, it creates a blank frame and inserts it into a "" topic "" slot in the frame that was handed to it.', '']","['how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'however, the method we are currently using in the atis domain ( #AUTHOR_TAG ) represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices such as clause ( for a sentence - level concept, such as request ), predicate ( for a functional operation ), reference ( essentially proper noun ), and qset ( for a set of objects']","['how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'however, the method we are currently using in the atis domain ( #AUTHOR_TAG ) represents our most promising approach to this problem.', 'we have decided to limit semantic frame types to a small set of choices such as clause ( for a sentence - level concept, such as request ), predicate ( for a functional operation ), reference ( essentially proper noun ), and qset ( for a set of objects ).', 'the process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'each node receives a frame in both a top - down and a bottom - up cycle, and modifies the frame according to specifications based on its broad - class identity ( as one of noun, noun - phrase, predicate, quantifier, etc. ).', 'for example, a [ subject ] is a noun - phrase node with the label "" topic. ""', 'during the top - down cycle, it creates a blank frame and inserts it into a "" topic "" slot in the frame that was handed to it.', 'it passes the blank frame to its children, who will then fill it appropriately, labeling it as a qset or as a reference.', 'it then passes along to the right sibling the same frame that was handed to it from above, with the completed topic slot filled with the information delivered by the children']",3
"['a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( #AUTHOR_TAG et al. 1991 ), is a system for accessing']","['a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( #AUTHOR_TAG et al. 1991 ), is a system for accessing']","['a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( #AUTHOR_TAG et al. 1991 ), is a system for accessing data']","['currently have two application domains that can carry on a spoken dialog with a user.', 'one, the voyager domain ( zue et al. 1990 ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( #AUTHOR_TAG et al. 1991 ), is a system for accessing data in the official 80 stephanie seneff tina : a natural language system for spoken language applications airline guide and booking flights.', 'work continues on improving all aspects of these domains. our current research is directed at a number of different remaining issues']",5
"['make the final decision on whether to accept the proposals.', 'this approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions.', 'the semantic conditions that pass']","['make the final decision on whether to accept the proposals.', 'this approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions.', 'the semantic conditions that pass']","[', one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions.', 'the semantic conditions that pass']","['far, we have added all semantic filters by hand, and they are implemented in a hard - fail mode, i. e., if the semantic restrictions fail, the node dies.', 'this strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'in principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'this approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions.', 'the semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'there is obviously a great deal more work to be done in this important area']",1
"['out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model ( #AUTHOR_TAG )']","['appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in "" three hundred and sixteen. ""', 'included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'this is a problem to be aware of in building grammars from example sentences.', 'in the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model ( #AUTHOR_TAG )']","['appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in "" three hundred and sixteen. ""', 'included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'this is a problem to be aware of in building grammars from example sentences.', 'in the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model ( #AUTHOR_TAG )']","['appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in "" three hundred and sixteen. ""', 'included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'this is a problem to be aware of in building grammars from example sentences.', 'in the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an n - gram back - off model ( #AUTHOR_TAG )']",0
"['node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded ( #AUTHOR_TAG ) by']","['to activators at higher levels of the parse tree.', 'that is to say, the current - focus is a feature, like verb - mode, that is blocked when an [ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded ( #AUTHOR_TAG ) by']","['to activators at higher levels of the parse tree.', 'that is to say, the current - focus is a feature, like verb - mode, that is blocked when an [ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded ( #AUTHOR_TAG ) by']","['. 5. 1 gaps.', 'the mechanism to deal with gaps resembles in certain respects the hold register idea of atns, but with an important difference, reflecting the design philoso - phy that no node can have access to information outside of its immediate domain.', 'the mechanism involves two slots that are available in the feature vector of each parse node.', 'these are called the current - focus and the float - object, respectively.', 'the current - focus slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'if the float - object slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the float - object.', 'the process of getting into the float - object slot ( which is analogous to the hold register ) requires two steps, executed independently by two different nodes.', 'the first node, the generator, fills the current - focus slot with the subparse returned to it by its children.', 'the second node, the activator, moves the current - focus into the float - object position, for its children, during the top - down cycle.', 'it also requires that the float - object be absorbed somewhere among its descendants by a designated absorber node, a condition that is checked during the bottom - up cycle.', 'the current - focus only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.', 'that is to say, the current - focus is a feature, like verb - mode, that is blocked when an [ end ] node is encountered.', 'to a first approximation, a current - focus reaches only nodes that are c - commanded ( #AUTHOR_TAG ) by its generator.', 'finally, certain blocker nodes block the transfer of the float - object to their children']",0
"['a user.', 'one, the voyager domain ( #AUTHOR_TAG ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( seneff et al. 1991 ), is a system for accessing data in the official airline guide and book']","['a user.', 'one, the voyager domain ( #AUTHOR_TAG ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( seneff et al. 1991 ), is a system for accessing data in the official airline guide and booking flights.', 'work continues on improving all aspects of these domains']","['a user.', 'one, the voyager domain ( #AUTHOR_TAG ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( seneff et al. 1991 ), is a system for accessing data in the official airline guide and booking flights.', 'work continues on improving all aspects of these domains']","['_ currently have two application domains that can carry on a spoken dialog with a user.', 'one, the voyager domain ( #AUTHOR_TAG ), answers questions about places of interest in an urban area, in our case, the vicinity of mit and harvard university.', 'the second one, atis ( seneff et al. 1991 ), is a system for accessing data in the official airline guide and booking flights.', 'work continues on improving all aspects of these domains']",5
"[""example used to illustrate the power of atns ( #AUTHOR_TAG ), ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", 'cases of crossed traces are automatically blocked because the second current - focus gets moved into the float - object position at the time of the second activator, overriding the preexisting float - object set up by the earlier activator.', 'the wrong float - object is available at the position of the first trace, and the parse dies : * ( which books ) / did you ask john ( where ) j bill bought ( ti ) ( tj )?', 'the current - focus slot is not']","[""example used to illustrate the power of atns ( #AUTHOR_TAG ), ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", 'cases of crossed traces are automatically blocked because the second current - focus gets moved into the float - object position at the time of the second activator, overriding the preexisting float - object set up by the earlier activator.', 'the wrong float - object is available at the position of the first trace, and the parse dies : * ( which books ) / did you ask john ( where ) j bill bought ( ti ) ( tj )?', 'the current - focus slot is not']","[""example used to illustrate the power of atns ( #AUTHOR_TAG ), ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", 'cases of crossed traces are automatically blocked because the second current - focus gets moved into the float - object position at the time of the second activator, overriding the preexisting float - object set up by the earlier activator.', 'the wrong float - object is available at the position of the first trace, and the parse dies : * ( which books ) / did you ask john ( where ) j bill bought ( ti ) ( tj )?', 'the current - focus slot is not']","[""example used to illustrate the power of atns ( #AUTHOR_TAG ), ` ` john was believed to have been shot,'' also parses correctly, because the [ object ] node following the verb ` ` believed'' acts as both an absorber and a ( re ) generator."", 'cases of crossed traces are automatically blocked because the second current - focus gets moved into the float - object position at the time of the second activator, overriding the preexisting float - object set up by the earlier activator.', 'the wrong float - object is available at the position of the first trace, and the parse dies : * ( which books ) / did you ask john ( where ) j bill bought ( ti ) ( tj )?', 'the current - focus slot is not restricted to nodes that represent nouns.', 'some of the generators are adverbial or adjectival parts of speech ( pos ).', 'an absorber checks for agreement in pos before it can accept the float - object as its subparse.', 'as an example, the question, "" ( how oily ) / do you like your salad dressing ( ti )? "" contains a [ q - subject ] "" how oily "" that is an adjective.', 'the absorber [ pred - adjective ] accepts the available float - object as its subparse, but only after confirming that pos is adjective']",1
"[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), #AUTHOR_TAG, Niemann ( 1990 ), and Young ( 1989 )']","['example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), #AUTHOR_TAG, Niemann ( 1990 ), and Young ( 1989 )']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), #AUTHOR_TAG, Niemann ( 1990 ), and Young ( 1989 )']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), #AUTHOR_TAG, Niemann ( 1990 ), and Young ( 1989 )']",0
"['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( #AUTHOR_TAG ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( #AUTHOR_TAG ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( #AUTHOR_TAG ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']","['date, four distinct domain - specific versions of tina have been implemented.', 'the first version ( timit ) was developed for the 450 phonetically rich sentences of the timit database ( lamel et al. 1986 ).', 'the second version ( rm ) concerns the resource management task ( #AUTHOR_TAG ) that has been popular within the darpa community in recent years.', 'the third version ( voyager ) serves as an interface both with a recognizer and with a functioning database back - end ( zue et al. 1990 ).', 'the voyager system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'a fourth domain - specific version is under development for the atis ( air travel information system ) task, which has recently been designated as the new common task for the darpa community']",5
"['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm ( #AUTHOR_TAG ) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']","['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm ( #AUTHOR_TAG ) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']","['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm ( #AUTHOR_TAG ) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']","['modification of this scheme is necessary when the input stream is not deterministic.', 'for the a * algorithm ( #AUTHOR_TAG ) as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion.', 'unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'with a deterministic word sequence it seems reasonable to assume probability 1. 0 for what has been found']",5
"['access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( #AUTHOR_TAG ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( viterbi 1967 ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( #AUTHOR_TAG ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( viterbi 1967 ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( #AUTHOR_TAG ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( viterbi 1967 ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']","['present, we have available at mit two systems, voyager and atis, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'in both of these systems, tina provides the interface between the recognizer and the application back - end.', 'in this section, i will describe our current interfaces between tina and the recognizer and our future plans in this area.', 'in addition, i will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'this aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'the recognizer for these systems is the summit system ( #AUTHOR_TAG ), which uses a segmental - based framework and includes an auditory model in the front - end processing.', 'the lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'the search algorithm is the standard viterbi search ( viterbi 1967 ), except that the match involves a network - to - network alignment problem rather than sequence - to - sequence']",5
"['for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bre']","['section describes how tina handles several issues that are often considered to be part of the task of a parser.', 'these include agreement constraints, semantic restrictions, subject - tagging for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bresnan 1982, p. 235 ff']","['section describes how tina handles several issues that are often considered to be part of the task of a parser.', 'these include agreement constraints, semantic restrictions, subject - tagging for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bre']","['section describes how tina handles several issues that are often considered to be part of the task of a parser.', 'these include agreement constraints, semantic restrictions, subject - tagging for verbs, and long distance movement ( often referred to as gaps, or the trace, as in "" ( which article ) / do you think i should read ( ti )? "" ) ( chomsky 1977 ).', 'the gap mechanism resembles the hold register idea of atns ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( lfgs ) ( bresnan 1982, p. 235 ff. ), but it is different from these in that the process of filling the hold register equivalent involves two steps separately initiated by two independent nodes']",1
"[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and #AUTHOR_TAG']","['example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and #AUTHOR_TAG']","[', in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and #AUTHOR_TAG']","['the past few years, there has been a gradual paradigm shift in speech recognition research both in the u. s. and in europe.', 'in addition to continued research on the transcription problem, i. e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 this shift is at least partly brought on by the realization that many of the applications involving human / machine interface using speech require an "" understanding "" of the intended message.', 'in fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the darpa speech and natural language workshops, as well as in publications from participants of the esprit sundial project.', 'representative systems are described in Boisen et al. ( 1989 ), De Mattia and Giachin ( 1989 ), Niedermair ( 1989 ), Niemann ( 1990 ), and #AUTHOR_TAG']",0
"['we first integrated this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 196""]","['we first integrated this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 1968, jelinek 1976 )."", 'both the a * and the viterbi search are left - to - right search algorithms.', 'however, the a * search is contrasted with the viterbi search in that the set of active hypotheses take up unequal segments of time.', 'that is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold']","['we first integrated this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 1968, jelinek 1976 )."", 'both the a * and the viterbi search are left - to - right search algorithms.', 'however, the a * search is contrasted with the viterbi search in that the set of active hypotheses take up unequal segments of time.', 'that is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold']","['we first integrated this recognizer with tina, we used a "" wire "" connection, in that the recognizer produced a single best output, which was then passed to tina for parsing.', 'a simple word - pair grammar constrained the search space.', 'if the parse failed, then the sentence was rejected.', ""we have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) to produce these ` ` n - best'' alternatives, we make use of a standard a * search algorithm ( hart 1968, jelinek 1976 )."", 'both the a * and the viterbi search are left - to - right search algorithms.', 'however, the a * search is contrasted with the viterbi search in that the set of active hypotheses take up unequal segments of time.', 'that is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold']",5
"['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance (""]","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( goodine et al. 1991 )."", ""ultimately we want to incorporate tina'sprobabilities directly into the a * search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model""]","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance (""]","['in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'following the viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""we have not yet made use of tina'sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort n - best outputs, giving a significant improvement in performance ( goodine et al. 1991 )."", ""ultimately we want to incorporate tina'sprobabilities directly into the a * search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model""]",5
"['explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', 'this description can then be given the standard set - theoretical interpretation of King (1989, 1994 )']","['explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', 'this description can then be given the standard set - theoretical interpretation of King (1989, 1994 )']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', 'this description can then be given the standard set - theoretical interpretation of King (1989, 1994 )']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', 'this description can then be given the standard set - theoretical interpretation of King (1989, 1994 )']",0
"['lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'the reader is referred to #AUTHOR_TAG']","['lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'the reader is referred to #AUTHOR_TAG']","['to factor out c in ( c a d1 ) v -.. v ( c a dk ) : we compute c a ( d1 v... v dk ), where the r ) are assumed to contain no further common factors.', 'once we have computed c, we use it to make the extended lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'the reader is referred to #AUTHOR_TAG']","['most specific generalization does not necessarily provide additional constrain - ing information.', 'however, usually it is the case that lexical entries resulting from lexical rule application differ in very few specifications compared to the number of specifica - tions in a base lexical entry.', 'most of the specifications of a lexical entry are assumed to be passed unchanged via the automatically generated frame specification.', 'therefore, after lifting the common information into the extended lexical entry, the out - argument in many cases contains enough information to permit a postponed execution of the interaction predicate.', 'when c is the common information, and d1,..., dk are the definitions of the interaction predicate called, we use distributivity to factor out c in ( c a d1 ) v -.. v ( c a dk ) : we compute c a ( d1 v... v dk ), where the r ) are assumed to contain no further common factors.', 'once we have computed c, we use it to make the extended lexical entry more specific.', 'this technique closely resembles the off - line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'the reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation']",0
"['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to']","['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to']","['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dbrre and eisele 1991 ; d6rre and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['way these predicates interconnect is represented in figure 19.', '27 #AUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction']","['way these predicates interconnect is represented in figure 19.', '27 #AUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction']","['way these predicates interconnect is represented in figure 19.', '27 #AUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', '']","['way these predicates interconnect is represented in figure 19.', '27 #AUTHOR_TAG argue that semi - productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry.', '28 in order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', 'since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given']",0
"['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement extraction lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['example, adopted in the lkb system ( #AUTHOR_TAG ).', 'both the input']","['example, adopted in the lkb system ( #AUTHOR_TAG ).', 'both the input']","['example, adopted in the lkb system ( #AUTHOR_TAG ).', 'both the input']","['disjunction thus constitutes the base lexicon.', 'the disjuncts in the constraint on derived - word, on the other hand, encode the lexical rules.', 'the in - specification of a lexical rule specifies the in feature, the out - specification, the derived word itself.', 'note that the value of the in feature is of type word and thus also has to satisfy either a base lexical entry or an out - specification of a lexical rule.', 'while this introduces the recursion necessary to permit successive lexical rule application, it also grounds the recursion in a word described by a base lexical entry.', 'contrary to the mlr setup, the dlr formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'this conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as, for example, adopted in the lkb system ( #AUTHOR_TAG ).', 'both the input and output of a lexical rule, i. e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.', 'as a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.', 'the computational treatment of lexical rules that we propose in this paper is essentially a domain - specific refinement of such an approach to lexical rules.', '']",0
"['can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ).', 'given a lexical']","['can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ).', 'given a lexical']","[', unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of 29 this improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ).', 'given a lexical entry as in figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.', 'to eliminate the frame predicates completely, we can successively unfold the frame predicates']","['automata resulting from word class specialization group the lexical entries into natural classes.', 'in case the automata corresponding to two lexical entries are identical, the entries belong to the same natural class.', 'however, each lexical rule application, i. e., each transition in an automaton, calls a frame predicate that can have a large number of defining clauses.', 'intuitively understood, each defining clause of a frame predicate corresponds to a subclass of the class of lexical entries to which a lexical rule can be applied.', 'during word class specialization, though, when the finite - state automaton representing global lexical rule application is pruned with respect to a particular base lexical entry, we know which subclass we are dealing with.', 'for each interaction definition we can therefore check which of the flame clauses are applicable and discard the non - applicable ones.', 'we thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'the elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of 29 this improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ).', 'given a lexical entry as in figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.', 'to eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.', '3 the successive unfolding steps are schematically represented in figure 20']",1
"['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( maxwell and kaplan 1989 ; eisele and dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation.', '']","['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( maxwell and kaplan 1989 ; eisele and dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation.', '']","['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( maxwell and kaplan 1989 ; eisele and dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']","['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( maxwell and kaplan 1989 ; eisele and dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']",0
"['is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in']","['is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG.', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 Hinrichs and Nakazawa (1996) show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']",0
"[', meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an']","['meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an']","[', meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by #AUTHOR_TAG, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
"['.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ; #AUTHOR_TAG ).', '5 an in - depth discussion including a comparison of both approaches is provided in calc']","['attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ; #AUTHOR_TAG ).', '5 an in - depth discussion including a comparison of both approaches is provided in']","['.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ; #AUTHOR_TAG ).', '5 an in - depth discussion including a comparison of both approaches is provided in calc']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and king 1994 ; #AUTHOR_TAG ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by Hinrichs and Nakazawa (1994, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
"['), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG )']","['properties can be considered an instance of the well - known frame problem in ai ( mccarthy and hayes 1969 ), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG )']","['[ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in ai ( mccarthy and hayes 1969 ), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG )']","['the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'this is so since the lexical rule in figure 2 "" ( like all lexical rules in hpsg ) preserves all properties of the input not mentioned in the rule. ""', '( pollard and sag [ 1994, 314 ], following flickinger [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in ai ( mccarthy and hayes 1969 ), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG )']",0
"['made in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in #AUTHOR_TAG this is a well - motivated convention since it']","['made in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in #AUTHOR_TAG this is a well - motivated convention since it']","['computational treatment expanding out the lexicon cannot be used for the increasing number of hpsg analyses that propose lexical rules that would result in an infinite lexicon.', 'most current hpsg analyses of dutch, german, italian, and french fall into that category.', '1 furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run - time.', 'finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'this conflicts with the standard assumption made in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in #AUTHOR_TAG this is a well - motivated convention since']","['computational treatment expanding out the lexicon cannot be used for the increasing number of hpsg analyses that propose lexical rules that would result in an infinite lexicon.', 'most current hpsg analyses of dutch, german, italian, and french fall into that category.', '1 furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run - time.', 'finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'this conflicts with the standard assumption made in hpsg that only the properties changed by a lexical rule need be mentioned.', 'as shown in #AUTHOR_TAG this is a well - motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries']",4
"['nondeterminism is based on unfold / fold transformation techniques ( #AUTHOR_TAG ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding']","['nondeterminism is based on unfold / fold transformation techniques ( #AUTHOR_TAG ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( #AUTHOR_TAG ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( #AUTHOR_TAG ).', '29 the unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']",5
"['oliva 1994 ; frank 1994 ; #AUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; #AUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; #AUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; #AUTHOR_TAG ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and']","['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['#AUTHOR_TAG ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['#AUTHOR_TAG ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['#AUTHOR_TAG ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; #AUTHOR_TAG ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; #AUTHOR_TAG ) and']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; #AUTHOR_TAG ) and']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; #AUTHOR_TAG ) and']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; #AUTHOR_TAG ) and the.', 'lexical rules ( dlrs ; meurers 1995 ).', '']",0
"['martini for the controll system ( gerdemann and #AUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar implementing an hpsg analysis covering the so - called aux - flip phenomenon, and partial - vp topicalization in the three clause types of german ( hin']","['the authors in cooperation with dieter martini for the controll system ( gerdemann and #AUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar implementing an hpsg analysis covering the so - called aux - flip phenomenon, and partial - vp topicalization in the three clause types of german ( hinrichs, meurers, and nakazawa 1994 ).', 'this test grammar includes eight lexical rules ; some serve syntactic purposes, like the partial - vp topicalization lexical rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'our compiler distinguished seven word classes.', 'some nouns and most verbal lexical']","['the authors in cooperation with dieter martini for the controll system ( gerdemann and #AUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar implementing an hpsg analysis covering the so - called aux - flip phenomenon, and partial - vp topicalization in the three clause types of german ( hin']","['computational treatment of lexical rules as covariation in lexical entries was implemented in prolog by the authors in cooperation with dieter martini for the controll system ( gerdemann and #AUTHOR_TAG ; gotz and meurers 1997a ).', 'we tested the covariation approach with a complex grammar implementing an hpsg analysis covering the so - called aux - flip phenomenon, and partial - vp topicalization in the three clause types of german ( hinrichs, meurers, and nakazawa 1994 ).', 'this test grammar includes eight lexical rules ; some serve syntactic purposes, like the partial - vp topicalization lexical rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'our compiler distinguished seven word classes.', 'some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations']",5
"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; #AUTHOR_TAG ; calcagno and pollard 1995 ) and the']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; #AUTHOR_TAG ; calcagno and pollard 1995 ) and the']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; #AUTHOR_TAG ; calcagno and pollard 1995 ) and the.', 'lexical rules (']","['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; #AUTHOR_TAG ; calcagno and pollard 1995 ) and the.', 'lexical rules ( dlrs ; meurers 1995 ).', '']",0
"['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['example, in the ale system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical']","['example, in the ale system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical']","['example, in the ale system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile - time.', 'while this provides a front - end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'we mentioned in section 2. 2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'a related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'in the ale system,']","['common computational treatment of lexical rules adopted, for example, in the ale system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile - time.', 'while this provides a front - end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'we mentioned in section 2. 2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'a related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'in the ale system, for example, a depth bound can be specified for this purpose.', 'finally, as shown in section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding']",1
"['logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']","['logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']","['logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']","['logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG, 1994 ).', 'the formal language of king allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the ( token ) identity of objects.', 'these atomic expressions can be combined using conjunction, disjunction, and negation.', 'the expressions are interpreted by a set - theoretical semantics']",0
"['nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by #AUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']","['nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by #AUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by #AUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']","['elimination of redundant nondeterminism is based on unfold / fold transformation techniques ( tamaki and sato 1984 ). 29', 'the unfolding transformation is also referred to as partial execution, for example, by #AUTHOR_TAG.', 'intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile - time.', 'as a result, the literal can be removed from the body of the clause.', 'whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation']",0
['##ele 1991 ; done and dorna 1993b ) or the tfs system ( #AUTHOR_TAG ;'],"['example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( #AUTHOR_TAG ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",['##ele 1991 ; done and dorna 1993b ) or the tfs system ( #AUTHOR_TAG ;'],"['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dorre and eisele 1991 ; done and dorna 1993b ) or the tfs system ( #AUTHOR_TAG ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['explicit description of the kind shown in figure 1.', ""this description can then be given the standard set - theoretical interpretation of #AUTHOR_TAG, 1994 ). '"", '11 10']","['explicit description of the kind shown in figure 1.', ""this description can then be given the standard set - theoretical interpretation of #AUTHOR_TAG, 1994 ). '"", '11 10']","['into the theory.', 'the formalization of dlrs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', ""this description can then be given the standard set - theoretical interpretation of #AUTHOR_TAG, 1994 ). '"", '11 10 note that the passivization lexical rule in figure 2 is only intended to illustrate the mechanism.', 'we do not make the linguistic claim that']","['thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'the formalization of dlrs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps : a rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in figure 1.', ""this description can then be given the standard set - theoretical interpretation of #AUTHOR_TAG, 1994 ). '"", '11 10 note that the passivization lexical rule in figure 2 is only intended to illustrate the mechanism.', 'we do not make the linguistic claim that passives should be analyzed using such a lexical rule.', '']",0
"['as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; #AUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; #AUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; #AUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; #AUTHOR_TAG ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
['#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],['#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],['#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],['#AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language'],0
['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ;'],['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; eisele and dorre 1990 ; griffith'],['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ;'],"['in certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; eisele and dorre 1990 ; griffith 1996 ) can be used to circumvent constraint propagation.', 'encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'for analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing']",0
"['powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']","['powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']","['powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']","['powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems.', 'in this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper']",0
"['properties can be considered an instance of the well - known frame problem in at ( #AUTHOR_TAG ), and we will therefore refer to the specifications left']","['properties can be considered an instance of the well - known frame problem in at ( #AUTHOR_TAG ), and we will therefore refer to the specifications left']","['[ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in at ( #AUTHOR_TAG ), and we will therefore refer to the specifications left']","['the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'this is so since the lexical rule in figure 2 "" ( like all lexical rules in hpsg ) preserves all properties of the input not mentioned in the rule. ""', '( pollard and sag [ 1994, 314 ], following flickinger [ 1987 ] ).', 'this idea of preserving properties can be considered an instance of the well - known frame problem in at ( #AUTHOR_TAG ), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( meurers 1994 )']",1
"['is rather straightforward.', 'in fact, one can view the representations as notational variants of one another.', 'each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'using an accumulator passing technique ( #AUTHOR_TAG ), we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']","['is rather straightforward.', 'in fact, one can view the representations as notational variants of one another.', 'each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'using an accumulator passing technique ( #AUTHOR_TAG ), we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']","['a finite - state automaton as definite relations is rather straightforward.', 'in fact, one can view the representations as notational variants of one another.', 'each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'using an accumulator passing technique ( #AUTHOR_TAG ), we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']","['a finite - state automaton as definite relations is rather straightforward.', 'in fact, one can view the representations as notational variants of one another.', 'each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'using an accumulator passing technique ( #AUTHOR_TAG ), we ensure that upon execution of a call to the interaction predicate q _ 1 a new lexical entry is derived as the result of successive application of a number of lexical rules.', 'because of the word class specialization step discussed in section 3. 3, the execution avoids trying out many lexical rule applications that are guaranteed to fail']",5
"['be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']","['be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']","['be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']","['translation of the lexical rule into a predicate is trivial.', 'the result is displayed description language.', '12 in order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in Meurers (1995).', 'the reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 a more detailed presentation can be found in minnen ( in preparation ).', '14 we use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'we here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form ( g6tz 1994 ).', 'computationally, a subsumption test could equally well be used in our compiler']",0
"['#AUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['#AUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['#AUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; #AUTHOR_TAG ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['oliva 1994 ; #AUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['oliva 1994 ; #AUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['oliva 1994 ; #AUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; #AUTHOR_TAG ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and #AUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calc']","['attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and #AUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in']","['.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and #AUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calc']","['terminology used in the literature varies.', 'types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'to avoid confusion, we will only use the terminology introduced in the text.', '4 this interpretation of the signature is sometimes referred to as closed world ( gerdemann and #AUTHOR_TAG ; gerdemann 1995 ).', '5 an in - depth discussion including a comparison of both approaches is provided in calcagno, meurers, and pollard ( in preparation ).', '6 the partial - vp topicalization lexical rule proposed by Hinrichs and Nakazawa (1994, 10 ) is a linguistic example.', 'the in - specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'in the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement']",0
"['##s and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['##s and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', 'de url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb / b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( pollard and sag 1994 ) or the complement cliticization lexical rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
['a linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs'],['a linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs'],['a linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs'],"['a linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i. e., changing the prd value of substantive signs from - to +, much like the lexical rule for nps given by Pollard and Sag (1994, p. 360, fn. 20 ).', 'in such a predicative lexical rule ( which we only note as an example and not as a linguistic proposal ) the subtype of the head object undergoing the rule as well as the value of the features only appropriate for the subtypes of substantive either is lost or must be specified by a separate rule for each of the subtypes']",0
"['with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and #AUTHOR_TAG, 1996, 1997b ) for encoding the main building block of hpsg grammars - - the implicative constraints -']","['with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and #AUTHOR_TAG, 1996, 1997b ) for encoding the main building block of hpsg grammars - - the implicative constraints - -']","['with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and #AUTHOR_TAG, 1996, 1997b ) for encoding the main building block of hpsg grammars - - the implicative constraints -']","['relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding : we show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.', 'the resulting encoding allows the execution of lexical rules on - the - fly, i. e., coroutined with other constraints at some time after lexical lookup.', 'the computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by gotz and #AUTHOR_TAG, 1996, 1997b ) for encoding the main building block of hpsg grammars - - the implicative constraints - - as a logic program']",2
"['as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['frank 1994 ; opalka 1995 ; #AUTHOR_TAG ).', 'the lexical entries are only partially specified, and']","['frank 1994 ; opalka 1995 ; #AUTHOR_TAG ).', 'the lexical entries are only partially specified, and']","['frank 1994 ; opalka 1995 ; #AUTHOR_TAG ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generaliza - tions over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; krieger and nerbonne 1992 ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; #AUTHOR_TAG ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( #AUTHOR_TAG ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( #AUTHOR_TAG ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( #AUTHOR_TAG ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']","['the authors are listed alphabetically.', 'sfb 340, kleine wilhelmstr.', '113, d - 72074 tiibingen, germany.', 'email : { dm, minnen } @ sfs. nphil. uni - tuebingen. de', 'url : http : / / www. sfs. nphil. uni - tuebingen. de / sfb', '/ b4home. html 1 this is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( hinrichs and nakazawa 1989 ) that also use lexical rules such as the complement extraction lexical rule ( #AUTHOR_TAG ) or the complement cliticization lexical rule ( miller and sag 1993 ) to operate on those raised elements.', 'also an analysis treating adjunct extraction via lexical rules ( van noord and bouma 1994 ) results in an infinite lexicon']",0
"['on the research results reported in #AUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in hpsg.', 'we developed a compiler that takes as its input a set of lexical rules, deduces the nec - essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'the definite clauses thereby introduce what we refer to as systematic covariationinlexicalentr']","['on the research results reported in #AUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in hpsg.', 'we developed a compiler that takes as its input a set of lexical rules, deduces the nec - essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'the definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries']","['on the research results reported in #AUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in hpsg.', 'we developed a compiler that takes as its input a set of lexical rules, deduces the nec - essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'the definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries']","['on the research results reported in #AUTHOR_TAG, 1996 ), we propose a new computational treatment of lexical rules that overcomes these short - comings and results in a more efficient processing of lexical rules as used in hpsg.', 'we developed a compiler that takes as its input a set of lexical rules, deduces the nec - essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'the definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries']",4
"['example, in lkb ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to']","['example, in lkb ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to']","['example, in lkb ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to']","['common approach to lexical rules is to encode them as unary phrase structure rules.', 'this approach is taken, for example, in lkb ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( copestake 1993, 31 ).', 'a similar method is included in patr - ii ( shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the cuf system ( dbrre and eisele 1991 ; d6rre and dorna 1993b ) or the tfs system ( emele and zajac 1990 ; emele 1994 ).', 'the covariation approach described in this paper can be viewed as a domain - specific refinement of such a treatment of lexical rules']",1
"['up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by #AUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called framepredicate, which can have multiple defining clauses.', 'so for the lexical rule 1, the frame specification is taken care']","['up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by #AUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called framepredicate, which can have multiple defining clauses.', 'so for the lexical rule 1, the frame specification is taken care']","['ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by #AUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called framepredicate, which can have multiple defining clauses.', 'so for the lexical rule 1, the frame specification is taken care']","['ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'in the above example, this would result in two lexical rules : one for words with tl as their c value and one for those with t2 as their c value.', 'in the latter case, we can also take care of transferring the value of z.', 'however, as discussed by #AUTHOR_TAG, creating several instances of lexical rules can be avoided.', 'instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'this is accomplished by having each lexical rule predicate call a so - called framepredicate, which can have multiple defining clauses.', 'so for the lexical rule 1, the frame specification is taken care of by extending the predicate in figure 6 with a call to a frame predicate, as shown in figure 8.']",4
"['a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'consider, for example, the lexical rule in figure 2, which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG, ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']",0
"['#AUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['#AUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['#AUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and']","['rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'in a number of proposals, lexical generalizations are captured using lexical underspecification ( kathol 1994 ; #AUTHOR_TAG ; riehemann 1993 ; oliva 1994 ; frank 1994 ; opalka 1995 ; sanfilippo 1995 ).', 'the lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy']",1
"['a passive lexical rule like the one presented by #AUTHOR_TAG, 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['a passive lexical rule like the one presented by #AUTHOR_TAG, 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['a passive lexical rule like the one presented by #AUTHOR_TAG, 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']","['as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'consider, for example, the lexical rule in figure 2, which encodes a passive lexical rule like the one presented by #AUTHOR_TAG, 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch.', 'this lexical rule could be used in a grammar of english to relate past participle forms of verbs to their passive form2  the rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'the index that the subject bore in the input is assigned to an optional prepositional complement in the output']",1
[''],[''],[''],"['the setup of king provides a clear formal basis for basic hpsg grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'two formalizations of lexical rules as used by hpsg linguists have been proposed, the meta - level lexical rules ( mlrs ; calcagno 1995 ; calcagno and pollard 1995 ) and the description - level lexical rules ( dlrs ;']",0
"['task of mention detection is closely related to named entity recognition ( ner ).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'they aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'however, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models,']","['task of mention detection is closely related to named entity recognition ( ner ).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'they aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'however, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models,']","['task of mention detection is closely related to named entity recognition ( ner ).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'they aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'however, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models,']","['task of mention detection is closely related to named entity recognition ( ner ).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'they aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'however, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of non - local features, and integration of external knowledge.', 'ner can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e. g.', 'hidden markov model (Rabiner, 1989) or conditional random fields (Sarawagi and Cohen, 2004).', 'the typical bio representation was introduced in Ramshaw and Marcus (1995) ; oc representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities']",0
"[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in #AUTHOR_TAG']","[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in #AUTHOR_TAG']","[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in #AUTHOR_TAG']","[', y u, v = 1 iff mentions u, v are directly linked.', 'thus, we can construct a forest and the mentions in the same connected component ( i. e., in the same tree ) are co - referred.', 'for this mention - pair coreference model i ( u, v ), we use the same set of features used in #AUTHOR_TAG']",5
['in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG )'],['in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG )'],['in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG )'],"[', phrases in the brackets are mentions and the underlined simple phrases are mention heads.', 'moreover, mention boundaries can be nested ( the boundary of a mention is inside the boundary of another mention ), but mention heads never overlap.', 'this property also simplifies the problem of mention head candidate generation.', 'in the example above, the first "" they "" refers to "" multinational companies investing in china "" and the second "" they "" refers to "" domestic manufacturers, who are also suffering "".', 'in both cases, the mention heads are sufficient to support the decisions : "" they "" refers to "" companies "", and "" they "" refers to "" manufacturers "".', 'in fact, most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG )']",0
"['##ll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and Kuhn (2014).', 'for berkeley system, we use the reported results from #AUTHOR_TAG']","['latest scorer is version v8. 01, but muc, b, ceaf e and conll average scores are not changed.', 'for evaluation on ace - 2004, we convert the system output and gold annotations into conll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and Kuhn (2014).', 'for berkeley system, we use the reported results from #AUTHOR_TAG']","['##ll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and Kuhn (2014).', 'for berkeley system, we use the reported results from #AUTHOR_TAG']","['latest scorer is version v8. 01, but muc, b, ceaf e and conll average scores are not changed.', 'for evaluation on ace - 2004, we convert the system output and gold annotations into conll format. 12', 'we do not provide results from berkeley and hotcoref on ace - 2004 dataset as they do not directly support ace input.', 'results for hotcoref are slightly different from the results reported in bjorkelund and Kuhn (2014).', 'for berkeley system, we use the reported results from #AUTHOR_TAG']",1
"['on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system ( Lee et al. , 2011 ), berkeley system ( #AUTHOR_TAG ) and hotcoref system ( bj [UNK] orkelund and Kuhn , 2014 ).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl']","['on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system ( Lee et al. , 2011 ), berkeley system ( #AUTHOR_TAG ) and hotcoref system ( bj [UNK] orkelund and Kuhn , 2014 ).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl']","['. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system ( Lee et al. , 2011 ), berkeley system ( #AUTHOR_TAG ) and hotcoref system ( bj [UNK] orkelund and Kuhn , 2014 ).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', 'when the cl 3 m coreference system uses gold mentions or heads, we call the system gold ; when it uses predicted mentions or heads, we call the system predicted.', 'the mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it h - m - md.', 'we can feed the predicted mentions from h - m - md']","['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system ( Lee et al. , 2011 ), berkeley system ( #AUTHOR_TAG ) and hotcoref system ( bj [UNK] orkelund and Kuhn , 2014 ).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', 'when the cl 3 m coreference system uses gold mentions or heads, we call the system gold ; when it uses predicted mentions or heads, we call the system predicted.', 'the mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it h - m - md.', 'we can feed the predicted mentions from h - m - md directly into the mention - pair coref - 9 no parsing information is needed at evaluation time. 10', 'we use gurobi v5. 0. 1 as our ilp solver.', '3 : performance of coreference resolution for all systems on the conll - 2012 dataset.', 'subscripts ( m, h ) indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 9,  1 = 0. 25 and  2 = 0. 2.', 'erence model that we implemented, resulting in a traditional pipelined end - to - end coreference system, namely h - m - coref.', 'we name our new proposed end - to - end coreference resolution system incorporating both the mention head candidate generation module']",1
"['contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ).', 'the ontonotes - 5. 0']","['contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']",5
"['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ collins head rules ( #AUTHOR_TAG ) to identify']","['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ collins head rules ( #AUTHOR_TAG ) to identify']","['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ collins head rules ( #AUTHOR_TAG ) to identify']","['of the head mentions proposed by the algorithms described in sec. 3 are positive examples.', 'we ensure a balanced training of the mention head detection model by adding sub - sampled invalid mention head candidates as negative examples.', 'specifically, after mention head candidate generation ( described in sec.', '3 ), we train on a set of candidates with precision larger than 50 %.', 'we then use illinois chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ collins head rules ( #AUTHOR_TAG ) to identify their heads.', 'when these extracted heads do not overlap with gold mention heads, we treat them as negative examples']",5
"['it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  bj [UNK] orkelund and Kuhn , 2014 ).', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', '']","['detection is rarely studied as a stand - alone research problem ( Recasens et al. (2013) is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  bj [UNK] orkelund and Kuhn , 2014 ).', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in table 1.', 'current state - of - the - art systems show a very significant drop in performance when running on system generated mentions.', 'these performance gaps are worrisome, since the real goal of nlp']","['detection is rarely studied as a stand - alone research problem ( Recasens et al. (2013) is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  bj [UNK] orkelund and Kuhn , 2014 ).', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', '']","['detection is rarely studied as a stand - alone research problem ( Recasens et al. (2013) is one key exception ).', 'most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ;  bj [UNK] orkelund and Kuhn , 2014 ).', 'however, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in table 1.', 'current state - of - the - art systems show a very significant drop in performance when running on system generated mentions.', 'these performance gaps are worrisome, since the real goal of nlp systems is to process raw data.', '1 : performance gaps between using gold mentions and predicted mentions for three state - of - the - art coreference resolution systems.', 'performance gaps are always larger than 10 %.', ""illinois's system (Chang et al., 2013) is evaluated on conll ( 2012conll (, 2011 ) shared task and ace - 2004 datasets."", 'it reports an average f1 score of muc, b and ceaf e metrics using conll v7. 0 scorer.', ""berkeley's system (Durrett and Klein, 2013) reports the same average score on the conll - 2011 shared task dataset."", ""results of stanford's system (Lee et al., 2011) are for b 3 metric on ace - 2004 dataset."", 'this paper focuses on improving end - to - end coreference performance.', 'we do this by : 1 ) developing a new ilp - based joint learning and inference formulation for coreference and mention head detection.', '2 ) developing a better mention head candidate generation algorithm.', 'importantly, we focus on heads rather than mention boundaries since those can be identified more robustly and used effectively in an end - to - end system.', 'as we show, this results in a dramatic improvement in the quality of the md component and, consequently, a significant reduction in the performance gap between coreference on gold mentions and coreference on raw data']",0
"['in #AUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then']","['on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the bilou - representation as it has advantages over traditional bio - representation, as shown, e. g. in #AUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then']","['in #AUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then']","['on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the bilou - representation as it has advantages over traditional bio - representation, as shown, e. g. in #AUTHOR_TAG.', 'the bilourepresentation suggests learning classifiers that identify the beginning, inside and last tokens of multi - token chunks as well as unit - length chunks.', 'the problem is then transformed into a simple, but constrained, 5 - class classification problem']",4
"[', which is released for the conll - 2012 shared task ( #AUTHOR_TAG ), contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']","[', which is released for the conll - 2012 shared task ( #AUTHOR_TAG ), contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task ( #AUTHOR_TAG ), contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012 shared task ( #AUTHOR_TAG ), contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']",5
"['on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by #AUTHOR_TAG, using constrained latent left - linking model ( cl']","['on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by #AUTHOR_TAG, using constrained latent left - linking model ( cl3m ) as our mention - pair coreference model in the joint framework10.', 'when the cl']","['. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by #AUTHOR_TAG, using constrained latent left - linking model ( cl']","['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by #AUTHOR_TAG, using constrained latent left - linking model ( cl3m ) as our mention - pair coreference model in the joint framework10.', 'when the cl 3 m coreference system uses gold mentions or heads, we call the system gold ; when it uses predicted mentions or heads, we call the system predicted.', 'the mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it h - m - md.', 'we can feed the predicted mentions from h - m - md directly into the mention - pair coref - 9 no parsing information is needed at evaluation time. 10', 'we use gurobi v5. 0. 1 as our ilp solver.', '3 : performance of coreference resolution for all systems on the conll - 2012 dataset.', 'subscripts ( m, h ) indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 9,  1 = 0. 25 and  2 = 0. 2.', 'erence model that we implemented, resulting in a traditional pipelined end - to - end coreference system, namely h - m - coref.', 'we name our new proposed end - to - end coreference resolution system incorporating both the mention head candidate generation module and']",5
"['##ference resolution has been extensively studied, with several state - of - the - art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013; bjorkelund and Kuhn, 2014;Song et al., 2012).', 'many of the early rule - based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'the introduction of ilp methods has influenced the coreference area too Denis and Baldridge, 2007).', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in #AUTHOR_TAG in our experiments']","['##ference resolution has been extensively studied, with several state - of - the - art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013; bjorkelund and Kuhn, 2014;Song et al., 2012).', 'many of the early rule - based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'the introduction of ilp methods has influenced the coreference area too Denis and Baldridge, 2007).', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in #AUTHOR_TAG in our experiments']","['##ference resolution has been extensively studied, with several state - of - the - art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013; bjorkelund and Kuhn, 2014;Song et al., 2012).', 'many of the early rule - based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'the introduction of ilp methods has influenced the coreference area too Denis and Baldridge, 2007).', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in #AUTHOR_TAG in our experiments']","['##ference resolution has been extensively studied, with several state - of - the - art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013; bjorkelund and Kuhn, 2014;Song et al., 2012).', 'many of the early rule - based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'the early designs were easy to understand and the rules were designed manually.', 'machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'the introduction of ilp methods has influenced the coreference area too Denis and Baldridge, 2007).', 'in this paper, we use the constrained latent left - linking model ( cl3m ) described in #AUTHOR_TAG in our experiments']",5
"['contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ).', 'the ontonotes - 5. 0']","['contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']","['ace - 2004 dataset contains 443 documents.', 'we use a standard split of 268 training documents, 68 development documents, and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ).', 'the ontonotes - 5. 0 dataset, which is released for the conll - 2012shared task (Pradhan et al., 2012,  contains 3, 145 annotated documents.', 'these documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'we report results on the test documents for both datasets.', ') indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 8,  1 = 0. 2 and  2 = 0. 3']",5
"['details can be found in #AUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in #AUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in #AUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']","['details can be found in #AUTHOR_TAG et al. ( 2013 ).', 'the difference here is that we also consider the validity of mention heads using ( u ), ( m']",0
"['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different']","['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different']","['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different']","['recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity - linking.', 'the work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'while their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different']",0
"['has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted']","['has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted']","['has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads']","['ace - 2004 dataset is annotated with both mention and mention heads, while the ontonotes - 5. 0', 'dataset only has mention annotations.', 'therefore, we preprocess ontonote - 5. 0 to derive mention heads using collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information.', 'the parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'we set these extracted heads as gold, which enables us to train the two layer bilou - classifier described in sec.', '3. 1. 1.', 'the nonoverlapping mention head assumption in sec.', '3. 1. 1', 'can be verified empirically on both ace - 2004 and ontonotes - 5. 0', 'datasets.', 'baseline systems we choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : stanford system (Lee et al., 2011), berkeley system (Durrett and Klein, 2014) and hotcoref system ( bjorkelund and Kuhn, 2014).', 'developed systems our developed system is built on the work by Chang et al. (2013), using constrained latent left - linking model ( cl 3 m ) as our mention - pair coreference model in the joint framework 10.', 'when the cl 3 m coreference system uses gold mentions or heads, we call the system gold ; when it uses predicted mentions or heads, we call the system predicted.', 'the mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it h - m - md.', 'we can feed the predicted mentions from h - m - md directly into the mention - pair coref - 9 no parsing information is needed at evaluation time. 10', 'we use gurobi v5. 0. 1 as our ilp solver.', '3 : performance of coreference resolution for all systems on the conll - 2012 dataset.', 'subscripts ( m, h ) indicate evaluations on ( mentions, mention heads ) respectively.', 'for gold mentions and mention heads, they yield the same performance for coreference.', 'our proposed h - joint - m system achieves the highest performance.', 'parameters of our proposed system are tuned as  = 0. 9,  = 0. 9,  1 = 0. 25 and  2 = 0. 2.', 'erence model that we implemented, resulting in a traditional pipelined end - to - end coreference system, namely h - m - coref.', 'we name our new proposed end - to - end coreference resolution system incorporating both the mention head candidate generation module and']",5
"['section describes our joint coreference resolution and mention head detection framework.', 'our work is inspired by the latent left - linking model in #AUTHOR_TAG and the ilp formulation from Chang et al. ( 2011 ).', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and jointly ( 1 ) determines if they are indeed mention heads and (']","['section describes our joint coreference resolution and mention head detection framework.', 'our work is inspired by the latent left - linking model in #AUTHOR_TAG and the ilp formulation from Chang et al. ( 2011 ).', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and jointly ( 1 ) determines if they are indeed mention heads and']","['section describes our joint coreference resolution and mention head detection framework.', 'our work is inspired by the latent left - linking model in #AUTHOR_TAG and the ilp formulation from Chang et al. ( 2011 ).', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and jointly ( 1 ) determines if they are indeed mention heads and (']","['section describes our joint coreference resolution and mention head detection framework.', 'our work is inspired by the latent left - linking model in #AUTHOR_TAG and the ilp formulation from Chang et al. ( 2011 ).', 'the joint learning and inference model takes as input mention head candidates ( sec.', '3 ) and jointly ( 1 ) determines if they are indeed mention heads and ( 2 ) learns a similarity metric between mentions.', 'this is done by simultaneously learning a binary mention head detection classifier and a mention - pair coreference classifier.', 'the mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'by learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'this joint framework aims to improve performance on both mention head detection and on coreference']",5
"['present experiments on the two standard coreference resolution datasets, ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0 ( #AUTHOR_TAG ).', '(Hovy et al., 2006).', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']","['present experiments on the two standard coreference resolution datasets, ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0 ( #AUTHOR_TAG ).', '(Hovy et al., 2006).', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']","['present experiments on the two standard coreference resolution datasets, ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0 ( #AUTHOR_TAG ).', '(Hovy et al., 2006).', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']","['present experiments on the two standard coreference resolution datasets, ace - 2004 ( nist, 2004 ) and ontonotes - 5. 0 ( #AUTHOR_TAG ).', '(Hovy et al., 2006).', 'our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat - of - the - art results on coreference resolution ; in addition, it achieves significant performance improvement on md for both datasets']",5
"['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar ( #AUTHOR_TAG ).', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in  5 ) with probability p del, or a standard ccg category c']","['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar ( #AUTHOR_TAG ).', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in  5 ) with probability p del, or a standard ccg category c']","['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar ( #AUTHOR_TAG ).', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in  5 ) with probability p del, or a standard ccg category c']","['the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross - linguistically - plausible categories.', 'to formalize the notion of what it means for a category to be more "" plausible "", we extend the category generator of our previous work, which we will call p cat.', 'we can define pcat using a probabilistic grammar ( #AUTHOR_TAG ).', 'the grammar may first generate a start or end category ( s, e ) with probability p se or a special tokendeletion category ( d ; explained in  5 ) with probability p del, or a standard ccg category c']",0
"[""' s ccm is an unlabeled bracketing model that generates the span of part - of - speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non - constituent )."", 'they found that modeling constituent context aids in parser learning because it is able to capture the']","[""' s ccm is an unlabeled bracketing model that generates the span of part - of - speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non - constituent )."", 'they found that modeling constituent context aids in parser learning because it is able to capture the']","[""' s ccm is an unlabeled bracketing model that generates the span of part - of - speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non - constituent )."", 'they found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', 'while ccm is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels']","[""' s ccm is an unlabeled bracketing model that generates the span of part - of - speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non - constituent )."", 'they found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', 'while ccm is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels']",0
"['dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors']","['dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors']","['this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors']","['this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'in order to estimate the parameters of our model, we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities.', 'however, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a metropolis - hastings step that allows us to sample efficiently from the correct posterior.', 'our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability - preferring priors on those parameters yields further gains in many scenarios']",5
"['ccgbank ( Hockenmaier and Steedman , 2007 ), which is a transformation of the penn treebank ( #AUTHOR_TAG ) ; the ctbccg ( Tse and Curran , 2010 ) transformation of the penn chinese treebank ( Xue et al. , 2005 ) ; and the ccg - tut corpus ( Bos et al. , 2009 ), built from the tut corpus of italian text ( Bosco et al. , 2000 )']","['ccgbank ( Hockenmaier and Steedman , 2007 ), which is a transformation of the penn treebank ( #AUTHOR_TAG ) ; the ctbccg ( Tse and Curran , 2010 ) transformation of the penn chinese treebank ( Xue et al. , 2005 ) ; and the ccg - tut corpus ( Bos et al. , 2009 ), built from the tut corpus of italian text ( Bosco et al. , 2000 )']","['our evaluation we compared our supertagcontext approach to ( our reimplementation of ) the best - performing model of our previous work (Garrette et al., 2015), which scm extends.', 'we evaluated on the english ccgbank ( Hockenmaier and Steedman , 2007 ), which is a transformation of the penn treebank ( #AUTHOR_TAG ) ; the ctbccg ( Tse and Curran , 2010 ) transformation of the penn chinese treebank ( Xue et al. , 2005 ) ; and the ccg - tut corpus ( Bos et al. , 2009 ), built from the tut corpus of italian text ( Bosco et al. , 2000 )']","['our evaluation we compared our supertagcontext approach to ( our reimplementation of ) the best - performing model of our previous work (Garrette et al., 2015), which scm extends.', 'we evaluated on the english ccgbank ( Hockenmaier and Steedman , 2007 ), which is a transformation of the penn treebank ( #AUTHOR_TAG ) ; the ctbccg ( Tse and Curran , 2010 ) transformation of the penn chinese treebank ( Xue et al. , 2005 ) ; and the ccg - tut corpus ( Bos et al. , 2009 ), built from the tut corpus of italian text ( Bosco et al. , 2000 )']",5
"[""right - side context of a non - terminal category - - the probability of generating a category to the right of the current constituent's category - - corresponds directly to the category transitions used for the hmm supertagger of #AUTHOR_TAG."", ""thus, the right - side context prior mean  rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]","[""right - side context of a non - terminal category - - the probability of generating a category to the right of the current constituent's category - - corresponds directly to the category transitions used for the hmm supertagger of #AUTHOR_TAG."", ""thus, the right - side context prior mean  rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]","[""right - side context of a non - terminal category - - the probability of generating a category to the right of the current constituent's category - - corresponds directly to the category transitions used for the hmm supertagger of #AUTHOR_TAG."", ""thus, the right - side context prior mean  rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]","[""right - side context of a non - terminal category - - the probability of generating a category to the right of the current constituent's category - - corresponds directly to the category transitions used for the hmm supertagger of #AUTHOR_TAG."", ""thus, the right - side context prior mean  rctx - 0 t can be biased in exactly the same way as the hmm supertagger's transitions : toward context supertags that connect to the constituent label""]",1
"[', a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as #AUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of']","['a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as #AUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of']","[', a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as #AUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of raw data available to the sampler, we supplemented the english data with raw, unannotated newswire sentences from the nyt giga - word 5 corpus (Parker et al., 2011) and']","['corpus was divided into four distinct data sets : a set from which we extract the tag dictionaries, a set of raw ( unannotated ) sentences, a development set, and a test set.', 'we use the same splits as #AUTHOR_TAG.', 'since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form ( x \\ x ) / x rather than introducing special conjunction rules.', 'in order to increase the amount of raw data available to the sampler, we supplemented the english data with raw, unannotated newswire sentences from the nyt giga - word 5 corpus (Parker et al., 2011) and supplemented italian with the out - of - domain wacky corpus (Baroni et al., 1999).', 'for english and italian, this allowed us to use 100k raw tokens for training ( chinese uses 62k ).', 'for chinese and italian, for training efficiency, we used only raw sentences that were 50 words or fewer ( note that we did not drop tag dictionary set or test set sentences )']",5
"['by #AUTHOR_TAG, but we do it']","['by #AUTHOR_TAG, but we do it']","['by #AUTHOR_TAG, but we do it']","['evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence : every dependency would be "" wrong "".', 'thus, it is important that we make a best effort to find a parse.', 'to accomplish this, we implemented a parsing backoff strategy.', 'the parser first tries to find a valid parse that has either s dcl or np at its root.', 'if that fails, then it searches for a parse with any root.', 'if no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent ( first with a restricted root set, then without ).', 'this is similar to the "" deletion "" strategy employed by #AUTHOR_TAG, but we do it directly in the grammar.', 'we add unary rules of the form d u for every potential supertag u in the tree.', 'then, at each node spanning exactly two tokens ( but no higher in the tree ), we allow rules t d, v and t v, d.', 'recall that in  3. 1, we stated that d is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.', 'additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents']",1
['this intuition into a bayesian prior can help train a ccg supertagger ( #AUTHOR_TAG )'],['this intuition into a bayesian prior can help train a ccg supertagger ( #AUTHOR_TAG )'],"['further notes that due to the natural associativity of ccg, adjacent categories tend to be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger ( #AUTHOR_TAG )']","['##ridge observed is that, cross - linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.', 'in previous work, we were able to incorporate this preference into a bayesian parsing model, biasing pcfg productions toward sim - pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015).', 'baldridge further notes that due to the natural associativity of ccg, adjacent categories tend to be combinable.', 'we previously showed that incorporating this intuition into a bayesian prior can help train a ccg supertagger ( #AUTHOR_TAG )']",2
"['employ the same procedure as our previous work for setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ). 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'these counts are then combined with estimates of the openness of each tag in order to assess its likelihood of appearing with new words']","['employ the same procedure as our previous work for setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ). 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'these counts are then combined with estimates of the openness of each tag in order to assess its likelihood of appearing with new words']","['employ the same procedure as our previous work for setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ). 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'these counts are then combined with estimates of the openness of each tag in order to assess its likelihood of appearing with new words']","['employ the same procedure as our previous work for setting the terminal production prior distributions _ term - 0 ( w ) by estimating word - given - category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ). 4', 'this procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'these counts are then combined with estimates of the openness of each tag in order to assess its likelihood of appearing with new words']",5
"['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties ( #AUTHOR_TAG )']","['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties ( #AUTHOR_TAG )']","['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties ( #AUTHOR_TAG )']","['ccm, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'since we are not generating from the model, this does not introduce difficulties ( #AUTHOR_TAG )']",4
['#AUTHOR_TAG'],['#AUTHOR_TAG'],['#AUTHOR_TAG'],"['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules for combining with punctuation to the left and right and allow for the merge rule x a x x of #AUTHOR_TAG']",5
"['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non - terminal position spanning words w i through w j1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j1 via any arrangement of productions that is rooted by y ij = t']","['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non - terminal position spanning words w i through w j1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j1 via any arrangement of productions that is rooted by y ij = t']","['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non - terminal position spanning words w i through w j1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j1 via any arrangement of productions that is rooted by y ij = t']","['sample from our proposal distribution, we use a blocked gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees.', 'for a sentence w, the strategy is to use the inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non - terminal position spanning words w i through w j1 and category t, going "" up "" the tree, the probability of generating w i,...', ', w j1 via any arrangement of productions that is rooted by y ij = t']",5
"['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by #AUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'to efficiently sample new model parameters, we exploit dirichlet - multinomial conjugacy.', 'by repeating these alternating steps and accumu - lating the productions, we obtain an approximation of the required posterior quantities']","['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by #AUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'to efficiently sample new model parameters, we exploit dirichlet - multinomial conjugacy.', 'by repeating these alternating steps and accumu - lating the productions, we obtain an approximation of the required posterior quantities']","['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by #AUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'to efficiently sample new model parameters, we exploit dirichlet - multinomial conjugacy.', 'by repeating these alternating steps and accumu - lating the productions, we obtain an approximation of the required posterior quantities']","['wish to infer the distribution over ccg parses, given the model we just described and a corpus of sentences.', 'since there is no way to analytically compute these modes, we resort to gibbs sampling to find an approximate solution.', 'our strategy is based on the approach presented by #AUTHOR_TAG.', 'at a high level, we alternate between resampling model parameters ( _ root, _ bin, _, _, _, _, _ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'to efficiently sample new model parameters, we exploit dirichlet - multinomial conjugacy.', 'by repeating these alternating steps and accumu - lating the productions, we obtain an approximation of the required posterior quantities']",5
"['important example is the constituentcontext model ( ccm ) of #AUTHOR_TAG, which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear.', 'this phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'for example, the part - of - speech ( pos ) sequence adj noun frequently occurs between the tags det and verb.', 'this det - verb context also frequently applies to the single - word sequence noun and to']","['important example is the constituentcontext model ( ccm ) of #AUTHOR_TAG, which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear.', 'this phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'for example, the part - of - speech ( pos ) sequence adj noun frequently occurs between the tags det and verb.', 'this det - verb context also frequently applies to the single - word sequence noun and to']","['important example is the constituentcontext model ( ccm ) of #AUTHOR_TAG, which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear.', 'this phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'for example, the part - of - speech ( pos ) sequence adj noun frequently occurs between the tags det and verb.', 'this det - verb context also frequently applies to the single - word sequence noun and to adj adj noun.', 'from this, we might deduce that det - verb is a likely context for a noun phrase.', 'ccm is able to learn which pos contexts are likely,']","['important example is the constituentcontext model ( ccm ) of #AUTHOR_TAG, which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear.', 'this phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'for example, the part - of - speech ( pos ) sequence adj noun frequently occurs between the tags det and verb.', 'this det - verb context also frequently applies to the single - word sequence noun and to adj adj noun.', 'from this, we might deduce that det - verb is a likely context for a noun phrase.', 'ccm is able to learn which pos contexts are likely, and does so via a probabilistic generative model, providing a statistical, data - driven take on substitutability.', 'however, since there is nothing intrinsic about the pos pair det - verb that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data.', 'Baldridge (2008) observed that unlike opaque, atomic pos labels, the rich structures of combinatory categorial grammar ( ccg ) (Steedman, 2000;Steedman and Baldridge, 2011) categories reflect universal grammatical properties.', 'ccg is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'for example, a category might encode that "" this constituent can combine with a noun phrase to the right ( an object ) and then a noun phrase to the left ( a subject ) to produce a sentence "" instead of simply verb.', 'ccg has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical - expansion algorithms (Bisk and Hockenmaier, 2012; 2013 ) or encoding that information as priors within a bayesian framework (Garrette et al., 2015)']",0
"['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add']","['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add']","['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add']","['of adjacent constituents can be combined using one of a set of combination rules to form categories of higher - level constituents, as seen in figure 1.', 'the direction of the slash operator gives the behavior of the function.', 'a category ( s \\ np ) / pp might describe an intransitive verb with a prepositional phrase complement ; it combines on the right ( / ) with a constituent with category pp, and then on the left ( \\ ) with a noun phrase ( np ) that serves as its subject.', 'we follow #AUTHOR_TAG in allowing a small set of generic, linguistically - plausible unary and binary grammar rules.', 'we further add rules for combining with punctuation to the left and right and allow for the merge rule x  x x of Clark and Curran (2007)']",5
"['##boost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by #AUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by']","['##boost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by #AUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by']","['##boost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by #AUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by']","['##boost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let rank - boost choose the relevant ones.', 'in total, we used 3, 291 features in training the spr.', 'features were discovered from the actual sentence plan trees that the spg generated through the feature derivation process described below, in a manner similar to that used by #AUTHOR_TAG.', 'the motivation for the features was to capture declaratively decisions made by the randomized spg.', 'we avoided features specific to particular text plans by discarding those that occurred fewer than 10 times']",1
['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG )'],['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG )'],['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG )'],"['work in sentence planning in the natural language generation ( nlg ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ).', 'this approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.', 'presumably, this is the reason why dialog systems to date have not used this kind of sentence planning']",0
"['the top - ranked output to input to the surface realizer. our primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 )']","['large sample of possible sentence plans for a', 'given text - plan input. in the second phase, the sentence - plan - ranker ( spr', ') ranks the sample sentence plans, and then selects the top - ranked output to input to the surface realizer. our primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 )']","['confirm ( month : 9 ) implicit - confirm ( day -', 'number : 1 ) request ( depart - time ) in this paper, we present spot, for "" sentence planner, trainable "". we also present a new methodology for automatically training spot on the basis of feedback provided by human judges. in order to train spot, we reconceptualize its task as consisting of two distinct phases. in the first phase, the', 'sentence - plan - generator ( spg ) generates a potentially large sample of possible sentence plans for a', 'given text - plan input. in the second phase, the sentence - plan - ranker ( spr', ') ranks the sample sentence plans, and then selects the top - ranked output to input to the surface realizer. our primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 )']","['example, consider the required capabilities of a sentence planner for a mixed - initiative spoken dialog system for travel planning : ( d1 ) system1 : welcome.... what airport would you like to fly out of?', 'user2 : i need to go to dallas. system3 : flying to dallas. what departure airport was that? user4 : from newark on september the 1st. system5 : what time would you like to', 'travel on september the 1st to dallas from new', '##ark? utterance system1 requests information about', ""the caller's departure"", 'airport, but in user2, the caller takes the', ""initiative to provide information about her destination. in system3, the system's goal is to implicitly confirm the"", ""destination ( because of the possibility of error in the speech recognition component ), and request information ( for the second time ) of the caller's departure airport"", "". in user4, the caller provides this information but also provides the month and day of travel. given the system's dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has"", 'provided so far, i. e. the departure and destination cities and the month and day information,', ""as well as to request information about the time of travel. the system's representation of its communicative goals for utterance system5 is in figure 1. the job of the sentence planner is to decide among the large number of potential realizations of these communicative goals. some example alternative realizations are in figure"", '2. 2 implicit - confirm ( orig - city : newark ) implicit - confirm (', 'dest - city : dallas ) implicit - confirm ( month : 9 ) implicit - confirm ( day -', 'number : 1 ) request ( depart - time ) in this paper, we present spot, for "" sentence planner, trainable "". we also present a new methodology for automatically training spot on the basis of feedback provided by human judges. in order to train spot, we reconceptualize its task as consisting of two distinct phases. in the first phase, the', 'sentence - plan - generator ( spg ) generates a potentially large sample of possible sentence plans for a', 'given text - plan input. in the second phase, the sentence - plan - ranker ( spr', ') ranks the sample sentence plans, and then selects the top - ranked output to input to the surface realizer. our primary contribution is a method for training the', 'spr. the spr uses rules automatically learned from training data, using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 )']",1
"['a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ), although the various merge']","['a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ), although the various merge']","['a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ), although the various merge operations are, to our knowledge, novel in this form']","['already mentioned, we divide the sentence planning task into two phases.', 'in the first phase, the sentenceplan - generator ( spg ) generates 12 - 20 possible sentence plans for a given input text plan.', ""each speech act is assigned a canonical lexico - structural representation ( called a dsynts - deep syntactic structure ( mel'cuk, 1988 ) )."", 'the sentence plan is a tree recording how these elementary dsynts are combined into larger dsyntss ; the dsynts for the entire input text plan is associated with the root node of the tree.', 'in the second phase, the sentence plan ranker ( spr ) ranks sentence plans generated by the spg, and then selects the top - ranked output as input to the surface realizer, realpro (Lavoie and Rambow, 1997) the research presented here is primarily concerned with creating a trainable spr.', 'a strength of our approach is the ability to use a very simple spg, as we explain below.', 'the basis of our spg is a set of clausecombining operations that incrementally transform a list of elementary predicate - argument representations ( the dsyntss corresponding to elementary speech acts, in our case ) into a single lexico - structural representation, by combining these representations using the following combining operations.', 'examples can be found in figure adjective.', 'this transforms a predicative use of an adjective into an adnominal construction.', 'period.', 'joins two complete clauses with a period.', 'these operations are not domain - specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ), although the various merge operations are, to our knowledge, novel in this form']",0
"['by (Lavoie and Rambow, 1998).', 'the representations used by Danlos ( 2000 ), Gardent and Webber ( 1998 ), or #AUTHOR_TAG are similar, but do']","['by (Lavoie and Rambow, 1998).', 'the representations used by Danlos ( 2000 ), Gardent and Webber ( 1998 ), or #AUTHOR_TAG are similar, but do']","['by a period and whose daughters are the two daughter dsyntss. )', 'if a clause combination fails, the sp - tree is discarded ( for example, if we try to create a relative clause of a structure which already contains a period ).', 'as a result, the dsynts for the entire turn is associated with the root node.', 'this dsynts can be sent to realpro, which returns a sentence ( or several sentences, if the dsynts contains period nodes ).', 'the spg is designed in such a way that if a dsynts is associated with the root node, it is a valid structure which can be realized.', 'figure 2 shows some of the realizations of alternative sentence plans generated by our spg for utterance sys - 3 the sp - tree is inspired by (Lavoie and Rambow, 1998).', 'the representations used by Danlos ( 2000 ), Gardent and Webber ( 1998 ), or #AUTHOR_TAG are similar, but do not ( always )']","['result of applying the operations is a sentence plan tree ( or sp - tree for short ), which is a binary tree with leaves labeled by all the elementary speech acts from the input text plan, and with its interior nodes labeled with clause - combining operations 3.', 'each node is also associated with a dsynts : the leaves ( which correspond to elementary speech acts from the input text plan ) are linked to a canonical dsynts for that speech act ( by lookup in a hand - crafted dictionary ).', 'the interior nodes are associated with dsyntss by executing their clausecombing operation on their two daughter nodes.', '( a pe - riod node results in a dsynts headed by a period and whose daughters are the two daughter dsyntss. )', 'if a clause combination fails, the sp - tree is discarded ( for example, if we try to create a relative clause of a structure which already contains a period ).', 'as a result, the dsynts for the entire turn is associated with the root node.', 'this dsynts can be sent to realpro, which returns a sentence ( or several sentences, if the dsynts contains period nodes ).', 'the spg is designed in such a way that if a dsynts is associated with the root node, it is a valid structure which can be realized.', 'figure 2 shows some of the realizations of alternative sentence plans generated by our spg for utterance sys - 3 the sp - tree is inspired by (Lavoie and Rambow, 1998).', 'the representations used by Danlos ( 2000 ), Gardent and Webber ( 1998 ), or #AUTHOR_TAG are similar, but do not ( always ) explicitly represent the clause combining operations as labeled nodes.', '4 shows the result of applying the soft - merge operation when args 1 and 2 are implicit confirmations of the origin and destination cities. figure 8 illustrates the relationship between the sp - tree and the dsynts for alternative 8.', 'the labels and arrows show the dsyntss associated with each node in the sp - tree ( in figure 7 ), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'the complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'in our approach, we do not need to encode such constraints.', 'rather, we generate a random sample of possible sentence plans for each text plan, up to a pre - specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution']",0
"['##the algorithm was implemented by the the authors, following the description in #AUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in #AUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in #AUTHOR_TAG']","['##the algorithm was implemented by the the authors, following the description in #AUTHOR_TAG']",5
"['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system ( #AUTHOR_TAG )']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system ( #AUTHOR_TAG )']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system ( #AUTHOR_TAG )']","['paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner, and the ica system ( #AUTHOR_TAG )']",1
"['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by #AUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by #AUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by #AUTHOR_TAG']","['data used in the experiment was selected from the penn treebank wall street journal, and is the same used by #AUTHOR_TAG']",5
['a the ica algorithm ( #AUTHOR_TAG )'],['a the ica algorithm ( #AUTHOR_TAG )'],['a the ica algorithm ( #AUTHOR_TAG )'],"[""## the regular tbl, as described in section 2 ; a an improved version of tbl, which makes extensive use of indexes to speed up the rules'update ; a the fasttbl algorithm ; a the ica algorithm ( #AUTHOR_TAG )""]",1
['ica system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],['ica system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],['ica system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],['ica system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance'],0
"['the tree - cut technique described above, our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from wordnet.', 'in this section, we g i v']","['the tree - cut technique described above, our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from wordnet.', 'in this section, we g i v']","['the tree - cut technique described above, our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']","['the tree - cut technique described above, our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from wordnet.', 'in this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method']",2
"['is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut (Li and Abe, 1998).', 'in our previous work ( #AUTHOR_TAG ), we applied this method to a small subset of wordnet nouns and showed potential applicability.', 'in the current work, we applied the method to all nouns and verbs in wordnet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'we report results of comparing our lexicon with the wordnet cousins as well as the inter - annotator disagreement observed between two semantically annotated corpora : wordnet semcor (Landes et al., 1998) and dso (Ng and Lee, 1996).', 'the results are quite promising : our extraction method discovered 89 % of the wordnet cousins, and the sense partitions in our']","['is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut (Li and Abe, 1998).', 'in our previous work ( #AUTHOR_TAG ), we applied this method to a small subset of wordnet nouns and showed potential applicability.', 'in the current work, we applied the method to all nouns and verbs in wordnet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'we report results of comparing our lexicon with the wordnet cousins as well as the inter - annotator disagreement observed between two semantically annotated corpora : wordnet semcor (Landes et al., 1998) and dso (Ng and Lee, 1996).', 'the results are quite promising : our extraction method discovered 89 % of the wordnet cousins, and the sense partitions in our']","['this paper, we describes a lexicon organized around systematic polysemy.', 'the lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut (Li and Abe, 1998).', 'in our previous work ( #AUTHOR_TAG ), we applied this method to a small subset of wordnet nouns and showed potential applicability.', 'in the current work, we applied the method to all nouns and verbs in wordnet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'we report results of comparing our lexicon with the wordnet cousins as well as the inter - annotator disagreement observed between two semantically annotated corpora : wordnet semcor (Landes et al., 1998) and dso (Ng and Lee, 1996).', 'the results are quite promising : our extraction method discovered 89 % of the wordnet cousins, and the sense partitions in our']","['this paper, we describes a lexicon organized around systematic polysemy.', 'the lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree - cut (Li and Abe, 1998).', 'in our previous work ( #AUTHOR_TAG ), we applied this method to a small subset of wordnet nouns and showed potential applicability.', 'in the current work, we applied the method to all nouns and verbs in wordnet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'we report results of comparing our lexicon with the wordnet cousins as well as the inter - annotator disagreement observed between two semantically annotated corpora : wordnet semcor (Landes et al., 1998) and dso (Ng and Lee, 1996).', 'the results are quite promising : our extraction method discovered 89 % of the wordnet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data']",2
"['order to obtain semantic representations of each word, we apply our previous strategy ( #AUTHOR_TAG ).', 'rather than using a termdocument matrix, we had followed an approach']","['order to obtain semantic representations of each word, we apply our previous strategy ( #AUTHOR_TAG ).', 'rather than using a termdocument matrix, we had followed an approach']","['order to obtain semantic representations of each word, we apply our previous strategy ( #AUTHOR_TAG ).', 'rather than using a termdocument matrix, we had followed an approach akin to that of schutze ( 1993 ), who performed svd on a nx2n term - term matrix.', 'the n here represents the n - 1 most - frequent words as well as a glob position to account for all other words not in the top n - 1.', ""the matrix is structured such that for a given word w's row, the first n columns denote words that - ncs ( [UNK], 1""]","['order to obtain semantic representations of each word, we apply our previous strategy ( #AUTHOR_TAG ).', 'rather than using a termdocument matrix, we had followed an approach akin to that of schutze ( 1993 ), who performed svd on a nx2n term - term matrix.', 'the n here represents the n - 1 most - frequent words as well as a glob position to account for all other words not in the top n - 1.', ""the matrix is structured such that for a given word w's row, the first n columns denote words that - ncs ( [UNK], 1""]",2
"['##poke remediation question after an incorrect user answer would be at one greater depth than the prior question ), a word occurrence vector for the automatically recognized text of the user turn, an automatic ( in ) correctness label, and lastly, the number of user turns since the last correct turn ( "" incorrect runs "" ).', 'we also included two user - based features, gender and pretest score.', 'note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( forbes - Riley and Litman , 2011 a ; #AUTHOR_TAG ), we have also experimented with other features,']","['turn number, the current itspoke question\'s name ( e. g., t 3 in figure 1 has a unique identifier ) and depth in the discourse structure ( e. g., an itspoke remediation question after an incorrect user answer would be at one greater depth than the prior question ), a word occurrence vector for the automatically recognized text of the user turn, an automatic ( in ) correctness label, and lastly, the number of user turns since the last correct turn ( "" incorrect runs "" ).', 'we also included two user - based features, gender and pretest score.', 'note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( forbes - Riley and Litman , 2011 a ; #AUTHOR_TAG ), we have also experimented with other features,']","['turn number, the current itspoke question\'s name ( e. g., t 3 in figure 1 has a unique identifier ) and depth in the discourse structure ( e. g., an itspoke remediation question after an incorrect user answer would be at one greater depth than the prior question ), a word occurrence vector for the automatically recognized text of the user turn, an automatic ( in ) correctness label, and lastly, the number of user turns since the last correct turn ( "" incorrect runs "" ).', 'we also included two user - based features, gender and pretest score.', 'note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( forbes - Riley and Litman , 2011 a ; #AUTHOR_TAG ), we have also experimented with other features, including state - of - theart acoustic - prosodic features used in the last interspeech challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009 b ) and made freely']","['train our dise model, we first extracted the set of speech and dialogue features shown in figure 2 from the user turns in our corpus.', 'as shown, the acoustic - prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'the lexical and dialogue features consist of the current dialogue name ( i. e., one of the six physics problems ) and turn number, the current itspoke question\'s name ( e. g., t 3 in figure 1 has a unique identifier ) and depth in the discourse structure ( e. g., an itspoke remediation question after an incorrect user answer would be at one greater depth than the prior question ), a word occurrence vector for the automatically recognized text of the user turn, an automatic ( in ) correctness label, and lastly, the number of user turns since the last correct turn ( "" incorrect runs "" ).', 'we also included two user - based features, gender and pretest score.', 'note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( forbes - Riley and Litman , 2011 a ; #AUTHOR_TAG ), we have also experimented with other features, including state - of - theart acoustic - prosodic features used in the last interspeech challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009 b ) and made freely available in the opensmile toolkit ( Florian et al. , 2010 ).', 'to date, however, these features have only decreased the crossvalidation performance of our models.', '8 while some of our features are tutoring - specific, these have similar counterparts in other applications ( i. e., answer ( in ) correctness corresponds to a more general notion of "" response appropriateness "" in other domains, while pretest score corresponds to the general notion of domain expertise ).', 'moreover, all of our features are fully automatic and available in real - time, so that the model can be directly implemented and deployed.', 'to that end, we now describe the results of our intrinsic and extrinsic evaluations of our dise model, aimed at determining whether it is ready to be evaluated with real users']",2
"['state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation ( Amengual et al. , 2000 ), as have been bilingual stochastic grammars ( #AUTHOR_TAG )']","['state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation ( Amengual et al. , 2000 ), as have been bilingual stochastic grammars ( #AUTHOR_TAG )']","['state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation ( Amengual et al. , 2000 ), as have been bilingual stochastic grammars ( #AUTHOR_TAG )']","['state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation ( Amengual et al. , 2000 ), as have been bilingual stochastic grammars ( #AUTHOR_TAG )']",0
['also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG )'],['also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG )'],['also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG )'],['also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG )'],5
['can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values. 3 there is no requirement that the components of f represent disjoint or statistically independent events'],['can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values. 3 there is no requirement that the components of f represent disjoint or statistically independent events'],['can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values. 3 there is no requirement that the components of f represent disjoint or statistically independent events'],['can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information - theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values. 3 there is no requirement that the components of f represent disjoint or statistically independent events'],4
['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],['describe an efficient algorithm for accomplishing this in which approximations to pst ( tis ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t'],0
['for nlp is maximum entropy / minimum divergence ( memd ) modeling ( #AUTHOR_TAG )'],['for nlp is maximum entropy / minimum divergence ( memd ) modeling ( #AUTHOR_TAG )'],['for nlp is maximum entropy / minimum divergence ( memd ) modeling ( #AUTHOR_TAG )'],['statistical technique which has recently become popular for nlp is maximum entropy / minimum divergence ( memd ) modeling ( #AUTHOR_TAG )'],5
"['( #AUTHOR_TAG ) show, lexical']","['( #AUTHOR_TAG ) show, lexical']","['( #AUTHOR_TAG ) show, lexical information improves on np and vp chunking as well']","['( #AUTHOR_TAG ) show, lexical information improves on np and vp chunking as well']",3
"['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by #AUTHOR_TAG, Collins ( 1997 ), and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by #AUTHOR_TAG, Collins ( 1997 ), and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by #AUTHOR_TAG, Collins ( 1997 ), and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by #AUTHOR_TAG, Collins ( 1997 ), and Ratnaparkhi ( 1997 ), and became a common testbed']",1
"['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), #AUTHOR_TAG, Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), #AUTHOR_TAG, Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), #AUTHOR_TAG, Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), #AUTHOR_TAG, Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']",0
"['is not aimed at handling dependencies, which require heavy use of lexical information ( #AUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information ( #AUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information ( #AUTHOR_TAG, for pp attachment )']","['is not aimed at handling dependencies, which require heavy use of lexical information ( #AUTHOR_TAG, for pp attachment )']",1
['approach for partial parsing was presented by #AUTHOR_TAG'],['approach for partial parsing was presented by #AUTHOR_TAG'],['approach for partial parsing was presented by #AUTHOR_TAG'],['approach for partial parsing was presented by #AUTHOR_TAG'],0
"['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), Collins ( 1997 ), #AUTHOR_TAG, and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), Collins ( 1997 ), #AUTHOR_TAG, and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), Collins ( 1997 ), #AUTHOR_TAG, and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), Magerman ( 1995 ), Collins ( 1997 ), #AUTHOR_TAG, and Sekine ( 1998 ) )']",0
"['results are lower than those of full parsers, e. g., #AUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']","['results are lower than those of full parsers, e. g., #AUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']","['results are lower than those of full parsers, e. g., #AUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']","['results are lower than those of full parsers, e. g., #AUTHOR_TAG as might be expected since much less structural data, and no lexical data are being used']",1
"['a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG, the method extends an existing flat shallow - parsing method to handle composite structures']",3
"['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), #AUTHOR_TAG, Collins ( 1997 ), Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), #AUTHOR_TAG, Collins ( 1997 ), Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), #AUTHOR_TAG, Collins ( 1997 ), Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']","['variety of statistical methods were proposed over the recent years for learning to produce a full parse of free - text sentences ( e. g., Bod ( 1992 ), #AUTHOR_TAG, Collins ( 1997 ), Ratnaparkhi ( 1997 ), and Sekine ( 1998 ) )']",0
"['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), Collins ( 1997 ), and #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), Collins ( 1997 ), and #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), Collins ( 1997 ), and #AUTHOR_TAG, and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), Collins ( 1997 ), and #AUTHOR_TAG, and became a common testbed']",1
"['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), #AUTHOR_TAG, and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), #AUTHOR_TAG, and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), #AUTHOR_TAG, and Ratnaparkhi ( 1997 ), and became a common testbed']","['system was trained on the penn treebank ( Marcus et al. , 1993 ) wsj sections 221 and tested on section 23 ( table 1 ), same as used by Magerman ( 1995 ), #AUTHOR_TAG, and Ratnaparkhi ( 1997 ), and became a common testbed']",1
"['approach to partial parsing was presented by #AUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by #AUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by #AUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']","['approach to partial parsing was presented by #AUTHOR_TAG, who extended a shallow - parsing technique to partial parsing']",0
"['a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ), the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ), the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ), the method extends an existing flat shallow - parsing method to handle composite structures']","['a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ), the method extends an existing flat shallow - parsing method to handle composite structures']",3
"[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences, #AUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a  b is']","[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences, #AUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a  b is']","[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences, #AUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a  b is']","[', the evidence for the first order is quite a bit stronger than the evidence for the second.', 'the first ordered pairs are more frequent, as are the individual adjectives involved.', 'to quantify the relative strengths of these transitive inferences, #AUTHOR_TAG propose to assign a weight to each link.', 'say the order a, b occurs m times and the pair { a, b } occurs n times in total.', 'then the weight of the pair a  b is']",0
"['the combined mbl method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'future work will pursue at least two directions for improving the results.', 'first, while semantic information is not available for all adjectives, it is clearly available for some.', 'furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ; #AUTHOR_TAG )']","['the combined mbl method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'future work will pursue at least two directions for improving the results.', 'first, while semantic information is not available for all adjectives, it is clearly available for some.', 'furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ; #AUTHOR_TAG )']","['the combined mbl method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'future work will pursue at least two directions for improving the results.', 'first, while semantic information is not available for all adjectives, it is clearly available for some.', 'furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself.', 'since the constraints on adjective ordering in english depend largely on semantic classes, the addition of semantic information to the model']","['the combined mbl method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'future work will pursue at least two directions for improving the results.', 'first, while semantic information is not available for all adjectives, it is clearly available for some.', 'furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'more generally, distributional clustering techniques ( sch [UNK] utze, 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself.', 'since the constraints on adjective ordering in english depend largely on semantic classes, the addition of semantic information to the model ought to improve the results']",3
"['simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']","['simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']","['simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']","['simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method.', 'to order the pair { a, b }, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often']",0
"['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'in particular, boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'in particular, boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'in particular, boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']","['second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'the technique method described in section 3. 7 is a fairly crude method for combining frequency information with symbolic data.', 'it would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'in particular, boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly']",3
"['on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a  b.', 'if the re - verse is true, and b, a is found more often than a, b, then b  a.', 'if neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', 'that is, if a  c and c  b, we can conclude that a  b.', 'to take an example from the bnc, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'however, the pairs large, new and new, green occur fairly frequently.', 'therefore, in the face of this evidence we can assign this pair the order large, green, which not coincidently is the correct english word order']","['on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a  b.', 'if the re - verse is true, and b, a is found more often than a, b, then b  a.', 'if neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', 'that is, if a  c and c  b, we can conclude that a  b.', 'to take an example from the bnc, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'however, the pairs large, new and new, green occur fairly frequently.', 'therefore, in the face of this evidence we can assign this pair the order large, green, which not coincidently is the correct english word order']","['way to think of the direct evidence method is to see that it defines a relation  on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a  b.', 'if the re - verse is true, and b, a is found more often than a, b, then b  a.', 'if neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', 'that is, if a  c and c  b, we can conclude that a  b.', 'to take an example from the bnc, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'however, the pairs large, new and new, green occur fairly frequently.', 'therefore, in the face of this evidence we can assign this pair the order large, green, which not coincidently is the correct english word order']","['way to think of the direct evidence method is to see that it defines a relation  on the set of english adjectives.', 'given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a, then a  b.', 'if the re - verse is true, and b, a is found more often than a, b, then b  a.', 'if neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation.', 'that is, if a  c and c  b, we can conclude that a  b.', 'to take an example from the bnc, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'however, the pairs large, new and new, green occur fairly frequently.', 'therefore, in the face of this evidence we can assign this pair the order large, green, which not coincidently is the correct english word order']",0
"['ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator ( #AUTHOR_TAG a""]","['ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator ( #AUTHOR_TAG a""]","['problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator ( #AUTHOR_TAG a""]","['problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""one approach to this more general problem, taken by the ` nitrogen'generator ( #AUTHOR_TAG a ; Langkilde and Knight , 1998 b ), takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model."", 'langkilde and knight report that this strategy yields good results for problems like generating verb / object collocations and for selecting the correct morphological form of a word.', 'it also should be straightforwardly applicable to the more specific problem we are addressing here.', 'to determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'this has the advantage of reducing the problem of adjective ordering to the problem of estimating n - gram probabilities, something which is relatively well understood']",5
"['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
"['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand ( #AUTHOR_TAG ) can pay off here, since only part of [UNK] may be needed subsequently.']","['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand ( #AUTHOR_TAG ) can pay off here, since only part of [UNK] may be needed subsequently.']","['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand ( #AUTHOR_TAG ) can pay off here, since only part of [UNK] may be needed subsequently.']","['traditionally log ( strength ) values are called weights, but this paper uses "" weight "" to mean something else.', '[UNK] on demand ( #AUTHOR_TAG ) can pay off here, since only part of [UNK] may be needed subsequently.']",0
"['denominator of equation ( 1 ) is the total probability of all accepting paths in x i  f  y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset  must be (  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations  and  on k. thus  is used to combine arc weights into a path weight and  is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * =  i = 0 k i.', 'the usual finite - state algorithms work if ( k, , , * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring ( r 0, +, , * ).']","['denominator of equation ( 1 ) is the total probability of all accepting paths in x i  f  y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset  must be (  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations  and  on k. thus  is used to combine arc weights into a path weight and  is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * =  i = 0 k i.', 'the usual finite - state algorithms work if ( k, , , * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring ( r 0, +, , * ).']","['denominator of equation ( 1 ) is the total probability of all accepting paths in x i  f  y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset  must be (  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations  and  on k. thus  is used to combine arc weights into a path weight and  is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * =  i = 0 k i.', 'the usual finite - state algorithms work if ( k, , , * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring ( r 0, +, , * ).']","['denominator of equation ( 1 ) is the total probability of all accepting paths in x i  f  y i.', 'but while computing this, we will also compute the numerator.', 'the idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'we will enforce an invariant : the weight of any pathset  must be (  p (  ),  p (  ) val (  ) )  r 0  v, from which ( 1 ) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite - state framework to allow this : weights may fall in any set k ( instead of r ).', 'multiplication and addition are replaced by binary operations  and  on k. thus  is used to combine arc weights into a path weight and  is used to combine the weights of alternative paths.', 'to sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k * =  i = 0 k i.', 'the usual finite - state algorithms work if ( k, , , * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring ( r 0, +, , * ). 16', 'our novel weights fall in a novel 14 formal derivation of ( 1 )']",5
"['11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ).', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs ( more generally, values in v ) forward to the probabilities.', 'this is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together']","['in many cases of interest, t i is an acyclic graph. 20', ""hen tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of  and  operations."", 'for hmms ( footnote 11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ).', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs ( more generally, values in v ) forward to the probabilities.', 'this is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together']","['11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ).', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs ( more generally, values in v ) forward to the probabilities.', 'this is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together']","['in many cases of interest, t i is an acyclic graph. 20', ""hen tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of  and  operations."", 'for hmms ( footnote 11 ), ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ).', 'but notice that it has no backward pass.', 'in place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs ( more generally, values in v ) forward to the probabilities.', 'this is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together']",0
"['availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical nlp.', 'such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation ( knight and al - Onaizan, 1998).', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical nlp.', 'such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation ( knight and al - Onaizan, 1998).', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical nlp.', 'such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation ( knight and al - Onaizan, 1998).', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical nlp.', 'such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation ( knight and al - Onaizan, 1998).', 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
"['kinds of models and training regimens.', 'for example, the forward - backward algorithm ( Baum , 1972 ) trains only hidden markov models, while ( #AUTHOR_TAG ) trains only stochastic edit distance']","['kinds of models and training regimens.', 'for example, the forward - backward algorithm ( Baum , 1972 ) trains only hidden markov models, while ( #AUTHOR_TAG ) trains only stochastic edit distance']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm ( Baum , 1972 ) trains only hidden markov models, while ( #AUTHOR_TAG ) trains only stochastic edit distance']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm ( Baum , 1972 ) trains only hidden markov models, while ( #AUTHOR_TAG ) trains only stochastic edit distance']",0
"['equivalently, by solving a sparse linear system of equations over r, a much - studied problem at o ( n ) space, o ( nm ) time, and faster approximations ( #AUTHOR_TAG )']","['equivalently, by solving a sparse linear system of equations over r, a much - studied problem at o ( n ) space, o ( nm ) time, and faster approximations ( #AUTHOR_TAG )']","['equivalently, by solving a sparse linear system of equations over r, a much - studied problem at o ( n ) space, o ( nm ) time, and faster approximations ( #AUTHOR_TAG )']","['therefore reintroduce a backward pass that lets us avoid  and  when computing t i ( so they are needed only to construct t i ).', 'this speedup also works for cyclic graphs and for any v.', 'write w jk as ( p jk, v jk ), and let w 1 jk = ( p 1 jk, v 1 jk ) denote the weight of the edge from j to k. 19 then it can be shown that w 0n = ( p 0n, j, k p 0j v 1 jk p kn ).', 'the forward and backward probabilities, p0j and pkn, can be computed using single - source algebraic path for the simpler semiring ( r, +, x, a ) - - or equivalently, by solving a sparse linear system of equations over r, a much - studied problem at o ( n ) space, o ( nm ) time, and faster approximations ( #AUTHOR_TAG )']",0
"[', express f as an fst and apply the well - known kleene - sch [UNK] utzenberger construction ( #AUTHOR_TAG ), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a']","[', express f as an fst and apply the well - known kleene - sch [UNK] utzenberger construction ( #AUTHOR_TAG ), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a']","[', express f as an fst and apply the well - known kleene - sch [UNK] utzenberger construction ( #AUTHOR_TAG ), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of']","['##to prove ( 1 ) a ( 3 ), express f as an fst and apply the well - known kleene - sch [UNK] utzenberger construction ( #AUTHOR_TAG ), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', '']",5
"['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'efficient hardware implementation is also possible via chip - level parallelism ( #AUTHOR_TAG )']","['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'efficient hardware implementation is also possible via chip - level parallelism ( #AUTHOR_TAG )']","['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'efficient hardware implementation is also possible via chip - level parallelism ( #AUTHOR_TAG )']","['and subtraction are also possible :  ( p, v ) = ( p, v ) and ( p, v ) 1 = ( p 1, p 1 vp 1 ).', 'division is commonly used in defining f  ( for normalization ). 19', 'multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'efficient hardware implementation is also possible via chip - level parallelism ( #AUTHOR_TAG )']",3
"['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a   ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given  is j, a dj, a  ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization ( #AUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a   ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given  is j, a dj, a  ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization ( #AUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a   ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given  is j, a dj, a  ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization ( #AUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']","['for per - state conditional normalization, let dj, a be the set of arcs from state j with input symbol a   ; their weights are normalized to sum to 1.', 'besides computing c, the e step must count the expected number dj, a of traversals of arcs in each dj, a.', 'then the predicted vector given  is j, a dj, a  ( expected feature counts on a randomly chosen arc in dj, a ).', 'per - state joint normalization ( #AUTHOR_TAG b, a  8. 2 ) is similar but drops the dependence on a.', 'the difficult case is global conditional normalization.', 'it arises, for example, when training a joint model of the']",1
"['em algorithm ( #AUTHOR_TAG ) can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f , which fst paths stand a chance of having been the path used?', '( guessing the path also guesses the exact input and output. )', 'the m step updates  to make those paths more likely.', 'em alternates these steps and converges to a local optimum.', ""the m step's form depends on the parameterization and the e step serves the m step's needs""]","['em algorithm ( #AUTHOR_TAG ) can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f , which fst paths stand a chance of having been the path used?', '( guessing the path also guesses the exact input and output. )', 'the m step updates  to make those paths more likely.', 'em alternates these steps and converges to a local optimum.', ""the m step's form depends on the parameterization and the e step serves the m step's needs""]","['em algorithm ( #AUTHOR_TAG ) can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f , which fst paths stand a chance of having been the path used?', '( guessing the path also guesses the exact input and output. )', 'the m step updates  to make those paths more likely.', 'em alternates these steps and converges to a local optimum.', ""the m step's form depends on the parameterization and the e step serves the m step's needs""]","['em algorithm ( #AUTHOR_TAG ) can maximize these functions.', 'roughly, the e step guesses hidden information : if ( x i, y i ) was generated from the current f , which fst paths stand a chance of having been the path used?', '( guessing the path also guesses the exact input and output. )', 'the m step updates  to make those paths more likely.', 'em alternates these steps and converges to a local optimum.', ""the m step's form depends on the parameterization and the e step serves the m step's needs""]",5
"['by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ), ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']","['by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ), ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']","['by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ), ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']","['defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways : ( 1 ) via fsts as in fig. 1c, ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ), ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem ( #AUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ ( yi _ _ ), since then the real work is done by an _ - closure step (Mohri, 2002) that implements the all - pairs version of algebraic path, whereas']","['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem ( #AUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ ( yi _ _ ), since then the real work is done by an _ - closure step (Mohri, 2002) that implements the all - pairs version of algebraic path, whereas']","['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem ( #AUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ ( yi _ _ ), since then the real work is done by an _ - closure step (Mohri, 2002) that implements the all - pairs version of algebraic path, whereas']","['for some important remarks on efficiency : a computing ti is an instance of the well - known algebraic path problem ( #AUTHOR_TAG ; tar an, 1981a ). then ti is the total semiring weight w0n of paths in ti from initial state 0 to final state n ( assumed wlog to be unique and un - weighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( _ _ xi ) _ f _ ( yi _ _ ), since then the real work is done by an _ - closure step (Mohri, 2002) that implements the all - pairs version of algebraic path, whereas all we need is the single - source version.', 'if n and m are the number of states and edges, 19 then both problems are o ( n3 ) in the worst case, but the single - source version can be solved in essentially o ( m ) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981 b ).', 'for a general graph Ti, Tarjan (1981 b ) shows how to partition into hard subgraphs that localize the cyclicity or irreducibility, then run the o ( n3 ) algorithm on each subgraph ( thereby reducing n to as little as 1 ), and recombine the results.', 'the overhead of partitioning and recombining is essentially only o ( m )']",0
['b ; #AUTHOR_TAG )'],"['in restricted cases ( mc Callum et al. , 2000 ; Eisner , 2001 b ; #AUTHOR_TAG )']","['.', 'such approaches have been tried recently in restricted cases ( mc Callum et al. , 2000 ; Eisner , 2001 b ; #AUTHOR_TAG )']","[': e , and a : ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( mc Callum et al. , 2000 ; Eisner , 2001 b ; #AUTHOR_TAG )']",0
['( #AUTHOR_TAG )'],"['- posterior estimation tries to maximize p (  )  i f  ( x i, y i ) where p (  ) is a prior probability.', 'in a log - linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG )']","['- posterior estimation tries to maximize p (  )  i f  ( x i, y i ) where p (  ) is a prior probability.', 'in a log - linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG )']","['- posterior estimation tries to maximize p (  )  i f  ( x i, y i ) where p (  ) is a prior probability.', 'in a log - linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG )']",5
"['represents a completely hidden state sequence ( cfxxx #AUTHOR_TAG, who allows any regular set ),']","['represents a completely hidden state sequence ( cfxxx #AUTHOR_TAG, who allows any regular set ),']","['represents a completely hidden state sequence ( cfxxx #AUTHOR_TAG, who allows any regular set ),']","['training data we are given a set of observed ( input, output ) pairs, ( xi, yi ).', 'these are assumed to be independent random samples from a joint dis - tribution of the form f _ ( x, y ) ; the goal is to recover the true _.', 'samples need not be fully observed ( partly supervised training ) : thus xi _ _ _, yi _ _ may be given as regular sets in which input and output were observed to fall.', 'for example, in ordinary hmm training, xi = e * and represents a completely hidden state sequence ( cfxxx #AUTHOR_TAG, who allows any regular set ), while yi is a single string representing a completely observed emission sequence.']",0
"['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( knight and al - Onaizan , 1998 )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( knight and al - Onaizan , 1998 )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( knight and al - Onaizan , 1998 )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']","['availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical nlp.', ""such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy - channel decoding,'including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( knight and al - Onaizan , 1998 )."", 'moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources']",0
"['of o ( yixe ), since then the real work is done by an c - closure step ( #AUTHOR_TAG ) that implements the all - pairs version of algebraic path, whereas']","['of o ( yixe ), since then the real work is done by an c - closure step ( #AUTHOR_TAG ) that implements the all - pairs version of algebraic path, whereas']","['of o ( yixe ), since then the real work is done by an c - closure step ( #AUTHOR_TAG ) that implements the all - pairs version of algebraic path, whereas']","['computing t i is an instance of the well - known algebraic path problem (Lehmann, 1977;Tarjan, 1981 a ).', 'let t i = x i  f  y i.', 'then t i is the total semiring weight w 0n of paths in t i from initial state 0 to final state n ( assumed wlog to be unique and unweighted ).', 'it is wasteful to compute ti as suggested earlier, by minimizing ( cxxi ) of o ( yixe ), since then the real work is done by an c - closure step ( #AUTHOR_TAG ) that implements the all - pairs version of algebraic path, whereas all we need is the single - source version.', 'if n and m are the number of states and edges, 19 then both problems are o ( n 3 ) in the worst case, but the single - source version can be solved in essentially o ( m ) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981 b ).', 'for a general graph t i, Tarjan (1981 b ) shows how to partition into "" hard "" subgraphs that localize the cyclicity or irreducibility, then run the o ( n 3 ) algorithm on each subgraph ( thereby reducing n to as little as 1 ), and recombine the results.', 'the overhead of partitioning and recombining is essentially only o ( m )']",0
"['##s that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias "" ( #AUTHOR_TAG ).', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']","['an easy approach is to normalize the options at each state to make the fst markovian.', 'unfortunately, the result may differ for equivalent fsts that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias "" ( #AUTHOR_TAG ).', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']","['an easy approach is to normalize the options at each state to make the fst markovian.', 'unfortunately, the result may differ for equivalent fsts that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias "" ( #AUTHOR_TAG ).', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']","['an easy approach is to normalize the options at each state to make the fst markovian.', 'unfortunately, the result may differ for equivalent fsts that express the same weighted relation.', 'undesirable consequences of this fact have been termed "" label bias "" ( #AUTHOR_TAG ).', 'also, in the conditional case such per - state normalization is only correct if all states accept all input suffixes ( since "" dead ends "" leak probability mass ). 8', 'a better - founded approach is global normalization, which simply divides each f ( x, y )']",0
"['brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']","['brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAG a ).', 'a leisurely journal - length version with more details has been prepared and is available']",2
"['##s ( Nederhof , 2000 ; #AUTHOR_TAG ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pc']","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic (Berstel and Reutenauer, 1988).', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs ( Nederhof , 2000 ; #AUTHOR_TAG ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]","['##as that approximate pcfgs ( Nederhof , 2000 ; #AUTHOR_TAG ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic (Berstel and Reutenauer, 1988).', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs ( Nederhof , 2000 ; #AUTHOR_TAG ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]",0
"['by compilation of weighted rewrite rules ( #AUTHOR_TAG ), ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']","['by compilation of weighted rewrite rules ( #AUTHOR_TAG ), ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']","['by compilation of weighted rewrite rules ( #AUTHOR_TAG ), ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']","['defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways : ( 1 ) via fsts as in fig. 1c, ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ), ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ), ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( gerdemann and van Noord , 1999 ), 5 ( 5 ) by conditionalization of a joint relation as discussed below']",0
"['.', 'such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001 b ; Lafferty et al. , 2001 )']","['weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001 b ; Lafferty et al. , 2001 )']","['.', 'such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001 b ; Lafferty et al. , 2001 )']","[': e , and a : ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001 b ; Lafferty et al. , 2001 )']",0
"['##s ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pc']","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic (Berstel and Reutenauer, 1988).', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]","['##as that approximate pcfgs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]","[', x, y p ( v | w ) p ( w, x ) p ( y | x ) p ( z | y ), implemented by composing 4 machines. 6, 7', 'here are also procedures for defining weighted fsts that are not probabilistic (Berstel and Reutenauer, 1988).', 'arbitrary weights such as 2. 7 may be assigned to arcs or sprinkled through a regexp ( to be compiled into : / 2. 7  arcs ).', 'a more subtle example is weighted fsas that approximate pcfgs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ), or to extend the idea, weighted fsts that approximate joint or conditional synchronous pcfgs built for translation.', ""these are parameterized by the pcfg's parameters, but add or remove strings of the pcfg to leave an improper probability distribution""]",0
"[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm ( #AUTHOR_TAG ) trains only hidden markov models, while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm ( #AUTHOR_TAG ) trains only hidden markov models, while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm ( #AUTHOR_TAG ) trains only hidden markov models, while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance']","[', there is a stumbling block : where do the weights come from?', 'after all, statistical models require supervised or unsupervised training.', 'currently, finite - state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'for example, the forward - backward algorithm ( #AUTHOR_TAG ) trains only hidden markov models, while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance']",0
"['.', 'such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAG b ; Lafferty et al. , 2001 )']","['weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAG b ; Lafferty et al. , 2001 )']","['.', 'such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAG b ; Lafferty et al. , 2001 )']","[': e , and a : ae  share a contextual "" vowel - fronting "" feature, then their weights rise and fall together with the strength of that feature.', 'the resulting machine must be normalized, either per - state or globally, to obtain a joint or a conditional distribution as desired.', 'such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAG b ; Lafferty et al. , 2001 )']",0
"['0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della #AUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['ec f ( x i, y i ), where ec ( x, y ) denotes the expected vector of total feature counts along a random path in f  whose ( input, output ) matches ( x, y ).', 'the m step then treats c as fixed, observed data and adjusts 0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della #AUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della #AUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']","['if arc probabilities ( or even , , [UNK],  ) have loglinear parameterization, then the e step must compute c = i ec f ( x i, y i ), where ec ( x, y ) denotes the expected vector of total feature counts along a random path in f  whose ( input, output ) matches ( x, y ).', 'the m step then treats c as fixed, observed data and adjusts 0 until the predicted vector of total feature counts equals c, using improved iterative scaling ( della #AUTHOR_TAG ; chen and', 'for globally normalized, joint models, the predicted vector is ec f (  *,  * ).', 'if the log - linear probabilities are conditioned on the state and / or the input, the predicted vector is harder to describe ( though usually much easier to compute ).']",5
"['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ).', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a (  ) ; reading another b (  ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x (  ). 4 to prove ( 1 )  ( 3 ), express f as an fst and apply the well - known kleene - schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of (']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ).', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a (  ) ; reading another b (  ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x (  ). 4 to prove ( 1 )  ( 3 ), express f as an fst and apply the well - known kleene - schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of ( 3 )  ( 2 ),']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ).', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a (  ) ; reading another b (  ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x (  ). 4 to prove ( 1 )  ( 3 ), express f as an fst and apply the well - known kleene - schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of (']","['central technique is to define a joint relation as a noisy - channel model, by composing a joint relation with a cascade of one or more conditional relations as in fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ).', 'the general form is illustrated by 3 conceptually, the parameters represent the probabilities of reading another a (  ) ; reading another b (  ) ; transducing b to p rather than q ( [UNK] ) ; starting to transduce p to rather than x (  ). 4 to prove ( 1 )  ( 3 ), express f as an fst and apply the well - known kleene - schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'a full proof is straightforward, as are proofs of ( 3 )  ( 2 ), ( 2 )  ( 1 )']",0
"['( genpept [ 11 ], refseq #AUTHOR_TAG, and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) [ 14 ], and', 'additionally, several model organism']","['( genpept [ 11 ], refseq #AUTHOR_TAG, and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) [ 14 ], and', 'additionally, several model organism']","['( genpept [ 11 ], refseq #AUTHOR_TAG, and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) [ 14 ], and', 'additionally, several model organism']","['system utilizes several large size biological databases including three ncbi databases ( genpept [ 11 ], refseq #AUTHOR_TAG, and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) [ 14 ], and', 'additionally, several model organism databases or nomenclature databases were used.', 'correspondences among records from these databases are identified using the rich cross - reference information provided by the iproclass database of pir [ 14 ].', 'the following provides a brief description of each of the database']",5
"[') #AUTHOR_TAG, rat - - rat genome database ( rgd ) [ 21']","[') #AUTHOR_TAG, rat - - rat genome database ( rgd ) [ 21']","[') #AUTHOR_TAG, rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) #AUTHOR_TAG, rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[') #AUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome database (']","['or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) #AUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome database (']","[') #AUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome database (']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) #AUTHOR_TAG, fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[', worm - - wormbase #AUTHOR_TAG, human nomenclature database']","[', worm - - wormbase #AUTHOR_TAG, human nomenclature database']","[') [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase #AUTHOR_TAG, human nomenclature database']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase #AUTHOR_TAG, human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"['the unified medical language system ( umls ) has been developed and maintained by national library of medicine ( nlm ) #AUTHOR_TAG.', 'it contains three knowledge']","['the unified medical language system ( umls ) has been developed and maintained by national library of medicine ( nlm ) #AUTHOR_TAG.', 'it contains three knowledge sources : the metathesaurus ( meta ), the specialist lexicon, and the semantic network.', 'the meta provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'the specialist']","['the unified medical language system ( umls ) has been developed and maintained by national library of medicine ( nlm ) #AUTHOR_TAG.', 'it contains three knowledge']","['umls - - the unified medical language system ( umls ) has been developed and maintained by national library of medicine ( nlm ) #AUTHOR_TAG.', 'it contains three knowledge sources : the metathesaurus ( meta ), the specialist lexicon, and the semantic network.', 'the meta provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'the specialist lexicon contains syntactic information for many terms, component words, and english words, including verbs, which do not appear in the meta.', 'the semantic network contains information about the types or categories ( e. g., "" disease or syndrome "", "" virus "" ) to which all meta concepts have been assigned']",0
"['the use of computers in storing the explosive amount of biological information, natural language processing ( nlp ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely : synonymy ( i. e., different terms refer to the same entity ), ambiguity ( i. e., one term is associated with different entities ), and coverage ( i. e., entity terms or entities are not present in']","['the use of computers in storing the explosive amount of biological information, natural language processing ( nlp ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely : synonymy ( i. e., different terms refer to the same entity ), ambiguity ( i. e., one term is associated with different entities ), and coverage ( i. e., entity terms or entities are not present in']","['the use of computers in storing the explosive amount of biological information, natural language processing ( nlp ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely : synonymy ( i. e., different terms refer to the same entity ), ambiguity ( i. e., one term is associated with different entities ), and coverage ( i. e., entity terms or entities are not present in']","['the use of computers in storing the explosive amount of biological information, natural language processing ( nlp ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG.', 'one requirement for nlp is the ability to accurately recognize terms that represent biological entities in free text.', 'another requirement is the ability to associate these terms with corresponding biological entities ( i. e., records in biological databases ) in order to be used by other automated systems for literature mining.', 'such task is called biological entity tagging.', 'biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely : synonymy ( i. e., different terms refer to the same entity ), ambiguity ( i. e., one term is associated with different entities ), and coverage ( i. e., entity terms or entities are not present in databases or knowledge bases )']",0
"['hugo ) [ 23 ], online mendelian inheritance in man ( omim ) #AUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26']","['hugo ) [ 23 ], online mendelian inheritance in man ( omim ) #AUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26']","['hugo ) [ 23 ], online mendelian inheritance in man ( omim ) #AUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) #AUTHOR_TAG, and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[', refseq, genpept, and pdb.', 'three uniref tables uniref100, uniref90 and uniref50 ) are available for download : uniref100 combines identical sequences and sub - fragments into a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity, respectively, to the representative sequence.', '']","['resources - there are three databases in pir : the protein sequence database ( psd ), iproclass, and pir - nref.', 'psd database includes functionally annotated protein sequences.', 'the iproclass database is a central point for exploration of protein information, which provides summary descriptions of protein family, function and structure for all protein sequences from pir, swiss - prot, and trembl ( now uniprot ).', 'additionally, it links to over 70 biological databases in the world.', 'the pir - nref database is a comprehensive database for sequence searching and protein identification.', 'it contains non - redundant protein sequences from psd, swiss - prot, trembl, refseq, genpept, and pdb.', 'three uniref tables uniref100, uniref90 and uniref50 ) are available for download : uniref100 combines identical sequences and sub - fragments into a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity, respectively, to the representative sequence.', '']","['( now uniprot ).', 'additionally, it links to over 70 biological databases in the world.', 'the pir - nref database is a comprehensive database for sequence searching and protein identification.', 'it contains non - redundant protein sequences from psd, swiss - prot, trembl, refseq, genpept, and pdb.', 'three uniref tables uniref100, uniref90 and uniref50 ) are available for download : uniref100 combines identical sequences and sub - fragments into a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity, respectively, to the representative sequence.', '']","['resources - there are three databases in pir : the protein sequence database ( psd ), iproclass, and pir - nref.', 'psd database includes functionally annotated protein sequences.', 'the iproclass database is a central point for exploration of protein information, which provides summary descriptions of protein family, function and structure for all protein sequences from pir, swiss - prot, and trembl ( now uniprot ).', 'additionally, it links to over 70 biological databases in the world.', 'the pir - nref database is a comprehensive database for sequence searching and protein identification.', 'it contains non - redundant protein sequences from psd, swiss - prot, trembl, refseq, genpept, and pdb.', 'three uniref tables uniref100, uniref90 and uniref50 ) are available for download : uniref100 combines identical sequences and sub - fragments into a single uniref entry ; and uniref90 and uniref50 are built by clustering uniref100 sequences into clusters based on the cd - hit algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity, respectively, to the representative sequence.', 'ncbi resources - three data sources from ncbi were used in this study : genpept, refseq, and entrez gene.', 'genpept entries are those translated from the genbanknucleotide sequence database.', 'refseq is a comprehensive, integrated, non - redundant set of sequences, including genomic dna, transcript ( rna ), and protein products, for major research organisms.', ""entrez gene provides a unified query environment for genes defined by sequence and / or in ncbi's map viewer."", 'it records gene names, symbols, and many other attributes associated with genes and the products they encode']",5
"[', rat - - rat genome database ( rgd ) #AUTHOR_TAG, worm - - wormbase [ 22']","[', rat - - rat genome database ( rgd ) #AUTHOR_TAG, worm - - wormbase [ 22']","[') [ 20 ], rat - - rat genome database ( rgd ) #AUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) #AUTHOR_TAG, worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"['( genpept [ 11 ], refseq [ 12 ], and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) #AUTHOR_TAG, and', 'additionally, several model organism']","['( genpept [ 11 ], refseq [ 12 ], and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) #AUTHOR_TAG, and', 'additionally, several model organism']","['( genpept [ 11 ], refseq [ 12 ], and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) #AUTHOR_TAG, and', 'additionally, several model organism']","['system utilizes several large size biological databases including three ncbi databases ( genpept [ 11 ], refseq [ 12 ], and entrez gene [ 13 ] ), psd database from protein information resources ( pir ) #AUTHOR_TAG, and', 'additionally, several model organism databases or nomenclature databases were used.', 'correspondences among records from these databases are identified using the rich cross - reference information provided by the iproclass database of pir [ 14 ].', 'the following provides a brief description of each of the database']",5
"[', fly flybase #AUTHOR_TAG, yeast saccharomyces genome database (']","[', fly flybase #AUTHOR_TAG, yeast saccharomyces genome database (']","[') [ 18 ], fly flybase #AUTHOR_TAG, yeast saccharomyces genome database (']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase #AUTHOR_TAG, yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) [ 25, 26 ]']",5
"[', human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) #AUTHOR_TAG']","[', human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) #AUTHOR_TAG']","[') [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) #AUTHOR_TAG']","['molecular biology databases we also included several model organism databases or nomenclature databases in the construction of the dictionary, i. e., mouse mouse genome database ( mgd ) [ 18 ], fly flybase [ 19 ], yeast saccharomyces genome database ( sgd ) [ 20 ], rat - - rat genome database ( rgd ) [ 21 ], worm - - wormbase [ 22 ], human nomenclature database ( hugo ) [ 23 ], online mendelian inheritance in man ( omim ) [ 24 ], and enzyme nomenclature database ( ecnum ) #AUTHOR_TAG']",5
"['our experiments, we use naive bayes as the learning algorithm.', 'the knowledge sources we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work ( #AUTHOR_TAG )']","['our experiments, we use naive bayes as the learning algorithm.', 'the knowledge sources we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work ( #AUTHOR_TAG )']","['our experiments, we use naive bayes as the learning algorithm.', 'the knowledge sources we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work ( #AUTHOR_TAG )']","['our experiments, we use naive bayes as the learning algorithm.', 'the knowledge sources we use include parts - of - speech, local collocations, and surrounding words.', 'these knowledge sources were effectively used to build a state - of - the - art wsd program in one of our prior work ( #AUTHOR_TAG )']",2
"['., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov ( 2002 ) ), #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexical']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov ( 2002 ) ), #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city,']","['., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov ( 2002 ) ), #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city,']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov ( 2002 ) ), #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. (2005) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. (2004) ) or wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an np ( see Bean and Riloff (2004) )']",0
"['knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced']","['knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced']","['with their scs.', 'this provides us with a train - ing set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']","['ace participants have also adopted a corpus - based approach to sc deter - mination that is investigated as part of the mention detection ( md ) task ( e. g., Florian et al. (2006) ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'un - like them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowl - edge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc clas - sifier ; instead, we use the bbn entity type corpus (Weischedel and Brunstein, 2005), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and an - notated with their scs.', 'this provides us with a train - ing set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., #AUTHOR_TAG ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",1
"['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov ( 1998 ), #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexical']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov ( 1998 ), #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city,']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov ( 1998 ), #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city,']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov ( 1998 ), #AUTHOR_TAG ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. (2005) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. (2004) ) or wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an np ( see Bean and Riloff (2004) )']",0
"['., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus ( #AUTHOR_TAG ), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., Luo et al. (2004) ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g., Florian et al. (2006) ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus ( #AUTHOR_TAG ), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., Luo et al. (2004) ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']","['., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus ( #AUTHOR_TAG ), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., Luo et al. (2004) ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g., Florian et al. (2006) ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus ( #AUTHOR_TAG ), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., Luo et al. (2004) ) ; and for those that do report per - formance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",5
"['., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see #AUTHOR_TAG ), Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexical']","['Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see #AUTHOR_TAG ), Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city,']","['., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see #AUTHOR_TAG ), Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city,']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see #AUTHOR_TAG ), Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. (2005) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. (2004) ) or wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an np ( see Bean and Riloff (2004) )']",0
"['( 2 ) apply the four classifiers and soon et al. s method to independently make predictions for the remaining 20 % of the instances ; and ( 3 ) train an svm classifier ( using the libsvm package ( #AUTHOR_TAG ) ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more specifically, let l =']","['addition, we train an svm classifier for sc determination by combining the output of five clas - sification methods : dl, 1 - nn, me, nb, and soon et al. s method as described in the introduction, 8 with the goal of examining whether sc classifica - tion accuracy can be improved by combining the output of individual classifiers in a supervised man - ner.', 'specifically, we ( 1 ) use 80 % of the instances generated from the bbn entity type corpus to train the four classifiers ; ( 2 ) apply the four classifiers and soon et al. s method to independently make predictions for the remaining 20 % of the instances ; and ( 3 ) train an svm classifier ( using the libsvm package ( #AUTHOR_TAG ) ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more specifically, let l =']","['addition, we train an svm classifier for sc determination by combining the output of five clas - sification methods : dl, 1 - nn, me, nb, and soon et al. s method as described in the introduction, 8 with the goal of examining whether sc classifica - tion accuracy can be improved by combining the output of individual classifiers in a supervised man - ner.', 'specifically, we ( 1 ) use 80 % of the instances generated from the bbn entity type corpus to train the four classifiers ; ( 2 ) apply the four classifiers and soon et al. s method to independently make predictions for the remaining 20 % of the instances ; and ( 3 ) train an svm classifier ( using the libsvm package ( #AUTHOR_TAG ) ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more specifically, let l = i']","['addition, we train an svm classifier for sc determination by combining the output of five clas - sification methods : dl, 1 - nn, me, nb, and soon et al. s method as described in the introduction, 8 with the goal of examining whether sc classifica - tion accuracy can be improved by combining the output of individual classifiers in a supervised man - ner.', 'specifically, we ( 1 ) use 80 % of the instances generated from the bbn entity type corpus to train the four classifiers ; ( 2 ) apply the four classifiers and soon et al. s method to independently make predictions for the remaining 20 % of the instances ; and ( 3 ) train an svm classifier ( using the libsvm package ( #AUTHOR_TAG ) ) on these 20 % of the instances, where each instance, i, is represented by a per org gpe fac loc oth training test 19. 8 9. 6 11. 4 1. 6 1. 2 56. 3 19. 5 9. 0 9. 6 1. 8 1. 1 59. 0 set of 31 binary features.', 'more specifically, let l = i li ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step ( 2 ).', 'to represent i, we generate one feature from each non - empty subset of li']",5
"[""4 ) ne : we use bbn's identifinder ( #AUTHOR_TAG ), a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']","[""4 ) ne : we use bbn's identifinder ( #AUTHOR_TAG ), a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']","[""4 ) ne : we use bbn's identifinder ( #AUTHOR_TAG ), a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']","[""4 ) ne : we use bbn's identifinder ( #AUTHOR_TAG ), a muc - style ne recognizer to determine the ne type of npz."", 'if npi is determined to be a person or organization, we create an ne feature whose value is simply its muc ne type.', 'however, if npi is determined to be a location, we create a feature with value gpe ( because most of the muc loca - tion nes are ace gpe nes ).', 'otherwise, no ne feature will be created ( because we are not interested in the other muc ne types )']",5
"['. g., Soon et al. ( 2001 ), #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g., Soon et al. ( 2001 ), #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['. g., Soon et al. ( 2001 ), #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g., Soon et al. ( 2001 ), #AUTHOR_TAG ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]",0
"['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in #AUTHOR_TAG, motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']",5
"['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer ( #AUTHOR_TAG ), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps']","['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer ( #AUTHOR_TAG ), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps']","['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer ( #AUTHOR_TAG ), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps']","['in sc induction, we use the ace phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer ( #AUTHOR_TAG ), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, np i, correctly resolved if np i and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps automatically extracted by an in - house np chunker and identifinder']",5
"['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following #AUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']","['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following #AUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']","['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following #AUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']","['training, the decision tree classifier is used to select an antecedent for each np in a test text.', 'following #AUTHOR_TAG, we select as the antecedent of each np, npj, the closest preceding np that is classified as coreferent with npj.', 'if no such np exists, no antecedent is selected for npj']",4
"['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g., #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e.']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g., #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus (Weischedel and Brunstein, 2005), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g., #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e.']","['ace participants have also adopted a corpus - based approach to sc determination that is investigated as part of the mention detection ( md ) task ( e. g., #AUTHOR_TAG ).', 'briefly, the goal of md is to identify the boundary of a mention, its mention type ( e. g., pronoun, name ), and its semantic type ( e. g., person, location ).', 'unlike them, ( 1 ) we do not perform the full md task, as our goal is to investigate the role of sc knowledge in coreference resolution ; and ( 2 ) we do not use the ace training data for acquiring our sc classifier ; instead, we use the bbn entity type corpus (Weischedel and Brunstein, 2005), which consists of all the penn treebank wall street journal articles with the ace mentions manually identified and annotated with their scs.', 'this provides us with a training set that is approximately five times bigger than that of ace.', 'more importantly, the ace participants do not evaluate the role of induced sc knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e. g., Luo et al. (2004) ) ; and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced sc information is used in their coreference algorithms']",1
"['., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG, as described below']","['baseline coreference system uses the c4. 5 deci - sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG, as described below']","['., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG, as described below']","['baseline coreference system uses the c4. 5 deci - sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj positional features that have been employed by high - performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG, as described below']",1
"['baseline coreference system uses the c4. 5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work (']","['baseline coreference system uses the c4. 5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, np j, and its closest antecedent, np i ; and a negative instance is created for np j paired with each of the intervening nps, np i + 1, np i + 2,..', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work (']","['baseline coreference system uses the c4. 5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, np j, and its closest antecedent, np i ; and a negative instance is created for np j paired with each of the intervening nps, np i + 1, np i + 2,..', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']",5
"['., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below']","['., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi +, npi + 2,, npj.', 'each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high - performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below']",1
"['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer (Vilain et al., 1995), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following #AUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps']","['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer (Vilain et al., 1995), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following #AUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps']","['output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer (Vilain et al., 1995), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following #AUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps']","['in sc induction, we use the ace phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'we report performance in terms of two metrics : ( 1 ) the fmeasure score as computed by the commonly - used muc scorer (Vilain et al., 1995), and ( 2 ) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'following #AUTHOR_TAG, we consider an anaphoric reference, npi, correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition.', 'in all of our experiments, we use nps automatically extracted by an in - house np chunker and identifinder']",5
"['( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see #AUTHOR_TAG )']","['( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see #AUTHOR_TAG )']","['( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see #AUTHOR_TAG )']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. ( 2005 ) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. ( 2004 ) ) or wikipedia ( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see #AUTHOR_TAG )']",0
"['. g., #AUTHOR_TAG, Markert and Nissim ( 2005 ) ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g., #AUTHOR_TAG, Markert and Nissim ( 2005 ) ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['. g., #AUTHOR_TAG, Markert and Nissim ( 2005 ) ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]","['type of semantic knowledge that has been employed by coreference resolvers is the semantic class ( sc ) of an np, which can be used to disallow coreference between semantically incompatible nps.', 'however, learning - based resolvers have not been able to benefit from having an sc agreement feature, presumably because the method used to compute the sc of an np is too simplistic : while the sc of a proper name is computed fairly accurately using a named entity ( ne ) recognizer, many resolvers simply assign to a common noun the first ( i. e., most frequent ) wordnet sense as its sc ( e. g., #AUTHOR_TAG, Markert and Nissim ( 2005 ) ).', ""it is not easy to measure the accuracy of this heuristic, but the fact that the sc agreement feature is not used by soon et al.'s decision tree coreference classifier seems to suggest that the sc values of the nps are not computed accurately by this first - sense heuristic""]",0
"['. g., #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']","['. g., #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']","['baseline coreference system uses the c4. 5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two nps are coreferent.', 'following previous work ( e. g., #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ), we generate training instances as follows : a positive instance is created for each anaphoric np, npj, and its closest antecedent, npi ; and a negative instance is created for npj paired with each of the intervening nps, npi + 1, npi + 2,..., npj _ 1.', '., np j1.', 'each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below']",5
"['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in Collins and Singer ( 1999 ), motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in Collins and Singer ( 1999 ), motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in Collins and Singer ( 1999 ), motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']","['algorithms.', 'we experiment with four learners commonly employed in language learning : decision list ( dl ) : we use the dl learner as described in Collins and Singer ( 1999 ), motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and ne classification ( Collins and Singer , 1999 ).', 'we apply add - one smoothing to smooth the class posteriors']",4
"['., Poesio et al. ( 2004 ) ) or wikipedia ( #AUTHOR_TAG ), and the contextual role played by an np ( see Bean and Riloff ( 2004 )']","[', Ji et al. ( 2005 ) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. ( 2004 ) ) or wikipedia ( #AUTHOR_TAG ), and the contextual role played by an np ( see Bean and Riloff ( 2004 )']","['., Poesio et al. ( 2004 ) ) or wikipedia ( #AUTHOR_TAG ), and the contextual role played by an np ( see Bean and Riloff ( 2004 )']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., Ji et al. ( 2005 ) ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. ( 2004 ) ) or wikipedia ( #AUTHOR_TAG ), and the contextual role played by an np ( see Bean and Riloff ( 2004 ) )']",0
"[', #AUTHOR_TAG ), their semantic similarity as computed using wordnet (']","[', #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. ( 2004 ) ) or wikipedia ( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see Bean and Riloff ( 2004 )']","[', #AUTHOR_TAG ), their semantic similarity as computed using wordnet (']","['the past decade, knowledge - lean approaches have significantly influenced research in noun phrase ( np ) coreference resolution - the problem of determining which nps refer to the same real - world entity in a document.', 'in knowledge - lean approaches, coreference resolvers employ only morpho - syntactic cues as knowledge sources in the resolution process ( e. g., Mitkov (1998), Tetreault (2001) ).', 'while these approaches have been reasonably successful ( see Mitkov (2002) ), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'in fact, semantics plays a crucially important role in the resolution of common nps, allowing us to identify the coreference relation between two lexically dissimilar common nouns ( e. g., talks and negotiations ) and to eliminate george w. bush from the list of candidate antecedents of the city, for instance.', 'as a result, researchers have re - adopted the once - popular knowledge - rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two nps ( e. g., #AUTHOR_TAG ), their semantic similarity as computed using wordnet ( e. g., Poesio et al. ( 2004 ) ) or wikipedia ( Ponzetto and Strube , 2006 ), and the contextual role played by an np ( see Bean and Riloff ( 2004 ) )']",0
"['by research in lexical semantics ( e. g., Hearst (1992) ).', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', 'an example extraction would be < eastern airlines, the carrier >, where the first entry is a proper noun labeled with either one of the seven muc - style ne types 4 or others 5 and the second entry is a common noun.', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see #AUTHOR_TAG']","['by research in lexical semantics ( e. g., Hearst (1992) ).', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', 'an example extraction would be < eastern airlines, the carrier >, where the first entry is a proper noun labeled with either one of the seven muc - style ne types 4 or others 5 and the second entry is a common noun.', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see #AUTHOR_TAG']","['others appear to be correlated with these ace scs. 2 ( 6 ) induced class : since the first - sense heuristic used in the previous feature may not be accurate in capturing the sc of an np, we employ a corpusbased method for inducing scs that is motivated by research in lexical semantics ( e. g., Hearst (1992) ).', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', 'an example extraction would be < eastern airlines, the carrier >, where the first entry is a proper noun labeled with either one of the seven muc - style ne types 4 or others 5 and the second entry is a common noun.', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see #AUTHOR_TAG']","['3 ) verb obj : a verb obj feature is created in a similar fashion as subj verb if np i participates in a verb - object relation.', 'again, this represents our attempt to coarsely model subcategorization.', ""( 4 ) ne : we use bbn's identifinder (Bikel et al., 1999) ( 5 ) wn class : for each keyword w shown in the right column of table 1, we determine whether the head noun of np i is a hyponym of w in wordnet, using only the first wordnet sense of np i. 1 if so, we create a wn class feature with w as its value."", 'these keywords are potentially useful features because some of them are subclasses of the ace scs shown in the left column of table 1, while others appear to be correlated with these ace scs. 2 ( 6 ) induced class : since the first - sense heuristic used in the previous feature may not be accurate in capturing the sc of an np, we employ a corpusbased method for inducing scs that is motivated by research in lexical semantics ( e. g., Hearst (1992) ).', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', 'an example extraction would be < eastern airlines, the carrier >, where the first entry is a proper noun labeled with either one of the seven muc - style ne types 4 or others 5 and the second entry is a common noun.', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see #AUTHOR_TAG a ) ).', ""motivated by this observation, we create for each of np i's ten most semantically similar nps a neigh - bor feature whose value is the surface string of the np."", ""to determine the ten nearest neighbors, we use the semantic similarity values provided by lin's dependency - based thesaurus, which is constructed using a distributional approach combined with an information - theoretic definition of similarity""]",4
"[', #AUTHOR_TAG ).', 'given a large, unann']","[', #AUTHOR_TAG ).', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', 'an example extraction would be < eastern airlines, the carrier >, where the first entry is a proper noun labeled with either one of the seven muc - style ne types 4 or others 5 and the second entry is a common noun.', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see Lin (1998']","[', #AUTHOR_TAG ).', 'given a large, unann']","['3 ) verb obj : a verb obj feature is created in a similar fashion as subj verb if np i participates in a verb - object relation.', 'again, this represents our attempt to coarsely model subcategorization.', ""( 4 ) ne : we use bbn's identifinder (Bikel et al., 1999) ( 5 ) wn class : for each keyword w shown in the right column of table 1, we determine whether the head noun of np i is a hyponym of w in wordnet, using only the first wordnet sense of np i. 1 if so, we create a wn class feature with w as its value."", 'these keywords are potentially useful features because some of them are subclasses of the ace scs shown in the left column of table 1, while others appear to be correlated with these ace scs. 2 ( 6 ) induced class : since the first - sense heuristic used in the previous feature may not be accurate in capturing the sc of an np, we employ a corpusbased method for inducing scs that is motivated by research in lexical semantics ( e. g., #AUTHOR_TAG ).', 'given a large, unannotated corpus 3, we use identi - finder to label each ne with its ne type and mini - par to extract all the appositive relations.', 'an example extraction would be < eastern airlines, the carrier >, where the first entry is a proper noun labeled with either one of the seven muc - style ne types 4 or others 5 and the second entry is a common noun.', 'we then infer the sc of a common noun as follows : ( 1 ) we compute the probability that the common noun co - occurs with each of the eight ne types 6 based on the extracted appositive relations, and ( 2 ) if the most likely ne type has a co - occurrence probability above a certain threshold ( we set it to 0. 7 ), we create a induced class fea - ture for np i whose value is the most likely ne type.', '( 7 ) neighbor : research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps ( see Lin (1998 a ) ).', ""motivated by this observation, we create for each of np i's ten most semantically similar nps a neigh - bor feature whose value is the surface string of the np."", ""to determine the ten nearest neighbors, we use the semantic similarity values provided by lin's dependency - based thesaurus, which is constructed using a distributional approach combined with an information - theoretic definition of similarity""]",4
"['communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between']","['communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between']","['and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme ( #AUTHOR_TAG ).', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin attributes has been used, i. e.', 'smile, laughter, scowl, faceother for facial expressions, and nod, jerk, tilt, sideturn, shake, waggle, other for head movements']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme ( #AUTHOR_TAG ).', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin attributes has been used, i. e.', 'smile, laughter, scowl, faceother for facial expressions, and nod, jerk, tilt, sideturn, shake, waggle, other for head movements']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme ( #AUTHOR_TAG ).', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin attributes has been used, i. e.', 'smile, laughter, scowl, faceother for facial expressions, and nod, jerk, tilt, sideturn, shake, waggle, other for head movements']","['communicative head gestures in the videos were found and annotated with anvil using a subset of the attributes defined in the mumin annotation scheme ( #AUTHOR_TAG ).', 'the mu - min scheme is a general framework for the study of gestures in interpersonal communication.', 'in this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'therefore, only a subset of the mumin attributes has been used, i. e.', 'smile, laughter, scowl, faceother for facial expressions, and nod, jerk, tilt, sideturn, shake, waggle, other for head movements']",5
"['- damsl corpus using lexical, syntactic and prosodic cues, while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task -']","['act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between']","['communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between']","['and eye gaze in interaction with embodied communication agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG'],['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG'],['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG'],"['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"['an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['general, dialogue act, agreement and turn anno - tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']",5
"['dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, pros']","['corpus of task - oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system ( #AUTHOR_TAG ).', 'we experiment']","['we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system ( #AUTHOR_TAG ).', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our']","['we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system ( #AUTHOR_TAG ).', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our']","['multimodal data we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system ( #AUTHOR_TAG ).', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb ) (Zhang et al., 2005).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']",5
"['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']","['general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'however, one dialogue was coded independently and in parallel by two expert annotators to measure inter - coder agreement.', 'a measure was derived for each annotated feature using the agreement analysis facility provided in anvil.', ""agreement between two annotation sets is calculated here in terms of cohen's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2."", 'anvil divides the annotations in slices and compares each slice.', 'we used slices of 0. 04 seconds.', 'the inter - coder agreement figures obtained for the three types of annotation are given in table 2']",5
"['communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between']","['communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between']","['and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",0
"['.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annot']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG, it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']",0
"[').', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG :']","['overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG :']","['an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG :']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc Clave (2000) for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"[').', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) :']","['overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) :']","['an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) :']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc Clave (2000) for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus']",0
"[').', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) :']","['overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) :']","['an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) :']","['authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see mc Clave (2000) for an overview ).', 'others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'for example, Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'related are also the studies by rieks op den Akker and Schulz (2008) and Murray and Renals (2008) : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi - modal corpus']",0
"['.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG )."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annot']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG )."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG )."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators']","['should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'for example, a yes can be an answer to a question, an agree and a turnelicit at the same time, thus making the semantic classification very fine - grained.', ""table 1 although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG )."", 'looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by anvil from the latest annotated element']",0
"['.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ), but are still sat - isfactory given the high number of categories provided by the scheme']","['non expert annotators ( one annotator per video ) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ), but are still sat - isfactory given the high number of categories provided by the scheme']","['non expert annotators ( one annotator per video ) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ), but are still sat - isfactory given the high number of categories provided by the scheme']","['head gestures in the danpass data have been coded by non expert annotators ( one annotator per video ) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'the annotations of this video were then used to measure inter - coder agreement in anvil as it was the case for the annotations on feedback expressions.', 'in the case of gestures we also measured agreement on gesture segmentation.', 'the figures obtained are given in table 3.', 'these results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ), but are still sat - isfactory given the high number of categories provided by the scheme']",1
"['in #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grain']","['in #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed']","['in #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed']","['has also been done on prosody and gestures in the specific domain of map - task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the switchboard - damsl corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated english map - task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'finally, feedback expressions ( head nods and shakes ) are successfully predicted from speech, prosody and eye gaze in interaction with embodied communication agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'in this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'our data are made up by a collection of eight video - recorded map - task dialogues in danish, which were annotated with phonetic and prosodic information.', 'we find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'the results, which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG, must be seen in light of the fact that our gesture annotation scheme comprises more fine - grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'the classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category']",1
"['were obtained using hidden naive bayes ( hnb ) ( #AUTHOR_TAG ).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['were obtained using hidden naive bayes ( hnb ) ( #AUTHOR_TAG ).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['were obtained using hidden naive bayes ( hnb ) ( #AUTHOR_TAG ).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']","['multimodal data we obtained by combining the linguistic annotations from danpass with the gesture annotation created in anvil, resulted into two different groups of data, one containing all yes and no expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in table 4.', 'these two sets of data were used for automatic dialogue act classification, which was run in the weka system (Witten and Frank, 2005).', 'we experimented with various weka classifiers, comprising hidden naive bayes, smo, id3, ladtree and decision table.', 'the best results on most of our data were obtained using hidden naive bayes ( hnb ) ( #AUTHOR_TAG ).', 'therefore, here we show the results of this classifier.', 'ten - folds crossvalidation was applied throughout']",5
"['##ssed ; 7 % of them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil ( #AUTHOR_TAG ).', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging iso 24617']","['in danpass are phonetically and prosodically annotated.', 'in the subset of the corpus considered here, 82 % of the feedback expressions bear stress or tone information, and 12 % are unstressed ; 7 % of them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil ( #AUTHOR_TAG ).', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging iso 24617 - 2 standard for semantic annotation of language resources.', 'this']","['##ssed ; 7 % of them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil ( #AUTHOR_TAG ).', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging iso 24617']","['already mentioned, all words in danpass are phonetically and prosodically annotated.', 'in the subset of the corpus considered here, 82 % of the feedback expressions bear stress or tone information, and 12 % are unstressed ; 7 % of them are marked with onset or offset hesitation, or both.', 'for this study, we added semantic labels - including dialogue acts - and gesture annotation.', 'both kinds of annotation were carried out using anvil ( #AUTHOR_TAG ).', 'to distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging iso 24617 - 2 standard for semantic annotation of language resources.', 'this subset comprises the categories accept, decline, repeatrephrase and answer.', 'moreover, all feedback expressions were annotated with an agreement feature ( agree, nonagree ) where relevant.', 'finally, the two turn management categories turn - take and turnelicit were also coded']",5
"['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used ( #AUTHOR_TAG )']","['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used ( #AUTHOR_TAG )']","['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used ( #AUTHOR_TAG )']","['and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see grnnum ( 2006 ).', 'the praat tool was used ( #AUTHOR_TAG )']",5
"['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on #AUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008)']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on #AUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008)']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on #AUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008)']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on #AUTHOR_TAG b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008)']",2
"['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['to the surface realizer based on the strategy decision, and a fuf / surge ( #AUTHOR_TAG ) generation system to produce']","['to the surface realizer based on the strategy decision, and a fuf / surge ( #AUTHOR_TAG ) generation system to produce']","['to the surface realizer based on the strategy decision, and a fuf / surge ( #AUTHOR_TAG ) generation system to produce']","[""strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer ( e. g., part of the answer to confirm ), is passed to content planning and generation."", 'the system uses a domain - specific content planner to produce input to the surface realizer based on the strategy decision, and a fuf / surge ( #AUTHOR_TAG ) generation system to produce the appropriate text.', 'templates are used to generate some stock phrases such as "" when you are ready, go on to the next slide.']",5
"[""indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","[""indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']",3
"['student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ).', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with']","['between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ).', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with']","['between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ).', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student']","['between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ).', 'the dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not - yet - mentioned parts of the answer.', 'once the complete answer has been accumulated, the system accepts it and moves on.', 'tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student']",5
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completi']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"[""indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG )."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","[""indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG )."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG )."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']","['dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""the analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example, using better terminology ) but doesn't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG )."", 'results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail']",3
"['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completi']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ).', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp (Hockey et al., 2003) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ).', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp (Hockey et al., 2003) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ).', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp (Hockey et al., 2003) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ).', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp (Hockey et al., 2003) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']",5
"['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG, and an ontology mapping mechanism ( Dzikovska et al. , 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG, and an ontology mapping mechanism ( Dzikovska et al. , 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG, and an ontology mapping mechanism ( Dzikovska et al. , 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG, and an ontology mapping mechanism ( Dzikovska et al. , 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']",1
"['beetle ii system architecture is designed to overcome these limitations ( #AUTHOR_TAG ).', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more""]","['beetle ii system architecture is designed to overcome these limitations ( #AUTHOR_TAG ).', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more""]","['beetle ii system architecture is designed to overcome these limitations ( #AUTHOR_TAG ).', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning""]","['beetle ii system architecture is designed to overcome these limitations ( #AUTHOR_TAG ).', 'it uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'this allows the system to consistently apply the same tutorial policy across a range of questions.', 'to some extent, this comes at the expense of being able to address individual student misconceptions.', ""however, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning""]",0
"[""), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #AUTHOR_TAG']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on Dzikovska et al. (2008 b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #AUTHOR_TAG']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on Dzikovska et al. (2008 b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #AUTHOR_TAG']","['knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""the diagnoser, based on Dzikovska et al. (2008 b ), outputs a diagnosis which consists of lists of correct, contradictory and non - mentioned objects and relations from the student's answer."", 'at present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #AUTHOR_TAG']",3
"['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completi']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ;  van Lehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ), and an ontology mapping mechanism ( #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ), and an ontology mapping mechanism ( #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ), and an ontology mapping mechanism ( #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then']","['use the trips dialogue parser (Allen et al., 2007) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ), and an ontology mapping mechanism ( #AUTHOR_TAG a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']",5
['factors such as student confidence could be considered as well ( #AUTHOR_TAG )'],['factors such as student confidence could be considered as well ( #AUTHOR_TAG )'],['factors such as student confidence could be considered as well ( #AUTHOR_TAG )'],['factors such as student confidence could be considered as well ( #AUTHOR_TAG )'],3
"['student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context,']","['the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ).', 'however, most existing systems use pre - authored tutor responses for addressing student errors.', 'the advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step - by - step scaffolding and potentially suggesting additional problems.', 'the disadvantage is a lack of adaptivity and generality : students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'it also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand - authored remediations']",0
"['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp ( #AUTHOR_TAG ) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp ( #AUTHOR_TAG ) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp ( #AUTHOR_TAG ) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']","['tutorial policy makes a high - level decision as to which strategy to use ( for example, "" acknowledge the correct part and give a high specificity hint "" ) based on the answer analysis and dialogue context.', 'at present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 in addition to a remediation policy, the tutorial planner implements an error recovery policy.', 'since the system accepts unrestricted input, interpretation errors are unavoidable.', 'our recovery policy is modeled on the targetedhelp ( #AUTHOR_TAG ) policy used in task - oriented dialogue.', 'if the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, "" i\'m sorry, i\'m having a problem understanding.', 'i don\'t know the word power. ""', 'the help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non - interpretable answers']",2
"['use the trips dialogue parser ( #AUTHOR_TAG ) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is']","['use the trips dialogue parser ( #AUTHOR_TAG ) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']","['use the trips dialogue parser ( #AUTHOR_TAG ) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']","['use the trips dialogue parser ( #AUTHOR_TAG ) to parse the utterances.', 'the parser provides a domain - independent semantic representation including high - level word senses and semantic role labels.', ""the contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008 a ) to produce a domain - specific semantic representation of the student's output."", 'utterance content is represented as a set of extracted objects and relations between them.', 'negation is supported, together with a heuristic scoping algorithm.', 'the interpreter also performs basic ellipsis resolution.', 'for example, it can determine that in the answer to the question "" which bulbs will be on and which bulbs will be off in this diagram? "",', '"" off "" can be taken to mean "" all bulbs in the di - agram will be off. ""', 'the resulting output is then passed on to the domain reasoning and diagnosis components']",5
"['system uses a knowledge base implemented in the km representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the km representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the km representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']","['system uses a knowledge base implemented in the km representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world.', 'at present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits']",5
"['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( Tatu and Moldovan , 2005 ), or adopting transformation - based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( Tatu and Moldovan , 2005 ), or adopting transformation - based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( Tatu and Moldovan , 2005 ), or adopting transformation - based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such information ranges from i ) lexical paraphrases ( textual equivalences between terms ) to ii ) lexical relations preserving entailment between words, and iii ) wordlevel similarity / relatedness scores.', 'wordnet, the most widely used resource in te, provides all the three types of information.', 'synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'hypernymy / hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'paths between concepts and glosses can be used to calculate similarity / relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( Tatu and Moldovan , 2005 ), or adopting transformation - based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such information ranges from i ) lexical paraphrases ( textual equivalences between terms ) to ii ) lexical relations preserving entailment between words, and iii ) wordlevel similarity / relatedness scores.', 'wordnet, the most widely used resource in te, provides all the three types of information.', 'synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'hypernymy / hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'paths between concepts and glosses can be used to calculate similarity / relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis']",0
"['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger (Schmid, 1994) for tokenization, and used the giza + + (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit (Koehn et al., 2007).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",0
"[', dirt ( #AUTHOR_TAG ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","[', dirt ( #AUTHOR_TAG ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","[', dirt ( #AUTHOR_TAG ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","[', dirt ( #AUTHOR_TAG ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are fre - quently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']",0
"['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( tatu and #AUTHOR_TAG ), or adopting transformation - based techniques ( Kouleykov and Magnini , 2005 ;  bar - Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( tatu and #AUTHOR_TAG ), or adopting transformation - based techniques ( Kouleykov and Magnini , 2005 ;  bar - Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( tatu and #AUTHOR_TAG ), or adopting transformation - based techniques ( Kouleykov and Magnini , 2005 ;  bar - Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such information ranges from i ) lexical paraphrases ( textual equivalences between terms ) to ii']","['current approaches to monolingual te, either syntactically oriented ( Rus et al. , 2005 ), or applying logical inference ( tatu and #AUTHOR_TAG ), or adopting transformation - based techniques ( Kouleykov and Magnini , 2005 ;  bar - Haim et al. , 2008 ), incorporate different types of lexical knowledge to support textual inference.', 'such information ranges from i ) lexical paraphrases ( textual equivalences between terms ) to ii ) lexical relations preserving entailment between words, and iii ) wordlevel similarity / relatedness scores.', 'wordnet, the most widely used resource in te, provides all the three types of information.', 'synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'hypernymy / hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'paths between concepts and glosses can be used to calculate similarity / relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis']",0
"['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by ( #AUTHOR_TAG ).', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent']","['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by ( #AUTHOR_TAG ).', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent']","['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by ( #AUTHOR_TAG ).', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent back to workers in a new translation job.', 'although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired clte corpus.', 'the validation, carried out by a spanish native speaker']","['dataset used for our experiments is an english - spanish entailment corpus obtained from the original rte3 dataset by translating the english hypothesis into spanish.', 'it consists of 1600 pairs derived from the rte3 development and test sets ( 800 + 800 ).', 'translations have been generated by the crowdflower3 channel to amazon mechanical turk4 ( mturk ), adopting the methodology proposed by ( #AUTHOR_TAG ).', ""the method relies on translation - validation cycles, defined as separate jobs routed to mturk's workforce."", 'translation jobs return one spanish version for each hypothesis.', 'validation jobs ask multiple workers to check the correctness of each translation using the original english sentence as reference.', 'at each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the clte corpus, while wrong translations are sent back to workers in a new translation job.', 'although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired clte corpus.', 'the validation, carried out by a spanish native speaker on 100 randomly selected pairs after two translation - validation cycles, showed the good quality of the collected material, with only 3 minor "" errors "" consisting in controversial but substantially acceptable translations reflecting regional spanish variations']",5
"['- lingual textual entailment ( clte ) has been proposed by ( #AUTHOR_TAG ) as an extension of textual entailment ( Dagan and Glickman , 2004 ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal ex - pressions recognizers and normalizers ) has to con - front, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the ex - isting ones, and the burden of integrating language -']","['- lingual textual entailment ( clte ) has been proposed by ( #AUTHOR_TAG ) as an extension of textual entailment ( Dagan and Glickman , 2004 ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal ex - pressions recognizers and normalizers ) has to con - front, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the ex - isting ones, and the burden of integrating language -']","['- lingual textual entailment ( clte ) has been proposed by ( #AUTHOR_TAG ) as an extension of textual entailment ( Dagan and Glickman , 2004 ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal ex - pressions recognizers and normalizers ) has to con - front, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the ex - isting ones, and the burden of integrating language - specific components into the same cross - lingual ar - chitecture']","['- lingual textual entailment ( clte ) has been proposed by ( #AUTHOR_TAG ) as an extension of textual entailment ( Dagan and Glickman , 2004 ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal ex - pressions recognizers and normalizers ) has to con - front, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the ex - isting ones, and the burden of integrating language - specific components into the same cross - lingual ar - chitecture']",0
"[', of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison, we experiment with different sources of multil']","['of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions']","[', of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions']","['paper investigates the idea, still unexplored, of a tighter integration of mt and te algorithms and techniques.', 'our aim is to embed cross - lingual processing techniques inside the te recognition process in order to avoid any dependency on external mt components, and eventually gain full control of the systems behaviour.', 'along this direction, we start from the acquisition and use of lexical knowl - edge, which represents the basic building block of any te system.', 'using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions']",1
"['for the wmt10 1.', 'we run treetagger ( Schmid , 1994 ) for tokenization, and used the giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit (Koehn et al., 2007).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org']","['for the wmt10 1.', 'we run treetagger ( Schmid , 1994 ) for tokenization, and used the giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit (Koehn et al., 2007).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']","['for the wmt10 1.', 'we run treetagger ( Schmid , 1994 ) for tokenization, and used the giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit (Koehn et al., 2007).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger ( Schmid , 1994 ) for tokenization, and used the giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit (Koehn et al., 2007).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",5
"['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( #AUTHOR_TAG ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( #AUTHOR_TAG ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( #AUTHOR_TAG ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( #AUTHOR_TAG ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",4
"['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']",0
"['combine the phrasal matching scores obtained at each n - gram level, and optimize their relative weights, we trained a support vector machine classifier, svmlight ( #AUTHOR_TAG ), using each score as a feature']","['combine the phrasal matching scores obtained at each n - gram level, and optimize their relative weights, we trained a support vector machine classifier, svmlight ( #AUTHOR_TAG ), using each score as a feature']","['combine the phrasal matching scores obtained at each n - gram level, and optimize their relative weights, we trained a support vector machine classifier, svmlight ( #AUTHOR_TAG ), using each score as a feature']","['combine the phrasal matching scores obtained at each n - gram level, and optimize their relative weights, we trained a support vector machine classifier, svmlight ( #AUTHOR_TAG ), using each score as a feature']",5
"['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( #AUTHOR_TAG ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( #AUTHOR_TAG ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( #AUTHOR_TAG ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( #AUTHOR_TAG ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",4
"['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( #AUTHOR_TAG ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( #AUTHOR_TAG ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( #AUTHOR_TAG ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( #AUTHOR_TAG ), framenet ( Baker et al. , 1998 ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of sta - tistically learned inference rules, that is often inte - grated as a source of lexical paraphrases and entail - ment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similar - ity / relatedness scores']",0
"['aligned with the english wordnet ( e. g. multiwordnet ( #AUTHOR_TAG ) ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards']","['aligned with the english wordnet ( e. g. multiwordnet ( #AUTHOR_TAG ) ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards wikipedia, the cross - lingual links between pages in different languages offer a possibility to extract lexical']","[') the limited availability of multilingual lexical resources.', 'as regards the first issue, its worth noting that in the monolingual scenario simple bag of words ( or bag of n - grams ) approaches are per se sufficient to achieve results above baseline.', 'in contrast, their application in the cross - lingual setting is not a viable solution due to the impossibility to perform direct lex - ical matches between texts and hypotheses in different languages.', 'this situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'however, with the only exceptions represented by wordnet and wikipedia, most of the aforementioned resources are available only for english.', 'multilingual lexical databases aligned with the english wordnet ( e. g. multiwordnet ( #AUTHOR_TAG ) ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards']","['clte we have to face additional and more problematic issues related to : i ) the stronger need of lexical knowledge, and ii ) the limited availability of multilingual lexical resources.', 'as regards the first issue, its worth noting that in the monolingual scenario simple bag of words ( or bag of n - grams ) approaches are per se sufficient to achieve results above baseline.', 'in contrast, their application in the cross - lingual setting is not a viable solution due to the impossibility to perform direct lex - ical matches between texts and hypotheses in different languages.', 'this situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'however, with the only exceptions represented by wordnet and wikipedia, most of the aforementioned resources are available only for english.', 'multilingual lexical databases aligned with the english wordnet ( e. g. multiwordnet ( #AUTHOR_TAG ) ) have been created for several languages, with different degrees of coverage.', 'as an example, the 57, 424 synsets of the spanish section of multiwordnet aligned to english cover just around 50 % of the wordnets synsets, thus making the coverage issue even more problematic than for te.', 'as regards wikipedia, the cross - lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for clte.', 'however, due to their relatively small number ( especially for some languages ), bilingual lexicons extracted from wikipedia are still inadequate to provide acceptable coverage.', 'in addition, featuring a bias towards named entities, the information acquired through cross - lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources ( e. g bilingual dictionaries )']",0
"['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( #AUTHOR_TAG ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( #AUTHOR_TAG ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( #AUTHOR_TAG ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( Denkowski and Lavie , 2010 ), and te ( #AUTHOR_TAG ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",4
"['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",0
"['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( Baker et al. , 1998 ), and wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']",0
"['- lingual textual entailment ( clte ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of textual entailment ( #AUTHOR_TAG ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g']","['- lingual textual entailment ( clte ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of textual entailment ( #AUTHOR_TAG ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal expressions recognizers and normalizers ) has to confront, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating language -']","['- lingual textual entailment ( clte ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of textual entailment ( #AUTHOR_TAG ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g']","['- lingual textual entailment ( clte ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of textual entailment ( #AUTHOR_TAG ) that consists in deciding, given two texts t and h in different languages, if the meaning of h can be inferred from the meaning of t.', 'the task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'for instance, the reliance of cur - rent monolingual te systems on lexical resources ( e. g. wordnet, verbocean, framenet ) and deep processing components ( e. g. syntactic and semantic parsers, co - reference resolution tools, temporal expressions recognizers and normalizers ) has to confront, at the cross - lingual level, with the limited availability of lexical / semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating language - specific components into the same cross - lingual architecture']",0
"['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( #AUTHOR_TAG ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( #AUTHOR_TAG ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( #AUTHOR_TAG ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'these include, just to mention the most popular ones, dirt ( Lin and Pantel , 2001 ), verbocean ( Chklovski and Pantel , 2004 ), framenet ( #AUTHOR_TAG ), and wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']",0
"['parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']","['parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization ( mc Keown et al., 2002), automatic evaluation of mt (Denkowski and Lavie, 2010), and te (Dinu and Wang, 2009).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases']",0
"['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( #AUTHOR_TAG ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( #AUTHOR_TAG ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( #AUTHOR_TAG ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']","['##hrase tables ( ppht ) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'they proved to be useful in a number of nlp applications such as natural language generation ( Iordanskaja et al. , 1991 ), multidocument summarization ( mc Keown et al. , 2002 ), automatic evaluation of mt ( #AUTHOR_TAG ), and te ( Dinu and Wang , 2009 ).', 'one of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( bannard and callison - Burch, 2005).', 'with this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'after the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases']",4
"['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']","['the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'as emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words']",0
"[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by ( #AUTHOR_TAG )."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora']","[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by ( #AUTHOR_TAG )."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora']","[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by ( #AUTHOR_TAG )."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora']","[""the sake of completeness, we report in this section also the results obtained adopting the ` ` basic solution'' proposed by ( #AUTHOR_TAG )."", 'although it was presented as an approach to clte, the proposed method brings the problem back to the monolingual case by translating h into the language of t. the comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive mt system ( google translate ) in the same scenario']",1
"['use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ).', 'another interesting direction is to investigate the potential of paraphrase patterns ( i.']","['use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ).', 'another interesting direction is to investigate the potential of paraphrase patterns ( i. e.', 'patterns including partof - speech slots ), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'on the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'as a first step, the probability scores assigned to phrasal']","['use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ).', 'another interesting direction is to investigate the potential of paraphrase patterns ( i.']","['future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for te and clte.', 'on one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'one possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ).', 'another interesting direction is to investigate the potential of paraphrase patterns ( i. e.', 'patterns including partof - speech slots ), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'on the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'as a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.', '134']",3
"['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by (Kouylekov et al., 2009).', 'in this way we obtained 13760 word pairs']","['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by (Kouylekov et al., 2009).', 'in this way we obtained 13760 word pairs']","['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by (Kouylekov et al., 2009).', 'in this way we obtained 13760 word pairs']","['( wiki ).', 'we performed latent semantic analysis ( lsa ) over wikipedia using the jlsi tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset.', 'then, we filtered all the pairs with similarity lower than 0. 7 as proposed by (Kouylekov et al., 2009).', 'in this way we obtained 13760 word pairs']",5
"['.', 'first, we notice that dealing with mt - derived inputs, the optimal pruning threshold changes from 0. 2 to 0. 1, leading to the highest accuracy of 63. 50 %.', 'this suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'second, in line with the findings of ( #AUTHOR_TAG ), the results obtained over the mt - derived corpus are equal to those we achieve over the original rte3 dataset ( i.']","['comparison with the results achieved on monolingual data obtained by automatically translating the spanish hypotheses ( rte3 - g row in table 2 ) leads to four main observations.', 'first, we notice that dealing with mt - derived inputs, the optimal pruning threshold changes from 0. 2 to 0. 1, leading to the highest accuracy of 63. 50 %.', 'this suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'second, in line with the findings of ( #AUTHOR_TAG ), the results obtained over the mt - derived corpus are equal to those we achieve over the original rte3 dataset ( i. e. 63. 50 % ).', '63. 50 % ).', 'third, the accuracy obtained over the clte corpus using combined phrase and paraphrase tables ( 62. 88 %, as reported in table 1 ) is comparable to the best result gained over the automatically translated dataset ( 63. 50 % ).', 'in all the other cases, the use of phrase and paraphrase tables on clte data outperforms the results achieved on the same data after translation.', ""finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge""]","['comparison with the results achieved on monolingual data obtained by automatically translating the spanish hypotheses ( rte3 - g row in table 2 ) leads to four main observations.', 'first, we notice that dealing with mt - derived inputs, the optimal pruning threshold changes from 0. 2 to 0. 1, leading to the highest accuracy of 63. 50 %.', 'this suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'second, in line with the findings of ( #AUTHOR_TAG ), the results obtained over the mt - derived corpus are equal to those we achieve over the original rte3 dataset ( i.']","['comparison with the results achieved on monolingual data obtained by automatically translating the spanish hypotheses ( rte3 - g row in table 2 ) leads to four main observations.', 'first, we notice that dealing with mt - derived inputs, the optimal pruning threshold changes from 0. 2 to 0. 1, leading to the highest accuracy of 63. 50 %.', 'this suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'second, in line with the findings of ( #AUTHOR_TAG ), the results obtained over the mt - derived corpus are equal to those we achieve over the original rte3 dataset ( i. e. 63. 50 % ).', '63. 50 % ).', 'third, the accuracy obtained over the clte corpus using combined phrase and paraphrase tables ( 62. 88 %, as reported in table 1 ) is comparable to the best result gained over the automatically translated dataset ( 63. 50 % ).', 'in all the other cases, the use of phrase and paraphrase tables on clte data outperforms the results achieved on the same data after translation.', ""finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62. 12 %, which is lower than the result obtained using only phrase tables on cross - lingual data ( 62. 62 % )."", 'this demonstrates that phrase tables can successfully replace mt systems in the clte task']",1
"['##es toolkit ( #AUTHOR_TAG ).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org']","['for the wmt10 1.', 'we run treetagger (Schmid, 1994) for tokenization, and used the giza + + (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit ( #AUTHOR_TAG ).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']","['for the wmt10 1.', 'we run treetagger (Schmid, 1994) for tokenization, and used the giza + + (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit ( #AUTHOR_TAG ).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']","['tables ( pht ) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'they are widely used in mt as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'there are several methods to build phrase tables.', 'the one adopted in this work consists in learning phrase alignments from a word - aligned bilingual corpus.', 'in order to build english - spanish phrase tables for our experiments, we used the freely available europarl v. 4, news commentary and united nations spanish - english parallel corpora released for the wmt10 1.', 'we run treetagger (Schmid, 1994) for tokenization, and used the giza + + (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'subsequently, we extracted the bilingual phrase table from the aligned corpora using the moses toolkit ( #AUTHOR_TAG ).', 'since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'in addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0. 01, 0. 05, 0. 1, 0. 2, 0. 3, 0. 4 and 0. 5.', 'the resulting 1 http : / / www. statmt. org / wmt10 /', 'phrase tables range from 76 to 48 million entries, with an average of 3. 9 words per phrase']",5
"['wordnet, the rte literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ).', 'these include, just to mention the most popular ones, dirt (Lin and Pantel, 2001), verbocean (Chklovski and Pantel, 2004), framenet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ).', 'these include, just to mention the most popular ones, dirt (Lin and Pantel, 2001), verbocean (Chklovski and Pantel, 2004), framenet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ).', 'these include, just to mention the most popular ones, dirt (Lin and Pantel, 2001), verbocean (Chklovski and Pantel, 2004), framenet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']","['wordnet, the rte literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ).', 'these include, just to mention the most popular ones, dirt (Lin and Pantel, 2001), verbocean (Chklovski and Pantel, 2004), framenet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'dirt is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'verbocean is a graph of fine - grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'framenet is a knowledge - base of frames describing prototypical situations, and the role of the participants they involve.', 'it can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'wikipedia is often used to extract probabilistic entailment rules based word similarity / relatedness scores']",0
"['are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '( see sec. 2']","['are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '( see sec. 2']","['questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '( see sec. 2']","['questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks.', 'the reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '( see sec. 2 for models of morphological organization and access and related experiments )']",0
"['respective example sentences.', 'out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v']","['respective example sentences.', 'out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them']","['respective example sentences.', 'out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them']","['and processing of compound verbs in the mental lexicon compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'the verb v1 is known as pole and v2 is called as vector.', 'for example, "" [UNK] [UNK] "" ( getting up ) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'however, not all v1 + v2 combinations are cvs.', 'for example, expressions like, "" [UNK] [UNK] "" ( take and then go ) and "" [UNK] [UNK] [UNK] "" ( return back ) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as cv.', 'the key question linguists are trying to identify for a long time and debating a lot is whether to consider cvs as a single lexical units or consider them as two separate units.', 'since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'a clear understanding about these phenomena may help us to classify or extract actual cvs from other verb sequences.', 'in order to do so, presently we have applied three different techniques to collect user data.', 'in the first technique, we annotated 4500 v1 + v2 sequences, along with their example sentences, using a group of three linguists ( the expert subjects ).', 'we asked the experts to classify the verb sequences into three classes namely, cv, not a cv and not sure.', 'each linguist has received 2000 verb pairs along with their respective example sentences.', 'out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'we measure the inter annotator agreement using the fleiss kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0. 79.', 'next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them to 36 native bangla speakers.', 'we ask each subjects to give a compositionality score of each verb sequences under 1 - 10 point scale, 10 being highly compositional and 1 for noncompositional.', 'we found an agreement of  = 0. 69 among the subjects.', 'we also observe a continuum of compositionality score among the verb sequences.', 'this reflects that it is difficult to classify bangla verb sequences discretely into the classes of cv and not']",5
"['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg']","['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hp']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by #AUTHOR_TAG that points out v1 and v2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in ( #AUTHOR_TAG )']","['speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in ( #AUTHOR_TAG )']","['speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in ( #AUTHOR_TAG )']","['the agreement lies around 0. 79. next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 v1 + v2 pairs and presented them to 36', 'native bangla speakers. we ask each subjects to give a compositionality score of', 'each verb sequences under 1 - 10 point scale, 10 being highly compositional and 1 for noncompositional. we found an agreement of  =', '0. 69 among the subjects. we also observe a continuum of compositional', ""##ity score among the verb sequences. this reflects that it is difficult to classify bangla verb sequences discretely into the classes of cv and not a cv. we then, compare the compositionality score with that of the expert user's annotation. we found a significant correlation between the expert annotation and the"", 'compositionality score. we observe verb sequences that are annotated as cvs ( like, [UNK] [UNK], [UNK] [UNK], [UNK] [UNK] ) have got low compositionality score ( average score ranges between', '1 - 4 ) on the other hand high compositional values are in general tagged as not a cv ( [UNK] [UNK] ( come and get ), [UNK] [UNK] ( return back )', ', [UNK] [UNK] [UNK] ( kept ), [UNK] [UNK] ( roll on floor ) ). this reflects that verb', 'sequences which are not cv shows high degree of compositionality. in other words non cv verbs', 'can directly interpret from their constituent verbs. this leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be', 'greater than the non - compositional verbs which maps to a single expression of meaning. in order to validate such claim we', 'perform a lexical decision experiment using native bangla speakers with 92 different verb sequences', '. we followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for english polymorphemic words. however,', 'rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. the reaction time', '( rt ) of each subject is recorded. our preliminarily observation from the rt analysis shows that as per our claim, rt of verb sequences having high compositionality value is significantly higher than the', 'rts for low or noncompositional verbs. this proves our hypothesis that bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon', 'and thus follows the full - listing model whereas compositional verb phrases are individually', 'parsed. however, we do believe that our experiment is composed of', 'a very small set of data and it is premature to conclude anything concrete based only on the current experimental results']",5
"['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg']","['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hp']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts']",0
"['- listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ).', 'on the other hand, morph']","['paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ).', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the']","[': the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ).', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes']","['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'most of the studies are designed to support one of the two mutually exclusive paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ).', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981; mac Kay, 1978).', 'intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988)']",0
"['words of english, hebrew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of']","['of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg']","['on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hp']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['respect to this, we apply the different priming and other lexical decision experiments, described in literature ( #AUTHOR_TAG ; bent']","['respect to this, we apply the different priming and other lexical decision experiments, described in literature ( #AUTHOR_TAG ; bentin, s. and Feldman , 1990 )']","['respect to this, we apply the different priming and other lexical decision experiments, described in literature ( #AUTHOR_TAG ; bent']","['respect to this, we apply the different priming and other lexical decision experiments, described in literature ( #AUTHOR_TAG ; bentin, s. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of bangla.', 'our cross - modal and masked priming experiment on bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically / orthographically unrelated.', 'these observations are similar to those reported for english and indicate that derivationally suffixed words in bangla are in general accessed through decomposition of the word into its constituent morphemes.', 'further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of bangla polymorphemic words.', 'our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix']",5
"['( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair']","['( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair']","['( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']",5
"['##brew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; grainger, et al., 1991']","['of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; grainger, et al., 1991']","['##brew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; grainger, et al., 1991']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; grainger, et al., 1991 ; Drews and Zwitserlood , 1995 ).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair']","['( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair']","['( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']","['apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for bangla morphologically complex words.', 'here, the prime is morphologically derived form of the target presented auditorily ( for cross modal priming ) or visually ( for masked priming ).', 'the subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'the same target word is again probed but with a different audio or visual probe called the control word.', 'the control shows no relationship with the target.', 'for example, bayaska ( aged ) and bayasa ( age ) is a prime - target pair, for which the corresponding control - target pair could be nayana ( eye ) and bayasa ( age )']",5
"['storage in lexical resources like wordnet ( #AUTHOR_TAG ) and raises the questions like, how to store morphologically complex words, in a lexical resource like wordnet keeping in mind the storage and access efficiency']","['storage in lexical resources like wordnet ( #AUTHOR_TAG ) and raises the questions like, how to store morphologically complex words, in a lexical resource like wordnet keeping in mind the storage and access efficiency']","['storage in lexical resources like wordnet ( #AUTHOR_TAG ) and raises the questions like, how to store morphologically complex words, in a lexical resource like wordnet keeping in mind the storage and access efficiency']","['clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language.', 'further, these linguistically important and interesting questions are also highly significant for computational linguistics ( cl ) and natural language processing ( nlp ) applications.', 'their computational significance arises from the issue of their storage in lexical resources like wordnet ( #AUTHOR_TAG ) and raises the questions like, how to store morphologically complex words, in a lexical resource like wordnet keeping in mind the storage and access efficiency']",0
"['##s.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ).', 'therefore, the findings from experiments in one language cannot be generalized to']","['cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ).', 'therefore, the findings from experiments in one language cannot be generalized to']","['', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ).', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( marslen - Wilson et al., 2008;Frost et al., 1997;Grainger, et al., 1991;Drews and Zwitserlood, 1995).', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ).', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],"['the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'most of the studies are designed to support one of the two mutually exclusive paradigms : the full - listing and the morphemic model.', 'the full - listing model claims that polymorphic words are represented as a whole in the human mental lexicon (Bradley, 1980;Butterworth, 1983).', 'on the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'the affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981; mac Kay, 1978).', 'intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'for instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG )']",0
"['on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', '']","['on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', '']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2']","['plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb v2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues cv formations in hindi and urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on "" prepared "" and "" unprepared mind "".', 'similar findings have been proposed by Pandharipande (1993) that points out v1 and v2 are paired on the basis of their semantic compa - tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent bangla cvs in terms of hpsg formalism.', 'she proposes that the selection of a v2 by a v1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'since none of the linguistic formalism could satisfactorily explain the unique phenomena of cv formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb - verb combinations in the ml and compare these responses with that of the existing models']",0
"['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via']","['has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on english inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ;  taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'if it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts']",0
